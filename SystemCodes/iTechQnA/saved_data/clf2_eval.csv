q_qid,q_creation_date,q_year,q_score,q_title,q_body,q_tags,a_aid,a_body,seq_A,seq_B,predicted,actual,pred_clean,cos_sim_score
48353738,2018-01-20,2018,11,TypeError: descriptor &#39;__init__&#39; requires a &#39;super&#39; object but received a &#39;str&#39;,"<p>Using Python 3.X as the interpreter. I just had inherited the <code>Employee</code> class with two of the derived class as <code>Developer</code> and <code>Manager</code>.</p>

<p>The below code is throwing a Type Error:  <code>'__init__' requires a 'super' object but received a 'str'</code>.<br>
I'm not getting why exactly it's happening, I don't find any problem with the program yet.</p>

<pre><code>class Employee:
    raise_amount=1.05
    emp_count=0
    def __init__(self,first_name,last_name, amount):
        self.first_name=first_name
        self.last_name=last_name
        self.amount=amount
        self.email_id=""{0}.{1}@{1}.com"" .format(first_name,last_name)
        Employee.emp_count +=1

    def fullname(self):
        print (""%s %s""%(self.first_name,self.last_name))

    def print1(self):
        print (self.email)
        print (""Total no of Employee are :%d"" %(Employee.emp_count))


    def raise_amount(self):
        self.amount *=self.raise_amount
        return self.amount


class Developer(Employee):
    raise_amount = 1.10
    def __init__(self,f,l,a,prog):
        super.__init__(f,l,a)
        self.programming=prog

class Manager(Employee):
    def __init__(self,f,l,a,emp=None):
        super.__init__(f,l,a)
        if emp is None:
            self.my_employee=[]
        else:
            self.my_employee=emp



    def add_employee(self,emp):
        if emp in self.my_employee:
            print(""employee is already exist"")
        else:
            self.my_employee.append(emp)

    def remove_employee(self,emp):
        if emp in self.my_employee:
            self.my_employee.remove(emp)

    def print_employee(self):
        for emp in self.my_employee:
            print (emp.fullnamme())



dev1=Developer(""subhendu"",""panda"",500000,""Python"")
dev1.raise_amount()
dev2=Developer('Aditya','bishoyi',5688989,'java')
dev2.fullname()
dev1.fullname()
emp1=Employee(""tonu"",""trip"",30000)
emp1.raise_amount()
emp1.fullname()
mgr1=Manager(""Biplab"",""choudhury"",5000000)
mgr1.fullname()
mgr1.add_employee(dev1)
mgr1.add_employee(emp1)
mgr1.add_employee(dev2)
mgr1.print_employee()
mgr1.remove_employee(dev1)
mgr1.print_employee()
</code></pre>

<p><strong>Error trace:</strong></p>

<pre><code>Traceback (most recent call last): 
File ""C:/Users/Subhendu/PycharmProjects/hello/inheritance.py"", line 56, in 
&lt;module&gt; dev1=Developer(""subhendu"",""panda"",500000,""Python"") 
File ""C:/Users/Subhendu/PycharmProjects/hello/inheritance.py"", line 27, in
init super.__init__(f,l,a) TypeError: descriptor 'init' requires a 'super'
object but received a 'str'
</code></pre>
","['python', 'python-3.x', 'python-2.7']",48354117,"<p>Your <code>super</code> call giving the error in the <code>Developer</code> class should be:</p>

<pre><code>super(Developer, self).__init__(f, l, a) 
</code></pre>

<p>In the <code>Manager</code> class:</p>

<pre><code>super(Manager, self).__init__(f, l, a)
</code></pre>

<p>You have other issues, for example, in <code>Employee</code> you have two attributes called <code>raise_amount</code>, one is a <code>float</code> and one is a method (function).  That's not allowed and the <code>float</code> takes precedence, so  <code>dev1.raise_amount()</code> fails.</p>

<p>In <code>print_employee()</code> you mis-spell <code>emp.fullname</code></p>
",TypeError descriptor init requires super object received str Using Python X interpreter I inherited Employee class two derived class Developer Manager The code throwing Type Error init requires super object received str I getting exactly happening I find problem program yet class Employee raise amount emp count def init self first name last name amount self first name first name self last name last name self amount amount self email id com format first name last name Employee emp count def fullname self print self first name self last name def print self print self email print Total Employee Employee emp count def raise amount self self amount self raise amount return self amount class Developer Employee raise amount def init self f l prog super init f l self programming prog class Manager Employee def init self f l emp None super init f l emp None self employee,"startoftags, python, python3x, python27, endoftags",python python3x list endoftags,python python3x python27,python python3x list,0.67
48457134,2018-01-26,2018,2,Django and ManyToMany fields,"<p>I have a database in mysql and I am setting up a django project. I have some entities with many-to-many relations, which are handled through association tables. With django, I have understood that I can use ManyToMany entity to achieve many-to-many relations BUT how do I do when the association table holds more information besides just the many-to-many relation?
See the example below.</p>

<pre><code>CREATE TABLE `products` (
  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `name` char(150) DEFAULT NULL ...)

CREATE TABLE `pictures` (
  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `filename` char(100) NOT NULL ... )

CREATE TABLE `products_pictures` (
  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `fk_products_id` int(11) unsigned NOT NULL,
  `fk_pictures_id` int(11) unsigned NOT NULL,
  `priority` tinyint(4) unsigned DEFAULT NULL,
  `relation` varchar(25) DEFAULT 'same product family' ... )
</code></pre>
","['python', 'django', 'django-models']",48457504,"<p>you can use <a href=""https://docs.djangoproject.com/en/2.0/ref/models/fields/#django.db.models.ManyToManyField.through"" rel=""nofollow noreferrer"">through</a> option, for example</p>

<pre><code>class Products(models.Model):
    name = models.CharField(max_length=128)
    pictures = models.ManyToManyField(
        Pictures,
        through='ProductsPictures',
    )

class ProductsPictures(models.Model):
    product = models.ForeignKey(Products, on_delete=models.CASCADE)
    picture = models.ForeignKey(Pictures, on_delete=models.CASCADE)
    description = models.CharField(max_length=128)
</code></pre>
",Django ManyToMany fields I database mysql I setting django project I entities many many relations handled association tables With django I understood I use ManyToMany entity achieve many many relations BUT I association table holds information besides many many relation See example CREATE TABLE products id int unsigned NOT NULL AUTO INCREMENT name char DEFAULT NULL CREATE TABLE pictures id int unsigned NOT NULL AUTO INCREMENT filename char NOT NULL CREATE TABLE products pictures id int unsigned NOT NULL AUTO INCREMENT fk products id int unsigned NOT NULL fk pictures id int unsigned NOT NULL priority tinyint unsigned DEFAULT NULL relation varchar DEFAULT product family,"startoftags, python, django, djangomodels, endoftags",python django djangomodels endoftags,python django djangomodels,python django djangomodels,1.0
48587799,2018-02-02,2018,5,Creating a new column name based on a loop variable and an additional string,"<p>I want to create percentage change column for each column that is a float in my dataframe and stored it in a newn column each time with the name of the initial column and the add on ""_change""     </p>

<p>I tried this but it does not seem to work any idea?</p>

<pre><code>for col in df.columns:
        if df[col].dtypes == ""float"":
           df[ col&amp;'_change'] = (df.col - df.groupby(['New_ID']).col.shift(1))/ df.col
</code></pre>

<p>for example if my column is df[""Expenses""] I would like to save the percentage change in df[""Expenses_change""]
Edited for adding example data frame and output</p>

<p>df initially</p>

<pre><code>Index   ID  Reporting_Date  Sales_Am    Exp_Am
     0   1   01/01/2016        1000      900
     1   1   02/01/2016        1050      950
     2   1   03/01/2016        1060      960
     3   2   01/01/2016        2000      1850
     4   2   02/01/2016        2500      2350
     4   2   03/01/2016        3000      2850
</code></pre>

<p>after the loop</p>

<pre><code>Index   ID  Reporting_Date  Sales_Am  Sales_Am_chge  Exp_Am  Exp_Am_chge
0        1  01/01/2016         1000     Null          900      Null
1        1  02/01/2016         1050     5%            950      6%
2        1  03/01/2016         1060     1%            960      1%
3        2  01/01/2016         2000     Null          1850     Null
4        2  02/01/2016         2500     25%           2350     27%
4        2  03/01/2016         3000     20%           2850     21%
</code></pre>

<p>keep in mind that i have more than 2 columns on my dataframe.</p>
","['python', 'pandas', 'dataframe']",48588026,"<p>Why are you using '&amp;' instead of '+' in </p>

<pre><code>df[ col&amp;'_change']
</code></pre>

<p>?</p>
",Creating new column name based loop variable additional string I want create percentage change column column float dataframe stored newn column time name initial column add change I tried seem work idea col df columns df col dtypes float df col amp change df col df groupby New ID col shift df col example column df Expenses I would like save percentage change df Expenses change Edited adding example data frame output df initially Index ID Reporting Date Sales Am Exp Am loop Index ID Reporting Date Sales Am Sales Am chge Exp Am Exp Am chge Null Null Null Null keep mind columns dataframe,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
48821755,2018-02-16,2018,2,Iterating for loop Python,"<p>This is the problem: 
Write a program named q1() that prompts the user for the starting and ending distance in
Kilometers (km), and an increment value. It should then print the conversion table showing values for
Kilometers, Miles (M) and Feet (ft). Each value should be displayed as they are calculated. </p>

<p>this is my code:</p>

<pre><code> print(""This program prints a translation table between the Kilometers (km), Miles (M) and Feet (ft) scales."")

start_km = input(""Enter the Starting Kilometers (km): "")
start_km = int(start_km)
end_km = input(""Enter Ending Kilometers (km): "" )
end_km = int(end_km)
increment = input(""Increment value: "")
increment = int(increment)
km = start_km
M = km*(.621371)
ft = M*(5280)

print(""km"", ""   "", ""M"", ""   "", ""ft"")
print(""=========================="")
list = ['km', 'M', 'ft']
for i in range(start_km,end_km+1,increment):
    km = start_km
    M = km*(.621371)
    ft = M*(5280)
    print( km, ""    "", M, ""    "", ft)
</code></pre>

<p>my problem is that i am getting a list but it is repeating the input value and not incrementing the calculation. how do i get it to iterate ?</p>
","['python', 'python-3.x', 'python-2.7']",48821805,"<pre><code>for i in range(start_km,end_km+1,increment)
</code></pre>

<p>does not change the value of start_km, so you are just printing the same value again and again. The for loop changes the value of <code>i</code>, starting at start_km and ending at <code>end_km</code>, so you should do:</p>

<pre><code>for km in range(start_km,end_km+1,increment):
    M = km*(.621371)
    ft = M*(5280)
    print( km, ""    "", M, ""    "", ft)
</code></pre>

<p>which increments <code>km</code> automatically, just like you wanted</p>
",Iterating loop Python This problem Write program named q prompts user starting ending distance Kilometers km increment value It print conversion table showing values Kilometers Miles M Feet ft Each value displayed calculated code print This program prints translation table Kilometers km Miles M Feet ft scales start km input Enter Starting Kilometers km start km int start km end km input Enter Ending Kilometers km end km int end km increment input Increment value increment int increment km start km M km ft M print km M ft print list km M ft range start km end km increment km start km M km ft M print km M ft problem getting list repeating input value incrementing calculation get iterate,"startoftags, python, python3x, python27, endoftags",python python3x list endoftags,python python3x python27,python python3x list,0.67
48936502,2018-02-22,2018,3,"How to fill NaN values in a pandas dataframe, with variable values?","<p>I have a dataframe:</p>

<pre><code>   Isolate1 Isolate2 Isolate3 Isolate4
2  NaN      NaN      AGTCTA   AGT
5  NaN      GC       NaN      NaN
</code></pre>

<p>And want to replace the NaN values in the Isolate1 column with dashes, one dash for each letter in the non NaN values from the other columns (or the maximum number if other column has other different value), ending in something like these:</p>

<pre><code>  Isolate1 Isolate2 Isolate3 Isolate4
2 ------   NaN      AGTCTA   AGT
5 --       GC       NaN      NaN
</code></pre>

<p>I have tried the following:</p>

<pre><code>index_sizes_to_replace = {}
for row in df.itertuples():
    indel_sizes = []
    #0 pos is index
    for i, value in enumerate(row[1:]):
        if pd.notnull(value):
            indel_sizes.append((i, len(value)))
    max_size = max([size for i, size in indel_sizes])
    index_sizes_to_replace[row[0]]= max_size
</code></pre>

<p>Now I have the number of dashes to replace the NaN values, but dont know how to do the filling, tried this:</p>

<pre><code>for index, size in index_sizes_to_replace.iteritems():
    df.iloc[index].fillna(""-""*size, inplace=True)
</code></pre>

<p>But didnt work, any suggestion?</p>
","['python', 'pandas', 'dataframe']",48936775,"<p>Let's try:</p>

<pre><code>import pandas as pd
import numpy as np

data = dict(Isolate1=[np.NaN,np.NaN,'A'],
            Isolate2=[np.NaN,'ABC','A'],
            Isolate3=['AGT',np.NaN,'A'],
            Isolate4=['AGTCTA',np.NaN,'A'])

df = pd.DataFrame(data)
</code></pre>

<p>Original solution:</p>

<pre><code>df['Isolate1'] = df.apply(lambda x: '-' * x.str.len().max().astype(int), axis=1)
</code></pre>

<p>To ignore Isolate1:</p>

<pre><code>df['Isolate1'] = df.iloc[:,1:].apply(lambda x: x.str.len().max().astype(int)*'-', axis=1)
</code></pre>

<p>Output:</p>

<pre><code>  Isolate1 Isolate2 Isolate3 Isolate4
0   ------      NaN      AGT   AGTCTA
1      ---      ABC      NaN      NaN
2        -        A        A        A
</code></pre>

<hr>

<p>@Anton vBR Edit to handle not nan in col1.</p>

<pre><code># Create a mask
m = pd.isna(df['Isolate1'])
df.loc[m,'Isolate1'] = df[m].apply(lambda x: '-' * x.str.len().max().astype(int), axis=1)
</code></pre>

<p>Output:</p>

<pre><code>  Isolate1 Isolate2 Isolate3 Isolate4
0   ------      NaN      AGT   AGTCTA
1      ---      ABC      NaN      NaN
2        A        A        A        A
</code></pre>
",How fill NaN values pandas dataframe variable values I dataframe Isolate Isolate Isolate Isolate NaN NaN AGTCTA AGT NaN GC NaN NaN And want replace NaN values Isolate column dashes one dash letter non NaN values columns maximum number column different value ending something like Isolate Isolate Isolate Isolate NaN AGTCTA AGT GC NaN NaN I tried following index sizes replace row df itertuples indel sizes pos index value enumerate row pd notnull value indel sizes append len value max size max size size indel sizes index sizes replace row max size Now I number dashes replace NaN values dont know filling tried index size index sizes replace iteritems df iloc index fillna size inplace True But didnt work suggestion,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
49153471,2018-03-07,2018,2,Find 2nd and 3rd highest value in Python,"<p>I have below data frame of food items and its nutrient contents as below: </p>

<pre><code>import pandas as pd
import os
os.chdir('D:\\userdata\\adbharga\\Desktop\\AVA\\RTestCode\\Python')
data=pd.read_csv(""nutrient.csv"")
data.head()
</code></pre>

<p>Out[30]:</p>

<pre><code>Name  Calories  Fat  Carb  Fiber  Protein
0      Chonga Bagel       300    5    50      3       12
1      8-Grain Roll       380    6    70      7       10
2  Almond Croissant       410   22    45      3       10
3     Apple Fritter       460   23    56      2        7
4  Banana Nut Bread       420   22    52      2        6
</code></pre>

<p>Need to extract the Top Nutrient content and its value. For that used below code. </p>

<pre><code>data['Top Nutrient'] = data[['Calories','Fat','Carb','Fiber','Protein']].idxmax(axis=1)
data['Amount']= data[['Calories','Fat','Carb','Fiber','Protein']].max(axis=1)
data.head()
</code></pre>

<p>Out[33]: </p>

<pre><code>Name  Calories  Fat  Carb  Fiber  Protein Top Nutrient  Amount
0      Chonga Bagel       300    5    50      3       12     Calories     300
1      8-Grain Roll       380    6    70      7       10     Calories     380
2  Almond Croissant       410   22    45      3       10     Calories     410
3     Apple Fritter       460   23    56      2        7     Calories     460
4  Banana Nut Bread       420   22    52      2        6     Calories     420
</code></pre>

<p>Is there a way to show next 2 Top Nutrient and its Value.Expected Output will be like this:</p>

<pre><code>Name    NextTop2   NextTop2Amount
Chonga Bagel        Carb|Protein    50|12
8-Grain Roll        Carb|Protein    70|10
Almond Croissant    Carb|Fat        45|22
Apple Fritter       Carb|Fat        56|23
Banana Nut Bread    Carb|Fat        52|22
</code></pre>

<p>Thanks</p>
","['python', 'python-3.x', 'pandas']",49153723,"<p>Here is the best use <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html"" rel=""nofollow noreferrer""><code>numpy.argsort</code></a> because very fast.</p>

<p>First filter columns by <code>subset</code> - <code>[]</code> and get indices by <code>argsort</code> for 2. and 3. top:</p>

<pre><code>cols = ['Calories','Fat','Carb','Fiber','Protein']

arr = data[cols].values.argsort(axis=1)[:, [-2, -3]]
a = np.array(cols)[arr]
print (a)
[['Carb' 'Protein']
 ['Carb' 'Protein']
 ['Carb' 'Fat']
 ['Carb' 'Fat']
 ['Carb' 'Fat']]
</code></pre>

<p>Also select values by indices:</p>

<pre><code>b = data[cols].values[np.arange(len(arr))[:,None], arr]
print (b)
[[50 12]
 [70 10]
 [45 22]
 [56 23]
 [52 22]]
</code></pre>

<p>Last create <code>DataFrame</code>s and add join by <code>|</code> for one column:</p>

<pre><code>data['Top Nutrient'] = data[cols].idxmax(axis=1)
data['Amount']= data[cols].max(axis=1)
data['NextTop2'] = pd.DataFrame(a).apply('|'.join, 1)
data['NextTop2Amount'] = pd.DataFrame(b).astype(str).apply('|'.join, 1)
</code></pre>

<hr>

<pre><code>print (data)

               Name  Calories  Fat  Carb  Fiber  Protein Top Nutrient  Amount  \
0      Chonga Bagel       300    5    50      3       12     Calories     300   
1      8-Grain Roll       380    6    70      7       10     Calories     380   
2  Almond Croissant       410   22    45      3       10     Calories     410   
3     Apple Fritter       460   23    56      2        7     Calories     460   
4  Banana Nut Bread       420   22    52      2        6     Calories     420   

       NextTop2 NextTop2Amount  
0  Carb|Protein          50|12  
1  Carb|Protein          70|10  
2      Carb|Fat          45|22  
3      Carb|Fat          56|23  
4      Carb|Fat          52|22  
</code></pre>
",Find nd rd highest value Python I data frame food items nutrient contents import pandas pd import os os chdir D userdata adbharga Desktop AVA RTestCode Python data pd read csv nutrient csv data head Out Name Calories Fat Carb Fiber Protein Chonga Bagel Grain Roll Almond Croissant Apple Fritter Banana Nut Bread Need extract Top Nutrient content value For used code data Top Nutrient data Calories Fat Carb Fiber Protein idxmax axis data Amount data Calories Fat Carb Fiber Protein max axis data head Out Name Calories Fat Carb Fiber Protein Top Nutrient Amount Chonga Bagel Calories Grain Roll Calories Almond Croissant Calories Apple Fritter Calories Banana Nut Bread Calories Is way show next Top Nutrient Value Expected Output like Name NextTop NextTop Amount Chonga Bagel Carb Protein Grain Roll Carb Protein Almond Croissant Carb Fat Apple Fritter Carb Fat Banana Nut Bread Carb Fat Thanks,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
49325509,2018-03-16,2018,13,How to find alternating repetitive digit pair?,"<p><code>121426</code> &lt;- Here, 1 is an alternating repetitive digit.</p>

<p><code>523563</code> &lt;- Here, NO digit is an alternating repetitive digit.</p>

<p><code>552523</code> &lt;- Here, both 2 and 5 are alternating repetitive digits.</p>

<p><code>333567</code> &lt;- Here, 3 is an alternating repetitive digit.</p>

<p>I found <code>re.findall(r'(?=(\d)\d\1)',P)</code> as the solution in editorial but not able to understand it.</p>

<p>Edit - Not allowed to use <code>if</code> conditions.</p>
","['python', 'regex', 'python-3.x']",49325550,"<p>You may use this regex using lookaheads:</p>

<pre><code>(\d)(?=\d\1)
</code></pre>

<p><a href=""https://regex101.com/r/JpdXdl/2"" rel=""noreferrer"">RegEx Demo</a></p>

<p><strong>Explanation:</strong></p>

<ul>
<li><code>(\d)</code>: Match and capture a digit in group #1 </li>
<li><code>(?=</code>: Start lookahead

<ul>
<li><code>\d</code>: Match any digit</li>
<li><code>\1</code>: Back-reference to captured group #1</li>
</ul></li>
<li><code>)</code>: End lookahead</li>
</ul>
",How find alternating repetitive digit pair lt Here alternating repetitive digit lt Here NO digit alternating repetitive digit lt Here alternating repetitive digits lt Here alternating repetitive digit I found findall r P solution editorial able understand Edit Not allowed use conditions,"startoftags, python, regex, python3x, endoftags",python python3x numpy endoftags,python regex python3x,python python3x numpy,0.67
49331751,2018-03-17,2018,2,Pandas merging monthly data from one dataframe with daily data in another,"<p>I have a csv file (say A.csv) with an index from 1980-01-01 to 2018-02-28 (increased by a day) and one data column (say everyday's stock price). </p>

<p>I have another csv file  (say B.csv) with an index from 1980-01 to 2018-02 (increased by a month) and one data column (say monthly trade balance). </p>

<p>In such case, how do merge B.csv to A.csv (by maintaining daily index)?
i.e., daily index + one column for daily stock price + another column for monthly trade balance (I need to expand monthly trade balance to daily trade balance by maintaining the same trade balance values for each days in a month).</p>
","['python', 'pandas', 'csv']",49331813,"<p>You can do this with <code>pandas</code>.</p>

<p>One way to do this is to convert both date columns to <code>datetime</code> objects, and use <code>pd.Series.map</code> to perform the mapping from one table to the other.</p>

<p>Since day is not specified for your monthly data, for our mapping we normalise to the first day of the month.</p>

<pre><code>import pandas as pd

# first read in the 2 tables into dataframes
# df_daily = pd.read_csv('daily.csv')
# df_monthly = pd.read_csv('monthly.csv')

df_daily = pd.DataFrame({'Date': ['1980-01-01', '1980-01-02', '1980-01-03'],
                         'Value': [1, 2, 3]})

df_monthly = pd.DataFrame({'Month': ['1979-12', '1980-01', '1980-03'],
                           'Value': [100, 200, 300]})

# convert to datetime objects
df_daily['Date'] = pd.to_datetime(df_daily['Date'])
df_monthly['Month'] = pd.to_datetime(df_monthly['Month']+'-01')

# perform mapping after normalising to first day of month
df_daily['MonthValue'] = df_daily['Date'].map(lambda x: x.replace(day=1))\
                                         .map(df_monthly.set_index('Month')['Value'])

#         Date  Value  MonthValue
# 0 1980-01-01      1         200
# 1 1980-01-02      2         200
# 2 1980-01-03      3         200
</code></pre>
",Pandas merging monthly data one dataframe daily data another I csv file say A csv index increased day one data column say everyday stock price I another csv file say B csv index increased month one data column say monthly trade balance In case merge B csv A csv maintaining daily index e daily index one column daily stock price another column monthly trade balance I need expand monthly trade balance daily trade balance maintaining trade balance values days month,"startoftags, python, pandas, csv, endoftags",python pandas dataframe endoftags,python pandas csv,python pandas dataframe,0.67
49530918,2018-03-28,2018,15,Check if pandas dataframe is subset of other dataframe,"<p>I have two Python Pandas dataframes A, B, with the same columns (obviously with different data). I want to check A is a subset of B, that is, all rows of A are contained in B.</p>

<p>Any idea how to do it? </p>
","['python', 'pandas', 'dataframe']",49531052,"<p>Method <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html"" rel=""noreferrer""><code>DataFrame.merge(another_DF)</code></a> merges on the intersection of the columns by default (uses all columns with same names from both DFs) and uses <code>how='inner'</code> - so we expect to have the same # of rows after <code>inner join</code> (if neither of DFs has duplicates):</p>

<pre><code>len(A.merge(B)) == len(A)
</code></pre>

<p>PS it will NOT work properly if one of DFs have duplicated rows - see below for such cases</p>

<p>Demo:</p>

<pre><code>In [128]: A
Out[128]:
   A  B  C
0  1  2  3
1  4  5  6

In [129]: B
Out[129]:
   A  B  C
0  4  5  6
1  1  2  3
2  9  8  7

In [130]: len(A.merge(B)) == len(A)
Out[130]: True
</code></pre>

<hr>

<p>for data sets containing duplicates, we can remove duplicates and use the same method:</p>

<pre><code>In [136]: A
Out[136]:
   A  B  C
0  1  2  3
1  4  5  6
2  1  2  3

In [137]: B
Out[137]:
   A  B  C
0  4  5  6
1  1  2  3
2  9  8  7
3  4  5  6

In [138]: A.merge(B).drop_duplicates()
Out[138]:
   A  B  C
0  1  2  3
2  4  5  6

In [139]: len(A.merge(B).drop_duplicates()) == len(A.drop_duplicates())
Out[139]: True
</code></pre>
",Check pandas dataframe subset dataframe I two Python Pandas dataframes A B columns obviously different data I want check A subset B rows A contained B Any idea,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
49566130,2018-03-29,2018,2,regex: grabbing items from a list based on the word attached,"<p>I have a list like:</p>

<pre><code>x=['/category/Women-Dresses?size=0', '/brand/Free_People', '/closet/shopmyycloset', '/listing/559c0800568c896f6e019f2a/unlike', '/listing/Eyelet-drop-waist-dress-559c0800568c896f6e019f2a', '/listing/Eyelet-drop-waist-dress-559c0800568c896f6e019f2a', None, '#', '#', '#', '#', '#', None, ]
</code></pre>

<p>I want to grab the elements from the list that have the word listing in it so I want to have </p>

<pre><code>c=[/listing/559c0800568c896f6e019f2a/unlike,/listing/Eyelet-drop-waist-dress-559c0800568c896f6e019f2a,/listing/Eyelet-drop-waist-dress-559c0800568c896f6e019f2a]
</code></pre>

<p>I thought regular expressions would be a good tool for this but I'm a little lost, this is what I tried but it doesn't work</p>

<pre><code>import re
x=['/category/Women-Dresses?size=0', '/brand/Free_People', '/closet/shopmyycloset', '/listing/559c0800568c896f6e019f2a/unlike', '/listing/Eyelet-drop-waist-dress-559c0800568c896f6e019f2a', '/listing/Eyelet-drop-waist-dress-559c0800568c896f6e019f2a', None, '#', '#', '#', '#', '#', None, ]
c=[]
for list in x:
    if re.findall('/list',list) in list:
        c.append()
print(c)
</code></pre>
","['python', 'regex', 'python-3.x']",49566142,"<p>Try this list comprehension. It picks elements that have ""listing"" in them.</p>

<pre><code>c=['/category/Women-Dresses?size=0', '/brand/Free_People', '/closet/shopmyycloset', '/listing/559c0800568c896f6e019f2a/unlike', '/listing/Eyelet-drop-waist-dress-559c0800568c896f6e019f2a', '/listing/Eyelet-drop-waist-dress-559c0800568c896f6e019f2a', None, '#', '#', '#', '#', '#', None, ]
c=[e for e in c if e and ""listing"" in e]
</code></pre>
",regex grabbing items list based word attached I list like x category Women Dresses size brand Free People closet shopmyycloset listing c c f e f unlike listing Eyelet drop waist dress c c f e f listing Eyelet drop waist dress c c f e f None None I want grab elements list word listing I want c listing c c f e f unlike listing Eyelet drop waist dress c c f e f listing Eyelet drop waist dress c c f e f I thought regular expressions would good tool I little lost I tried work import x category Women Dresses size brand Free People closet shopmyycloset listing c c f e f unlike listing Eyelet drop waist dress c c f e f listing Eyelet drop waist dress c c f e f None None c list x findall list list list c append print c,"startoftags, python, regex, python3x, endoftags",python arrays numpy endoftags,python regex python3x,python arrays numpy,0.33
49594368,2018-04-01,2018,3,How to raise an array of numbers to a power with NumPy?,"<p>NumPy has <code>log</code>, <code>log2</code>, and <code>log10</code> methods which can perform vectorized log base e / 2 / 10 (respectively). However, for doing the inverse operation (exponentiation), I only see <code>exp</code>. Why is there no <code>exp2</code> / <code>exp10</code> / etc?</p>

<p>I've tried using <code>np.power(10, nums)</code>, but it won't let me raise to a negative power. <code>10 ** nums</code> does not work either.</p>
","['python', 'python-3.x', 'numpy']",49594398,"<p>It should work fine with <code>10 ** nums</code> provided you use the <code>float</code> dtype. Otherwise it will create an integer array:</p>

<pre><code>&gt;&gt;&gt; a = numpy.array([-1, 0, 1, 2, 3], dtype=int)
&gt;&gt;&gt; 2 ** a
array([0, 1, 2, 4, 8])
&gt;&gt;&gt; 10 ** a
array([   0,    1,   10,  100, 1000])
&gt;&gt;&gt; a = numpy.array([-1, 0, 1, 2, 3], dtype=float)
&gt;&gt;&gt; 10 ** a
array([  1.00000000e-01,   1.00000000e+00,   1.00000000e+01,
         1.00000000e+02,   1.00000000e+03])
</code></pre>

<p>You can also coerce to <code>float</code> by using <code>10.0</code> instead of <code>10</code>:</p>

<pre><code>&gt;&gt;&gt; a = numpy.array([-1, 0, 1, 2, 3], dtype=int)
&gt;&gt;&gt; 10.0 ** a
array([  1.00000000e-01,   1.00000000e+00,   1.00000000e+01,
         1.00000000e+02,   1.00000000e+03])
</code></pre>
",How raise array numbers power NumPy NumPy log log log methods perform vectorized log base e respectively However inverse operation exponentiation I see exp Why exp exp etc I tried using np power nums let raise negative power nums work either,"startoftags, python, python3x, numpy, endoftags",python python3x numpy endoftags,python python3x numpy,python python3x numpy,1.0
49617624,2018-04-02,2018,2,Using RegEx to find and print plural words in Turkish,"<p>I am fairly new in python. In the code, I've read a text file as input, and put each line readen on this text file into a list as elements.</p>

<p>I'm trying to write the code using RegEx to find and print plural words. In turkish, plural words are '-ler' or '-lar' suffixes.</p>

<p>my code is as follows: </p>

<pre><code>import re

f = open('C:/Users/ENE/Desktop/CSE &amp; Kodlar/nlp/utf8textfile.txt', encoding='utf-8-sig', errors='ignore')


with f as file:
    list = file.readlines()
list = [x.strip() for x in list]

print(list)

total = 0
for i in list:
    total += len(i)
ave_size = float(total) / float(len(list))
print(""Average word length = "" + str(ave_size))

#p = re.compile('.*l[ae]r.*')

for element in list:
    m = re.findall("".*l[ae]r.*"", element)
    if m:
        print(m)
</code></pre>

<p>which gives an output of</p>

<p>list = ['Aliler geldiler', 'Selam olsun sana', 'Merhabalar', 'Java kitabÄ± nerede']</p>

<p>for loop:
['Aliler geldiler']
['Merhabalar']</p>

<p>I am trying to print word by word, like ['Aliler'], ['geldiler'] and ['Merhabalar']. How can I do this?</p>
","['python', 'regex', 'python-3.x']",49617853,"<p>You may just find all words ending in <code>lar</code> or <code>ler</code> using a <code>\w*l[ea]r\b</code> regex:</p>

<pre><code>results = re.findall(r'\w*l[ea]r\b', s)
</code></pre>

<p>See the <a href=""https://regex101.com/r/TazRWw/2"" rel=""nofollow noreferrer"">regex demo</a>. In Python 3.x, <code>\b</code> word boundary is Unicode aware by default, in Python 2.x, I'd recommend adding <code>re.U</code> flag.</p>

<p>Here, <code>s</code> can be the whole line, or even the whole document.</p>

<p><strong>Details</strong></p>

<ul>
<li><code>\w*</code> - 0+ letters, digits and <code>_</code> (in Python 3.x, it will match all Unicode letters, digits or <code>_</code>, you may use <code>[^\W\d_]*</code> to only match letters)</li>
<li><code>l</code> - an <code>l</code> letter</li>
<li><code>[ea]</code> - <code>e</code> or <code>a</code></li>
<li><code>r</code> - an <code>r</code> letter</li>
<li><code>\b</code> - a word boundary (note the <code>r'..'</code> notation used to avoid double escaping <code>\b</code> to make the engine parse it as a word boundary).</li>
</ul>
",Using RegEx find print plural words Turkish I fairly new python In code I read text file input put line readen text file list elements I trying write code using RegEx find print plural words In turkish plural words ler lar suffixes code follows import f open C Users ENE Desktop CSE amp Kodlar nlp utf textfile txt encoding utf sig errors ignore f file list file readlines list x strip x list print list total list total len ave size float total float len list print Average word length str ave size p compile l ae r element list findall l ae r element print gives output list Aliler geldiler Selam olsun sana Merhabalar Java kitab nerede loop Aliler geldiler Merhabalar I trying print word word like Aliler geldiler Merhabalar How I,"startoftags, python, regex, python3x, endoftags",python django djangorestframework endoftags,python regex python3x,python django djangorestframework,0.33
49652950,2018-04-04,2018,2,Adding elements to list based on condition of previous elements,"<p>I have a txt file that I am reading with the following format:</p>

<pre><code>Event B     0     40
Event B     0     75    
Event B     1     30    
Event A         
Event B     1     50    
Event B     1     70    
Event A         
Event A                     
Event B     2     40
</code></pre>

<p>I am trying to code the following logic:</p>

<p>For every Event A:
    print columns 1 and 2 of first Event B SINCE the last Event A</p>

<p>So the output would be the following:</p>

<pre><code>Event B     0     40
Event B     0     75    
Event B     1     30    
Event A     0     40
Event B     1     50    
Event B     1     70    
Event A     1     50    
Event A     N/A   N/A               
Event B     2     40
etc...
</code></pre>

<p>I can read in the file ok as a list:</p>

<pre><code>with open(event_file) as schedule:
     schedule = schedule.readlines()


for i in range(0, len(schedule)):
     if schedule[i][0] == 'Event A':
          if schedule[i-X][0] == 'Event A':
               print(schedule[i-X+1])  # Where X is how many lines before Event A the last one was... but I really dont know how to determine this.. Nor do I know if any of this is the right way to go about it.
</code></pre>

<p>I hope I'm making sense.          </p>
","['python', 'python-3.x', 'python-2.7']",49653409,"<p>You simply have to remember the last Event B:</p>

<pre><code>txt = """"""Event B  ,   0 ,    40
Event B  ,   0    , 75    
Event B  ,   1    , 30    
Event A
Event B  ,   1    , 50    
Event B  ,   1    , 70    
Event A
Event A        
Event B  ,   2    , 40
""""""

# split your data:
data = [ [k.strip() for k in row.strip().split("","")] for row in txt.split(""\n"")]

rv = []
b = None
for d in data: 
    if d[0] == ""Event A"":
        # either add the remembered B or N/A's
        if b:
            rv.append([ d[0], b[1],b[2] ])
        else:
            rv.append([ d[0], ""N/A"",""N/A"" ])
        b = None     # delete remebered b
        continue
    elif b == None:  # remember first b
        b = d
    if d and d[0]:   # if not empty, add to results 
        rv.append(d)

print (rv) # print results
</code></pre>

<p>Output:</p>

<pre><code>[['Event B', '0', '40'], 
 ['Event B', '0', '75'], 
 ['Event B', '1', '30'], 
 ['Event A', '0', '40'], 
 ['Event B', '1', '50'], 
 ['Event B', '1', '70'], 
 ['Event A', '1', '50'], 
 ['Event A', 'N/A', 'N/A'], 
 ['Event B', '2', '40']]
</code></pre>
",Adding elements list based condition previous elements I txt file I reading following format Event B Event B Event B Event A Event B Event B Event A Event A Event B I trying code following logic For every Event A print columns first Event B SINCE last Event A So output would following Event B Event B Event B Event A Event B Event B Event A Event A N A N A Event B etc I read file ok list open event file schedule schedule schedule readlines range len schedule schedule Event A schedule X Event A print schedule X Where X many lines Event A last one I really dont know determine Nor I know right way go I hope I making sense,"startoftags, python, python3x, python27, endoftags",python pandas dataframe endoftags,python python3x python27,python pandas dataframe,0.33
49669492,2018-04-05,2018,2,pandas reference NaN value with multiple keys,"<p>I have 2 dataframes, the [Trade] column in first dataframe has 3 NaN values,
I need to fill them with a referenc dataframe, which you might see below this the first df.
You can use various methods to populate NaN value. Join, merge, concat, replace, whatever that works easily.
thank you</p>

<pre><code>    Trade        ID     Toy_company
0   D255/FE     192     Duplo
1   L217/SN     255     Duplo
2   NaN         195     Lego
3   NaN         256     Duplo
4   D255/FE     192     Duplo
5   D255/FE     192     Duplo
6   D250/AT     192     Duplo
7   D250/AT     192     Duplo
8   L195/AE     195     Lego
9   NaN         195     Lego
10  L256/PE     256     Lego
</code></pre>

<p>reference dataframe:</p>

<pre><code>   Trade_ID     Item    Company
0   D255/FE     192     Duplo
1   D217/SN     255     Duplo
2   L12A/BA     195     Lego
3   L999/CC     256     Lego
</code></pre>

<p>Requirement:
1. Only fill NaN value according to the info in reference dataframe, on TWO keys: Item+Company. Leave it NaN if can't be referenced.
2. Do not correct any exsiting info, only impact on NaN values</p>

<p>So the final result should be:</p>

<pre><code>        Trade        ID     Toy_company
    0   D255/FE     192     Duplo
    1   L217/SN     255     Duplo
    2   L12A/BA     195     Lego
    3   NaN         256     Duplo
    4   D255/FE     192     Duplo
    5   D255/FE     192     Duplo
    6   D250/AT     192     Duplo
    7   D250/AT     192     Duplo
    8   L195/AE     195     Lego
    9   L12A/BA     195     Lego
    10  L256/PE     256     Lego
</code></pre>
","['python', 'pandas', 'dataframe']",49669764,"<p>you can use <code>merge()</code> method:</p>

<pre><code>In [347]: d1['Trade'] = (d1['Trade'].fillna(
                             d1.merge(d2.rename(columns={'Item':'ID','Company':'Toy_company'}),
                                      how='left')['Trade_ID']))

In [348]: d1
Out[348]:
      Trade   ID Toy_company
0   D255/FE  192       Duplo
1   L217/SN  255       Duplo
2   L12A/BA  195        Lego
3       NaN  256       Duplo
4   D255/FE  192       Duplo
5   D255/FE  192       Duplo
6   D250/AT  192       Duplo
7   D250/AT  192       Duplo
8   L195/AE  195        Lego
9   L12A/BA  195        Lego
10  L256/PE  256        Lego
</code></pre>
",pandas reference NaN value multiple keys I dataframes Trade column first dataframe NaN values I need fill referenc dataframe might see first df You use various methods populate NaN value Join merge concat replace whatever works easily thank Trade ID Toy company D FE Duplo L SN Duplo NaN Lego NaN Duplo D FE Duplo D FE Duplo D AT Duplo D AT Duplo L AE Lego NaN Lego L PE Lego reference dataframe Trade ID Item Company D FE Duplo D SN Duplo L A BA Lego L CC Lego Requirement Only fill NaN value according info reference dataframe TWO keys Item Company Leave NaN referenced Do correct exsiting info impact NaN values So final result Trade ID Toy company D FE Duplo L SN Duplo L A BA Lego NaN Duplo D FE Duplo D FE Duplo D AT Duplo D AT Duplo L AE Lego L A,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
49781015,2018-04-11,2018,3,Max and min from two series in pandas groupby,"<p>Is it possible to get the min and max values from two series in a groupby? </p>

<p>For example in the following situation, when grouping by <code>c</code>, how can I get the min and max values for <code>a</code> and <code>b</code> at the same time? </p>

<pre><code>df = pd.DataFrame({'a': [10,20,3,40,55], 'b': [5,14,8,50,60], 'c': ['x','x','y','y','y']})
g = df.groupby(df.c)
for key, item in g:
    print (g.get_group(key), ""\n"")

    a   b  c
0  10   5  x
1  20  14  x

    a   b  c
2   3   8  y
3  40  50  y
4  55  60  y
</code></pre>

<p>I have resolved this by taking the min and max of each grouped series then by finding the min and max of the <code>_min</code>/<code>_max</code> series:</p>

<pre><code>df['a_min'] = g['a'].transform('min')
df['a_max'] = g['a'].transform('max')
df['b_min'] = g['b'].transform('min')
df['b_max'] = g['b'].transform('max')
df['min'] = df[['a_min', 'a_max', 'b_min', 'b_max']].min(axis=1)
df['max'] = df[['a_min', 'a_max', 'b_min', 'b_max']].max(axis=1)

    a   b  c  a_min  a_max  b_min  b_max  min  max
0  10   5  x     10     20      5     14    5   20
1  20  14  x     10     20      5     14    5   20
2   3   8  y      3     55      8     60    3   60
3  40  50  y      3     55      8     60    3   60
4  55  60  y      3     55      8     60    3   60
</code></pre>

<p>This produces the output that I want but with a lot of extra series. I am wondering if there is a better way to do this? </p>
","['python', 'pandas', 'pandas-groupby']",49781140,"<p>Using <code>transform</code>still ok , you just need add <code>min(axis=1)</code> for your <code>transform</code> result </p>

<pre><code>df['min'],df['max']=df.groupby('c').transform('min').min(1),df.groupby('c').transform('max').max(1)
df
Out[88]: 
    a   b  c  min  max
0  10   5  x    5   20
1  20  14  x    5   20
2   3   8  y    3   60
3  40  50  y    3   60
4  55  60  y    3   60
</code></pre>

<p>In an instance where there are series that you don't want included, for example excluding <code>f</code>, the series should be listed after the grouping</p>

<pre><code>    a   b  c   f
0  10   5  x   0
1  20  14  x  45
2   3   8  y  67
3  40  50  y  17
4  55  60  y  91

df['min'] = df.groupby('c')[['a', 'b']].transform('min').min(axis=1)
df['max'] = df.groupby('c')[['a', 'b']].transform('max').max(axis=1)
</code></pre>
",Max min two series pandas groupby Is possible get min max values two series groupby For example following situation grouping c I get min max values b time df pd DataFrame b c x x g df groupby df c key item g print g get group key n b c x x b c I resolved taking min max grouped series finding min max min max series df min g transform min df max g transform max df b min g b transform min df b max g b transform max df min df min max b min b max min axis df max df min max b min b max max axis b c min max b min b max min max x x This produces output I want lot extra series I wondering better way,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas numpy endoftags,python pandas pandasgroupby,python pandas numpy,0.67
49791146,2018-04-12,2018,2,Rolling array around,"<p>Let's say I have</p>

<pre><code>arr = np.arange(6)
arr
array([0, 1, 2, 3, 4, 5])
</code></pre>

<p>and I decide that I want to treat an array ""like a circle"": When I run out of material at the end, I want to start at index 0 again. That is, I want a convenient way of selecting <code>x</code> elements, starting at index <code>i</code>.</p>

<p>Now, if <code>x == 6</code>, I can simply do</p>

<pre><code>i = 3
np.hstack((arr[i:], arr[:i]))
Out[9]: array([3, 4, 5, 0, 1, 2])
</code></pre>

<p>But is there a convenient way of generally doing this, even if <code>x &gt; 6</code>, without having to manually breaking the array apart and thinking through the logic?</p>

<p>For example:</p>

<pre><code>print(roll_array_arround(arr)[2:17])
</code></pre>

<p>should return.</p>

<pre><code>array([2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0])
</code></pre>
","['python', 'arrays', 'numpy']",49791708,"<p>See mode='wrap' in ndarray.take:</p>

<p><a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.take.html"" rel=""nofollow noreferrer"">https://docs.scipy.org/doc/numpy/reference/generated/numpy.take.html</a></p>

<p>Taking your hypothetical function:</p>

<pre><code>print(roll_array_arround(arr)[2:17])
</code></pre>

<p>If it is implied that it is a true slice of the original array that you are after, that is not going to happen; a wrapped-around array cannot be expressed as a strided view of the original; so if you seek a function that maps an ndarray to an ndarray, this will necessarily involve a copy of your data.</p>

<p>That is, efficiency-wise, you shouldnt expect to find solution that significantly differs in performance from the expression below. </p>

<pre><code>print(arr.take(np.arange(2,17), mode='wrap'))
</code></pre>
",Rolling array around Let say I arr np arange arr array I decide I want treat array like circle When I run material end I want start index That I want convenient way selecting x elements starting index Now x I simply np hstack arr arr Out array But convenient way generally even x gt without manually breaking array apart thinking logic For example print roll array arround arr return array,"startoftags, python, arrays, numpy, endoftags",python python3x numpy endoftags,python arrays numpy,python python3x numpy,0.67
49961169,2018-04-21,2018,5,Show categorical x-axis values when making line plot from pandas Series in matplotlib,"<p>How do I get the x-axis values of [a, b, c] to show up?</p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt

s = pd.Series([1, 2, 10], index=['a', 'b', 'c'])
s.plot()
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/gb7Xz.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gb7Xz.png"" alt=""enter image description here""></a></p>
","['python', 'pandas', 'matplotlib']",49962070,"<p>You can get your xtick labels to show using <code>plt.xticks</code>:</p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt
s = pd.Series([1, 2, 10], index=['a', 'b', 'c'])
s.plot()
plt.xticks(np.arange(len(s.index)), s.index)
plt.show()
</code></pre>

<p>Output:</p>

<p><a href=""https://i.stack.imgur.com/gcNqL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gcNqL.png"" alt=""enter image description here""></a></p>
",Show categorical x axis values making line plot pandas Series matplotlib How I get x axis values b c show import pandas pd import matplotlib pyplot plt pd Series index b c plot plt show,"startoftags, python, pandas, matplotlib, endoftags",python pandas matplotlib endoftags,python pandas matplotlib,python pandas matplotlib,1.0
49984696,2018-04-23,2018,2,String of characters within filename using python,"<p>I want to read the below file using the pandas data frame. The letters and numbers after the timestamp changes dynamically. How can i use the Unix property like (filename*) in python?</p>

<p>filename - file_04_23_2018_<strong>5d4da460ab82496a</strong></p>

<pre><code>import pandas as pd
import time
T= time.strftime(""%m_%d_%Y"")
pd.read_csv(""file_{}"".format(T))
</code></pre>

<p>With the code above, I could not read the full name of file.</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",49984755,"<p>You may iterate over <code>glob.glob</code>:</p>

<pre><code>import glob

today = time.strftime(""%m_%d_%Y"")

df_list = []
for f in glob.glob('file_{}_*'.format(today)):
    df_list.append(pd.read_csv(f))

df = pd.concat([df_list], axis=0, ignore_index=True)
</code></pre>

<p>If it's just one file matching this format, this still works the same, though you may be able to optimise a bit:</p>

<pre><code>f = glob.glob('file_{}_*'.format(today))[0]
df = pd.read_csv(f)
</code></pre>
",String characters within filename using python I want read file using pandas data frame The letters numbers timestamp changes dynamically How use Unix property like filename python filename file da ab import pandas pd import time T time strftime Y pd read csv file format T With code I could read full name file,"startoftags, python, python3x, pandas, dataframe, endoftags",python python3x list endoftags,python python3x pandas dataframe,python python3x list,0.58
50053228,2018-04-26,2018,3,ListSerializer in Django Restful - When is it called?,"<p>I have the following code for my <code>serializers.py</code>: </p>

<pre><code>from rest_framework import serializers

from django.db import transaction
from secdata_finder.models import File

class FileListSerializer(serializers.ListSerializer):
    @transaction.atomic
    def batch_save_files(file_data):
        files = [File(**data) for data in file_data]
        return File.objects.bulk_create(files)

    def create(self, validated_data):
        print(""I am creating multiple rows!"")
        return self.batch_save_files(validated_data)

class FileSerializer(serializers.ModelSerializer):
    class Meta:
        list_serializer_class = FileListSerializer
        model = File
        fields = (...) # omitted
</code></pre>

<p>I'm experimenting with it on my Django test suite:</p>

<pre><code>def test_file_post(self):
    request = self.factory.post('/path/file_query', {""many"":False})
    request.data = {
        ... # omitted fields here
    }
    response = FileQuery.as_view()(request)
</code></pre>

<p>It prints <code>I am creating multiple rows!</code>, which is <strong>not what should happen</strong>.</p>

<p>Per the <a href=""http://www.django-rest-framework.org/api-guide/serializers/#listserializer"" rel=""nofollow noreferrer"">docs</a>:</p>

<blockquote>
  <p>... customize the create or update behavior of multiple objects.
  For these cases you can modify the class that is used when many=True is passed, by using the list_serializer_class option on the serializer Meta class.</p>
</blockquote>

<p>So what am I not understanding? I passed in <code>many:False</code> in my post request, and yet it still delegates the <code>create</code> function to the <code>FileListSerializer</code>!</p>
","['python', 'django', 'django-rest-framework']",50055585,"<p>Per the docs:</p>

<blockquote>
  <p>The ListSerializer class provides the behavior for serializing and
  validating multiple objects at once. You won't typically need to use
  ListSerializer directly, but should instead simply pass many=True when
  instantiating a serializer</p>
</blockquote>

<p>You can add <code>many=True</code> to your serializer</p>

<pre><code>class FileSerializer(serializers.ModelSerializer):

def __init__(self, *args, **kwargs):
    kwargs['many'] = kwargs.get('many', True)
    super().__init__(*args, **kwargs)
</code></pre>

<p>Potential dupe of <a href=""https://stackoverflow.com/questions/14666199/how-do-i-create-multiple-model-instances-with-django-rest-framework"">How do I create multiple model instances with Django Rest Framework?</a></p>
",ListSerializer Django Restful When called I following code serializers py rest framework import serializers django db import transaction secdata finder models import File class FileListSerializer serializers ListSerializer transaction atomic def batch save files file data files File data data file data return File objects bulk create files def create self validated data print I creating multiple rows return self batch save files validated data class FileSerializer serializers ModelSerializer class Meta list serializer class FileListSerializer model File fields omitted I experimenting Django test suite def test file post self request self factory post path file query many False request data omitted fields response FileQuery view request It prints I creating multiple rows happen Per docs customize create update behavior multiple objects For cases modify class used many True passed using list serializer class option serializer Meta class So I understanding I passed many False post request yet still delegates create function,"startoftags, python, django, djangorestframework, endoftags",python django djangorestframework endoftags,python django djangorestframework,python django djangorestframework,1.0
50117840,2018-05-01,2018,2,pandas dataframe: identifiy NaN and zero values in one statement,"<p>Is there any way to combine the two statements
<code>df.isnull().sum()</code> and
<code>(df == 0).sum()</code> to get the following overview?</p>

<p>Demo:</p>

<pre><code>df = pd.DataFrame({'a':[1,0,0,1,3], 'b':[0,NaN,1,NaN,1], 'c':[0,0,0,0,NaN]})

df

    a   b       c
0   1   0.0     0.0
1   0   NaN     0.0
2   0   1.0     0.0
3   1   NaN     0.0
4   3   1.0     NaN
</code></pre>

<p>Expected result:</p>

<pre><code>a    2
b    3
c    5
</code></pre>

<p>Probably very simple, but I can't find the solution... 
Thank's for your help</p>
","['python', 'pandas', 'dataframe']",50117901,"<p>You mean just this:</p>

<pre><code>In[27]:

(df==0).sum() + df.isnull().sum()
Out[27]: 
a    2
b    3
c    5
dtype: int64
</code></pre>

<p><strong>EDIT</strong></p>

<p>Thanks to @coldpseed for the suggestion, you can also do the following:</p>

<pre><code>In[28]:
df[df!=0].isnull().sum()

Out[28]: 
a    2
b    3
c    5
dtype: int64
</code></pre>

<p>which is more succinct, I've always been more in favour of clarity but shorter code sometimes wins.</p>
",pandas dataframe identifiy NaN zero values one statement Is way combine two statements df isnull sum df sum get following overview Demo df pd DataFrame b NaN NaN c NaN df b c NaN NaN NaN Expected result b c Probably simple I find solution Thank help,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
50334755,2018-05-14,2018,2,How feature columns work in tensorflow?,"<p>I read that <a href=""https://www.tensorflow.org/get_started/feature_columns"" rel=""nofollow noreferrer"">feature columns in tensorflow</a> are used to define our data but how and why? How do feature columns work and why they even exist if we can make a custom estimator without them too? </p>

<p>And if they are necessary, why libraries like keras don't use them?</p>
","['python', 'tensorflow', 'keras']",50334977,"<p><strong>Broadly Speaking</strong></p>

<p>This may be too general to answer. You may want to watch some videos or do more reading on machine learning, because this is a broad topic.</p>

<p>I will try to explain what features of data are used for.</p>

<p>A ""feature"" of the data is a meaningful variable that <em>should</em> separate two classes from each other. For example, if we choose the feature ""weight"", we can tell elephants apart from squirrels. They have very different weights, and our machine learning algorithm can learn to ""understand"" that an animal with a heavy weight is more likely to be an elephant than it is to be a squirrel. In a real scenario you would generally have more than one feature.</p>

<p>I'm not sure why you would say that Keras does not use features. They are a fundamental aspect of many classification problems. Some datasets may contain labelled data or labelled features, like this one: <a href=""https://keras.io/datasets/#cifar100-small-image-classification"" rel=""nofollow noreferrer"">https://keras.io/datasets/#cifar100-small-image-classification</a></p>

<p>When we ""don't use features"", I think a more accurate way to state that would be that the data is unlabelled. In this case, a machine learning algorithm can still find relationships in the data, but without human labels applied to the data.</p>

<p>If you <code>Ctrl+F</code> for the word ""features"" on this page you will see places where Keras accepts them as an argument: <a href=""https://keras.io/layers/core/"" rel=""nofollow noreferrer"">https://keras.io/layers/core/</a></p>

<p>I am not a machine learning expert so if anyone is able to correct my answer, I would appreciate that too.</p>

<p><strong>In Tensorflow</strong></p>

<p>My understanding of <a href=""https://www.tensorflow.org/get_started/feature_columns"" rel=""nofollow noreferrer"">Tensorflow's feature columns</a> implementation in particular is that they allow you to cast raw data into a typed column that allow the algorithm to better distinguish what type of data you are passing. For example Latitude and Longitude could be passed as two numerical columns, but as the docs say <a href=""https://www.tensorflow.org/get_started/feature_columns#crossed_column"" rel=""nofollow noreferrer"">here</a>, using a Crossed Column for Latitude X Longitude may allow the model to train on the data in a more meaningful/effective way. After all, what ""Latitude"" and ""Longitude"" really mean is ""Location."" As for why Keras does not have this functionality, I am not sure, hopefully someone else can offer insight on this topic.</p>
",How feature columns work tensorflow I read feature columns tensorflow used define data How feature columns work even exist make custom estimator without And necessary libraries like keras use,"startoftags, python, tensorflow, keras, endoftags",python django djangorestframework endoftags,python tensorflow keras,python django djangorestframework,0.33
50372272,2018-05-16,2018,23,How to add columns to an empty pandas dataframe?,"<p>I have an empty <code>dataframe</code>.</p>

<pre><code>df=pd.DataFrame(columns=['a'])
</code></pre>

<p>for some reason I want to generate df2, another empty dataframe, with two columns 'a' and 'b'.</p>

<p>If I do </p>

<pre><code>df.columns=df.columns+'b'
</code></pre>

<p>it does not work (I get the columns renamed to 'ab')
and neither does the following </p>

<pre><code>df.columns=df.columns.tolist()+['b']
</code></pre>

<p>How to add a separate column 'b' to df, and <code>df.emtpy</code> keep on being <code>True</code>?</p>

<p>Using .loc is also not possible</p>

<pre><code>   df.loc[:,'b']=None
</code></pre>

<p>as it returns </p>

<pre><code>  Cannot set dataframe with no defined index and a scalar
</code></pre>
","['python', 'pandas', 'dataframe']",50372722,"<p>Here are few ways to add an empty column to an empty dataframe:</p>

<pre><code>df=pd.DataFrame(columns=['a'])
df['b'] = None
df = df.assign(c=None)
df = df.assign(d=df['a'])
df['e'] = pd.Series(index=df.index)   
df = pd.concat([df,pd.DataFrame(columns=list('f'))])
print(df)
</code></pre>

<p><strong>Output</strong>:</p>

<pre><code>Empty DataFrame
Columns: [a, b, c, d, e, f]
Index: []
</code></pre>

<p>I hope it helps.</p>
",How add columns empty pandas dataframe I empty dataframe df pd DataFrame columns reason I want generate df another empty dataframe two columns b If I df columns df columns b work I get columns renamed ab neither following df columns df columns tolist b How add separate column b df df emtpy keep True Using loc also possible df loc b None returns Cannot set dataframe defined index scalar,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
50441576,2018-05-21,2018,3,Why id of a pandas dataframe cell changes with each execution?,"<p>I ran into this problem when I was trying to make sure some properties of data frame's view.</p>

<p>Suppose I have a dataframe defined as: <code>df = pd.DataFrame(columns=list('abc'), data=np.arange(18).reshape(6, 3))</code> and a view of this dataframe defined as: <code>df1 = df.iloc[:3, :]</code>. We now have two dataframes as following:</p>

<pre><code>print(df)
    a   b   c
0   0   1   2
1   3   4   5
2   6   7   8
3   9  10  11
4  12  13  14
5  15  16  17

print(df1)

   a  b  c
0  0  1  2
1  3  4  5
2  6  7  8
</code></pre>

<p>Now I want to output the id of a particular cell of these two dataframes:</p>

<pre><code>print(id(df.loc[0, 'a']))
print(id(df1.loc[0, 'a']))
</code></pre>

<p>and I have the output as:</p>

<pre><code>140114943491408
140114943491408
</code></pre>

<p>The weird thing is, if I continuously execute those two lines of 'print id' code, the ids change as well:</p>

<pre><code>140114943491480
140114943491480
</code></pre>

<p>I have to emphasize that I did not execute the 'df definition' code when I execute those two 'print id' code, so the df and df1 are not redefined. Then, in my opinion, the memory address of each element in the data frame should be fixed, so how could the output changes?</p>

<p>A more weird thing happens when I keep executing those two lines of 'print id' codes. In some rare scenarios, those two ids even do not equal to each other:</p>

<pre><code>140114943181088
140114943181112
</code></pre>

<p>But if I execute <code>id(df.loc[0, 'a']) == id(df1.loc[0, 'a'])</code> at the same time, python still output <code>True</code>. I know that since df1 is a view of df, their cells should share one memory, but how come the output of their ids could be different occasionally?</p>

<p>Those strange behaviors make me totally at lost. Could anyone explain those behaviors? Are they due to the characteristics of data frame or the id function in python? Thanks! </p>

<p>FYI, I am using <code>Python 3.5.2</code>.</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",50441641,"<p>You are not getting the id of a ""cell"", you are getting the <code>id</code> of the object returned by the <code>.loc</code> accessor, which is a boxed version of the underlying data.</p>

<p>So, </p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame(columns=list('abc'), data=np.arange(18).reshape(6, 3))
&gt;&gt;&gt; df1 = df.iloc[:3, :]
&gt;&gt;&gt; df.dtypes
a    int64
b    int64
c    int64
dtype: object
&gt;&gt;&gt; df1.dtypes
a    int64
b    int64
c    int64
dtype: object
</code></pre>

<p>But since <em>everything</em> in Python is an object, your <code>loc</code> method must return an object:</p>

<pre><code>&gt;&gt;&gt; x = df.loc[0, 'a']
&gt;&gt;&gt; x
0
&gt;&gt;&gt; type(x)
&lt;class 'numpy.int64'&gt;
&gt;&gt;&gt; isinstance(x, object)
True
</code></pre>

<p>However, the actual underlying buffer is a primitive array of C fixed-size 64-bit signed integers. They are not Python objects, they are ""boxed"" to borrow a term from other languages which mix primitive types with objects.</p>

<p>Now, the phenomenon you are seeing with all objects having the same <code>id</code>:</p>

<pre><code>&gt;&gt;&gt; id(df.loc[0, 'a']), id(df.loc[0, 'a'])
(4539673432, 4539673432)
&gt;&gt;&gt; id(df.loc[0, 'a']), id(df.loc[0, 'a']), id(df1.loc[0,'a'])
(4539673432, 4539673432, 4539673432)
</code></pre>

<p>Occurs because in Python, objects are free to re-use the memory address of recently reclaimed objects. Indeed, when you create your tuple of <code>id</code>'s, the object's returned by <code>loc</code> only exist long enough to get passed and processed by the first invocation of <code>id</code>, the second time you use <code>loc</code>, the object, already deallocated, simply re-uses the same memory. You can see the same behavior with any Python object, like a <code>list</code>:</p>

<pre><code>&gt;&gt;&gt; id([]), id([])
(4545276872, 4545276872)
</code></pre>

<p>Fundamentally, <code>id</code>'s are only guaranteed to be unique for the <em>lifetime</em> of the object. Read more about this phenomenon <a href=""https://stackoverflow.com/questions/20753364/why-is-the-id-of-a-python-class-not-unique-when-called-quickly"">here</a>. But, note, in the following case, it will always be different:</p>

<pre><code>&gt;&gt;&gt; x = df.loc[0, 'a']
&gt;&gt;&gt; x2 = df.loc[0, 'a']
&gt;&gt;&gt; id(x), id(x2)
(4539673432, 4539673408)
</code></pre>

<p>Since you maintain references around, the objects are not reclaimed, and require new memory.</p>

<p>Note, for many immutable objects, the interpreter is free to optimize and return <em>the same exact object</em>. In CPython, this is the case with ""small ints"", the so called small-int cache:</p>

<pre><code>&gt;&gt;&gt; x = 2
&gt;&gt;&gt; y = 2
&gt;&gt;&gt; id(x), id(y)
(4304820368, 4304820368)
</code></pre>

<p>But this is an implementation detail that should not be relied upon.</p>

<p>If you want to prove to yourself that your data-frames are sharing the same underlying buffer, just mutate them and you'll see the same change reflected across views:</p>

<pre><code>&gt;&gt;&gt; df
    a   b   c
0   0   1   2
1   3   4   5
2   6   7   8
3   9  10  11
4  12  13  14
5  15  16  17
&gt;&gt;&gt; df1
   a  b  c
0  0  1  2
1  3  4  5
2  6  7  8
&gt;&gt;&gt; df.loc[0, 'a'] = 99
&gt;&gt;&gt; df
    a   b   c
0  99   1   2
1   3   4   5
2   6   7   8
3   9  10  11
4  12  13  14
5  15  16  17
&gt;&gt;&gt; df1
    a  b  c
0  99  1  2
1   3  4  5
2   6  7  8
</code></pre>
",Why id pandas dataframe cell changes execution I ran problem I trying make sure properties data frame view Suppose I dataframe defined df pd DataFrame columns list abc data np arange reshape view dataframe defined df df iloc We two dataframes following print df b c print df b c Now I want output id particular cell two dataframes print id df loc print id df loc I output The weird thing I continuously execute two lines print id code ids change well I emphasize I execute df definition code I execute two print id code df df redefined Then opinion memory address element data frame fixed could output changes A weird thing happens I keep executing two lines print id codes In rare scenarios two ids even equal But I execute id df loc id df loc time python still output True I know since df view df cells,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
50503290,2018-05-24,2018,4,How to convert a column of numbers into a date in a dataframe in python,"<p>My dataframe contains a column for dates. It looks like this: </p>

<pre><code>Index    Date
 0       12018
 1      102018
 2       32018
 3      122018
 4      112019
 5       32019
 6       42019
</code></pre>

<p>The last four numbers show the year and the first (two) the month.
I want to change the column to:</p>

<pre><code>- 01-01-2018 
- 01-01-2018 
- 01-10-2018 
- 01-03-2018
...
</code></pre>

<p>or even better to a datetime format.</p>

<p>I've tried this function, which displays:</p>

<blockquote>
  <p>TypeError: can only concatenate list (not ""str"") to list</p>
</blockquote>

<pre><code>def adjust_date(dataset_in, col_name):
day = ""01""
for col in col_name:
    if len(col_name)&gt;5:
        month = col_name[0:1]
        year = col_name[2:5]
    else:
        month = col_name[0]
        year = col_name[1:4]

    result = year + ""-"" + month + ""-"" + day 
return result     
</code></pre>
","['python', 'pandas', 'dataframe']",50503324,"<p>I think <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html"" rel=""nofollow noreferrer""><code>to_datetime</code></a> with a specified <a href=""http://strftime.org/"" rel=""nofollow noreferrer"">format</a> should be enough:</p>

<pre><code>df['Date'] = pd.to_datetime(df['Date'], format='%m%Y')
print (df)
   Index       Date
0      0 2018-01-01
1      1 2018-10-01
2      2 2018-03-01
3      3 2018-12-01
4      4 2019-11-01
5      5 2019-03-01
6      6 2019-04-01

print (df.dtypes)
Index             int64
Date     datetime64[ns]
dtype: object
</code></pre>

<p>Thank you @Vivek Kalyanarangan for solution - add <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.strftime.html"" rel=""nofollow noreferrer""><code>strftime</code></a> for custom <code>string</code> format (but lost datetimes):</p>

<pre><code>df['Date'] = pd.to_datetime(df['Date'], format='%m%Y').dt.strftime('%d-%m-%Y')
print (df)
   Index        Date
0      0  01-01-2018
1      1  01-10-2018
2      2  01-03-2018
3      3  01-12-2018
4      4  01-11-2019
5      5  01-03-2019
6      6  01-04-2019

print (df.dtypes)
Index     int64
Date     object
dtype: object

print (df['Date'].apply(type))
0    &lt;class 'str'&gt;
1    &lt;class 'str'&gt;
2    &lt;class 'str'&gt;
3    &lt;class 'str'&gt;
4    &lt;class 'str'&gt;
5    &lt;class 'str'&gt;
6    &lt;class 'str'&gt;
Name: Date, dtype: object
</code></pre>
",How convert column numbers date dataframe python My dataframe contains column dates It looks like Index Date The last four numbers show year first two month I want change column even better datetime format I tried function displays TypeError concatenate list str list def adjust date dataset col name day col col name len col name gt month col name year col name else month col name year col name result year month day return result,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
50531031,2018-05-25,2018,2,Properly configure load button in tkinter and properly display values from dictionary,"<p>I'm new to coding. Probably 3 weeks coding at most. I need some help properly configuring my load button. In my program I have an entry box and button that auto generates entry boxes with which you then input the values for the dictionary. What I want my load button to do is create the proper amount of entry boxes and populate the text in the entry boxes for you. As of right now, all it does is populate the dictionary in the Label which is kind of useless. </p>

<p>Here is my code.</p>

<pre><code>from tkinter import *
import tkinter
from math import *
from tkinter.filedialog import askopenfilename
import json
import os

class App:
    def __init__(self,root):
        self.root = root
        self.entry = Entry(self.root)
        self.button = Button(self.root, text=""Input number of items in bag"", command=self.command)
        self.done = Button(self.root, text=""Save File"", command=self.save)
        self.load = Button(self.root, text=""Load File"", command=self.load_data)
        self.save = Button(self.root, text=""Save List"", command=self.dict)
        self.frame = Frame(self.root)
        self.browsebutton = Button(root, text=""Browse"", command=self.browsefunc)
        self.entry.pack(side=RIGHT)
        self.button.pack(side=RIGHT) 
        self.save.pack(side=LEFT)
        self.done.pack(side=BOTTOM)
        self.load.pack(side=BOTTOM)
        self.browsebutton.pack(side=BOTTOM)
        self.frame.pack()

        global pathlabel
        pathlabel= Label(self.root)
        pathlabel.pack(side=BOTTOM)

        Label(self.root, text=""Enter calculation below:"").pack()
        global entry
        entry = Entry(self.root)
        entry.bind(""&lt;Return&gt;"", self.evaluate)
        entry.pack()
        global res
        res = Label(self.root)
        res.pack()

        global lbl
        lbl = Label(self.root)
        lbl.pack(side=BOTTOM)

        global DND_label
        Label(self.root)
        DND_label = Label(self.root)
        DND_label.pack(side=TOP)


    def command(self):
        self.frame.destroy()
        self.frame = Frame(self.root)
        self.text = []

        for i in range(int(self.entry.get())):
            self.text.append(Entry(self.frame, text=""Item "" + str(i+1) + ': '))
            self.text[i].pack()
            self.frame.pack()
        lbl.config(text=""Enter amount and item name."")


    def dict(self):
        global DND
        DND = {}
        for i in range(len(self.text)):
            DND.update({self.text[i].cget(""text""): self.text[i].get()})
        for k, v in DND.items():
            print(v)
        DND_label.configure(text = ""Inventory: "" + str(DND.values()))

    def save(self):
        DND = {}
        for i in range(len(self.text)):
            DND.update({self.text[i].cget(""text""): self.text[i].get()})
        try:
            with open(filename, 'w') as f:
                newpath = os.path.splitext(filename)[0] + ""_NEW.txt""
                with open(newpath, 'w') as j:
                    json.dump(DND,j)
                    DND_label.configure(text = ""Inventory: "" + str(DND.values()))
            for k, v in DND.items():
                print(v)

        except FileNotFoundError:
            try:
                New_dir = os.mkdir(os.path.join(os.path.expanduser('~'), 'Documents', 'DND_player_inventories'))
                filepath = os.path.join(os.path.expanduser('~'), 'Documents', 'DND_player_inventories')
                file = 'DND_inventory.txt'
                newpath = os.path.join(filepath, file)
                with open(newpath, 'w') as j:
                    json.dump(DND,j)
                    DND_label.configure(text = ""Inventory: "" + str(DND.values()))
                for k, v in DND.items():
                    print(v)

            except FileExistsError:
                try:
                    filepath = os.path.join(os.path.expanduser('~'), 'Documents', 'DND_player_inventories')
                    file = 'DND_inventory.txt'
                    newpath = os.path.join(filepath, file)
                    with open(newpath, 'x') as j:
                        json.dump(DND,j)
                        DND_label.configure(text = ""Inventory: "" + str(DND.values()))
                    for k, v in DND.items():
                        print(v)

                except FileExistsError:
                    filepath = os.path.join(os.path.expanduser('~'), 'Documents', 'DND_player_inventories')
                    file = 'DND_inventory.txt'
                    file = os.path.join(filepath, file)
                    with open(file, 'r') as f:
                        newpath = os.path.join(filepath, file[0:-4]) + ""_NEW.txt""
                        with open(newpath, 'w') as j:
                            json.dump(DND,j)
                            DND_label.configure(text = ""Inventory: "" + str(DND.values()))
                        for k, v in DND.items():
                            print(v)

        except NameError:
            try:
                New_dir = os.mkdir(os.path.join(os.path.expanduser('~'), 'Documents', 'DND_player_inventories'))
                filepath = os.path.join(os.path.expanduser('~'), 'Documents', 'DND_player_inventories')
                file = 'DND_inventory.txt'
                newpath = os.path.join(filepath, file)
                with open(newpath, 'w') as j:
                    json.dump(DND,j)
                    DND_label.configure(text = ""Inventory: "" + str(DND.values()))
                for k, v in DND.items():
                    print(v)

            except FileExistsError:
                try:
                    filepath = os.path.join(os.path.expanduser('~'), 'Documents', 'DND_player_inventories')
                    file = 'DND_inventory.txt'
                    newpath = os.path.join(filepath, file)
                    with open(newpath, 'x') as j:
                        json.dump(DND,j)
                        DND_label.configure(text = ""Inventory: "" + str(DND.values()))
                    for k, v in DND.items():
                        print(v)

                except FileExistsError:
                    filepath = os.path.join(os.path.expanduser('~'), 'Documents', 'DND_player_inventories')
                    file = 'DND_inventory.txt'
                    file = os.path.join(filepath, file)
                    with open(file, 'r') as f:
                        newpath = os.path.join(filepath, file[0:-4]) + ""_NEW.txt""
                        with open(newpath, 'w') as j:
                            json.dump(DND,j)
                            DND_label.configure(text = ""Inventory: "" + str(DND.values()))
                    for k, v in DND.items():
                        print(v)

    def load_data(self):
        with open(filename, 'r') as f:
            try:
                DND = json.load(f)
            except ValueError:
                DND = {}
            for k, v in DND.items():
                DND_label.configure(text = ""Inventory: "" + str(DND.values()))
                print(v)

    @staticmethod
    def browsefunc():
        global filename
        filename = askopenfilename()
        pathlabel.config(text=filename)

    @staticmethod
    def evaluate(self):
        res.configure(text = ""Answer: "" + str(eval(entry.get())))



if __name__ == ""__main__"":
    global root
    root = Tk()
    root.title('DND Player inventories')
    App(root)
    root.mainloop()
</code></pre>

<p>This is what I have for the load button.</p>

<pre><code>    def load_data(self):
        with open(filename, 'r') as f:
            try:
                DND = json.load(f)
            except ValueError:
                DND = {}
            for k, v in DND.items():
                DND_label.configure(text = ""Inventory: "" + str(DND.values()))
                print(v)
</code></pre>

<p>The other thing I need help with is displaying the dictionary values without the extra wording in there. I use this code below which shows what is in the dictionary, it just doesn't look good.</p>

<pre><code>DND_label.configure(text = ""Inventory: "" + str(DND.values()))
</code></pre>

<p>Example dictionary, save it in a text file and browse to it to load it. Name of text file doesn't matter.</p>

<pre><code>{""Item 1: "": ""test"", ""Item 2: "": ""test 1"", ""Item 3: "": ""test 3"", ""Item 4: "": ""test 4""}
</code></pre>

<p>I am using tkinter for the gui. This is just a project to help me learn. In depth answers to how and why would be very much appreciated. I'm sure there is easier ways to code it but this is understandable to me as I'm learning.</p>
","['python', 'python-3.x', 'tkinter']",50532582,"<p>Ignoring all the other issues I can see with your code, here is a simple MCVE example of how to load a dictionary from a file into your program with values being placed in the entry field. </p>

<p>Please use my below example when formulating future questions as the below example shows a <a href=""https://stackoverflow.com/help/mcve"">Minimal, Complete, and Verifiable example</a>. The bare minimum to reproduce an issue or behavior related to your question. As you can see I do not include things like the title or various other functions or variables that are unrelated to the specific questions.</p>

<p>My example will use a couple class attributes to hold the dictionary and to hold a list used for creating the different fields.</p>

<p>I use a list in this way because I can get the index information easy and use that to programmatically produce the correct labels and entry fields in the correct rows and columns.</p>

<p>By producing this list we can also use it to store new values and save them to a file but that is an answer for another question if you are unable to work it out.</p>

<p>Here is my example dictionary:</p>

<pre><code>{""Age"":""30"", ""Name"":""Mike"", ""Job"":""Network Engineer""}
</code></pre>

<p>Here is my code:
Keep in mind you will need to use your own file path to your dictionary file.</p>

<pre><code>import tkinter as tk
import json


class App(tk.Tk):
    def __init__(self):
        tk.Tk.__init__(self)
        self.button_frame = tk.Frame(self)
        self.button_frame.grid(row=0, column=0)
        self.data_frame = tk.Frame(self)
        self.data_frame.grid(row=0, column=2)
        self.loaded_dict = {}
        self.data_list = []

        tk.Button(self.button_frame, text=""Load dictionary data"",
                  command=self.load_dict).grid(row=0, column=0)

    def load_dict(self):
        self.loaded_dict = {}
        with open("".path/to/dict_file.txt"", ""r"") as f:
            self.loaded_dict = json.load(f)
        if self.loaded_dict != {}:
            for key, value in self.loaded_dict.items():
                self.data_list.append([tk.Label(self.data_frame, text=key), tk.Entry(self.data_frame), value])
            for ndex, item in enumerate(self.data_list):
                item[0].grid(row=ndex, column=0)
                item[1].grid(row=ndex, column=1)
                item[1].insert(0, item[2])

if __name__ == ""__main__"":
    root = App()
    root.mainloop()
</code></pre>

<p>Results:</p>

<p>Before pressing the button:</p>

<p><a href=""https://i.stack.imgur.com/AkyXa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AkyXa.png"" alt=""enter image description here""></a></p>

<p>After:</p>

<p><a href=""https://i.stack.imgur.com/U3eFy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U3eFy.png"" alt=""enter image description here""></a></p>

<p>Now this example is very basic and one can do a lot more to manage formatting and resetting of the frame to load something else however this example is the very basic way of loading the dictionary in the manor you requested.</p>

<p>Update:</p>

<p>To answer you question in the comments below we can load just the value to the label by changing what we are doing the <code>for</code> loop. In the for loop we are checking the key and the key value of each key/value pair in the dictionary and then doing something with that info. All we need to do is tell the text of your label to be the <code>value</code> portion of that pair.</p>

<p>Change this:</p>

<pre><code>for key, value in self.loaded_dict.items():
                    self.data_list.append([tk.Label(self.data_frame, text=key), tk.Entry(self.data_frame), value])
                for ndex, item in enumerate(self.data_list):
                    item[0].grid(row=ndex, column=0)
                    item[1].grid(row=ndex, column=1)
                    item[1].insert(0, item[2])
</code></pre>

<p>To this:</p>

<pre><code>for key, value in self.loaded_dict.items():
                self.data_list.append([tk.Label(self.data_frame, text=value), tk.Entry(self.data_frame), value])
            for ndex, item in enumerate(self.data_list):
                item[0].grid(row=ndex, column=0)
                item[1].grid(row=ndex, column=1)
</code></pre>

<p>Results:</p>

<p><a href=""https://i.stack.imgur.com/dAJnz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dAJnz.png"" alt=""enter image description here""></a></p>

<p>As stated before this is just a simple example. You can worry about things like formatting later after you get the basics working.</p>
",Properly configure load button tkinter properly display values dictionary I new coding Probably weeks coding I need help properly configuring load button In program I entry box button auto generates entry boxes input values dictionary What I want load button create proper amount entry boxes populate text entry boxes As right populate dictionary Label kind useless Here code tkinter import import tkinter math import tkinter filedialog import askopenfilename import json import os class App def init self root self root root self entry Entry self root self button Button self root text Input number items bag command self command self done Button self root text Save File command self save self load Button self root text Load File command self load data self save Button self root text Save List command self dict self frame Frame self root self browsebutton Button root text Browse command self browsefunc self entry pack,"startoftags, python, python3x, tkinter, endoftags",python python3x list endoftags,python python3x tkinter,python python3x list,0.67
50552478,2018-05-27,2018,4,Is there an easier way to produce a numpy matrix where the numbers worm up and down vertically,"<p>I'm sorry I don't know the correct terminology hence 'worm vertically' basically I want the numbers to start at the bottom left and as it goes up the column it adds, then the next column has its lowest number at the top and adds as it goes down. eg:</p>

<pre><code>[2, 3, 8
 1, 4, 7
 0, 5, 6]
</code></pre>

<p>so far I came up with this very bloated code that basically makes a column then copies it, flips it, adds the max value of the last column plus 1, then hstack's them together eg:</p>

<p>first column</p>

<pre><code>[2,
 1,
 0,]
</code></pre>

<p>flip it:</p>

<pre><code>[0,
 1,
 2,]
</code></pre>

<p>add the max value of the last plus 1 (3 in this case):</p>

<pre><code> [3,
  4,
  5,]
</code></pre>

<p>hstack them:</p>

<pre><code>[2, 3
 1, 4
 0, 5]
</code></pre>

<p>I then just repeat this for as many columns I want. to speed things up, if I wanted 4 I can just take the final hstack and copy both columns and add the max value. no flipping is needed because its an even number of columns.</p>

<p>The matrix I need has 252 items in a 12 by 21 matrix. This is the code I have so far:  </p>

<pre><code>import numpy as np

a = np.arange(20,-1,-1).reshape(21,1)
b = a+1 + a[::-1]*2
c = b+1 + a*2
d = np.hstack((a,b,c))
e = np.hstack((d,np.flip(d,0)+ d.shape[0] * d.shape[1]))
f = np.hstack((e,e + e.shape[0] * e.shape[1]))
del a,b,c,d,e

&gt;&gt;&gt; f
array([[ 20,  21,  62,  63, 104, 105, 146, 147, 188, 189, 230, 231],
       [ 19,  22,  61,  64, 103, 106, 145, 148, 187, 190, 229, 232],
       [ 18,  23,  60,  65, 102, 107, 144, 149, 186, 191, 228, 233],
       [ 17,  24,  59,  66, 101, 108, 143, 150, 185, 192, 227, 234],
       [ 16,  25,  58,  67, 100, 109, 142, 151, 184, 193, 226, 235],
       [ 15,  26,  57,  68,  99, 110, 141, 152, 183, 194, 225, 236],
       [ 14,  27,  56,  69,  98, 111, 140, 153, 182, 195, 224, 237],
       [ 13,  28,  55,  70,  97, 112, 139, 154, 181, 196, 223, 238],
       [ 12,  29,  54,  71,  96, 113, 138, 155, 180, 197, 222, 239],
       [ 11,  30,  53,  72,  95, 114, 137, 156, 179, 198, 221, 240],
       [ 10,  31,  52,  73,  94, 115, 136, 157, 178, 199, 220, 241],
       [  9,  32,  51,  74,  93, 116, 135, 158, 177, 200, 219, 242],
       [  8,  33,  50,  75,  92, 117, 134, 159, 176, 201, 218, 243],
       [  7,  34,  49,  76,  91, 118, 133, 160, 175, 202, 217, 244],
       [  6,  35,  48,  77,  90, 119, 132, 161, 174, 203, 216, 245],
       [  5,  36,  47,  78,  89, 120, 131, 162, 173, 204, 215, 246],
       [  4,  37,  46,  79,  88, 121, 130, 163, 172, 205, 214, 247],
       [  3,  38,  45,  80,  87, 122, 129, 164, 171, 206, 213, 248],
       [  2,  39,  44,  81,  86, 123, 128, 165, 170, 207, 212, 249],
       [  1,  40,  43,  82,  85, 124, 127, 166, 169, 208, 211, 250],
       [  0,  41,  42,  83,  84, 125, 126, 167, 168, 209, 210, 251]])
</code></pre>

<p>Is there a super simple way of doing this that I'm not seeing or don't know?</p>
","['python', 'python-3.x', 'numpy']",50552648,"<p>You can <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html"" rel=""nofollow noreferrer"">transpose</a> the array and <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.flipud.html"" rel=""nofollow noreferrer"">flip vertically</a> every second column:</p>

<pre><code>import numpy as np
row = 4
col = 5
a = np.transpose(np.arange(row * col).reshape(col, row))
a[:, ::2] = np.flipud(a[:, ::2])

print(a)
#Output
[[ 3  4 11 12 19]
 [ 2  5 10 13 18]
 [ 1  6  9 14 17]
 [ 0  7  8 15 16]]
</code></pre>

<p>There is of course also a <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.fliplr.html#numpy.fliplr"" rel=""nofollow noreferrer"">horizontal version of this flip function</a>. </p>
",Is easier way produce numpy matrix numbers worm vertically I sorry I know correct terminology hence worm vertically basically I want numbers start bottom left goes column adds next column lowest number top adds goes eg far I came bloated code basically makes column copies flips adds max value last column plus hstack together eg first column flip add max value last plus case hstack I repeat many columns I want speed things I wanted I take final hstack copy columns add max value flipping needed even number columns The matrix I need items matrix This code I far import numpy np np arange reshape b c b np hstack b c e np hstack np flip shape shape f np hstack e e e shape e shape del b c e gt gt gt f array Is super simple way I seeing know,"startoftags, python, python3x, numpy, endoftags",python pandas dataframe endoftags,python python3x numpy,python pandas dataframe,0.33
50587191,2018-05-29,2018,4,Refining the dataframe according to words list,"<p>I have a word list of type <code>list</code> containing large number of English words.</p>

<p>I also have a dataframe which looks like:-</p>

<pre><code>    FileName        PageNo     LineNo   GOODS_DESC  
1   17743633 - 1    TM000002    69      Abuj Cen Le 
31  17743633 - 1    TM000007    126     Mr USD  
33  17743633 - 1    TM000008    22      TABLEAU EMBALLAGE
34  17743633 - 1    TM000008    24      LISA e EMBALV
46  17743633 - 1    TM000008    143     Cen 
47  17743633 - 1    TM000008    146     A Gl
50  17743633 - 1    TM000009    121     Ppvv Tn Ppvv In 
51  17743633 - 1    TM000009    129     SPECIFY
52  17743633 - 1    TM000009    136     Decrp G 
58  17743633 - 1    TM000009    97      Je ugn  
60  17743633 - 1    TM000009    108     De Veel 
61  17743633 - 1    TM000014    44      TYRE CHIPS SHREDDED TYRES   
63  17743633 - 1    TM000014    48      TYRE CHIPS SHREDDED TYRES
</code></pre>

<p>I want to keep only those words in the 'GOODS_DESC' column that are present in the words list.</p>

<p>My desired output is:-</p>

<pre><code>    FileName        PageNo     LineNo   GOODS_DESC  
1   17743633 - 1    TM000002    69      NaN
31  17743633 - 1    TM000007    126     Mr USD  
33  17743633 - 1    TM000008    22      TABLEAU
34  17743633 - 1    TM000008    24      LISA  
46  17743633 - 1    TM000008    143     NaN 
47  17743633 - 1    TM000008    146     NaN
50  17743633 - 1    TM000009    121     NaN 
51  17743633 - 1    TM000009    129     SPECIFY
52  17743633 - 1    TM000009    136     NaN
58  17743633 - 1    TM000009    97      NaN 
60  17743633 - 1    TM000009    108     NaN
61  17743633 - 1    TM000014    44      TYRE CHIPS SHREDDED TYRES   
63  17743633 - 1    TM000014    48      TYRE CHIPS SHREDDED TYRES
</code></pre>

<p>My approach is also giving output but I'm using lists and it is slow. I want to make it fast.</p>

<pre><code>for rows in df.itertuples():
    a = []
    flat_list = []
    a.append(rows.GOODS_DESC)
    flat_list = [item.strip() for sublist in a for item in sublist.split(' ') if item.strip()]
    flat_list = list(sorted(set(flat_list), key=flat_list.index))
    flat_list = [i for i in flat_list if i.lower() in word_list]
    if(not flat_list):
        df.drop(rows.Index,inplace=True)
        continue
    s=' '.join(flat_list)
    df.loc[rows.Index,'GOODS_DESC']=s

df['GOODS_DESC'] = df['GOODS_DESC'].str.upper()
</code></pre>
","['python', 'pandas', 'dataframe']",50587319,"<p>Your logic seems overly complicated. You can use a single list comprehension with <code>pd.Series.apply</code>. I recommend, as below, you use <code>set</code> for O(1) lookup and <code>str.casefold</code> to match strings irrespective of case.</p>

<pre><code>s = pd.Series(['Abuj Cen Le', 'Mr USD', 'TABLEAU EMBALLAGE', 'LISA e EMBALV'])

word_set = {i.casefold() for i in ['Mr', 'USD', 'TABLEAU', 'LISA']}

def apply_filter(x):
    out = ' '.join([i for i in x.split() if i.casefold() in word_set])
    return out if out else np.nan

res = s.apply(apply_filter)

print(res)

0        NaN
1     Mr USD
2    TABLEAU
3       LISA
dtype: object
</code></pre>
",Refining dataframe according words list I word list type list containing large number English words I also dataframe looks like FileName PageNo LineNo GOODS DESC TM Abuj Cen Le TM Mr USD TM TABLEAU EMBALLAGE TM LISA e EMBALV TM Cen TM A Gl TM Ppvv Tn Ppvv In TM SPECIFY TM Decrp G TM Je ugn TM De Veel TM TYRE CHIPS SHREDDED TYRES TM TYRE CHIPS SHREDDED TYRES I want keep words GOODS DESC column present words list My desired output FileName PageNo LineNo GOODS DESC TM NaN TM Mr USD TM TABLEAU TM LISA TM NaN TM NaN TM NaN TM SPECIFY TM NaN TM NaN TM NaN TM TYRE CHIPS SHREDDED TYRES TM TYRE CHIPS SHREDDED TYRES My approach also giving output I using lists slow I want make fast rows df itertuples flat list append rows GOODS DESC flat list item strip sublist item sublist,"startoftags, python, pandas, dataframe, endoftags",python python3x list endoftags,python pandas dataframe,python python3x list,0.33
51049326,2018-06-26,2018,2,pandas Dataframe Replace NaN values with with previous value based on a key column,"<p>I have a pd.dataframe that looks like this:</p>

<pre><code>key_value    a    b    c    d    e
value_01     1    10   x   NaN  NaN
value_01    NaN   12  NaN  NaN  NaN
value_01    NaN   7   NaN  NaN  NaN
value_02     7    4    y   NaN  NaN 
value_02    NaN   5   NaN  NaN  NaN
value_02    NaN   6   NaN  NaN  NaN
value_03     19   15   z   NaN  NaN
</code></pre>

<p>So now based on the key_value,</p>

<p>For column 'a' &amp; 'c', I want to copy over the last cell's value from the same column 'a' &amp; 'c' based off of the key_value.</p>

<p>For another column 'd', I want to copy over the row 'i - 1' cell value from column 'b' to column 'd' i'th cell.</p>

<p>Lastly, for column 'e' I want to copy over the sum of 'i - 1' cell's from column 'b' to column 'e' i'th cell . </p>

<p><strong>For every key_value</strong> the columns 'a', 'b' &amp; 'c' have some value in their first  row, based on which the next values are being copied over or for different columns the values are being generated for.</p>

<pre><code>key_value    a    b    c    d    e
value_01     1    10   x   NaN  NaN
value_01     1    12   x    10   10
value_01     1    7    x    12   22
value_02     7    4    y   NaN  NaN
value_02     7    5    y    4    4
value_02     7    6    y    5    9
value_03     8    15   z   NaN  NaN
</code></pre>

<p>My current approach:</p>

<pre><code>size = df.key_value.size
for i in range(size):
    if pd.isna(df.a[i]) and df.key_value[i] == output.key_value[i - 1]:
        df.a[i] = df.a[i - 1]
        df.c[i] = df.c[i - 1]
        df.d[i] = df.b[i - 1]
        df.e[i] = df.e[i] + df.b[i - 1]
</code></pre>

<p>For columns like 'a' and 'b' the <strong><em>NaN</em></strong> values are all in the same row indexes.</p>

<p>My approach works but takes very long since my datframe has over 50000 records, I was wondering if there is a different way to do this, since I have multiple columns like 'a' &amp; 'b' where values need to be copied over based on 'key_value' and some columns where the values are being computed using say a column like 'b'</p>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",51049753,"<h2><code>pd.concat</code> with <code>groupby</code> and <code>assign</code></h2>

<pre><code>pd.concat([
    g.ffill().assign(d=lambda d: d.b.shift(), e=lambda d: d.d.cumsum())
    for _, g in df.groupby('key_value')
])

  key_value     a  b  c    d    e
0  value_01   1.0  1  x  NaN  NaN
1  value_01   1.0  2  x  1.0  1.0
2  value_01   1.0  3  x  2.0  3.0
3  value_02   7.0  4  y  NaN  NaN
4  value_02   7.0  5  y  4.0  4.0
5  value_02   7.0  6  y  5.0  9.0
6  value_03  19.0  7  z  NaN  NaN
</code></pre>

<hr>

<h2><code>groupby</code> and <code>apply</code></h2>

<pre><code>def h(g):
    return g.ffill().assign(
        d=lambda d: d.b.shift(), e=lambda d: d.d.cumsum())

df.groupby('key_value', as_index=False, group_keys=False).apply(h)
</code></pre>
",pandas Dataframe Replace NaN values previous value based key column I pd dataframe looks like key value b c e value x NaN NaN value NaN NaN NaN NaN value NaN NaN NaN NaN value NaN NaN value NaN NaN NaN NaN value NaN NaN NaN NaN value z NaN NaN So based key value For column amp c I want copy last cell value column amp c based key value For another column I want copy row cell value column b column th cell Lastly column e I want copy sum cell column b column e th cell For every key value columns b amp c value first row based next values copied different columns values generated key value b c e value x NaN NaN value x value x value NaN NaN value value value z NaN NaN My current approach size df key value size range size,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas dataframe pandasgroupby,python pandas dataframe,0.87
51052867,2018-06-27,2018,2,Find whether two columns match based on existence of words in the same row,"<p>I have a dataframe with two columns of interest. I want to try to compare the two columns word-wise and find out whether any words overlap. If so, I want to append a column that indicates that matching words were found within the row.</p>

<p>Here's an example of my dataframe:</p>

<pre><code>df
    name1      name2
0   cat nip    giant cat
1   bad dog    blue sky
2   slow snail slimy snail
3   tall tree  big boy
</code></pre>

<p>Here's what I want:</p>

<pre><code>df
    name1      name2       found
0   cat nip    giant cat   True
1   bad dog    blue sky    False
2   slow snail slimy snail True
3   tall tree  big boy     False
</code></pre>

<p>I've tried many methods. One method was using this code:</p>

<pre><code>df['found'] = df['name1'].apply(lambda x: any(i in df['name2'] for i in x))
</code></pre>

<p>Which didn't work. Second method was using this code:</p>

<pre><code>glossary = list(set(df['name1']))
pattern = '|'.join(glossary)
check = df[(df.name1.str.contains(pattern))&amp; 
        (df.name2.str.contains(pattern))]
</code></pre>

<p>This code didn't work either, and it was creating a new dataframe (which I don't want). Plus both methods were really slow. Any ideas how to do it correctly?</p>

<p>Also, I've already tried the <code>isin</code> method:</p>

<pre><code>df['found'] = df['name1'].isin(df['name2'])
</code></pre>

<p>which doesn't work either. It gives me many false <code>True</code> labels.</p>
","['python', 'pandas', 'dataframe']",51053044,"<p>You can use a simple solution using <code>&amp;</code> between series. </p>

<p>First <a href=""http://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.Series.transform.html"" rel=""nofollow noreferrer""><code>transform</code></a> your lists to <code>set</code></p>

<pre><code>transf_1 = df.name1.str.split("" "").transform(set)
transf_2 = df.name2.str.split("" "").transform(set)
</code></pre>

<p>Then just </p>

<pre><code>&gt;&gt;&gt; transf_1 &amp; transf_2.values

0     True
1    False
2     True
3    False
</code></pre>
",Find whether two columns match based existence words row I dataframe two columns interest I want try compare two columns word wise find whether words overlap If I want append column indicates matching words found within row Here example dataframe df name name cat nip giant cat bad dog blue sky slow snail slimy snail tall tree big boy Here I want df name name found cat nip giant cat True bad dog blue sky False slow snail slimy snail True tall tree big boy False I tried many methods One method using code df found df name apply lambda x df name x Which work Second method using code glossary list set df name pattern join glossary check df df name str contains pattern amp df name str contains pattern This code work either creating new dataframe I want Plus methods really slow Any ideas correctly Also I already,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
51089334,2018-06-28,2018,10,What is the difference between tf.keras.layers versus tf.layers?,"<p>What is the difference between tf.keras.layers versus tf.layers?<br>
E.g. both of them have Conv2d, do they provide different outputs?<br>
 Is there any benefits if you mix them (something like a tf.keras.layers.Conv2d in one hidden layer and in the next, tf.layers.max_pooling2d)?</p>
","['python', 'tensorflow', 'keras']",54718798,"<p>Since TensorFlow 1.12, <code>tf.layers</code> are merely wrappers around <code>tf.keras.layers</code>.</p>

<p>A few examples:</p>

<p>Convolutional <code>tf.layers</code> just inherit from the convolutional <code>tf.keras.layers</code>, see source code <a href=""https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/layers/convolutional.py#L217"" rel=""noreferrer"">here</a>:</p>

<pre><code>@tf_export('layers.Conv2D')
class Conv2D(keras_layers.Conv2D, base.Layer):
</code></pre>

<p>The same is true for all <a href=""https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/layers/core.py"" rel=""noreferrer"">core <code>tf.layers</code></a>, e.g.:</p>

<pre><code>@tf_export('layers.Dense')
class Dense(keras_layers.Dense, base.Layer):
</code></pre>

<p>With the integration of Keras into TensorFlow, it would make little sense to maintain several different layer implementations. <code>tf.keras</code> is becoming the de-facto high-level API for TensorFlow, therefore <code>tf.layers</code> are now just wrappers around <code>tf.keras.layers</code>.</p>
",What difference tf keras layers versus tf layers What difference tf keras layers versus tf layers E g Conv provide different outputs Is benefits mix something like tf keras layers Conv one hidden layer next tf layers max pooling,"startoftags, python, tensorflow, keras, endoftags",python django djangorestframework endoftags,python tensorflow keras,python django djangorestframework,0.33
51110171,2018-06-29,2018,6,How to assign a unique ID to detect repeated rows in a pandas dataframe?,"<p>I am working with a large pandas dataframe, with several columns pretty much like this:</p>

<pre><code>A      B         C    D   

John   Tom       0    1
Homer  Bart      2    3
Tom    Maggie    1    4 
Lisa   John      5    0
Homer  Bart      2    3
Lisa   John      5    0
Homer  Bart      2    3
Homer  Bart      2    3
Tom    Maggie    1    4
</code></pre>

<p>How can I assign an unique id to each repeated row? For example:</p>

<pre><code>A      B         C    D      new_id

John   Tom       0    1.2      1
Homer  Bart      2    3.0      2
Tom    Maggie    1    4.2      3
Lisa   John      5    0        4
Homer  Bart      2    3        5
Lisa   John      5    0        4
Homer  Bart      2    3.0      2
Homer  Bart      2    3.0      2
Tom    Maggie    1    4.1      6
</code></pre>

<p>I know that I can use <code>duplicate</code> to detect the duplicated rows, however I can not visualize were are reapeting those rows. I tried to:</p>

<pre><code>df.assign(id=(df.columns).astype('category').cat.codes)
df
</code></pre>

<p>However, is not working. How can I get a unique id for detecting groups of duplicated rows?</p>
","['python', 'python-3.x', 'pandas']",51110205,"<p>For small dataframes, you can convert your rows to tuples, which can be hashed, and then use <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.factorize.html"" rel=""nofollow noreferrer""><code>pd.factorize</code></a>.</p>

<pre><code>df['new_id'] = pd.factorize(df.apply(tuple, axis=1))[0] + 1
</code></pre>

<p><code>groupby</code> is more efficient for larger dataframes:</p>

<pre><code>df['new_id'] = df.groupby(df.columns.tolist(), sort=False).ngroup() + 1
</code></pre>
",How assign unique ID detect repeated rows pandas dataframe I working large pandas dataframe several columns pretty much like A B C D John Tom Homer Bart Tom Maggie Lisa John Homer Bart Lisa John Homer Bart Homer Bart Tom Maggie How I assign unique id repeated row For example A B C D new id John Tom Homer Bart Tom Maggie Lisa John Homer Bart Lisa John Homer Bart Homer Bart Tom Maggie I know I use duplicate detect duplicated rows however I visualize reapeting rows I tried df assign id df columns astype category cat codes df However working How I get unique id detecting groups duplicated rows,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
51151510,2018-07-03,2018,2,Dictionary list from a for loop with if else hasattr,"<p>following code to start: </p>

<pre><code>products = [{
  ""id"": x.id,
  ""fabric"": x.fabric.name,
  ""fabricimg"": x.fabric.fabric_cover.url,
  } for x in entry_obj.all()]

cart_data = {
  ""products"": products,
  ""total"": cart_obj.total
}

return JsonResponse(cart_data)
</code></pre>

<p>This works fine for creating my list products with the dictionaries from the <code>x(objects)</code> in <code>entry_obj.all()</code>.
But now I have the scenario that I have some <code>x(objects)</code> with no <code>x.fabric.name</code>, and instead will have to use a filler for example a simple string like <code>""noname""</code>. </p>

<p>How can I use an if statement in the existing for loop to catch the case of name not existing and instead setting the key fabric to my string value?</p>

<p>I thought of using: </p>

<pre><code>if hasattr(entry_obj,""name"") &gt; ""fabric"": x.fabric.name
else &gt; ""fabric"": ""noname""
</code></pre>

<p>But I'm unsure where to put it in the for loop plus how to iterate through the <code>x(objects)</code> in <code>entry_obj</code> for that matter so I can still give <code>Json</code> the right <code>cart_data</code>.</p>
","['python', 'list', 'dictionary']",51151599,"<p>Use <a href=""https://docs.python.org/3/library/functions.html#getattr"" rel=""nofollow noreferrer""><code>getattr</code></a> with the 3rd <em>default</em> argument:</p>

<pre><code>products = [{'id': x.id, 'fabric': getattr(x.fabric, 'name', 'noname'),
             'fabricimg': x.fabric.fabric_cover.url} for x in entry_obj.all()]
</code></pre>
",Dictionary list loop else hasattr following code start products id x id fabric x fabric name fabricimg x fabric fabric cover url x entry obj cart data products products total cart obj total return JsonResponse cart data This works fine creating list products dictionaries x objects entry obj But I scenario I x objects x fabric name instead use filler example simple string like noname How I use statement existing loop catch case name existing instead setting key fabric string value I thought using hasattr entry obj name gt fabric x fabric name else gt fabric noname But I unsure put loop plus iterate x objects entry obj matter I still give Json right cart data,"startoftags, python, list, dictionary, endoftags",python python3x list endoftags,python list dictionary,python python3x list,0.67
51167542,2018-07-04,2018,2,Tensorflow creating new variables despite reuse set to true,"<p>I am trying to build a basic RNN, but I get errors trying to use the network after training.
I hold network architecture in a function <code>inference</code></p>

<pre><code>def inference(inp):
    with tf.name_scope(""inference""):
        layer = SimpleRNN(1, activation='sigmoid',   return_sequences=False)(inp)
        layer = Dense(1)(layer)

    return layer 
</code></pre>

<p>but everytime i call it, another set of variables gets created despite using the same scope in training:</p>

<pre><code>def train(sess, seq_len=2, epochs=100):
    x_input, y_input = generate_data(seq_len)

    with tf.name_scope('train_input'):
        x = tf.placeholder(tf.float32, (None, seq_len, 1))
        y = tf.placeholder(tf.float32, (None, 1))

    with tf.variable_scope('RNN'):
        output = inference(x)

    with tf.name_scope('training'):
        loss = tf.losses.mean_squared_error(labels=y, predictions=output)
        train_op = tf.train.RMSPropOptimizer(learning_rate=0.1).minimize(loss=loss, global_step=tf.train.get_global_step())

    with sess.as_default():
        sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])

        for i in tqdm.trange(epochs):
            ls, res, _ = sess.run([loss, output, train_op], feed_dict={x:x_input, y:y_input})
            if i%100==0:
                print(f'{ls}: {res[10]} - {y_input[10]}')
            x_input, y_input = generate_data(seq_len)
</code></pre>

<p>and prediction:</p>

<pre><code>def predict_signal(sess, x, seq_len):   
    # Preparing signal (omitted)
    # Predict
    inp = tf.convert_to_tensor(prepared_signal, tf.float32)
    with sess.as_default():
        with tf.variable_scope('RNN', reuse=True) as scope:
            output = inference(inp)
            result = output.eval()

    return result
</code></pre>

<p>I have spent couple of hours reading about variables scopes by now, but on running prediction I still get an error <code>Attempting to use uninitialized value RNN_1/inference/simple_rnn_2/kernel</code>, with the number by RNN_1 increasing with each call</p>
","['python', 'tensorflow', 'keras']",51167774,"<p>This is just speculation until you show us the <code>SimpleRNN</code> implementation. However, I suspect that <code>SimpleRNN</code> is very badly implemented. There is a different getween <code>tf.get_variable</code> and <code>tf.Variable</code>. I expect your <code>SimpleRNN</code> to use <code>tf.Variable</code>.</p>

<p>To reproduce this behaviour:</p>

<pre><code>import tensorflow as tf


def inference(x):
    w = tf.Variable(1., name='w')
    layer = x + w
    return layer


x = tf.placeholder(tf.float32)

with tf.variable_scope('RNN'):
    output = inference(x)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(output, {x: 10}))

    with sess.as_default():
        with tf.variable_scope('RNN', reuse=True):
            output2 = inference(x)

    print(sess.run(output2, {x: 10}))
</code></pre>

<p>This gives exactly the same error:</p>

<blockquote>
  <p>Attempting to use uninitialized value RNN_1/w</p>
</blockquote>

<p>However the version with <code>w = tf.get_variable('w', initializer=1.)</code> instead of <code>w = tf.Variable(1., name='w')</code> makes it work.</p>

<p>Why? See the docs:</p>

<p><a href=""http://devdocs.io/tensorflow~python/tf/get_variable"" rel=""nofollow noreferrer""><code>tf.get_variable:</code></a></p>

<blockquote>
  <p>Gets an existing variable with these parameters or create a new one.
  This function prefixes the name with the current variable scope and performs <strong>reuse checks</strong>. </p>
</blockquote>

<p><strong>edit</strong>
Thank you for the question (I added the keras flag to your question). This is now becoming my favorite reason to show people why using Keras is the worst decision they ever made.</p>

<p>SimpleRNN <a href=""https://github.com/keras-team/keras/blob/468f080c98f06780c950e5a78c9eeeaf9fff002e/keras/layers/recurrent.py#L826-L830"" rel=""nofollow noreferrer"">creates it variables here:</a></p>

<pre><code>self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                      name='kernel',...)
</code></pre>

<p>This <a href=""https://github.com/keras-team/keras/blob/468f080c98f06780c950e5a78c9eeeaf9fff002e/keras/engine/base_layer.py#L249-L252"" rel=""nofollow noreferrer"">executes the line</a> </p>

<pre><code>weight = K.variable(initializer(shape),
                    dtype=dtype,
                    name=name,
                    constraint=constraint)
</code></pre>

<p>which <a href=""https://github.com/keras-team/keras/blob/468f080c98f06780c950e5a78c9eeeaf9fff002e/keras/backend/tensorflow_backend.py#L399"" rel=""nofollow noreferrer"">ends up here</a></p>

<pre><code>v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)
</code></pre>

<p>And this is an obvious flaw in the implementation.
Until Keras uses TensorFlow in the correct way (respecting at least <code>scopes</code> and <code>variable-collections</code>), you should look for alternatives. The best advice somebody can give you is to switch to something better like the official <code>tf.layers</code>.</p>
",Tensorflow creating new variables despite reuse set true I trying build basic RNN I get errors trying use network training I hold network architecture function inference def inference inp tf name scope inference layer SimpleRNN activation sigmoid return sequences False inp layer Dense layer return layer everytime call another set variables gets created despite using scope training def train sess seq len epochs x input input generate data seq len tf name scope train input x tf placeholder tf float None seq len tf placeholder tf float None tf variable scope RNN output inference x tf name scope training loss tf losses mean squared error labels predictions output train op tf train RMSPropOptimizer learning rate minimize loss loss global step tf train get global step sess default sess run tf global variables initializer tf local variables initializer tqdm trange epochs ls res sess run loss output train op feed dict,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
51183238,2018-07-05,2018,2,Get text inside one-pair-brackets but not double square brackets,"<ol>
<li>In the text below I only want to get ""4th of July""</li>
</ol>

<blockquote>
  <p><code>Hello, happy [4th of July]. I love the [[firework]]</code></p>
</blockquote>

<ol start=""2"">
<li><p>I have these text:</p>

<blockquote>
  <p>text = {{Hey, how are you.}} I've watched John Mulaney all night.
  [[Category: Comedy]] [[Image: John Mulaney]]</p>
</blockquote></li>
</ol>

<p>I'm trying to remove {{Hey, how are you.}}, [[Category: Comedy]], and [[Image: John Mulaney]]. This is what I have tried so far, but it doesn't seem to work:</p>

<pre><code>hey_how_are_you = re.compile('\{\{.*\}\}')
category = re.compile('\[\[Category:.*?\]\]')
image = re.compile('\[\[Image:.*?\]\]')
text = hey_how_are_you.sub('', text)
text = category.sub('', text)
text = image.sub('', text)
</code></pre>
","['python', 'regex', 'python-3.x']",51185070,"<pre><code># 1.
text=""Hello, happy [4th of July]. I love the [[firework]]. ""
l=re.findall(r""(?&lt;!\[)\[([^\[\]]+)\]"",text)
print(l,""\n"",l[0])
# 2.
text2="" {{Hey, how are you.}} I've watched John Mulaney all night. [[Category: Comedy]] [[Image: John Mulaney]]""
print(re.sub(r""\{\{.*?\}\}|\[\[\s*Category:.*?\]\]|\[\[\s*Image:.*?\]\]"","""",text2))

Output:
['4th of July'] 
 4th of July
  I've watched John Mulaney all night.  

In the 1st problem you can use negative lookbehind: (?&lt;!\[)
Your regexp in the 2nd problem works for me. (What error you have?) However, it can be solved in one pass, too.
</code></pre>
",Get text inside one pair brackets double square brackets In text I want get th July Hello happy th July I love firework I text text Hey I watched John Mulaney night Category Comedy Image John Mulaney I trying remove Hey Category Comedy Image John Mulaney This I tried far seem work hey compile category compile Category image compile Image text hey sub text text category sub text text image sub text,"startoftags, python, regex, python3x, endoftags",python python3x list endoftags,python regex python3x,python python3x list,0.67
51234778,2018-07-08,2018,13,What are the differences between Bot and Client?,"<p>I've been going through some examples on how to make a Discord Python Bot and I've been seeing <code>client</code> and <code>bot</code> being used almost interchangeably and I'm not able to find when you would use which one when.</p>

<p>For example:</p>

<pre><code>client = discord.Client()
@client.event
async def on_message(message):
    # we do not want the bot to reply to itself
    if message.author == client.user:
        return

    if message.content.startswith('$guess'):
        await client.send_message(message.channel, 'Guess a number between 1 to 10')

    def guess_check(m):
        return m.content.isdigit()

@client.event
async def on_ready():
    print('Logged in as')
    print(client.user.name)
    print(client.user.id)
    print('------')

client.run('token')
</code></pre>

<p>vs.</p>

<pre><code>bot = commands.Bot(command_prefix='?', description=description)
@bot.event
async def on_ready():
    print('Logged in as')
    print(bot.user.name)
    print(bot.user.id)
    print('------')

@bot.command()
async def add(left : int, right : int):
    """"""Adds two numbers together.""""""
    await bot.say(left + right)

bot.run('token')
</code></pre>

<p>I'm beginning to think they have very similar qualities and can do the same things but is a personal preference to go with a client vs. a bot. However they do have their differences where clients have an <code>on_message</code> while bots wait for a <code>prefix command</code>.</p>

<p>Can someone please clarify the difference between <code>client</code> and <code>bot</code>?</p>
","['python', 'discord', 'discord.py']",51235308,"<h1>Tl;dr</h1>
<p>Just use <code>commands.Bot</code>.</p>
<hr />
<p><code>Bot</code> is an extended version of <code>Client</code> (it's in a <em>subclass</em> relationship). Ie. it's an extension of Client with commands enabled, thus the name of the subdirectory <code>ext/commands</code>.</p>
<p>The <code>Bot</code> class inherits all the functionalities of <code>Client</code>, which means that everything you can do with <code>Client</code>, <code>Bot</code> can do it too. The most noticeable addition was becoming command-driven (<code>@bot.command()</code>), whereas you would have to manually work with handling events when using <code>Client</code>. A downside of <code>Bot</code> is that you will have to learn the additional functionalities from looking at examples or source codes since the commands extension isn't much documented. UPD: Now it is documented <a href=""https://discordpy.readthedocs.io/en/stable/ext/commands/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>If you simply want your bots to accept commands and handle them, it would be a lot easier to work with the <code>Bot</code> since all the handling and preprocessing are done for you. But if you're eager to write your own handles and do crazy stunts with discord.py, then by all means, use the base <code>Client</code>.</p>
<hr />
<p>In case you're stumped by which to choose, I recommend you to use <code>commands.Bot</code> since it's a lot easier to work with and it's in addition of everything <code>Client</code> can already do. And please remember that you <strong>do not need both</strong>.</p>
<p><strong>WRONG:</strong></p>
<pre><code>client = discord.Client()
bot = commands.Bot(&quot;.&quot;)

# do stuff with bot
</code></pre>
<p><strong>CORRECT:</strong></p>
<pre><code>bot = commands.Bot(&quot;.&quot;)

# do stuff with bot
</code></pre>
",What differences Bot Client I going examples make Discord Python Bot I seeing client bot used almost interchangeably I able find would use one For example client discord Client client event async def message message want bot reply message author client user return message content startswith guess await client send message message channel Guess number def guess check return content isdigit client event async def ready print Logged print client user name print client user id print client run token vs bot commands Bot command prefix description description bot event async def ready print Logged print bot user name print bot user id print bot command async def add left int right int Adds two numbers together await bot say left right bot run token I beginning think similar qualities things personal preference go client vs bot However differences clients message bots wait prefix command Can someone please clarify difference,"startoftags, python, discord, discordpy, endoftags",python discord discordpy endoftags,python discord discordpy,python discord discordpy,1.0
51276845,2018-07-11,2018,5,How can I override Django Rest Framework viewset perform_create() method to set a default value to a field,"<p>I have a model called <code>Product</code> and two viewsets that manage the same model in different ways. Each one must set a different value for field <code>product_type</code>, which is set read_only in the serializer <code>SuplementSerializer</code>.</p>

<p>I trying to override <code>perform_create()</code> method and set the value for field <code>product_type</code>, but it always takes the default value.</p>

<p>This is my model:</p>

<pre><code>class Product(models.Model):
    ROOM = 'ROOM'
    SUPLEMENT = 'SUPLEMENT'
    PRODUCT_TYPE_CHOICES = (
        (ROOM, _('Room')),
        (SUPLEMENT, _('Suplement'))
    )

    hotel = models.ForeignKey(Hotel, on_delete=models.PROTECT, related_name='products', verbose_name=_('Hotel'))
    name = models.CharField(max_length=100, verbose_name=_('Name'))
    product_type = models.CharField(max_length=9, choices=PRODUCT_TYPE_CHOICES, default=ROOM, verbose_name=_('Product Type'))
    room_type = models.ForeignKey(RoomType, null=True, blank=True, on_delete=models.PROTECT, related_name='products', verbose_name=_('Room Type'))
    plan_type = models.ForeignKey(PlanType, null=True, blank=True, on_delete=models.PROTECT, related_name='products', verbose_name=_('Plan Type'))
    content_type = models.ForeignKey(ContentType, null=True, blank=True, on_delete=models.PROTECT, related_name='rate_base_products', verbose_name=_('Rate Base'))
    object_id = models.PositiveIntegerField(null=True, blank=True)
    rate_base = GenericForeignKey('content_type', 'object_id')

    class Meta:
        verbose_name = _('Product')
        verbose_name_plural = _('Products')

    def __str__(self):
        return ""[{}][{}]{}"".format(self.id, self.hotel, self.name)

    def save(self, *vars, **kwargs):
        self.full_clean()
        return super().save(*vars, **kwargs)

    def clean(self, *vars, **kwargs):
        if self.content_type != None:
            if self.content_type.model != 'ages' and self.content_type.model != 'roombase' and self.content_type.model != 'product':
                raise CustomValidation(_(""rate_base must be an instance of either 'Ages' or a 'RoomBase'""), 'rate_base', status.HTTP_400_BAD_REQUEST)
</code></pre>

<p>These are my viewsets:</p>

<pre><code>class SuplementViewSet(viewsets.ModelViewSet):
    permission_classes = (permissions.IsAuthenticated,)
    queryset = models.Product.objects.filter(product_type=models.Product.SUPLEMENT)
    serializer_class = serializers.SuplementSerializer
    filter_backends = (DjangoFilterBackend, SearchFilter, OrderingFilter,)
    search_fields = ('hotel__name', 'name')

    def perform_create(self, instance):
        instance.product_type = models.Product.SUPLEMENT
        instance.save()

class ProductViewSet(viewsets.ModelViewSet):
    permission_classes = (permissions.IsAuthenticated,)
    queryset = models.Product.objects.all()
    serializer_class = serializers.ProductSerializer
    filter_backends = (DjangoFilterBackend, SearchFilter, OrderingFilter,)
    search_fields = ('hotel__name', 'name')
    filter_fields = ('hotel__name', 'name')
</code></pre>

<p>And this is my serializer:</p>

<pre><code>class SuplementSerializer(serializers.ModelSerializer):
    class Meta:
        model = models.Product
        fields = ('id', 'hotel', 'product_type', 'name')
        read_only_fields = ('product_type',)

    def __init__(self, *args, **kwargs):
        exclude = kwargs.pop('exclude', None)

        super(SuplementSerializer, self).__init__(*args, **kwargs)

        if exclude is not None:
            for field_name in exclude:
                self.fields.pop(field_name)
</code></pre>

<p>I don't know why I can't set the value for field <code>product_type</code>.</p>
","['python', 'django', 'django-rest-framework']",51276972,"<p>Argument to <code>perform_create()</code> method is serializer instance not an object instance. You can set the field as below</p>

<pre><code>def perform_create(self, serz):
    serz.save(product_type=models.Product.SUPLEMENT)
</code></pre>

<p>See explanation here <a href=""http://www.django-rest-framework.org/tutorial/6-viewsets-and-routers/#refactoring-to-use-viewsets"" rel=""noreferrer"">http://www.django-rest-framework.org/tutorial/6-viewsets-and-routers/#refactoring-to-use-viewsets</a></p>
",How I override Django Rest Framework viewset perform create method set default value field I model called Product two viewsets manage model different ways Each one must set different value field product type set read serializer SuplementSerializer I trying override perform create method set value field product type always takes default value This model class Product models Model ROOM ROOM SUPLEMENT SUPLEMENT PRODUCT TYPE CHOICES ROOM Room SUPLEMENT Suplement hotel models ForeignKey Hotel delete models PROTECT related name products verbose name Hotel name models CharField max length verbose name Name product type models CharField max length choices PRODUCT TYPE CHOICES default ROOM verbose name Product Type room type models ForeignKey RoomType null True blank True delete models PROTECT related name products verbose name Room Type plan type models ForeignKey PlanType null True blank True delete models PROTECT related name products verbose name Plan Type content type models ForeignKey ContentType null,"startoftags, python, django, djangorestframework, endoftags",python django djangorestframework endoftags,python django djangorestframework,python django djangorestframework,1.0
51306491,2018-07-12,2018,3,applying a method to a few selected columns in a pandas dataframe,"<p>I would like to apply a little method to several columns in a Dataframe. The method color_negative can't be applied to columns with strings, therefore I need to skip these columns somehow. I could think of two approaches to solve the problem, though sadly none is working.</p>
<ul>
<li><p><strong>In approach 1:</strong></p>
<p>I try to apply the method to every column one by one skipping the first one by using the index of the Dataframe and by setting the incrementing counter of the while loop to 1. When executing this approach I get the error, that the 'Series' object has no attribute 'style', so apparently, I can't apply a method to a single column.</p>
</li>
<li><p><strong>In approach 2:</strong></p>
<p>I try to use the subset to apply the method only to those columns with numeric values, though I'm not sure if I use subset correctly. When executing this approach I get the error that the object of type 'Styler' has no <code>len()</code>.</p>
</li>
</ul>
<p>Here a simplified example:</p>
<pre><code>import pandas as pd

d = {'col1': ['a', 'b'], 'col2': [21, 22], 'col3': [3, 51]}
df = pd.DataFrame(data=d)

def color_negative_red(val):
    color = 'black'
    if val &lt; -1 : color = 'red'
    if val &gt; 1 :  color = 'green'
    return 'color: %s' % color    
    
i=1
while i &lt;= len(df):
    #Approach 1
    df.iloc[:, i] = df.iloc[:, i].style.applymap(color_negative_red)
    #Approach 2
    df = df.style.applymap(color_negative_red, subset = df.iloc[:, i])
    i+=1    

df
</code></pre>
<p>Has anyone a suggestion on how to solve this problem?</p>
","['python', 'pandas', 'dataframe']",51306764,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.formats.style.Styler.apply.html"" rel=""nofollow noreferrer""><code>style.Styler.apply</code></a> with <code>DataFrame of styles</code> with <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.select.html"" rel=""nofollow noreferrer""><code>numpy.select</code></a> for filling:</p>

<pre><code>d = {'col1': ['a', 'b'], 'col2': [21, 22], 'col3': [0, -51]}
df = pd.DataFrame(data=d)

def color_negative_red(x):
    #select only numeric columns
    x1 = x.select_dtypes(np.number)
    c1 = 'color: red'
    c2 = 'color: green'
    c3 = '' 
    #boolean masks
    m1 = x1 &lt; -1
    m2 = x1 &gt; 1
    #numpy array by conditions
    arr = np.select([m1, m2], [c1, c2], default=c3)
    df1 =  pd.DataFrame(arr, index=df.index, columns=x1.columns)
    #added strings columns filled by c3 string 
    df1 = df1.reindex(columns=x.columns, fill_value=c3)
    return df1

df.style.apply(color_negative_red, axis=None)
</code></pre>

<p><a href=""https://i.stack.imgur.com/1SFqW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1SFqW.png"" alt=""pic""></a></p>
",applying method selected columns pandas dataframe I would like apply little method several columns Dataframe The method color negative applied columns strings therefore I need skip columns somehow I could think two approaches solve problem though sadly none working In approach I try apply method every column one one skipping first one using index Dataframe setting incrementing counter loop When executing approach I get error Series object attribute style apparently I apply method single column In approach I try use subset apply method columns numeric values though I sure I use subset correctly When executing approach I get error object type Styler len Here simplified example import pandas pd col b col col df pd DataFrame data def color negative red val color black val lt color red val gt color green return color color lt len df Approach df iloc df iloc style applymap color negative red Approach df,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
51312587,2018-07-12,2018,3,Printing employee object information from a dictionary of employees,"<p>I am attempting to print employee object information from a dictionary of employee objects without using a for loop. Here is what I have done so far:</p>

<pre><code>employee_dict = {}

class Employee:

  def __init__(self, id, salary):
    self.id = id
    self.salary = salary
    self.employee_dictionary(self)

  def info(self):
    return ""Employee ID:{} \nSalary:{}"".format(self.id, self.salary)

  def employee_dictionary(self):
    employee_dict = {self.id: self}

emp = Employee(1, 10)
emp1 = Employee(2, 5)

employee = employee_dict[1]

print(employee.info())
</code></pre>

<p>Could someone point me in the right direction? I feel like I am close. The error this code gives me is:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/rob/PycharmProjects/untitled4/sdfghsfdgjrtyrty.py"", line 19, in &lt;module&gt;
    employee = employee_dict[1]
KeyError: 1
</code></pre>
","['python', 'python-3.x', 'dictionary']",51313554,"<p>The underlying problem here is that you're not using the dictionary right. Instead of adding the new employee to the existing global <code>employee_dict</code>, you're trying to <em>replace</em> that global dict with a new one, containing just the new employee:</p>

<pre><code>def employee_dictionary(self):
  employee_dict = {self.id: self}
</code></pre>

<p>If you fix this, the problem you're actually asking about won't even come up:</p>

<pre><code>def employee_dictionary(self):
  employee_dict[self.id] = self
</code></pre>

<p>This will add a new mapping from <code>self.id</code> to <code>self</code> in the dict. So, after your two <code>Employee</code> constructions, with IDs 1 and 2, you'll end up with two entries in the dict, for keys <code>1</code> and <code>2</code>, and everything will just work.</p>

<hr>

<p>Also, you should definitely at least consider two other changes:</p>

<ul>
<li>Replace the global variable with a class attribute, and possibly also replace the <code>employee_dictionary</code> method with a class method, as detailed in <a href=""https://stackoverflow.com/a/51312650/908494"">scharette's answer</a>.</li>
<li>Rename the method from <code>employee_dictionary</code> to something (a) marked private (methods starting with <code>_</code> are private by convention, meaning your users know they aren't supposed to be calling it unless they have a weird use case), and (b) more reflective of what it does. Maybe <code>_register</code> or <code>_add_to_dict</code>?</li>
</ul>

<p>The first one of these would <em>also</em> (as, again, detailed in scharette's answer) have made the problem you're asking about go away. (But you still need the main fix anyway.)</p>

<hr>

<p>But you probably want to understand the more immediate scope problem anywayâand how to solve it when you <em>can't</em> just make it go away.</p>

<p>In Python, any name that you assign to in a function is a local variable. If there's a <code>spam = â¦</code> anywhere in the function (or a <code>with eggs as spam:</code> or certainly other kinds of things that also count as assignment), every reference to <code>spam</code> in that function is to that local. So, <code>employee_dict = {self.id: self}</code> creates a local variable named <code>employee_dict</code>, assigns a value to it, and then returns, at which point all the locals go away. The fact that it happens to have the same name as a global doesn't mean anything to Python (although to a human reader, like you, it's obviously confusing).</p>

<p>Any name that you use <em>without</em> assigning to it anywhere is a local-or-enclosing-or-global-or-builtin variable. When you do <code>employee_dict[self.id]</code>, because there's no <code>employee_dict = â¦</code> anywhere, Python searches the local, enclosing, global, and builtin scopes to find what you meant by <code>employee_dict</code>, and it finds the global.</p>

<p>You can force Python to treat a name as a global variable, even if you assign to it, with a <code>global</code> statement at the top of the function:</p>

<pre><code>def employee_dictionary(self):
  global employee_dict
  employee_dict = {self.id: self}
</code></pre>

<p>Adding <code>global</code> is safe even if the name would already be a global. This is usually pointlessâbut if you're not sure whether something counts as an assignment (or just not sure future readers of your code would be sureâ¦), and you want to make it clear that the variable is a global, you can declare it:</p>

<pre><code>def employee_dictionary(self):
  global employee_dict
  employee_dict[self.id] = self
</code></pre>
",Printing employee object information dictionary employees I attempting print employee object information dictionary employee objects without using loop Here I done far employee dict class Employee def init self id salary self id id self salary salary self employee dictionary self def info self return Employee ID nSalary format self id self salary def employee dictionary self employee dict self id self emp Employee emp Employee employee employee dict print employee info Could someone point right direction I feel like I close The error code gives Traceback recent call last File home rob PycharmProjects untitled sdfghsfdgjrtyrty py line lt module gt employee employee dict KeyError,"startoftags, python, python3x, dictionary, endoftags",python django djangorestframework endoftags,python python3x dictionary,python django djangorestframework,0.33
51411713,2018-07-18,2018,3,pygame.time.wait() crashes the program,"<p>In a pygame code I wannted to do a title that changes colors. 
I tried to do a simple title that changes colors, but it not even turned the color to blue (or do it for a second), and the program crash. The code:</p>

<pre><code>title_font = pygame.font.SysFont(""monospace"", TITLE_SIZE)

while True:
    title = title_font.render(""game"", 5, RED)
    game_display.blit(title, TITLE_POS)
    pygame.display.update()
    pygame.time.wait(2000)

    title = title_font.render(""game"", 5, BLUE)
    game_display.blit(title, TITLE_POS)
    pygame.display.update()
    pygame.time.wait(3000)

    title = title_font.render(""game"", 5, RED)
    game_display.blit(title, TITLE_POS)
    pygame.display.update()
    pygame.time.wait(2000)
</code></pre>

<p>It also happens with pygame.time.delay(), and I don't know where is the problem...</p>
","['python', 'python-3.x', 'pygame']",51412997,"<p>Don't use <code>pygame.time.wait</code> or <code>delay</code> because these functions make your program sleep for the given time and the window becomes unresponsive. You also need to handle the events (with one of the <a href=""https://www.pygame.org/docs/ref/event.html#pygame.event.pump"" rel=""nofollow noreferrer"">pygame.event</a> functions) each frame to avoid this.</p>

<p>Here are some timer examples which don't block: <a href=""https://stackoverflow.com/questions/30720665/countdown-timer-in-pygame"">Countdown timer in Pygame</a></p>

<p>To switch the colors, you can just assign the next color to a variable and use it to render the text.</p>

<pre><code>import pygame


pygame.init()
screen = pygame.display.set_mode((640, 480))
clock = pygame.time.Clock()
title_font = pygame.font.SysFont('monospace', 50)
BACKGROUND_COLOR = pygame.Color('gray12')
BLUE = pygame.Color('blue')
RED = pygame.Color('red')
# Assign the current color to the color variable.
color = RED
timer = 2
dt = 0

done = False
while not done:
    # Handle the events.
    for event in pygame.event.get():
        # This allows the user to quit by pressing the X button.
        if event.type == pygame.QUIT:
            done = True

    timer -= dt  # Decrement the timer by the delta time.
    if timer &lt;= 0:  # When the time is up ...
        # Swap the colors.
        if color == RED:
            color = BLUE
            timer = 3
        else:
            color = RED
            timer = 2

    screen.fill(BACKGROUND_COLOR)
    title = title_font.render('game', 5, color)
    screen.blit(title, (200, 50))

    pygame.display.flip()
    # dt is the passed time since the last clock.tick call.
    dt = clock.tick(60) / 1000  # / 1000 to convert milliseconds to seconds.

pygame.quit()
</code></pre>
",pygame time wait crashes program In pygame code I wannted title changes colors I tried simple title changes colors even turned color blue second program crash The code title font pygame font SysFont monospace TITLE SIZE True title title font render game RED game display blit title TITLE POS pygame display update pygame time wait title title font render game BLUE game display blit title TITLE POS pygame display update pygame time wait title title font render game RED game display blit title TITLE POS pygame display update pygame time wait It also happens pygame time delay I know problem,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
51424453,2018-07-19,2018,17,Adding list with different length as a new column to a dataframe,"<p>I am willing to add or insert the list values in the dataframe. The dataframe len is <code>49</code>, whereas the length of list id <code>47</code>. I am getting the following error while implementing the code.   </p>

<pre><code>print(""Lenght of dataframe: "",datasetTest.open.count())
print(""Lenght of array: "",len(test_pred_list))
datasetTest['predict_close'] = test_pred_list
</code></pre>

<p>The error is:  </p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-105-68114a4e9a82&gt; in &lt;module&gt;()
      5 # datasetTest = datasetTest.dropna()
      6 # print(datasetTest.count())
----&gt; 7 datasetTest['predict_close'] = test_pred_list
      8 # test_shifted['color_predicted'] = test_shifted.apply(determinePredictedcolor, axis=1)
      9 # test_shifted['color_original'] =

c:\python35\lib\site-packages\pandas\core\frame.py in __setitem__(self, key, value)
   2517         else:
   2518             # set column
-&gt; 2519             self._set_item(key, value)
   2520 
   2521     def _setitem_slice(self, key, value):

c:\python35\lib\site-packages\pandas\core\frame.py in _set_item(self, key, value)
   2583 
   2584         self._ensure_valid_index(value)
-&gt; 2585         value = self._sanitize_column(key, value)
   2586         NDFrame._set_item(self, key, value)
   2587 

c:\python35\lib\site-packages\pandas\core\frame.py in _sanitize_column(self, key, value, broadcast)
   2758 
   2759             # turn me into an ndarray
-&gt; 2760             value = _sanitize_index(value, self.index, copy=False)
   2761             if not isinstance(value, (np.ndarray, Index)):
   2762                 if isinstance(value, list) and len(value) &gt; 0:

c:\python35\lib\site-packages\pandas\core\series.py in _sanitize_index(data, index, copy)
   3119 
   3120     if len(data) != len(index):
-&gt; 3121         raise ValueError('Length of values does not match length of ' 'index')
   3122 
   3123     if isinstance(data, PeriodIndex):

ValueError: Length of values does not match length of index
</code></pre>

<p>How I can get rid of this error. Please help me.</p>
","['python', 'python-3.x', 'pandas']",51424527,"<p>If you convert the list to a Series then it will just work:</p>

<pre><code>datasetTest.loc[:,'predict_close'] = pd.Series(test_pred_list)
</code></pre>

<p>example:</p>

<pre><code>In[121]:
df = pd.DataFrame({'a':np.arange(3)})
df

Out[121]: 
   a
0  0
1  1
2  2

In[122]:
df.loc[:,'b'] = pd.Series(['a','b'])
df

Out[122]: 
   a    b
0  0    a
1  1    b
2  2  NaN
</code></pre>

<p>The docs refer to this as <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#setting-with-enlargement"" rel=""noreferrer"">setting with enlargement</a> which talks about adding or expanding but it also works where the length is less than the pre-existing index.</p>

<p>To handle where the index doesn't start at <code>0</code> or in fact is not an int:</p>

<pre><code>In[126]:
df = pd.DataFrame({'a':np.arange(3)}, index=np.arange(3,6))
df

Out[126]: 
   a
3  0
4  1
5  2

In[127]:
s = pd.Series(['a','b'])
s.index = df.index[:len(s)]
s

Out[127]: 
3    a
4    b
dtype: object

In[128]:
df.loc[:,'b'] = s
df

Out[128]: 
   a    b
3  0    a
4  1    b
5  2  NaN
</code></pre>

<p>You can optionally replace the <code>NaN</code> if you wish calling <code>fillna</code></p>
",Adding list different length new column dataframe I willing add insert list values dataframe The dataframe len whereas length list id I getting following error implementing code print Lenght dataframe datasetTest open count print Lenght array len test pred list datasetTest predict close test pred list The error ValueError Traceback recent call last lt ipython input e gt lt module gt datasetTest datasetTest dropna print datasetTest count gt datasetTest predict close test pred list test shifted color predicted test shifted apply axis test shifted color original c python lib site packages pandas core frame py setitem self key value else set column gt self set item key value def setitem slice self key value c python lib site packages pandas core frame py set item self key value self ensure valid index value gt value self sanitize column key value NDFrame set item self key value c python lib site,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
51499385,2018-07-24,2018,3,How to add values to a new column in pandas dataframe?,"<p>I want to create a new named column in a Pandas dataframe, insert first value into it, and then add another values to the same column:</p>

<p>Something like:</p>

<pre><code>import pandas

df = pandas.DataFrame()
df['New column'].append('a')
df['New column'].append('b')
df['New column'].append('c')

etc.
</code></pre>

<p>How do I do that?</p>
","['python', 'pandas', 'dataframe']",51499563,"<p>Dont do it, <a href=""https://stackoverflow.com/a/24871316/2901002"">because it's slow</a>:</p>
<blockquote>
<ol start=""6"">
<li><strong>updating an empty frame a-single-row-at-a-time</strong>. I have seen this method used WAY too much. It is by far the slowest. It is probably common place (and reasonably fast for some python structures), but a DataFrame does a fair number of checks on indexing, so this will always be very slow to update a row at a time. Much better to create new structures and concat.</li>
</ol>
</blockquote>
<p>Better to create a list of data and create <code>DataFrame</code> by contructor:</p>
<pre><code>vals = ['a','b','c']

df = pandas.DataFrame({'New column':vals})
</code></pre>
",How add values new column pandas dataframe I want create new named column Pandas dataframe insert first value add another values column Something like import pandas df pandas DataFrame df New column append df New column append b df New column append c etc How I,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
51513386,2018-07-25,2018,4,How to ensure the __del__ function is called on a Python class as is commonly (but incorrectly) expected?,"<p>I understand that the <code>__del__</code> function of a Python class <a href=""https://stackoverflow.com/questions/6104535/i-dont-understand-this-python-del-behaviour"">is not treated in the way that many people might expect</a>: as a destructor.</p>

<p>I also understand that there are more 'pythonic' and arguably more elegant ways to tidy up, particularly with use of the <a href=""https://stackoverflow.com/a/865272/5196274"">with construct</a>.</p>

<p>However, when writing code that may be used by an audience less versed in pythonic ways, when cleanup is important, is there an elegant way I can simply get <code>__del__</code> to work as a destructor reliably, without interfering with python's natural use of <code>__del__</code>?</p>

<p>The expectation that <code>__del__</code> behave as a destructor doesn't seem <em>unreasonable</em> and at the same time is quite common.  So I'm simply wondering if there is an elegant way to make it work as per that expectation - disregarding the many debates that can be had over the merits of how pythonic it is.</p>
","['python', 'python-3.x', 'python-2.7']",51513663,"<p>If you understand all that, why not do it in the Pythonic way? Compare another class where cleanup is important: <code>tempfile.TemporaryDirectory</code>.</p>

<pre><code>with TemporaryDirectory() as tmp:
    # ...
# tmp is deleted

def foo():
    tmp = TemporaryDirectory()
foo()
# tmp is deleted
</code></pre>

<p>How do they do this? Here's the relevant bit:</p>

<pre><code>import weakref
class Foo():
    def __init__(self, name):
        self.name = name
        self._finalizer = weakref.finalize(self, self._cleanup, self.name)
        print(""%s reporting for duty!"" % name)

    @classmethod
    def _cleanup(cls, name):
        print(""%s feels forgotten! Bye!"" % name)

    def cleanup(self):
        if self._finalizer.detach():
            print(""%s told to go away! Bye!"" % self.name)

def foo():
    print(""Calling Arnold"")
    tmpfoo = Foo(""Arnold"")
    print(""Finishing with Arnold"")

foo()
# =&gt; Calling Arnold
# =&gt; Arnold reporting for duty
# =&gt; Finishing with Arnold
# =&gt; Arnold feels forgotten. Bye!

def bar():
    print(""Calling Rocky"")
    tmpbar = Foo(""Rocky"")
    tmpbar.cleanup()
    print(""Finishing with Rocky"")

bar()
# =&gt; Calling Rocky
# =&gt; Rocky reporting for duty!
# =&gt; Rocky told to go away! Bye!
# =&gt; Finishing with Rocky
</code></pre>

<p><code>weakref.finalize</code> will trigger <code>_cleanup</code> when the object is garbage-collected, or at the end of the program if it's still around. We can keep the finaliser around so that we can explicitly kill the object (using <code>detach</code>) and mark it as dead so the finaliser is not called (when we want to manually handle the cleanup).</p>

<p>If you want to support the context usage with <code>with</code>, it is trivial to add <code>__enter__</code> and <code>__exit__</code> methods, just invoke <code>cleanup</code> in <code>__exit__</code> (""manual cleanup"" as discussed above).</p>
",How ensure del function called Python class commonly incorrectly expected I understand del function Python class treated way many people might expect destructor I also understand pythonic arguably elegant ways tidy particularly use construct However writing code may used audience less versed pythonic ways cleanup important elegant way I simply get del work destructor reliably without interfering python natural use del The expectation del behave destructor seem unreasonable time quite common So I simply wondering elegant way make work per expectation disregarding many debates merits pythonic,"startoftags, python, python3x, python27, endoftags",python arrays numpy endoftags,python python3x python27,python arrays numpy,0.33
51778413,2018-08-10,2018,6,"What is the difference, if any, between using single quote and double quote in a python dictionary?","<p>I declare a python dictionary like this using double quotes;</p>

<pre><code>{
    ""Name"":""John"",
    ""Date"":""2/18/1998"",
    ""Profit"":25.12
}
</code></pre>

<p>The same python dictionary can be written like this using single quotes;</p>

<pre><code>{
    'Name':'John',
    'Date':'2/18/1998',
    'Profit':25.12
}
</code></pre>

<p>What is the difference, if any, between the two? What is the best practice in python? Practical-wise, I encounter no difference so far. I'm not sure if I missed out anything.</p>
","['python', 'python-3.x', 'dictionary']",51778426,"<p>The differences is that when you have a string like:</p>

<pre><code>''a''
</code></pre>

<p>It will throw a syntax-error because it contains two quotes side by side</p>

<p>so you need to do double quotes like:</p>

<pre><code>""'a'""
</code></pre>

<p>And same thing doing the opposite:</p>

<pre><code>""""a""""
</code></pre>

<p>Will throw an error.</p>

<pre><code>'""a""'
</code></pre>

<p>won't throw an error</p>

<p><em>(on stack-overflow the displayed code is already doesn't look right)</em></p>
",What difference using single quote double quote python dictionary I declare python dictionary like using double quotes Name John Date Profit The python dictionary written like using single quotes Name John Date Profit What difference two What best practice python Practical wise I encounter difference far I sure I missed anything,"startoftags, python, python3x, dictionary, endoftags",python python3x list endoftags,python python3x dictionary,python python3x list,0.67
51839496,2018-08-14,2018,5,"Tensorflow, Keras: How to set add_loss in Keras layer with stop gradient?","<h2>Question 1</h2>

<p>We know that we can use <code>tf.stop_gradient(B)</code> to prevent variable <code>B</code> being trained in backpropagation. But I have no idea how to stop <code>B</code> in certain loss. </p>

<p>To put is simple, assume our loss is: </p>

<pre><code>loss = categorical_crossentropy + my_loss
B = tf.stop_gradient(B)
</code></pre>

<p>where <strong>both <code>categorical_crossentropy</code> and <code>my_loss</code> all depends on <code>B</code></strong>. So, if we set stop gradient for <code>B</code>, both of them will take <code>B</code> as constant. </p>

<p>But how do I set only <code>my_loss</code> stop gradient w.r.t <code>B</code>, leave <code>categorical_crossentropy</code> unchanged? Something like <code>B = tf.stop_gradient(B, myloss)</code></p>

<p>My code for that would be:
</p>

<pre><code>my_loss = ...
B = tf.stop_gradient(B)
categorical_crossentropy = ...
loss = categorical_crossentropy + my_loss
</code></pre>

<p>Will that work? Or, how to make that work?</p>

<hr>

<h2>Question 2</h2>

<p>Okay, guys, if Q1 can be solved, my final quest is how to do that in custom layer?</p>

<p>To put it specific, assume we have a custom layer, which have trainable weights <code>A</code> and <code>B</code> and self loss <code>my_loss</code> for this layer only.</p>

<pre class=""lang-py prettyprint-override""><code>class My_Layer(keras.layers.Layer):
    def __init__(self, **kwargs):
        super(My_Layer, self).__init__(**kwargs)
    def build(self, input_shape):
        self.w = self.add_weight(name='w', trainable=True)
        self.B = self.add_weight(name='B', trainable=True)
        my_loss = w * B
        # tf.stop_gradient(w)
        self.add_loss(my_loss)
</code></pre>

<p>How do I make <code>w</code> only trainable for model loss (MSE, crossentropy etc.),  and <code>B</code> only trainable for <code>my_loss</code>?</p>

<p>If I add that <code>tf.stop_gradient(w)</code>, will that stop <code>w</code> for <code>my_loss</code> only or the final loss of the model?</p>
","['python', 'tensorflow', 'keras']",51975194,"<p><strong>Question 1</strong></p>

<p>When you run <code>y = tf.stop_gradient(x)</code>, you create a <code>StopGradient</code> operation whose input is <code>x</code> and output is <code>y</code>. This operation behaves like an identity, i.e. the value of <code>x</code> is the same as the value of <code>y</code> except that gradients don't flow from <code>y</code> to <code>x</code>.</p>

<p>If you want to have gradients flow to <code>B</code> only from some losses, you can simply do:</p>

<pre><code>B_no_grad = tf.stop_gradient(B)
loss1 = get_loss(B)  # B will be updated because of loss1
loss2 = get_loss(B_no_grad)   # B will not be updated because of loss2 
</code></pre>

<p>Things should become clear when you think about the computation graph you are building. <code>stop_gradient</code> allows you to create an ""identity"" node for any tensor (not just variable) that does not allow gradients to flow through it.</p>

<p><strong>Question 2</strong></p>

<p>I don't know how to do this while using a model loss that you specify using a string (e.g. <code>model.compile(loss='categorical_crossentropy', ...)</code> because you don't control its construction. However, you can do it by adding losses using <code>add_loss</code> or building a model-level loss yourself using model outputs. For the former, just create some losses using plain variables and some using <code>*_no_grad</code> versions, add them all using <code>add_loss()</code>, and compile your model with <code>loss=None</code>.</p>
",Tensorflow Keras How set add loss Keras layer stop gradient Question We know use tf stop gradient B prevent variable B trained backpropagation But I idea stop B certain loss To put simple assume loss loss categorical crossentropy loss B tf stop gradient B categorical crossentropy loss depends B So set stop gradient B take B constant But I set loss stop gradient w r B leave categorical crossentropy unchanged Something like B tf stop gradient B myloss My code would loss B tf stop gradient B categorical crossentropy loss categorical crossentropy loss Will work Or make work Question Okay guys Q solved final quest custom layer To put specific assume custom layer trainable weights A B self loss loss layer class My Layer keras layers Layer def init self kwargs super My Layer self init kwargs def build self input shape self w self add weight name w trainable,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
51896621,2018-08-17,2018,2,"In Pandas, how do I create a dataframe from a count of items in a column that are separated by commas?","<p>In python3 and pandas I have a dataframe which contains for each line informations about legal proceedings.</p>

<p>The column ""nome"" has names of people, the ""tipo"" column has the types of lawsuits, only two types <code>INQ</code> and <code>AP</code>.</p>

<p>And column ""resumo"" has crimes investigated for prosecution in court proceedings. But each legal process may consist of one or more crimes. And the crimes are separated by "","":</p>

<pre><code>Peculato,           Lavagem de Dinheiro
CorrupÃ§Ã£o passiva,  OcultaÃ§Ã£o de bens, Lavagem de dinheiro
CorrupÃ§Ã£o passiva,  Lavagem de dinheiro, Crimes Eleitorais
Crimes Eleitorais,  Lavagem de dinheiro
Peculato
Quadrilha ou Bando, Crimes da Lei de licitaÃ§Ãµes, Peculato
</code></pre>

<p>I need to count:</p>

<pre><code>For each name
Divided by INQ and AP processes
The appearance of each individual crime between "",""
</code></pre>

<p>Taking the example above the ""resumo"" column, and assuming they all concern the person ""John Doe"". </p>

<p>The first two lines are of type <code>AP</code> and the remaining <code>INQ</code>, then John Doe has:</p>

<pre><code>1 AP for Peculato
2 AP for Lavagem de dinheiro
1 AP for CorrupÃ§Ã£o passiva
1 AP for OcultaÃ§Ã£o de bens

1 INQ for CorrupÃ§Ã£o passiva
2 INQ for Lavagem de dinheiro
2 INQ for Crimes Eleitorais
2 INQ for Peculato
1 INQ for Quadrilha ou Bando
1 INQ for Crimes da Lei de licitaÃ§Ãµes
</code></pre>

<p>A sample of the rows look like</p>

<pre><code>df_selecao_atual[['tipo', 'resumo', 'nome']].head(5).to_dict()
{'tipo': {2: 'INQ', 3: 'AP', 4: 'INQ', 5: 'INQ', 6: 'AP'},
 'resumo': {2: 'Desvio de verbas pÃºblicas',
  3: 'Desvio de verbas pÃºblicas',
  4: nan,
  5: 'PrestaÃ§Ã£o de contas rejeitada',
  6: 'Peculato, GestÃ£o fraudulenta'},
 'nome': {2: 'CÃSAR MESSIAS',
  3: 'CÃSAR MESSIAS',
  4: 'FLAVIANO MELO',
  5: 'FLAVIANO MELO',
  6: 'FLAVIANO MELO'}}
</code></pre>

<p>On this database I already had an answer that worked very well in this link: <a href=""https://stackoverflow.com/questions/51798913/in-pandas-how-to-count-items-between-commas-dividing-between-column-types"">In pandas, how to count items between commas, dividing between column types?</a></p>

<p>But now I need to not only show on the screen, but create a dataframe. Like this:</p>

<pre><code>nome                tipo    resumo              count
Fulano de tal       INQ     Peculato            4
Fulano de tal       INQ     OcultaÃ§Ã£o de Bens   1
Fulano de tal       INQ     CorrupÃ§Ã£o ativa     2
Fulano de tal       INQ     InvestigaÃ§Ã£o Penal  3
Fulano de tal       AP      Peculato            1
Fulano de tal       AP      CorrupÃ§Ã£o passiva   2
Beltrano da Silva   INQ     Peculato            2
Beltrano da Silva   INQ     Lavagem de dinheiro 5
Beltrano da Silva   AP      Lavagem de dinheiro 1
</code></pre>

<p>Please, does anyone know how I could create this dataframe?</p>
","['python', 'pandas', 'dataframe']",51896892,"<p>You can create another <code>DataFrame</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html"" rel=""nofollow noreferrer""><code>split</code></a> <code>resumo</code> column and add to original by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html"" rel=""nofollow noreferrer""><code>join</code></a>, then for counting use <code>groupby</code> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.size.html"" rel=""nofollow noreferrer""><code>size</code></a>:</p>

<pre><code>s = (df.pop('resumo').str.split(',', expand=True)
       .stack()
       .reset_index(level=1, drop=True)
       .rename('resumo'))


df = df.join(s).groupby(['nome','tipo','resumo']).size().reset_index(name='count')
print (df)
            nome tipo                         resumo  count
0  CÃSAR MESSIAS   AP      Desvio de verbas pÃºblicas      1
1  CÃSAR MESSIAS  INQ      Desvio de verbas pÃºblicas      1
2  FLAVIANO MELO   AP             GestÃ£o fraudulenta      1
3  FLAVIANO MELO   AP                       Peculato      1
4  FLAVIANO MELO  INQ  PrestaÃ§Ã£o de contas rejeitada      1
</code></pre>

<p>If want use <code>Counter</code> solution with last solution:</p>

<pre><code>s = df.dropna().groupby(['nome', 'tipo']).resumo.agg(', '.join).str.split(', ').agg(Counter)
print (s)
nome           tipo
CÃSAR MESSIAS  AP              {'Desvio de verbas pÃºblicas': 1}
               INQ             {'Desvio de verbas pÃºblicas': 1}
FLAVIANO MELO  AP      {'Peculato': 1, 'GestÃ£o fraudulenta': 1}
               INQ         {'PrestaÃ§Ã£o de contas rejeitada': 1}
Name: resumo, dtype: object

df2 = (pd.DataFrame(s.values.tolist(), index=s.index)
         .stack()
         .astype(int)
         .reset_index(name='count')
         .rename(columns={'level_2':'resumo'}))
print (df2)
            nome tipo                         resumo  count
0  CÃSAR MESSIAS   AP      Desvio de verbas pÃºblicas      1
1  CÃSAR MESSIAS  INQ      Desvio de verbas pÃºblicas      1
2  FLAVIANO MELO   AP             GestÃ£o fraudulenta      1
3  FLAVIANO MELO   AP                       Peculato      1
4  FLAVIANO MELO  INQ  PrestaÃ§Ã£o de contas rejeitada      1
</code></pre>
",In Pandas I create dataframe count items column separated commas In python pandas I dataframe contains line informations legal proceedings The column nome names people tipo column types lawsuits two types INQ AP And column resumo crimes investigated prosecution court proceedings But legal process may consist one crimes And crimes separated Peculato Lavagem de Dinheiro Corrup passiva Oculta de bens Lavagem de dinheiro Corrup passiva Lavagem de dinheiro Crimes Eleitorais Crimes Eleitorais Lavagem de dinheiro Peculato Quadrilha ou Bando Crimes da Lei de licita es Peculato I need count For name Divided INQ AP processes The appearance individual crime Taking example resumo column assuming concern person John Doe The first two lines type AP remaining INQ John Doe AP Peculato AP Lavagem de dinheiro AP Corrup passiva AP Oculta de bens INQ Corrup passiva INQ Lavagem de dinheiro INQ Crimes Eleitorais INQ Peculato INQ Quadrilha ou Bando INQ Crimes da,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
51946916,2018-08-21,2018,4,How to add &#39;id&#39; to the fields in HyperlinkedModelSerializer in DRF,"<p>When using the <code>HyperlinkedModelSerializer</code> from Django REST Framework, the field <code>id</code> is not included in the <code>fields</code> by default. This <a href=""https://stackoverflow.com/questions/15014495/id-field-in-django-rest-framework-serializer"">question</a> has an answer that explains that well.</p>

<p>However I have a problem I'd like to solve in a particular way.</p>

<p>I have a model with custom ID and few dozens of other fields:</p>

<pre><code>class Foo(models.Model):
    id = models.IntegerField(primary_key=True)
    # 20-30 fields
</code></pre>

<p>In the <code>serializers.py</code> I'd like to include all fields from the model:</p>

<pre><code>class FooSerializer(serializers.HyperlinkedModelSerializer):
    class Meta:
        model = Foo
        fields = '__all__'
</code></pre>

<p>However this doesn't include the field <code>id</code>. Defining <code>id = serializers.ReadOnlyField()</code> doesn't help me either, as <code>id</code> shoud be editable.</p>

<p>Specifying all the fields manually like this:</p>

<pre><code>fields = ('id', # all other fields)
</code></pre>

<p>would be a solution I'm trying to circumvent because the model class has a lot of fields and they might change in future.</p>

<p>Is there an elegant possibility to add the field <code>id</code>? Maybe overriding the <code>__init__</code> method?</p>
","['python', 'django', 'django-rest-framework']",51947117,"<p>Add <strong><code>id</code></strong> attribute in <strong><code>FooSerializer</code></strong> serializer as:</p>

<pre><code>class FooSerializer(serializers.HyperlinkedModelSerializer):
    <b>id = serializers.IntegerField(read_only=True)</b>
    class Meta:
        model = Foo
        fields = '__all__'</code></pre>
",How add id fields DRF When using Django REST Framework field id included fields default This question answer explains well However I problem I like solve particular way I model custom ID dozens fields class Foo models Model id models IntegerField primary key True fields In serializers py I like include fields model class FooSerializer serializers class Meta model Foo fields However include field id Defining id serializers ReadOnlyField help either id shoud editable Specifying fields manually like fields id fields would solution I trying circumvent model class lot fields might change future Is elegant possibility add field id Maybe overriding init method,"startoftags, python, django, djangorestframework, endoftags",python tensorflow keras endoftags,python django djangorestframework,python tensorflow keras,0.33
52151163,2018-09-03,2018,2,Get sum of amount fields result in Django REST,"<p>Here is my serializer:</p>

<pre><code>class MySerializer(serializers.Serializer): 
    amount1 = serializers.SerializerMethodField(read_only=True)
    amount2 = serializers.SerializerMethodField(read_only=True)
    amount3 = serializers.SerializerMethodField(read_only=True)
    total = serializers.SerializerMethodField(read_only=True)

    class Meta:
        model = Amount
        fields = ""__all__""

    def get_amount1(self,obj):
        """"""very large calculation here""""""
        return 5

    def get_amount2(self,obj):
        """"""very large calculation here""""""
        return 10

    def get_amount3(self,obj):
        """"""very large calculation here""""""
        return 15

    def get_total(self,obj):
        return self.get_amount1 +self.get_amount2+self.get_amount3
</code></pre>

<p>Now I want to show the sum of all three amounts in <code>total</code> field, but it is taking too much time because of a large calculation in above methods and they are calculating twice only for getting <code>total</code>.</p>

<p>How can I get the sum of <code>amount1</code>, <code>amount2</code>, <code>amount3</code> without calculating <code>get_amount1</code>, <code>get_amount2</code>, <code>get_amount3</code> twice?</p>
","['python', 'django', 'django-rest-framework']",52151227,"<p>You can use <code>getattr</code> within single serializer instance:</p>

<pre><code>def get_amount1(self,obj):
    """"""very large calculation here""""""
    if getattr(self, 'amount1', None):
        return self.amount1
    self.amount1 = 5
    return self.amount1

def get_amount2(self,obj):
    """"""very large calculation here""""""
    if getattr(self, 'amount2', None):
        return self.amount2
    self.amount2 = 10
    return self.amount2

def get_amount3(self,obj):
    """"""very large calculation here""""""
    if getattr(self, 'amount3', None):
        return self.amount3
    self.amount3 = 15
    return self.amount4

def get_total(self,obj):
    return self.get_amount1(obj) +self.get_amount2(obj)+self.get_amount3(obj)
</code></pre>

<p>Or as @Willem-Van-Onsem mentioned in comment <a href=""https://docs.python.org/3/library/functools.html#functools.lru_cache"" rel=""nofollow noreferrer""><code>lru_cache</code></a> for more broad caching:</p>

<pre><code>@lru_cache
def get_amount1(self,obj):
    """"""very large calculation here""""""
    return 5

@lru_cache
def get_amount2(self,obj):
    """"""very large calculation here""""""
    return 10

@lru_cache
def get_amount3(self,obj):
    """"""very large calculation here""""""
    return 15
</code></pre>
",Get sum amount fields result Django REST Here serializer class MySerializer serializers Serializer amount serializers read True amount serializers read True amount serializers read True total serializers read True class Meta model Amount fields def get amount self obj large calculation return def get amount self obj large calculation return def get amount self obj large calculation return def get total self obj return self get amount self get amount self get amount Now I want show sum three amounts total field taking much time large calculation methods calculating twice getting total How I get sum amount amount amount without calculating get amount get amount get amount twice,"startoftags, python, django, djangorestframework, endoftags",python tensorflow keras endoftags,python django djangorestframework,python tensorflow keras,0.33
52473972,2018-09-24,2018,3,pandas convert Hour index integer to datetime,"<p>i have a Pandas dataframe like this:</p>

<pre><code>   Date    Hour Actual      
2018-06-01  0   0.000000
2018-06-01  1   0.012000
2018-06-01  2   0.065000
2018-06-01  3   0.560000
...
</code></pre>

<p>I want to convert these Hour integer indexes and add to date so that it is a Pandas' datetime object.
The result should be like this:</p>

<pre><code>       Date            Actual       
2018-06-01 00:00:00   0.000000
2018-06-01 01:00:00   0.012000
2018-06-01 02:00:00   0.065000
2018-06-01 03:00:00   0.560000
...
</code></pre>

<p>What would be an efficient way to do that? Does Panda provide functionality for converting integer indexes into datetime objects?</p>
","['python', 'pandas', 'datetime']",52474003,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html"" rel=""noreferrer""><code>to_datetime</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_timedelta.html"" rel=""noreferrer""><code>to_timedelta</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pop.html"" rel=""noreferrer""><code>pop</code></a> for extract column <code>time</code>:</p>

<pre><code>df['Date'] = pd.to_datetime(df['Date']) + pd.to_timedelta(df.pop('Hour'), unit='H')
print (df)
                 Date  Actual
0 2018-06-01 00:00:00   0.000
1 2018-06-01 01:00:00   0.012
2 2018-06-01 02:00:00   0.065
3 2018-06-01 03:00:00   0.560
</code></pre>
",pandas convert Hour index integer datetime Pandas dataframe like Date Hour Actual I want convert Hour integer indexes add date Pandas datetime object The result like Date Actual What would efficient way Does Panda provide functionality converting integer indexes datetime objects,"startoftags, python, pandas, datetime, endoftags",python pandas numpy endoftags,python pandas datetime,python pandas numpy,0.67
52554388,2018-09-28,2018,2,numpy masked ragged array,"<p>I want to fill a masked array whose dtype is object (because I need to store masked ragged arrays) with a non scalar fill_value.</p>

<p>Here's an example of a 2D array whose elements are 1D numpy arrays. Of course, I would like the fill_value to be an empty array.</p>

<pre><code>import numpy as np

arr = np.array([
    [np.arange(10), np.arange(5), np.arange(3)],
    [np.arange(1),  np.arange(2), np.array([])],
])

marr = np.ma.array(arr)

marr.mask = [[True, False, False],
             [True, False, True]]
marr.fill_value = np.array([])

marr.filled()
</code></pre>

<p>Unfortunately, it yields an error on the last line:</p>

<pre><code>ValueError: could not broadcast input array from shape (0) into shape (2,3)
</code></pre>

<p>I could manually extract the mask, and apply it on an element-by-element algorithm; but it does not seem to be the right direction to me.</p>

<p>Thank you !</p>
","['python', 'arrays', 'numpy']",52559604,"<p>I would not count on <code>MaskedArray</code> to work well with object dtype arrays. <code>filled</code> is trying to copy the fill value, an array, into a subset of the slots in the <code>data</code>.  Due to broadcasting that can be tricky, even without the masking layer.</p>

<p>Look at the full error:</p>

<pre><code>In [39]: marr.filled()                                                                                                  
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-39-219e016a84cf&gt; in &lt;module&gt;
----&gt; 1 marr.filled()

/usr/local/lib/python3.6/dist-packages/numpy/ma/core.py in filled(self, fill_value)
   3718             result = self._data.copy('K')
   3719             try:
-&gt; 3720                 np.copyto(result, fill_value, where=m)
   3721             except (TypeError, AttributeError):
   3722                 fill_value = narray(fill_value, dtype=object)

ValueError: could not broadcast input array from shape (0) into shape (2,3)
</code></pre>

<p><code>np.copyto</code> tries to broadcast <code>result</code>, <code>fill_value</code> and <code>m</code> (mask) against each other, and then copy the corresponding (mask==true) elements from <code>fill_value</code> to <code>result</code>.</p>

<p><code>marr.data</code> and <code>marr.mask</code> are both <code>(2,3)</code>.  But broadcasting a (0,) shape to (2,3) doesn't work, and isn't what you want anyways.</p>

<p>Filling with a scalar works, but not with an array (or list).</p>

<pre><code>In [56]: np.broadcast_to(np.array([]),(2,3))                                                                            
...
ValueError: operands could not be broadcast together with remapped shapes [original-&gt;remapped]: (0,) and requested shape (2,3)
</code></pre>

<p>A (1,) shape array will broadcast -</p>

<pre><code>In [57]: np.broadcast_to(np.array([1]),(2,3))                                                                           
Out[57]: 
array([[1, 1, 1],
       [1, 1, 1]])
</code></pre>

<p>But the filled result is not an array; it's a scalar:</p>

<pre><code>In [58]: marr.filled(np.array([1]))                                                                                     
Out[58]: 
array([[1, array([0, 1, 2, 3, 4]), array([0, 1, 2])],
       [1, array([0, 1]), 1]], dtype=object)
</code></pre>

<h2>A fill that works</h2>

<p>I can make this fill work if I define a (1,) object dtype array, and putting the (0,) array in it (as an object).</p>

<pre><code>In [97]: Ofill = np.array([None], object)                                                                               
In [98]: Ofill[0] = np.array([])                                                                                        
In [99]: Ofill                                                                                                          
Out[99]: array([array([], dtype=float64)], dtype=object)
In [100]: marr.filled(Ofill)                                                                                            
Out[100]: 
array([[array([], dtype=float64), array([0, 1, 2, 3, 4]),
        array([0, 1, 2])],
       [array([], dtype=float64), array([0, 1]),
        array([], dtype=float64)]], dtype=object)
</code></pre>

<p>This works because <code>Ofill</code> can be broadcasted to (2,3) without messing with the shape of the element</p>

<pre><code>In [101]: np.broadcast_to(Ofill,(2,3))                                                                                  
Out[101]: 
array([[array([], dtype=float64), array([], dtype=float64),
        array([], dtype=float64)],
       [array([], dtype=float64), array([], dtype=float64),
        array([], dtype=float64)]], dtype=object)
</code></pre>

<p>This works, but I wouldn't say it's pretty (or recommended).</p>

<p>Filling with <code>None</code> is prettier, but even then we have to make it a list:</p>

<pre><code>In [103]: marr.filled([None])                                                                                           
Out[103]: 
array([[None, array([0, 1, 2, 3, 4]), array([0, 1, 2])],
       [None, array([0, 1]), None]], dtype=object)
</code></pre>
",numpy masked ragged array I want fill masked array whose dtype object I need store masked ragged arrays non scalar fill value Here example D array whose elements D numpy arrays Of course I would like fill value empty array import numpy np arr np array np arange np arange np arange np arange np arange np array marr np array arr marr mask True False False True False True marr fill value np array marr filled Unfortunately yields error last line ValueError could broadcast input array shape shape I could manually extract mask apply element element algorithm seem right direction Thank,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
52625673,2018-10-03,2018,3,Recursive method to print the hierarchical dictionary,"<p>I want to create a recursive method with Python that prints this dictionary:</p>

<pre><code>partners = {
        'manager-1': {
            'name': 'Manager 1',
            'children': {
                'manager-2': {
                    'name': 'Manager 2',
                    'children': {
                        'employee-1': {
                            'name': 'Employee 1',
                            'children': {
                                'employee-7': {
                                    'name': 'Employee 7',
                                },
                                'employee-8': {
                                    'name': 'Employee 8',
                                }
                            }
                        },
                        'employee-2': {
                            'name': 'Employee 2',
                        },
                        'employee-3': {
                            'name': 'Employee 3',
                        },
                    },
                },
                'manager-3': {
                    'name': 'Manager 3',
                    'children': {
                        'employee-4': {
                            'name': 'Employee 4',
                        },
                        'employee-5': {
                            'name': 'Employee 5',
                        },
                        'employee-6': {
                            'name': 'Employee 6',
                        },
                    },
                },
                'manager-4': {
                    'name': 'Manager 4',
                },
                'manager-5': {
                    'name': 'Manager 5',
                }
            }
        }
    }
</code></pre>

<p>And gives it like this:</p>

<pre><code>--Manager 1
----Manager 2
------Employee 1
--------Employee 7
--------Employee 8
------Employee 2
------Employee 3
----Manager 3
------Manager 4
------Manager 5
------Manager 6
----Manager 4
----Manager 5
</code></pre>

<p>I made this method:</p>

<pre><code>def hierarch(partners):
    for partner in partners:
        if 'children' not in partner.keys(): 
            print(partner['name'])
        else:
            hierarch(partner['children'])
</code></pre>

<p>And I got this error when running the code:</p>

<pre><code>AttributeError: 'str' object has no attribute 'keys'
</code></pre>

<p>How can I make a script that allows me to print this hierarchy? I'm not really good with dictionaries. I'm working with Python 3. Any help please? Thanks.</p>
","['python', 'python-3.x', 'dictionary']",52625821,"<p>Do you need to reproduce the exact order?  You could use something like this to traverse the data structure:</p>

<pre><code>def recurse(data, level):
    if type(data) is dict:
        if ""name"" in data:
            print(""-"" * level + data[""name""])
        for (key, value) in data.items():
            recurse(value, level + 1)
recurse(partners, 1)
</code></pre>
",Recursive method print hierarchical dictionary I want create recursive method Python prints dictionary partners manager name Manager children manager name Manager children employee name Employee children employee name Employee employee name Employee employee name Employee employee name Employee manager name Manager children employee name Employee employee name Employee employee name Employee manager name Manager manager name Manager And gives like Manager Manager Employee Employee Employee Employee Employee Manager Manager Manager Manager Manager Manager I made method def hierarch partners partner partners children partner keys print partner name else hierarch partner children And I got error running code AttributeError str object attribute keys How I make script allows print hierarchy I really good dictionaries I working Python Any help please Thanks,"startoftags, python, python3x, dictionary, endoftags",python python3x list endoftags,python python3x dictionary,python python3x list,0.67
52650413,2018-10-04,2018,2,Changing a column&#39;s values into 1 or 0 based on a parameter in pandas,"<p>So I have this dataframe that contains a lot of columns and for an example have a look at this:</p>

<pre><code>id   Status   Name   Age   Job
213  Active   John   39    Unavailable
415  Inactive Sara   34    Unavailable
941  Inactive Micky  11    Unavailable
993  Active   Zack   45    Unavailable
</code></pre>

<p>What I want to do is to use the pandas library to assign a value of 1 to the job column if the status of a person is active and 0 if it is inactive.So the original dataframe becomes like: </p>

<pre><code>id   Status   Name   Age   Job
213  Active   John   39    1
415  Inactive Sara   34    0
941  Inactive Micky  11    0
993  Active   Zack   45    1
</code></pre>

<p>And to change the values of status column to 1 and 0 based if status is active or inactive respectively. </p>

<pre><code>id   Status   Name   Age   Job
213    1      John   39    Unavailable
415    0      Sara   34    Unavailable
941    0      Micky  11    Unavailable
993    1      Zack   45    Unavailable
</code></pre>

<p>I read a lot in their documentation but they haven't really explicitly declared such manipulations. Also I want to have these made separately.</p>
","['python', 'python-3.x', 'pandas']",52650464,"<p>There are many ways to do this. I like <code>map</code>:</p>

<pre><code>df['Job'] = df.Status.map({'Active':1, 'Inactive':0})
&gt;&gt;&gt; df
    id    Status   Name  Age  Job
0  213    Active   John   39    1
1  415  Inactive   Sara   34    0
2  941  Inactive  Micky   11    0
3  993    Active   Zack   45    1
</code></pre>

<p>Since you only have 2 options (<code>Active</code> or <code>Inactive</code>), you could also use <code>np.where</code>:</p>

<pre><code>df['Job'] = pd.np.where(df.Status == 'Active', 1, 0)
</code></pre>
",Changing column values based parameter pandas So I dataframe contains lot columns example look id Status Name Age Job Active John Unavailable Inactive Sara Unavailable Inactive Micky Unavailable Active Zack Unavailable What I want use pandas library assign value job column status person active inactive So original dataframe becomes like id Status Name Age Job Active John Inactive Sara Inactive Micky Active Zack And change values status column based status active inactive respectively id Status Name Age Job John Unavailable Sara Unavailable Micky Unavailable Zack Unavailable I read lot documentation really explicitly declared manipulations Also I want made separately,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
52676426,2018-10-06,2018,3,How to set transparency and background colour for a pie chart matplotlib,"<p>I'm sure this is somewhere in <code>SO</code> but I can't see to find it anywhere. I'm trying to alter the <code>colour</code> <code>transparency</code> or <code>alpha</code> for a <code>pie chart</code> in <code>matplotlib</code>. I'm also hoping to set the background <code>colour</code> to <code>grey</code>. </p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt

d = ({ 
    'C' : ['X','Y','Z','X','Y','Z','A','X','Y','Z'],
    })

df = pd.DataFrame(data=d)

fig, ax = plt.subplots(figsize = (20,12))
ax.grid(False)
plt.style.use('ggplot')

df['C'].value_counts().plot(kind = 'pie')

plt.show
</code></pre>

<p>I cant seem to change the <code>pie chart</code> <code>colour</code> <code>transparency</code> or use the background generally created by <code>'ggplot'</code>?</p>
","['python', 'pandas', 'matplotlib']",52678032,"<p>There are two similar ways: the basic idea is to define the wedge properties
using <code>wedgeprops</code>. On a side note, I would rather use <code>lightgrey</code> background as it looks much better than <code>grey</code> which is a bit dark.</p>

<p><strong>First</strong>: </p>

<pre><code>df['C'].value_counts().plot(kind = 'pie',wedgeprops={'alpha':0.5})
fig.set_facecolor('lightgrey')
</code></pre>

<p><a href=""https://i.stack.imgur.com/osOzW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/osOzW.png"" alt=""enter image description here""></a></p>

<p><strong>Second</strong>: using <code>plt.pie</code></p>

<pre><code>plt.pie(df['C'].value_counts(), wedgeprops={'alpha':0.5})
fig.set_facecolor('lightgrey')
</code></pre>

<p><a href=""https://i.stack.imgur.com/LX1qy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LX1qy.png"" alt=""enter image description here""></a></p>
",How set transparency background colour pie chart matplotlib I sure somewhere SO I see find anywhere I trying alter colour transparency alpha pie chart matplotlib I also hoping set background colour grey import pandas pd import matplotlib pyplot plt C X Y Z X Y Z A X Y Z df pd DataFrame data fig ax plt subplots figsize ax grid False plt style use ggplot df C value counts plot kind pie plt show I cant seem change pie chart colour transparency use background generally created ggplot,"startoftags, python, pandas, matplotlib, endoftags",python pandas matplotlib endoftags,python pandas matplotlib,python pandas matplotlib,1.0
52785101,2018-10-12,2018,69,seaborn scatterplot marker size for ALL markers,"<p>I can't find out anywhere how to change the marker size on seaborn scatterplots. There is a <code>size</code> option listed in the <a href=""https://seaborn.pydata.org/generated/seaborn.scatterplot.html"" rel=""noreferrer"">documentation</a> but it is only for when you want variable size across points. I want the same size for all points but larger than the default!</p>

<p>I tried making a new column of integers in my dataframe and set that as the size, but it looks like the actual value doesn't matter, it changes the marker size on a relative basis, so in this case all the markers were still the same size as the default.</p>

<p>Edit: here's some code</p>

<pre><code>ax = sns.scatterplot(x=""Data Set Description"", y=""R Squared"", data=mean_df)
plt.show()
</code></pre>

<p>I just tried something and it worked, not sure if it's the best method though. I added size=[1, 1, 1, 1, 1, 1] and sizes=(500, 500). So essentially I'm setting all sizes to be the same, and the range of sizes to be only at 500.</p>
","['python', 'matplotlib', 'seaborn']",52785672,"<p>You can do so by giving a value to the <code>s</code> argument to change the marker size.</p>

<p>Example:</p>

<pre><code>ax = sns.scatterplot(x=""Data Set Description"", y=""R Squared"", data=mean_df, s=10)
</code></pre>
",seaborn scatterplot marker size ALL markers I find anywhere change marker size seaborn scatterplots There size option listed documentation want variable size across points I want size points larger default I tried making new column integers dataframe set size looks like actual value matter changes marker size relative basis case markers still size default Edit code ax sns scatterplot x Data Set Description R Squared data mean df plt show I tried something worked sure best method though I added size sizes So essentially I setting sizes range sizes,"startoftags, python, matplotlib, seaborn, endoftags",python matplotlib seaborn endoftags,python matplotlib seaborn,python matplotlib seaborn,1.0
52882771,2018-10-18,2018,3,Read CSV file in Pandas with Blank lines in between,"<p>I have a data.csv file like this</p>

<pre><code>Col1,Col2,Col3,Col4,Col5  
10,12,14,15,16  
18,20,22,24,26  
28,30,32,34,36  
38,40,42,44,46  
48,50,52,54,56

Col6,Col7  
11,12  
13,14  
...
</code></pre>

<p>Now, I want to read only the data of columns Col1 to Col5 and I don't require Col6 and Col7.</p>

<p>I tried reading this file using </p>

<pre><code>df = pd.read_csv('data.csv',header=0)
</code></pre>

<p>then its throwing an error saying</p>

<pre><code>UnicodeDecodeError : 'utf-8' codec cant decode byte 0xb2 in position 3: invalid start byte
</code></pre>

<p>Then, I tried this  </p>

<pre><code>df = pd.read_csv('data.csv',header=0,error_bad_lines=True)
</code></pre>

<p>But this is also not giving the desired result. How can we read only till the first blank line in the csv file?</p>
","['python', 'pandas', 'csv']",52882986,"<p>You can create a generator which reads a file line by line. The result is passed to <code>pandas</code>:</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import io


def file_reader(filename):
    with open(filename) as f:
        for line in f:
            if line and line != '\n':
                yield line
            else:
                break


data = io.StringIO(''.join(file_reader('data.csv')))
df = pd.read_csv(data)
</code></pre>
",Read CSV file Pandas Blank lines I data csv file like Col Col Col Col Col Col Col Now I want read data columns Col Col I require Col Col I tried reading file using df pd read csv data csv header throwing error saying UnicodeDecodeError utf codec cant decode byte xb position invalid start byte Then I tried df pd read csv data csv header error bad lines True But also giving desired result How read till first blank line csv file,"startoftags, python, pandas, csv, endoftags",python pandas dataframe endoftags,python pandas csv,python pandas dataframe,0.67
52885878,2018-10-19,2018,4,How to downsampling time series data in pandas?,"<p>I have a time series in pandas that looks like this (order by id):</p>

<pre><code>id    time    value
 1       0        2
 1       1        4
 1       2        5
 1       3       10
 1       4       15
 1       5       16
 1       6       18
 1       7       20
 2      15        3
 2      16        5
 2      17        8
 2      18       10
 4       6        5
 4       7        6
</code></pre>

<p>I want downsampling time from 1 minute to 3 minutes for each group id.
And value is a maximum of group (id and 3 minutes).</p>

<p>The output should be like:</p>

<pre><code>id    time    value
 1       0        5
 1       1       16
 1       2       20
 2       0        8
 2       1       10
 4       0        6
</code></pre>

<p>I tried loop it take long time process.</p>

<p>Any idea how to solve this for large dataframe?</p>

<p>Thanks!</p>
","['python', 'pandas', 'dataframe']",52885988,"<p>Use <code>np.r_</code> and <code>.iloc</code> with <code>groupby</code>:</p>

<pre><code>df.groupby('id')['value'].apply(lambda x: x.iloc[np.r_[2:len(x):3,-1]])
</code></pre>

<p>Output:</p>

<pre><code>id    
1   2      5
    5     16
    7     20
2   10     8
    11    10
4   13     6
Name: value, dtype: int64
</code></pre>

<p>Going a little further with column naming etc..</p>

<pre><code>df_out = df.groupby('id')['value']\
           .apply(lambda x: x.iloc[np.r_[2:len(x):3,-1]]).reset_index()
df_out.assign(time=df_out.groupby('id').cumcount()).drop('level_1', axis=1)
</code></pre>

<p>Output:</p>

<pre><code>   id  value  time
0   1      5     0
1   1     16     1
2   1     20     2
3   2      8     0
4   2     10     1
5   4      6     0
</code></pre>
",How downsampling time series data pandas I time series pandas looks like order id id time value I want downsampling time minute minutes group id And value maximum group id minutes The output like id time value I tried loop take long time process Any idea solve large dataframe Thanks,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
52895104,2018-10-19,2018,4,Get rightmost non-zero value position for each row in dataframe (non lambda method),"<p>I have a large data frame and I need to get the last zero value index for each row in it if there are zeros from the right. </p>

<p>If there is not a zero in the row I need the last index.</p>

<p>Working code below. with correct output. </p>

<p>Is there a way to vectorize this code (not use a lambda)</p>

<p>Example Code: </p>

<pre><code>df = pd.DataFrame.from_dict(
    {'a': {0: 14, 1: 0, 2: 105, 3: 67},
     'b': {0: 67, 1: 0, 2: 0, 3: 63},
     'c': {0: 35, 1: 0, 2: 530, 3: 431},
     'd': {0: 500, 1: 0, 2: 0, 3: 500},
     'e': {0: 13, 1: 0, 2: 0, 3: 12},
     'f': {0: 123, 1: 0, 2: 0, 3: 0}}
)

# if row has no zeros use last index
def func(row):
    # if row is all zeros return first index
    if sum(row == 0) == len(row):
        return row.index[0]

    # if row is all non zero return last index
    if sum(row != 0)== len(row):
        return row.index[-1]

    # else return index of right most non zero value
    return row.loc[row != 0].index[-1]

df.apply(lambda row: func(row), axis=1)
</code></pre>

<p>Output:</p>

<pre><code>0    f
1    a
2    c
3    e
</code></pre>
","['python', 'python-3.x', 'pandas']",52895240,"<p>Find where it's not equal to 0, cumsum and then find the first instance where this is the maximum.</p>

<pre><code>df.ne(0).cumsum(1).idxmax(1)

0    f
1    a
2    c
3    e
dtype: object
</code></pre>
",Get rightmost non zero value position row dataframe non lambda method I large data frame I need get last zero value index row zeros right If zero row I need last index Working code correct output Is way vectorize code use lambda Example Code df pd DataFrame dict b c e f row zeros use last index def func row row zeros return first index sum row len row return row index row non zero return last index sum row len row return row index else return index right non zero value return row loc row index df apply lambda row func row axis Output f c e,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
52898919,2018-10-19,2018,2,Tkinter module - program will just run then stop and won&#39;t open the window,"<p>I need help, I am doing a budget calculator and using <code>tkinter</code> for the first time and wondered why it is not working...</p>

<p>When I run it, it will just end straight away and when I put the <code>root = Tk()</code> at the end it comes up with an error.</p>

<p>I really need help, my code is below...</p>

<pre><code>from time import sleep
from tkinter import * 
from tkinter import messagebox, ttk, Tk

root = Tk()

class GUI():

    def taskbar(self):

        menu = Menu()
        file = Menu(menu)
        file.add_command(label=""Exit"", command=self.exit_GUI)
        file.add_command(label = ""Information"", command=self.info_popup)        

    def Main_Menu(self):
        topFrame = Frame(root)
        topFrame.pack()
        bottomFrame = Frame(root)
        bottomFrame.pack(side=BOTTOM)

        Income_button = Button(topFrame, text=""Enter your incomes"", command=self.Income)
        Expense_button = Button(topFrame, text=""Enter your expenses"", command=self.Expense)
        Total_button = Button(bottomFrame, text=""View Results"", command=self.Total)
        Income_button.pack()
        Expense_button.pack()
        Total_button.pack()

    def Income(self):
        pass

    def Expense(self):
        pass

    def Total(self):
        pass

    def exit_GUI(self):
        exit()

    def info_popup():
        pass

g = GUI()
g.Main_Menu()
g.taskbar()
g.Income()
g.Expense()
g.Total()
g.exit_GUI()
g.info_popup()

root.mainloop()
</code></pre>
","['python', 'python-3.x', 'tkinter']",52898988,"<p>You are exiting before you ever get to the <code>mainloop</code> with:</p>

<pre><code>g.exit_GUI()
</code></pre>

<p>That method is calling the standard <code>exit()</code> and stopping the entire script. Remove or comment out the above call. You will also need to add <code>self</code> as an argument to <code>info_popup</code> to get your script to run:</p>

<pre><code>def info_popup(self):
    pass
</code></pre>
",Tkinter module program run stop open window I need help I budget calculator using tkinter first time wondered working When I run end straight away I put root Tk end comes error I really need help code time import sleep tkinter import tkinter import messagebox ttk Tk root Tk class GUI def taskbar self menu Menu file Menu menu file add command label Exit command self exit GUI file add command label Information command self info popup def Main Menu self topFrame Frame root topFrame pack bottomFrame Frame root bottomFrame pack side BOTTOM Income button Button topFrame text Enter incomes command self Income Expense button Button topFrame text Enter expenses command self Expense Total button Button bottomFrame text View Results command self Total Income button pack Expense button pack Total button pack def Income self pass def Expense self pass def Total self pass def exit GUI self exit def,"startoftags, python, python3x, tkinter, endoftags",python python3x list endoftags,python python3x tkinter,python python3x list,0.67
52909610,2018-10-20,2018,2,pandas - getting first and last value from each day in a datetime dataframe,"<p>I have a dataframe of a month excluding Saturday and Sunday, which was logged every 1 minute.</p>

<pre><code>                         v1         v2  
2017-04-03 09:15:00     35.7       35.4  
2017-04-03 09:16:00     28.7       28.5
      ...               ...        ...
2017-04-03 16:29:00     81.7       81.5
2017-04-03 16:30:00     82.7       82.6
      ...               ...        ...
2017-04-04 09:15:00     24.3       24.2  
2017-04-04 09:16:00     25.6       25.5
      ...               ...        ...
2017-04-04 16:29:00     67.0       67.2
2017-04-04 16:30:00     70.2       70.6
      ...               ...        ...
2017-04-28 09:15:00     31.7       31.4  
2017-04-28 09:16:00     31.5       31.0
      ...               ...        ...
2017-04-28 16:29:00     33.2       33.5
2017-04-28 16:30:00     33.0       33.7
</code></pre>

<p>how to resample dataframe to get 1st and last value from each day. The required data-frame :</p>

<pre><code>                        v1         v2  
2017-04-03 09:15:00     35.7       35.4  
2017-04-03 16:30:00     82.7       82.6
2017-04-04 09:15:00     24.3       24.2  
2017-04-04 16:30:00     70.2       70.6
      ...               ...        ...
2017-04-28 09:15:00     31.7       31.4  
2017-04-28 16:30:00     33.0       33.7
</code></pre>
","['python', 'pandas', 'pandas-groupby']",52909738,"<p>Here's one way:</p>

<pre><code>res = df.groupby(df.index.date).apply(lambda x: x.iloc[[0, -1]])
res.index = res.index.droplevel(0)

print(res)

                       v1    v2
2017-04-03 09:15:00  35.7  35.4
2017-04-03 16:30:00  82.7  82.6
2017-04-04 09:15:00  24.3  24.2
2017-04-04 16:30:00  70.2  70.6
2017-04-28 09:15:00  31.7  31.4
2017-04-28 16:30:00  33.0  33.7
</code></pre>
",pandas getting first last value day datetime dataframe I dataframe month excluding Saturday Sunday logged every minute v v resample dataframe get st last value day The required data frame v v,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
53091343,2018-10-31,2018,7,Pandas groupby sequential values,"<p>I have no idea how to call this operation, so I couldn't really google anything, but here's what I'm trying to do:</p>

<p>I have this dataframe:</p>

<pre><code>df = pd.DataFrame({""name"": [""A"", ""B"", ""B"", ""B"", ""A"", ""A"", ""B""], ""value"":[3, 1, 2, 0, 5, 2, 3]})
df
  name  value
0    A      3
1    B      1
2    B      2
3    B      0
4    A      5
5    A      2
6    B      3
</code></pre>

<p>And I want to group it on <code>df.name</code> and apply a <code>max</code> function on <code>df.values</code> but only if the names are in sequence. So my desired result is as follows:</p>

<pre><code>df.groupby_sequence(""name"")[""value""].agg(max)
  name  value
0    A      3
1    B      2
2    A      5
3    B      3
</code></pre>

<p>Any clue how to do this?</p>
","['python', 'pandas', 'pandas-groupby']",53091412,"<p>Using <code>pandas</code>, you can groupby when the name changes from row to row, using <code>(df.name!=df.name.shift()).cumsum()</code>, which essentially groups together consecutive names:</p>

<pre><code>&gt;&gt;&gt; df.groupby((df.name!=df.name.shift()).cumsum()).max().reset_index(drop=True)
  name  value
0    A      3
1    B      2
2    A      5
3    B      3
</code></pre>
",Pandas groupby sequential values I idea call operation I really google anything I trying I dataframe df pd DataFrame name A B B B A A B value df name value A B B B A A B And I want group df name apply max function df values names sequence So desired result follows df groupby sequence name value agg max name value A B A B Any clue,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
53156714,2018-11-05,2018,5,Confusion about when NumPy array slices are references and when they are copies,"<p>I have the following piece of code:</p>

<pre><code>import numpy as np
arr = np.arange(10)
slice = arr[2:5]
slice[:] = 12
print(arr)
slice = slice / 2
print(arr)
</code></pre>

<p>The output will be:</p>

<pre><code>[ 0  1 12 12 12  5  6  7  8  9]
[6. 6. 6.]
[ 0  1 12 12 12  5  6  7  8  9]
</code></pre>

<p>So the first time around <code>slice</code> is just a reference to part of <code>arr</code> so modifying it also changes the array, but the second time around it has become a copy of that part of the array and modifying it makes no difference in <code>arr</code>. Why does this happen? What makes <code>slice = slice / 2</code> different?</p>
","['python', 'arrays', 'numpy']",53156974,"<p>Indexing with a slice object always returns a view (reference) of an array.  Modifying the slice will modify the original array. In your second example you assign to the slice object.  That does not modify the object.  A new object is created with the specified values, in this case slice / 2.  You can use <code>/=</code> to modify the object in place if that's the desired behavior, or index into the slice (<code>[:]</code>) which numpy interprets as modifying the entries at those indices.</p>
",Confusion NumPy array slices references copies I following piece code import numpy np arr np arange slice arr slice print arr slice slice print arr The output So first time around slice reference part arr modifying also changes array second time around become copy part array modifying makes difference arr Why happen What makes slice slice different,"startoftags, python, arrays, numpy, endoftags",python python3x numpy endoftags,python arrays numpy,python python3x numpy,0.67
53188883,2018-11-07,2018,3,Pandas Dataframe automatically renames duplicate columns name,"<p>I have a dataframe with 10 columns and 160 rows. Column names are based on month and year for e.g Jun'17, July'17, Mar'18 etc.
However in excel some columns are repeating like Jun'17 two times
When I import them to pandas dataframe it renames duplicate columns to Jun'17 and Jun'17.1</p>

<p>This '.1' is extra and disturbing my whole calculation.</p>
","['python', 'pandas', 'dataframe']",53209046,"<p>I dont think it is a good idea have more columns with the same name, and i wouldnt suggest this, but if you want to go with that, you can do in this way:</p>

<pre><code>df = df.rename(columns = {""Jun'17.1"":""Jun'17""})
</code></pre>

<p>To access to the 2 different columns then do in this way:</p>

<pre><code>df[""Jun'17""].iloc[:,0]
df[""Jun'17""].iloc[:,1]
</code></pre>
",Pandas Dataframe automatically renames duplicate columns name I dataframe columns rows Column names based month year e g Jun July Mar etc However excel columns repeating like Jun two times When I import pandas dataframe renames duplicate columns Jun Jun This extra disturbing whole calculation,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
53211498,2018-11-08,2018,7,pandas groupby() with custom aggregate function and put result in a new column,"<p>Suppose I have a dataframe with 3 columns. I want to group it by one of the columns and compute a new value for each group using a custom aggregate function. </p>

<p>This new value has a totally different meaning and its column just is not present in the original dataframe. So, in effect, I want to change the shape of the dataframe during the <code>groupby() + agg()</code> transformation. The original dataframe looks like <code>(foo, bar, baz)</code> and has a range index while the resulting dataframe needs to have only <code>(qux)</code> column and <code>baz</code> as an index.</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'foo': [1, 2, 3], 'bar': ['a', 'b', 'c'], 'baz': [0, 0, 1]})
df.head()

#        foo    bar    baz
#   0      1      a      0
#   1      2      b      0
#   2      3      c      1    

def calc_qux(gdf, **kw):
    qux = ','.join(map(str, gdf['foo'])) + ''.join(gdf['bar'])
    return (None, None)  # but I want (None, None, qux)

df = df.groupby('baz').agg(calc_qux, axis=1)  # ['qux'] but then it fails, since 'qux' is not presented in the frame.
df.head()

#      qux
# baz       
#   0  1,2ab
#   1  3c
</code></pre>

<p>The code above produces an error <code>ValueError: Shape of passed values is (2, 3), indices imply (2, 2)</code> if I'm trying to return from the aggregation function different amount of values than the number of columns in the original dataframe.</p>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",53212175,"<p>You want to use <code>apply()</code> here since you are not operating on a single column (in which case <code>agg()</code> would be appropriate):</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'foo': [1, 2, 3], 'bar': ['a', 'b', 'c'], 'baz': [0, 0, 1]})

def calc_qux(x):

    return ','.join(x['foo'].astype(str).values) + ''.join(x['bar'].values)

df.groupby('baz').apply(calc_qux).to_frame('qux')
</code></pre>

<p>Yields:</p>

<pre><code>       qux
baz       
0    1,2ab
1       3c
</code></pre>
",pandas groupby custom aggregate function put result new column Suppose I dataframe columns I want group one columns compute new value group using custom aggregate function This new value totally different meaning column present original dataframe So effect I want change shape dataframe groupby agg transformation The original dataframe looks like foo bar baz range index resulting dataframe needs qux column baz index import pandas pd df pd DataFrame foo bar b c baz df head foo bar baz b c def calc qux gdf kw qux join map str gdf foo join gdf bar return None None I want None None qux df df groupby baz agg calc qux axis qux fails since qux presented frame df head qux baz ab c The code produces error ValueError Shape passed values indices imply I trying return aggregation function different amount values number columns original dataframe,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python pandas numpy endoftags,python pandas dataframe pandasgroupby,python pandas numpy,0.58
53217879,2018-11-08,2018,2,Access dictionary values using list values as subsequent keys,"<pre><code>keys = ['prop1', 'prop2', 'prop3']
dict = { prop1: { prop2: { prop3: True } } }
</code></pre>

<p>How do I get the value <code>True</code> out of the dict using the list?</p>

<p>Not having any success with</p>

<pre><code>val = reduce((lambda a, b: dict[b]), keys)
</code></pre>

<p>update:</p>

<p><code>keys</code> and <code>dict</code> can be arbitrarily long, but will always have matching properties/keys.</p>
","['python', 'python-3.x', 'dictionary']",53218082,"<p>Using a loop:</p>

<pre><code>&gt;&gt;&gt; a = ['prop1', 'prop2', 'prop3'] 
&gt;&gt;&gt; d = {'prop1': {'prop2': {'prop3': True}}}
&gt;&gt;&gt; result = d
&gt;&gt;&gt; for k in a: 
...     result = result[k] 
...
&gt;&gt;&gt; result
True
</code></pre>

<p>Using a functional style:</p>

<pre><code>&gt;&gt;&gt; from functools import reduce
&gt;&gt;&gt; reduce(dict.get, a, d)
True
</code></pre>
",Access dictionary values using list values subsequent keys keys prop prop prop dict prop prop prop True How I get value True dict using list Not success val reduce lambda b dict b keys update keys dict arbitrarily long always matching properties keys,"startoftags, python, python3x, dictionary, endoftags",python python3x list endoftags,python python3x dictionary,python python3x list,0.67
53267432,2018-11-12,2018,2,Pandas - check if subset of dataframe is in another dataframe,"<p>I have the following dataframe which I'll call 'names':</p>

<pre><code>date       name    code  
6/1/2018   A       5     
6/1/2018   B       5     
7/1/2018   A       5     
7/1/2018   B       5     
</code></pre>

<p>I have the following df which I need to alter:</p>

<pre><code>date       name    comment   
5/1/2018   A       'Good'    
6/1/2018   A       'Good'    
6/1/2018   B       'Good'    
6/1/2018   C       'Good'    
7/1/2018   A       'Good'    
7/1/2018   B       'Good'    
</code></pre>

<p>I need to change the comment to 'Bad' if the name isn't in the names dataframe <em>for that date</em></p>

<p>Right now I have:</p>

<pre><code>df['comment'] = np.where(~df['name'].isin(names['name']), 'Bad', df['comment'])
</code></pre>

<p>Though obviously that doesn't work because it doesn't take into account name AND date. </p>

<p>Final output: </p>

<pre><code>date       name    comment   
5/1/2018   A       'Bad'     
6/1/2018   A       'Good'    
6/1/2018   B       'Good'    
6/1/2018   C       'Bad'     
7/1/2018   A       'Good'    
7/1/2018   B       'Good'    
</code></pre>

<p>The first row was changed because there's no A entry for 5/1 in the names dataframe. The C row was changed because there's no C entry for 6/1 in the names df (or rather no C entry at all).</p>

<p>Note: Both dataframes (names and df) are larger than I've shown, both row and column-wise. </p>
","['python', 'pandas', 'dataframe']",53267471,"<p>Performant solution using <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.get_indexer.html"" rel=""nofollow noreferrer""><code>pd.Index.get_indexer</code></a>:</p>

<pre><code>v = names.set_index(['date', 'name'])
m = v.index.get_indexer(pd.MultiIndex.from_arrays([df.date, df.name])) == -1
df.loc[m, 'comment'] = '\'Bad\''

print(df)
      date name comment
0  5/1/2018    A   'Bad'
1  6/1/2018    A  'Good'
2  6/1/2018    B  'Good'
3  6/1/2018    C   'Bad'
4  7/1/2018    A  'Good'
5  7/1/2018    B  'Good'
</code></pre>

<hr>

<p>Alternatively, do a LEFT OUTER <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a>, determine missing values in the right DataFrame, and use that to <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mask.html"" rel=""nofollow noreferrer""><code>mask</code></a> rows:</p>

<pre><code>m = df.merge(names, how='left', on=['date', 'name']).code.isna()
df['comment'] = df['comment'].mask(m, '\'Bad\'')

print(df)
       date name comment
0  5/1/2018    A   'Bad'
1  6/1/2018    A  'Good'
2  6/1/2018    B  'Good'
3  6/1/2018    C   'Bad'
4  7/1/2018    A  'Good'
5  7/1/2018    B  'Good'
</code></pre>
",Pandas check subset dataframe another dataframe I following dataframe I call names date name code A B A B I following df I need alter date name comment A Good A Good B Good C Good A Good B Good I need change comment Bad name names dataframe date Right I df comment np df name isin names name Bad df comment Though obviously work take account name AND date Final output date name comment A Bad A Good B Good C Bad A Good B Good The first row changed A entry names dataframe The C row changed C entry names df rather C entry Note Both dataframes names df larger I shown row column wise,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
53408717,2018-11-21,2018,2,getting error while removing duplicates from csv using pandas,"<p><strong>My csv file is on this link:</strong></p>
<p><a href=""https://drive.google.com/file/d/1Pac9-YLAtc7iaN0qEuiBOpYYf9ZPDDaL/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1Pac9-YLAtc7iaN0qEuiBOpYYf9ZPDDaL/view?usp=sharing</a></p>
<p>I want to remove the duplicate from the csv by checking length of <strong>genres</strong> against each artist ID. If an artist have 2 records in csv (e.g., ed sheeran's id <strong>6eUKZXaKkcviH0Ku9w2n3V</strong> have 2 records one record have 1 genres while row#5 have 5 genres so i want to keep the row which has largest genres length)</p>
<p>I'm using this script for now:</p>
<pre><code>import pandas
import ast


df = pandas.read_csv('39K.csv', encoding='latin-1')

df['lst_len'] = df['genres'].map(lambda x: len(ast.literal_eval(str(x))))
print(df['lst_len'][0])

df = df.sort_values('lst_len', ascending=False)

# Drop duplicates, preserving first (longest) list by ID
df = df.drop_duplicates(subset='ID')


# Remove extra column that we introduced, write to file
df = df.drop('lst_len', axis=1)
df.to_csv('clean_39K.csv', index=False)
</code></pre>
<p>but this script works for 500 record (may be i have illusion that size of records matters),</p>
<p>but when I run this script for my largest file <strong>39K.csv</strong> I'm getting this error:</p>
<pre><code>Traceback (most recent call last):
******* error in line 5, in &lt;module&gt;....
    df['lst_len'] = df['genres'].map(lambda x: len(list(x)))
    df['lst_len'] = df['genres'].map(lambda x: len(list(x)))
TypeError: 'float' object is not iterable
</code></pre>
<p>Please point me where i am doing wrong?
Thanks</p>
","['python', 'pandas', 'csv']",53408921,"<p>You have bad data at (at least) line 16553 of your input csv file:</p>

<pre><code>52lUXCmpmAIVsgNd1uADOy,Moosh &amp; Twist,NULL
</code></pre>

<p><code>pandas</code> interprets <code>NULL</code> as <code>nan</code> when it reads the file, which is of type <code>float</code> and is not iterable. There are a few other <code>NULL</code> entries in there too, so you could either manually remove or fix them (preferred), or handle this case in your code.</p>

<p>For example, if you actually want to pretend that <code>NULL</code> should be interpreted as an empty list, you can preprocess the data like this (just after reading the csv):</p>

<pre><code>df.loc[df['genres'].isnull(),['genres']] = df.loc[df['genres'].isnull(),'genres'].apply(lambda x: [])
</code></pre>

<p>Or more elegantly, switch to reading the csv using <code>na_filter=False</code>:</p>

<pre><code>df = pandas.read_csv('39K.csv', encoding='latin-1', na_filter=False)
</code></pre>

<p>which will prevent pandas from replacing these values with <code>nan</code> in the first place.</p>

<p>Finally, the code doesn't quite do what we ant because it is counting the number of characters in the string representation of the list. The solution is to preprocess the NULL values into strings representing empty lists, then use <code>ast.literal_eval</code> to turn all the strings back into lists:</p>

<pre><code>import pandas
import ast

    df = pandas.read_csv('39K.csv', encoding='latin-1', na_filter=False)
    df.replace(to_replace=""NULL"", value=""[]"", inplace=True)

    for item in df['genres']:

        print(str(item))
        print(ast.literal_eval(item))

    df['lst_len'] = df['genres'].map(lambda x: len(ast.literal_eval(x)))
</code></pre>
",getting error removing duplicates csv using pandas My csv file link https drive google com file Pac YLAtc iaN qEuiBOpYYf ZPDDaL view usp sharing I want remove duplicate csv checking length genres artist ID If artist records csv e g ed sheeran id eUKZXaKkcviH Ku w n V records one record genres row genres want keep row largest genres length I using script import pandas import ast df pandas read csv K csv encoding latin df lst len df genres map lambda x len ast literal eval str x print df lst len df df sort values lst len ascending False Drop duplicates preserving first longest list ID df df drop duplicates subset ID Remove extra column introduced write file df df drop lst len axis df csv clean K csv index False script works record may illusion size records matters I run script largest file K csv I getting,"startoftags, python, pandas, csv, endoftags",python pandas datetime endoftags,python pandas csv,python pandas datetime,0.67
53417477,2018-11-21,2018,2,convert multiple dictionaries to single dictionary in python,"<p>I have multiple dictionaries with its keys and values and I want to assign(transfer- all of them to a new-empty- dictionary with keeping all keys and values.
note: other question that i checked have dictionaries with same size</p>

<pre><code>n = {}
x = {'six':6,'thirteen':13,'fifty five':55}
y = {'two': 2, 'four': 4, 'three': 3, 'one': 1, 'zero': 0,'ten': 10}
z = {'nine': 9, 'four': 4, 'three': 3, 'eleven': 11, 'zero': 0, 'seven':7}
</code></pre>
","['python', 'python-3.x', 'dictionary']",53417597,"<h3><a href=""https://docs.python.org/3/library/collections.html#collections.ChainMap"" rel=""nofollow noreferrer""><code>ChainMap</code></a></h3>

<p>For many use cases, <code>collections.ChainMap</code> suffices and is efficient (assumes Python 3.x):</p>

<pre><code>from collections import ChainMap

n = ChainMap(x, y, z)

n['two']       # 2
n['thirteen']  # 13
</code></pre>

<p>If you need a dictionary, just call <code>dict</code> on the <code>ChainMap</code> object:</p>

<pre><code>d = dict(n)
</code></pre>

<h3>Dictionary unpacking</h3>

<p>With Python 3.x, (<a href=""https://www.python.org/dev/peps/pep-0448/"" rel=""nofollow noreferrer"">PEP448</a>), you can unpack your dictionaries as you define a new dictionary:</p>

<pre><code>d = {**x, **y, **z}
</code></pre>

<p>Related: <a href=""https://stackoverflow.com/questions/38987/how-to-merge-two-dictionaries-in-a-single-expression"">How to merge two dictionaries in a single expression?</a></p>
",convert multiple dictionaries single dictionary python I multiple dictionaries keys values I want assign transfer new empty dictionary keeping keys values note question checked dictionaries size n x six thirteen fifty five two four three one zero ten z nine four three eleven zero seven,"startoftags, python, python3x, dictionary, endoftags",python python3x list endoftags,python python3x dictionary,python python3x list,0.67
53496948,2018-11-27,2018,5,Tensorflow to Keras: import graph def error on Keras model,"<p>I have a Tensorflow code for classifying images which I want to convert to Keras code. But I'm having trouble with the higher level API not having all codes which I desire. The problem which I have been stuck at is: </p>

<pre><code>#net = get_vgg_model() &lt;- got tf.VGG16 model
net = tf.keras.applications.VGG16()


g1 = tf.Graph()
with tf.Session(graph=g1, config=config) as sess, g1.device('/cpu:0'):
    tf.import_graph_def(net['graph_def'], name='vgg')
</code></pre>

<p>this code gives the error: </p>

<pre><code>Traceback (most recent call last):
  File ""app.py"", line 16, in &lt;module&gt;
    from modules.xvision import Xvision
    File ""/app/modules/xvision.py"", line 84, in &lt;module&gt;
       tf.import_graph_def(net['graph_def'], name='vgg')
   TypeError: 'Model' object has no attribute '__getitem__'
</code></pre>

<p>Could someone help me with this graph?</p>
","['python', 'tensorflow', 'keras']",53541695,"<h2>Getting the graph</h2>

<p>You can get the graph from Keras with: </p>

<pre><code>import keras.backend as K
K.get_session().graph
</code></pre>

<p>You can probably pass it to <code>import_graph_def</code>, but I suspect it's already Tensorflow's default graph, since in the link below, the creator of Keras says there is only one graph. </p>

<p>More in: <a href=""https://github.com/keras-team/keras/issues/3223"" rel=""noreferrer"">https://github.com/keras-team/keras/issues/3223</a></p>

<h2>Working suggestion</h2>

<p>I don't know what you're trying to achieve, but if the idea is using Keras regularly, you'd probably never need to grab the graph.</p>

<p>In Keras, once you created your model with <code>net = tf.keras.applications.VGG16()</code>, you'd start using Keras methods from this model, such as:</p>

<pre><code>#compile for training
net.compile(optimizer=someKerasOptimizer, loss=someKerasLoss, metrics=[m1,m2])

#training
net.fit(trainingInputs, trainingTargets, epochs=..., batch_size=..., ...)    
net.fit_generator(someGeneratorThatLoadsBatches, steps_per_epoch=...., ....)

#predicting
net.predict(inputs)
net.predict_generator(someGeneratorThatLoadsInputImages, steps=howManyBatches)    
</code></pre>

<p>Accessing weights and layers would be done by:</p>

<pre><code>layer = net.layers[index]
layer = net.get_layer('layer_name')

weights = layer.get_weights()
layer.set_weights(someWeightsList)

allWeights = net.get_weights()
net.set_weights(listWithAllWeights)
</code></pre>
",Tensorflow Keras import graph def error Keras model I Tensorflow code classifying images I want convert Keras code But I trouble higher level API codes I desire The problem I stuck net get vgg model lt got tf VGG model net tf keras applications VGG g tf Graph tf Session graph g config config sess g device cpu tf import graph def net graph def name vgg code gives error Traceback recent call last File app py line lt module gt modules xvision import Xvision File app modules xvision py line lt module gt tf import graph def net graph def name vgg TypeError Model object attribute getitem Could someone help graph,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
53502036,2018-11-27,2018,2,Counting amount of people in building over time,"<p>I'm struggling in finding a ""simple"" way to perform this analysis with Pandas:</p>

<p>I have xlsx files that show the transits of people into a building.
Here after I show a simplified version of my raw data.</p>

<pre><code>       Full Name                 Time Direction
0  Uncle Scrooge  08-10-2018 09:16:52        In
1  Uncle Scrooge  08-10-2018 16:42:40       Out
2    Donald Duck  08-10-2018 15:04:07        In
3    Donald Duck  08-10-2018 15:06:42       Out
4    Donald Duck  08-10-2018 15:15:49        In
5    Donald Duck  08-10-2018 16:07:57       Out
</code></pre>

<p>My ideal final result is showing (in a tabular or better graphical way) how the total number of people into the building changes over the time.
So going back to the sample data I provided, I'd like to show that during the day 08-10-2018:</p>

<ul>
<li>before 09:16:52 there no one into the building</li>
<li>from 09:16:52 to 15:04:06 there 1 person (Uncle Scrooge)</li>
<li>from 15:04:07 to 15:06:42 there are 2 people (Uncle Scrooge and Donald Duck)</li>
<li>from 15:06:42 to 15:15:48 there is 1 person</li>
<li>from 15:15:49 to 16:07:57 there are 2 again</li>
<li>from 16:07:58 to 16:42:40 there is 1 again</li>
<li>from 16:42:41 to the end of the day there are none</li>
</ul>

<p>I used real data for that example, so you can see timestamps are accurate to the seconds, but I don't need to be that accurate, since that analysis have to be performed over a 2-months range data.</p>

<p>Any help is appreciated</p>

<p>thanks a lot</p>

<p>giorgio</p>

<p>===============
UPDATE:===============</p>

<p>@nixon and @ALollz thanks a lot you're awsome.
It' works perfectly, apart for a detail I dind't think about in my original question.</p>

<p>Infact, as I mentioned, I'm working with data spanning a period of 2 months.
Moreover, for some reason, it seems that not all the people entering the building have been tracked when exiting it.
So with the cumsum() function, I find the total number of people of a day being influenced by the people of the day before and so on, 
That shows an unjustifiable high number of people into the building during early and late hours of any days apart form the very first ones.</p>

<p>So I was thinking it could be solved by first performing a group_by on days and then appling your suggestion.</p>

<p>Could you help me in putting all together?
Thanks a lot</p>

<p>giorgio</p>
","['python', 'pandas', 'datetime']",53504384,"<p>You can start by setting the <code>Time</code> column as index, and sorting it using <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_index.html"" rel=""nofollow noreferrer""><code>sort_index</code></a>:</p>

<pre><code>df = df.set_index('Time').sort_index()
print(df)

                Direction      Full Name
Time                                        
2018-08-10 09:16:52        In  Uncle Scrooge
2018-08-10 15:04:07        In    Donald Duck
2018-08-10 15:06:42       Out    Donald Duck
2018-08-10 15:15:49        In    Donald Duck
2018-08-10 16:07:57       Out    Donald Duck
2018-08-10 16:42:40       Out  Uncle Scrooge
</code></pre>

<p>And create a mapping (as @ALollz suggests) of <code>{'In':1, 'Out':-1}</code>:</p>

<pre><code>mapper = {'In':1, 'Out':-1}
df = df.assign(Direction_mapped = df.Direction.map(mapper))
</code></pre>

<p>Which would give you:</p>

<pre><code>                   Direction      Full Name  Direction_mapped
Time                                                          
2018-08-10 09:16:52        In  Uncle Scrooge                 1
2018-08-10 15:04:07        In    Donald Duck                 1
2018-08-10 15:06:42       Out    Donald Duck                -1
2018-08-10 15:15:49        In    Donald Duck                 1
2018-08-10 16:07:57       Out    Donald Duck                -1
2018-08-10 16:42:40       Out  Uncle Scrooge                -1
</code></pre>

<p>Having mapped the Direction column, you can simply apply <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.cumsum.html"" rel=""nofollow noreferrer""><code>cumsum</code></a> to the result, which will give you the amount of people from a specific time onwards:</p>

<pre><code>df = df.assign(n_people = df.Direction_mapped.cumsum()).drop(['Direction_mapped'], axis = 1)
</code></pre>

<p>Which yields:</p>

<pre><code>                       Direction  Full Name  n_people
Time                                                  
2018-08-10 09:16:52        In  Uncle Scrooge         1
2018-08-10 15:04:07        In    Donald Duck         2
2018-08-10 15:06:42       Out    Donald Duck         1
2018-08-10 15:15:49        In    Donald Duck         2
2018-08-10 16:07:57       Out    Donald Duck         1
2018-08-10 16:42:40       Out  Uncle Scrooge         0
</code></pre>

<hr>

<h2> General solution </h2>

<p>A more general solution for the case that not everyone is tracked leaving the building. Lets try with a new df which includes more than one day. Also lets simulate this time that Donald Duck does get in twice, but is not tracked getting out on the second time:</p>

<pre><code>df = pd.DataFrame({'Full Name': ['Uncle Scrooge','Uncle Scrooge', 'Donald Duck', 'Donald Duck', 'Donald Duck',
                                 'Someone else', 'Someone else'],
                   'Time': ['08-10-2018 09:16:52','08-10-2018 16:42:40', '08-10-2018 15:04:07', '08-10-2018 15:06:42', '08-10-2018 15:15:49', 
                            '08-11-2018 10:42:40', '08-11-2018 10:48:40'],
                  'Direction': ['In','Out','In','Out', 'In','In', 'Out']})
print(df)

     Full Name          Time               Direction
0  Uncle Scrooge  08-10-2018 09:16:52        In
1  Uncle Scrooge  08-10-2018 16:42:40       Out
2    Donald Duck  08-10-2018 15:04:07        In
3    Donald Duck  08-10-2018 15:06:42       Out
4    Donald Duck  08-10-2018 15:15:49        In
5   Someone else  08-11-2018 10:42:40        In
6   Someone else  08-11-2018 10:48:40       Out
</code></pre>

<p>First the previous functionality can be encapsulated in a function</p>

<pre><code>def apply_by_day(x):
    mapper = {'In':1, 'Out':-1}
    x = x.assign(Direction_mapped = x.Direction.map(mapper))
    x = x.assign(n_people = x.Direction_mapped.cumsum())\
         .drop(['Direction_mapped'], axis = 1)
    return x
</code></pre>

<p>And then <code>apply_by_day</code> can be applied on daily groups using <a href=""https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.Grouper.html"" rel=""nofollow noreferrer""><code>pandas.Grouper</code></a>:</p>

<pre><code>df.Time = pd.to_datetime(df.Time)
df = df.set_index('Time').sort_index()
df.groupby(pd.Grouper(freq='D')).apply(lambda x: apply_by_day(x))

                                 Full Name       Direction  n_people
Time       Time                                                  
2018-08-10 2018-08-10 09:16:52  Uncle Scrooge        In         1
           2018-08-10 15:04:07    Donald Duck        In         2
           2018-08-10 15:06:42    Donald Duck       Out         1
           2018-08-10 15:15:49    Donald Duck        In         2
           2018-08-10 16:42:40  Uncle Scrooge       Out         1
2018-08-11 2018-08-11 10:42:40   Someone else        In         1
           2018-08-11 10:48:40   Someone else       Out         0
</code></pre>

<p>As the resulting dataframe shows, even though was not tracked out leaving the building on the 2018-08-10, the n_people starts from 0 on the following day, as defined function is applied for each day separately. </p>
",Counting amount people building time I struggling finding simple way perform analysis Pandas I xlsx files show transits people building Here I show simplified version raw data Full Name Time Direction Uncle Scrooge In Uncle Scrooge Out Donald Duck In Donald Duck Out Donald Duck In Donald Duck Out My ideal final result showing tabular better graphical way total number people building changes time So going back sample data I provided I like show day one building person Uncle Scrooge people Uncle Scrooge Donald Duck person end day none I used real data example see timestamps accurate seconds I need accurate since analysis performed months range data Any help appreciated thanks lot giorgio UPDATE nixon ALollz thanks lot awsome It works perfectly apart detail I dind think original question Infact I mentioned I working data spanning period months Moreover reason seems people entering building tracked exiting So cumsum function I,"startoftags, python, pandas, datetime, endoftags",python pandas dataframe endoftags,python pandas datetime,python pandas dataframe,0.67
53688988,2018-12-09,2018,18,Why does pandas merge on NaN?,"<p>I recently asked a question regarding missing values in pandas <a href=""https://stackoverflow.com/questions/53549030/comparing-pd-series-and-getting-what-appears-to-be-unusual-results-when-the-se"">here</a> and was directed to a <a href=""https://github.com/pandas-dev/pandas/issues/20442#issuecomment-375247173"" rel=""noreferrer"">github issue</a>. After reading through that page and the <a href=""http://pandas.pydata.org/pandas-docs/stable/missing_data.html"" rel=""noreferrer"">missing data documentation</a>.</p>

<p>I am wondering why <code>merge</code> and <code>join</code> treat NaNs as a match when ""they don't compare equal"": <code>np.nan != np.nan</code></p>

<pre><code># merge example
df = pd.DataFrame({'col1':[np.nan, 'match'], 'col2':[1,2]})
df2 = pd.DataFrame({'col1':[np.nan, 'no match'], 'col3':[3,4]})
pd.merge(df,df2, on='col1')

    col1    col2    col3
0   NaN      1       3

# join example with same dataframes from above
df.set_index('col1').join(df2.set_index('col1'))

      col2  col3
col1        
NaN     1   3.0
match   2   NaN
</code></pre>

<p>However, NaNs in <code>groupby</code> are excluded:</p>

<pre><code>df = pd.DataFrame({'col1':[np.nan, 'match', np.nan], 'col2':[1,2,1]})
df.groupby('col1').sum()

       col2
col1    
match   2
</code></pre>

<p>Of course you can <code>dropna()</code> or <code>df[df['col1'].notnull()]</code> but I am curious as to why NaNs are excluded in some pandas operations like <code>groupby</code> and not others like <code>merge</code>, <code>join</code>, <code>update</code>, and <code>map</code>?</p>

<p>Essentially, as I asked above, why does <code>merge</code> and <code>join</code> match on <code>np.nan</code> when they do not compare equal?</p>
","['python', 'python-3.x', 'pandas']",53719315,"<p>Yeah, this is <em>definitely</em> a bug. See <a href=""https://github.com/pandas-dev/pandas/issues/22491"" rel=""noreferrer"">GH22491</a> which documents exactly your issue, and <a href=""https://github.com/pandas-dev/pandas/issues/22618"" rel=""noreferrer"">GH22618</a> which notes the problem is also observed with <code>None</code>. based on the discussions, this does not appear to be intended behaviour.</p>

<p>A quick source dive shows that the issue *<strong>might</strong>* be inside the <a href=""https://github.com/pandas-dev/pandas/blob/master/pandas/core/reshape/merge.py#L1591-L1651"" rel=""noreferrer""><code>_factorize_keys</code></a> function in <code>pandas/core/reshape/merge.py</code>. This function appears to factorise the keys to determine what rows are to be matched with each other. </p>

<p>Specifically, this portion </p>

<pre><code># NA group
lmask = llab == -1
lany = lmask.any()
rmask = rlab == -1
rany = rmask.any()

if lany or rany:
    if lany:
        np.putmask(llab, lmask, count)
    if rany:
        np.putmask(rlab, rmask, count)
    count += 1
</code></pre>

<p>...seems to be the culprit. NaN keys are identified as a valid category (with categorical value equal to <code>count</code>).</p>

<p>Disclaimer: I am not a pandas dev, and this is only my speculation; so the real issue could be something else. But from first glance, this seems like it.</p>
",Why pandas merge NaN I recently asked question regarding missing values pandas directed github issue After reading page missing data documentation I wondering merge join treat NaNs match compare equal np nan np nan merge example df pd DataFrame col np nan match col df pd DataFrame col np nan match col pd merge df df col col col col NaN join example dataframes df set index col join df set index col col col col NaN match NaN However NaNs groupby excluded df pd DataFrame col np nan match np nan col df groupby col sum col col match Of course dropna df df col notnull I curious NaNs excluded pandas operations like groupby others like merge join update map Essentially I asked merge join match np nan compare equal,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
53764460,2018-12-13,2018,2,How best way to transform one list into new list with pairs of elements from list?,"<p>My problem was to transform the list <code>[1,2,3,4,5]</code> to <code>[[1, 2], [2, 3], [3, 4], [4, 5]]</code>.</p>

<p>I resolved it with:</p>

<pre><code>a = [1,2,3,4,5]

result = [[e, a[idx + 1]] for idx, e in enumerate(a) if idx + 1 != len(a)]
</code></pre>

<p>What the best way to do it?</p>
","['python', 'python-3.x', 'list']",53765038,"<p>You can use <a href=""https://docs.python.org/3/library/functions.html#zip"" rel=""nofollow noreferrer""><code>zip</code></a>:</p>

<pre><code>L = [1,2,3,4,5]

res = list(zip(L, L[1:]))
</code></pre>

<p>This gives a list of tuples. If a list <em>of lists</em> is a strict requirement, you can use <a href=""https://docs.python.org/3/library/functions.html#map"" rel=""nofollow noreferrer""><code>map</code></a>:</p>

<pre><code>res = list(map(list, zip(L, L[1:])))

print(res)

[[1, 2], [2, 3], [3, 4], [4, 5]]
</code></pre>

<p>For a generalised solution to your problem, see <a href=""https://stackoverflow.com/questions/6822725/rolling-or-sliding-window-iterator"">Rolling or sliding window iterator</a>.</p>
",How best way transform one list new list pairs elements list My problem transform list I resolved result e idx idx e enumerate idx len What best way,"startoftags, python, python3x, list, endoftags",python python3x list endoftags,python python3x list,python python3x list,1.0
53879149,2018-12-21,2018,3,Dataframe cummin column that starts at a specific index,"<p>I have a data frame of two columns. Date and an decimal number. 
I want to create a new column in the dataframe that displays the cummin of the decimal number column only for when time has past 9:30</p>

<p><a href=""https://i.stack.imgur.com/6L426.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6L426.png"" alt=""enter image description here""></a></p>
","['python', 'pandas', 'dataframe']",53879172,"<p>Use <code>mask</code> to mask, followed by <code>cummin</code>.</p>

<pre><code># df.index = pd.to_datetime(df.index, errors='coerce')
df['cummin'] = df.number.mask(df.index.strftime('%H:%M') &lt; '09:30').cummin()
</code></pre>

<p>You can also query the <code>hour</code> and <code>minute</code> attribute of the index to get the hours:</p>

<pre><code>df['cummin'] = df.loc[
    (df.index.hour &gt;= 9) &amp; (df.index.minute &gt; 30), 'number'].cummin()
</code></pre>

<hr>

<p>MCVE:</p>

<pre><code>df = pd.DataFrame([1.4, 4.5, 2.3], 
                  index=['9:00', '9:31', '9:45'], 
                  columns=['number'])
df.index = pd.to_datetime(df.index)
df
                     number
2018-12-21 09:00:00     1.4
2018-12-21 09:31:00     4.5
2018-12-21 09:45:00     2.3
</code></pre>

<p></p>

<pre><code>df.assign(number=(
    df.number.mask(df.index.strftime('%H:%M') &lt; '09:30').cummin()))

                     number  cummin
2018-12-21 09:00:00     NaN     NaN
2018-12-21 09:31:00     4.5     4.5
2018-12-21 09:45:00     2.3     2.3
</code></pre>

<p></p>

<pre><code>df.assign(number=df.loc[
    (df.index.hour &gt;= 9) &amp; (df.index.minute &gt; 30), 'number'].cummin())

                     number  cummin
2018-12-21 09:00:00     NaN     NaN
2018-12-21 09:31:00     4.5     4.5
2018-12-21 09:45:00     2.3     2.3
</code></pre>
",Dataframe cummin column starts specific index I data frame two columns Date decimal number I want create new column dataframe displays cummin decimal number column time past,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
53962844,2018-12-28,2018,6,Applying Regex across entire column of a Dataframe,"<p>I have a Dataframe with 3 columns:</p>

<pre><code>id,name,team 
101,kevin, marketing
102,scott,admin\n
103,peter,finance\n
</code></pre>

<p>I am trying to apply a regex function such that I remove the unnecessary spaces. I have got the code that removes these spaces how ever I am unable loop it through the entire Dataframe.</p>

<p>This is what I have tried thus far:</p>

<pre><code>df['team'] = re.sub(r'[\n\r]*','',df['team'])
</code></pre>

<p>But this throws an error <code>AttributeError: 'Series' object has no attribute 're'</code></p>

<p>Could anyone advice how could I loop this regex through the entire Dataframe <code>df['team']</code> column</p>
","['python', 'python-3.x', 'pandas']",53962889,"<p>You are almost there, there are two simple ways of doing this:</p>

<pre><code># option 1 - faster way
df['team'] =  [re.sub(r'[\n\r]*','', str(x)) for x in df['team']]

# option 2
df['team'] =  df['team'].apply(lambda x: re.sub(r'[\n\r]*','', str(x)))
</code></pre>
",Applying Regex across entire column Dataframe I Dataframe columns id name team kevin marketing scott admin n peter finance n I trying apply regex function I remove unnecessary spaces I got code removes spaces ever I unable loop entire Dataframe This I tried thus far df team sub r n r df team But throws error AttributeError Series object attribute Could anyone advice could I loop regex entire Dataframe df team column,"startoftags, python, python3x, pandas, endoftags",python pandas numpy endoftags,python python3x pandas,python pandas numpy,0.67
54057967,2019-01-06,2019,2,How to use Pandas groupby week when the week number spans more than one year,"<p>I need to groupby week, however a week like this one (the first week of the year) spans two years, 2018 and 2019.  </p>

<p>Typically I would do the following:  </p>

<pre><code>df.groupby([df.DATE.dt.year,df.DATE.dt.week]).sum()
</code></pre>

<p>which results in the single week characterized as two separate weeks in the output.  I am sure I can brute force with IF statements, however I am wondering if there is a more clean way to group by week during these year transitions.  </p>
","['python', 'pandas', 'pandas-groupby']",54058025,"<p>You can convert the dates to pandas Period objects, and then group on them.</p>

<pre><code>df = pd.DataFrame(
    {'Date': pd.DatetimeIndex(start='2018-12-24', end='2019-01-05', freq='d'),
     'Data': [1] * 8 + [2] * 5})
&gt;&gt;&gt; df
         Date  Data
0  2018-12-24     1
1  2018-12-25     1
2  2018-12-26     1
3  2018-12-27     1
4  2018-12-28     1
5  2018-12-29     1
6  2018-12-30     1
7  2018-12-31     1
8  2019-01-01     2
9  2019-01-02     2
10 2019-01-03     2
11 2019-01-04     2
12 2019-01-05     2

&gt;&gt;&gt; (df
     .assign(period=pd.PeriodIndex(df['Date'], freq='W-Sun'))  # Weekly periods ending Sundays.
     .groupby('period')['Data'].mean())
period
2018-12-24/2018-12-30    1.000000
2018-12-31/2019-01-06    1.833333  # (1 * 1 + 2 * 5) / 6 = 1.833 
Freq: W-SUN, Name: Data, dtype: float64
</code></pre>

<p>Note that there are only six days in the final period in the example above.</p>
",How use Pandas groupby week week number spans one year I need groupby week however week like one first week year spans two years Typically I would following df groupby df DATE dt year df DATE dt week sum results single week characterized two separate weeks output I sure I brute force IF statements however I wondering clean way group week year transitions,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas numpy endoftags,python pandas pandasgroupby,python pandas numpy,0.67
54124744,2019-01-10,2019,4,pandas add item to a series of list data type,"<p>How to properly add a single item to a series of list data type? I tried to make a copy and add an item to the list but this method affects the original dataframe instead</p>

<p>This is my code:</p>

<pre><code>df = pd.DataFrame({'num':[['one'],['three'],['five']]})

# make copy of original df
copy_df = df.copy()

# add 'thing' to every single list
copy_df.num.apply(lambda x: x.append('thing'))

# show results of copy_df
print(copy_df) # this will show [['one', 'thing'], ['three', 'things'], ...]

print(df) # this will also show [['one', 'thing'], ['three', 'things'], ...]
# WHY? 
</code></pre>

<p>My question is:</p>

<ol>
<li>why the method above adds element to original copy too?</li>
<li>Is there any better way to add element to a Series of list?</li>
</ol>
","['python', 'pandas', 'dataframe']",54124822,"<p>Because you are copying the dataframe but not the list in dataframe so inner series still have reference of list from original dataframe.</p>

<p>Better way to achieve it;</p>

<pre><code>copy_df.num = copy_df.num.apply(lambda x: x + ['thing'])
</code></pre>
",pandas add item series list data type How properly add single item series list data type I tried make copy add item list method affects original dataframe instead This code df pd DataFrame num one three five make copy original df copy df df copy add thing every single list copy df num apply lambda x x append thing show results copy df print copy df show one thing three things print df also show one thing three things WHY My question method adds element original copy Is better way add element Series list,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
54134300,2019-01-10,2019,3,How to sort a column of arrays based on another column of arrays in pandas?,"<p>I have a dataframe like this:</p>

<pre><code>df1= pd.DataFrame({
    'col1': [np.asarray([1,4,3,2]), np.asarray([9,10,7,5]), np.asarray([100,120,10,22])],
    'col2': [np.asarray([0,1,4,5]), np.asarray([100,101,102,103]), np.asarray([10,11,12,13])]
})

df1
                 col1                  col2
0        [1, 4, 3, 2]          [0, 1, 4, 5]
1       [9, 10, 7, 5]  [100, 101, 102, 103]
2  [100, 120, 10, 22]      [10, 11, 12, 13]
</code></pre>

<p>I want to sort the values of the arrays in column 2 based on the values of the arrays in column 1. </p>

<p>Here's my solution:</p>

<pre><code>sort_idx = df1['col1'].apply(np.argsort).values
for rowidxval, (index, row) in enumerate(df1.iterrows()):
    df1['col1'][index] = df1['col1'][index][sort_idx[rowidxval]]
    df1['col2'][index] = df1['col2'][index][sort_idx[rowidxval]]
</code></pre>

<p>Is there an elegant, pythonic way of doing it instead of brute force sort the dataframe row-wise? What if I want to re-sort more than one column based on the values in column 1? </p>
","['python', 'pandas', 'dataframe']",54134367,"<p>Lists in columns are never recommended (mixed dtypes and mutable dtypes introduce bottlenecks and performance reduction in code), but you can make this as fast as possible using a list comprehension:</p>

<pre><code>df['col2'] = [np.array(y)[np.argsort(x)] for x, y in zip(df.col1, df.col2)]
df

                 col1                  col2
0        [1, 4, 3, 2]          [0, 5, 4, 1]
1       [9, 10, 7, 5]  [103, 102, 100, 101]
2  [100, 120, 10, 22]      [12, 13, 10, 11]
</code></pre>

<hr>

<p>If they are both arrays, the solution simplifies:</p>

<pre><code>df['col2'] = [y[x.argsort()] for x, y in zip(df.col1, df.col2)]
df

                 col1                  col2
0        [1, 4, 3, 2]          [0, 5, 4, 1]
1       [9, 10, 7, 5]  [103, 102, 100, 101]
2  [100, 120, 10, 22]      [12, 13, 10, 11]
</code></pre>

<hr>

<p>For more information on performance related concerns, see the section on ""Mixed dtypes"" in <a href=""https://stackoverflow.com/questions/54028199/for-loops-with-pandas-when-should-i-care"">For loops with pandas - When should I care?</a>.</p>
",How sort column arrays based another column arrays pandas I dataframe like df pd DataFrame col np asarray np asarray np asarray col np asarray np asarray np asarray df col col I want sort values arrays column based values arrays column Here solution sort idx df col apply np argsort values rowidxval index row enumerate df iterrows df col index df col index sort idx rowidxval df col index df col index sort idx rowidxval Is elegant pythonic way instead brute force sort dataframe row wise What I want sort one column based values column,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
54165778,2019-01-13,2019,2,Pandas group by but keep another column,"<p>Say that I have a dataframe that looks something like this</p>

<pre><code>            date                                      location  year
 0    1908-09-17                           Fort Myer, Virginia  1908
 1    1909-09-07                       Juvisy-sur-Orge, France  1909
 2    1912-07-12                     Atlantic City, New Jersey  1912
 3    1913-08-06            Victoria, British Columbia, Canada  1912
</code></pre>

<p>I want to use pandas groupby function to create an output that shows the total number of incidents by year but also keep the location column that will display one of the locations that year. Any which one works. So it would look something like this:</p>

<pre><code>       total  location
 year                
 1908     1    Fort Myer, Virginia
 1909     1    Juvisy-sur-Orge, France
 1912     2    Atlantic City, New Jersey
</code></pre>

<p>Can this be done without doing funky joining? The furthest I can get is using the normal groupby</p>

<pre><code>df = df.groupby(['year']).count()
</code></pre>

<p>But that only gives me something like this</p>

<pre><code>               location
year                
1908     1         1
1909     1         1
1912     2         2
</code></pre>

<p>How can I display one of the locations in this dataframe?</p>
","['python', 'pandas', 'pandas-groupby']",54165794,"<p>You can use <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.agg.html"" rel=""nofollow noreferrer""><code>groupby.agg</code></a> and use <code>'first'</code> to extract the first location in each group:</p>

<pre><code>res = df.groupby('year')['location'].agg(['first', 'count'])

print(res)
#                           first  count
# year                                  
# 1908        Fort Myer, Virginia      1
# 1909    Juvisy-sur-Orge, France      1
# 1912  Atlantic City, New Jersey      2
</code></pre>
",Pandas group keep another column Say I dataframe looks something like date location year Fort Myer Virginia Juvisy sur Orge France Atlantic City New Jersey Victoria British Columbia Canada I want use pandas groupby function create output shows total number incidents year also keep location column display one locations year Any one works So would look something like total location year Fort Myer Virginia Juvisy sur Orge France Atlantic City New Jersey Can done without funky joining The furthest I get using normal groupby df df groupby year count But gives something like location year How I display one locations dataframe,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
54177047,2019-01-14,2019,2,How to incorporate lists as values in a dictionary?,"<p>I want to make a dictionary keeping track of all anime I've seen and manga I've read. I want the keys to be ""Anime"" and ""Manga"" and their values to be lists in which I can add the series I've seen. </p>

<pre><code>amanga = {}
#total = 0

for x in range (2):
    x = input(""Anime or Manga? "")
    print(""How many entries? "")
    n = int(input())
    for i in range (n):
        amanga[x[i]] = input(""Entry: "")

print (amanga)
</code></pre>

<p>The values are not in lists but are added just as they are. I've linked the output.</p>

<p><a href=""https://ibb.co/7b3MqBm"" rel=""nofollow noreferrer"">https://ibb.co/7b3MqBm</a></p>

<p>I want the output to be</p>

<pre><code>{'Anime' : [Monster, Legend of the Galactic Heroes],
 'Manga' : [Berserk, 20th Century Boys] }
</code></pre>
","['python', 'list', 'dictionary']",54177098,"<p>You are almost there. Try these modifications. It uses <code>defaultdict</code>, which can make your code more concise:</p>

<pre><code>from collections import defaultdict

# Create a dictionary, with the default value being `[]` (an empty list)
amanga = defaultdict(list)

for _ in range(2):
    media_type = input(""Anime or Manga? "")
    n = int(input(""How many entries?\n""))
    amanga[media_type].extend([input(""Entry: "") for _ in range(n)])

print(dict(amanga))
</code></pre>

<p>Output:</p>

<pre><code>Anime or Manga? Anime
How many entries?
2
Entry: Monster
Entry: Legend of the Galacic Heroes
Anime or Manga? Manga
How many entries?
2
Entry: Berserk
Entry: 20th Century Boys
{'Anime': ['Monster', 'Legend of the Galacic Heroes'], 'Manga': ['Berserk', '20th Century Boys']}
</code></pre>

<p>Also, you can run the same code again in the future, and it will simply add entries to <code>amanga</code>.</p>
",How incorporate lists values dictionary I want make dictionary keeping track anime I seen manga I read I want keys Anime Manga values lists I add series I seen amanga total x range x input Anime Manga print How many entries n int input range n amanga x input Entry print amanga The values lists added I linked output https ibb co b MqBm I want output Anime Monster Legend Galactic Heroes Manga Berserk th Century Boys,"startoftags, python, list, dictionary, endoftags",python python3x list endoftags,python list dictionary,python python3x list,0.67
54218633,2019-01-16,2019,2,dict.popitem() in python 3.5,"<p>In the last versions of python, the builtin dictionary {} preserves the order, just like the OrderedDict (even if it's not guaranteed to do so).</p>

<p>Does dict.popitem() always return the last key-value pair from the dictionary, or a random one?</p>
","['python', 'python-3.x', 'dictionary']",54218745,"<p>Yes. From the <a href=""https://docs.python.org/3/library/stdtypes.html#dict.popitem"" rel=""nofollow noreferrer"">Python Documentation</a> (3.7):</p>
<blockquote>
<p><strong>popitem()</strong></p>
<p>Remove and return a (key, value) pair from the dictionary. <strong>Pairs are returned in LIFO order.</strong></p>
</blockquote>
<p>The same <a href=""https://docs.python.org/3/library/collections.html#collections.OrderedDict.popitem"" rel=""nofollow noreferrer"">also applies</a> to <code>collections.OrderedDict</code> before Python 3.7.</p>
<p>However, do note that <code>dict.popitem</code> (the non-ordered version) has no guarantee in Python 3.6 or older versions.</p>
",dict popitem python In last versions python builtin dictionary preserves order like OrderedDict even guaranteed Does dict popitem always return last key value pair dictionary random one,"startoftags, python, python3x, dictionary, endoftags",python python3x list endoftags,python python3x dictionary,python python3x list,0.67
54641130,2019-02-12,2019,2,Some problem of comparing two multidimensional lists,"<p>Thanks for seeing my problem.
I designed to make a_list and erase every element in b_list if b_list have one.
But It doesn't work properly. It only works except a_list[0].
What is happening in my code?</p>

<pre><code>a_list = [[0, 1], [0, 2]]
b_list = [[0, [0, 1], [0, 2], '3', [0, 4]], ['1', [0,1], [0, 2], [0, 3], '4', [0, 5]]]

for i in b_list:
    for j in a_list:
        temp = [k for k in i if k != j]
    print(temp)

&gt;&gt;&gt; 0, [0, 1], '3', [0, 4]]
    ['1', [0, 1], [0, 3], '4', [0, 5]]
</code></pre>

<p>In this result, I cannot understand why [0, 1]s are in there!!!???</p>

<p>I want result like this</p>

<pre><code>&gt;&gt;&gt; [0, '3', [0, 4]]
&gt;&gt;&gt; ['1', [0, 3], '4', [0, 5]]
</code></pre>

<p>PLZ help me.</p>
","['python', 'python-3.x', 'list']",54641213,"<p>You were close. You can use <code>not in</code> for the elements of <code>b_list</code> to check their existence in <code>a_list</code>. </p>

<pre><code>a_list = [[0, 1], [0, 2]]
b_list = [[0, [0, 1], [0, 2], '3', [0, 4]], ['1', [0,1], [0, 2], [0, 3], '4', [0, 5]]]

for i in b_list:
    temp = [j for j in i if j not in a_list]
    print(temp)

# [0, '3', [0, 4]]
# ['1', [0, 3], '4', [0, 5]]
</code></pre>
",Some problem comparing two multidimensional lists Thanks seeing problem I designed make list erase every element b list b list one But It work properly It works except list What happening code list b list b list j list temp k k k j print temp gt gt gt In result I cannot understand I want result like gt gt gt gt gt gt PLZ help,"startoftags, python, python3x, list, endoftags",python python3x list endoftags,python python3x list,python python3x list,1.0
54687304,2019-02-14,2019,2,Python web scraping Zacks website error: [WinError 10054] An existing connection was forcibly closed by the remote host,"<p>I would like to get the data located on this page:
<a href=""https://www.zacks.com/stock/quote/MA"" rel=""nofollow noreferrer"">https://www.zacks.com/stock/quote/MA</a></p>

<p>I've tried to do this with Beautiful Soup in Python but I get an error: ""[WinError 10054] An existing connection was forcibly closed by the remote host"".</p>

<p>Can someone guide me?</p>

<pre><code>from bs4 import BeautifulSoup
import urllib
import re
import urllib.request

url = 'https://www.zacks.com/stock/quote/MA'

r = urllib.request.urlopen(url).read()
soup = BeautifulSoup(r, ""lxml"")
soup
</code></pre>

<p>Thanks!</p>
","['python', 'web-scraping', 'beautifulsoup']",54687858,"<p>Taken from this <a href=""https://stackoverflow.com/a/1434506/6418023"">answer here</a>:</p>

<blockquote>
  <p>It's fatal. The remote server has sent you a RST packet, which
  indicates an immediate dropping of the connection, rather than the
  usual handshake. This bypasses the normal half-closed state
  transition. I like this description:</p>
  
  <blockquote>
    <p>""Connection reset by peer"" is the TCP/IP equivalent of slamming the
    phone back on the hook. It's more polite than merely not replying,
    leaving one hanging. But it's not the FIN-ACK expected of the truly
    polite TCP/IP converseur.""</p>
  </blockquote>
</blockquote>

<p>This is because the User-Agent defined when making the Python Requests is not accepted by the queried site and hence the connection was dropped by the remote web server. Hence the connection reset error that you see. I tried doing a cURL request and it worked fine, so all you have to do is define your User-Agent in the header section. Something like this:</p>

<pre><code>&gt;&gt;&gt; header = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:32.0) Gecko/20100101 Firefox/32.0',}
&gt;&gt;&gt; url = 'https://www.zacks.com/stock/quote/MA'
&gt;&gt;&gt; r = requests.get(url, headers=header, verify=False)
&gt;&gt;&gt; soups = BS(r.text,""lxml"")
&gt;&gt;&gt; print(soups.prettify())
</code></pre>

<p>And then make the required get requests and I'm hoping you'll be good.</p>
",Python web scraping Zacks website error WinError An existing connection forcibly closed remote host I would like get data located page https www zacks com stock quote MA I tried Beautiful Soup Python I get error WinError An existing connection forcibly closed remote host Can someone guide bs import BeautifulSoup import urllib import import urllib request url https www zacks com stock quote MA r urllib request urlopen url read soup BeautifulSoup r lxml soup Thanks,"startoftags, python, webscraping, beautifulsoup, endoftags",python arrays numpy endoftags,python webscraping beautifulsoup,python arrays numpy,0.33
54714018,2019-02-15,2019,23,horizontal grid only (in python using pandas plot + pyplot),"<p>I would like to get only horizontal grid using pandas plot. </p>

<p>The integrated parameter of pandas only has <code>grid=True</code> or <code>grid=False</code>, so I tried with matplotlib pyplot, changing the axes parameters, specifically with this code:</p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt
fig = plt.figure()
ax2 = plt.subplot()
ax2.grid(axis='x')
df.plot(kind='bar',ax=ax2, fontsize=10, sort_columns=True)
plt.show(fig)
</code></pre>

<p>But I get no grid, neither horizontal nor vertical. Is Pandas overwriting the axes? Or am I doing something wrong?</p>
","['python', 'pandas', 'matplotlib']",54714063,"<p>Try setting the grid <strong>after</strong> plotting the DataFrame. Also, to get the horizontal grid, you need to use <code>ax2.grid(axis='y')</code>. Below is an answer using a sample DataFrame.</p>

<p>I have restructured how you define <code>ax2</code> by making use of <code>subplots</code>. </p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt

df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})

fig, ax2 = plt.subplots()

df.plot(kind='bar',ax=ax2, fontsize=10, sort_columns=True)
ax2.grid(axis='y')
plt.show()
</code></pre>

<p>Alternatively, you can also do the following: Use the axis object returned from the DataFrame plot directly to turn on the horizontal grid </p>

<pre><code>fig = plt.figure()

ax2 = df.plot(kind='bar', fontsize=10, sort_columns=True)
ax2.grid(axis='y')
</code></pre>

<p><strong>Third option</strong> as suggested by @ayorgo in the comments is to chain the two commands as</p>

<pre><code>df.plot(kind='bar',ax=ax2, fontsize=10, sort_columns=True).grid(axis='y')
</code></pre>

<p><a href=""https://i.stack.imgur.com/PKKgb.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/PKKgb.png"" alt=""enter image description here""></a></p>
",horizontal grid python using pandas plot pyplot I would like get horizontal grid using pandas plot The integrated parameter pandas grid True grid False I tried matplotlib pyplot changing axes parameters specifically code import pandas pd import matplotlib pyplot plt fig plt figure ax plt subplot ax grid axis x df plot kind bar ax ax fontsize sort columns True plt show fig But I get grid neither horizontal vertical Is Pandas overwriting axes Or I something wrong,"startoftags, python, pandas, matplotlib, endoftags",python pandas matplotlib endoftags,python pandas matplotlib,python pandas matplotlib,1.0
54733869,2019-02-17,2019,4,How to get the Rank of current row compared to previous rows,"<p>How to get the Rank of current row compared to previous rows </p>

<p>I have a dataframe like:</p>

<pre><code>Instru Price Volume
ABCD   1000  100258
ABCD   1000  100252
ABCD   1000  100168
ABCD   1000  100390
ABCD   1000  100470
ABCD   1000  100420
</code></pre>

<p>I want to get the rank of current row compared to all previous rows for Volume Column. </p>

<p>Desired Dataframe Data:</p>

<pre><code>Instru Price Volume  Rank
ABCD   1000  100258  1     =&gt; 1st Row so Rank 1
ABCD   1000  100252  2     =&gt; Rank 2 (Compare 100258,100252)
ABCD   1000  100168  3     =&gt; Rank 3 (Compare 100258,100252,100168)
ABCD   1000  100390  1     =&gt; Rank 1 (Compare 100390,100258,100252,100168)
ABCD   1000  100470  1     =&gt; Rank 1 (Compare 100470,100390,100258,100252,100168)
ABCD   1000  100420  2     =&gt; Rank 2 (Compare 100470,100420,100390,100258,100252,100168)
</code></pre>

<p>pandas.DataFrame.rank Function doesnot serve my purpose.</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",54734285,"<p>Use <a href=""https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.searchsorted.html"" rel=""noreferrer"">np.searchsorted</a> after a <em>cumulative sort</em>:</p>

<pre><code>df['Rank'] = np.array([i - np.searchsorted(sorted(df.Volume[:i]), v) for i, v in enumerate(df.Volume)]) + 1
print(df)
</code></pre>

<p><strong>Output</strong></p>

<pre><code>  Instru  Price  Volume  Rank
0   ABCD   1000  100258     1
1   ABCD   1000  100252     2
2   ABCD   1000  100168     3
3   ABCD   1000  100390     1
4   ABCD   1000  100470     1
5   ABCD   1000  100420     2
</code></pre>
",How get Rank current row compared previous rows How get Rank current row compared previous rows I dataframe like Instru Price Volume ABCD ABCD ABCD ABCD ABCD ABCD I want get rank current row compared previous rows Volume Column Desired Dataframe Data Instru Price Volume Rank ABCD gt st Row Rank ABCD gt Rank Compare ABCD gt Rank Compare ABCD gt Rank Compare ABCD gt Rank Compare ABCD gt Rank Compare pandas DataFrame rank Function doesnot serve purpose,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
54778714,2019-02-20,2019,2,python regex combine patterns with AND and group,"<p>I am trying to use regex to match something meets the following conditions:</p>

<ol>
<li>do not contain a ""//"" string</li>
<li>contain Chinese characters</li>
<li>pick up those Chinese characters</li>
</ol>

<p>I read line by line from a file:</p>

<pre><code>f = open(""test.js"", 'r')
lines = f.readlines()
for line in lines:
    matches = regex.findall(line)
    if matches:
        print(matches)
</code></pre>

<p>First I tried to match Chinese characters using following pattern:</p>

<pre><code>re.compile(r""[\u4e00-\u9fff]+"")
</code></pre>

<p>it works and give me the output:</p>

<pre><code>['ä¸è½½å¤±æå']
['ä¸è½½å¤±è´¥']
['ç»å®çå¬']
['è¯¥åè½ææªå¼æ¾']
</code></pre>

<p>Then I tried to exclude the ""//""  with the following pattern and combine it to the above pattern:</p>

<pre><code>re.compile(r""^(?=^(?:(?!//).)*$)(?=.*[\u4e00-\u9fff]+).*$"")
</code></pre>

<p>it gives me the output:</p>

<pre><code>['           showToastByText(""è¯¥åè½ææªå¼æ¾"");']
</code></pre>

<p>which is almost right but what I want is only the Chinese characters part.</p>

<p>I tried to add ""()"" but just can not pick up the part that I want.</p>

<p>Any advice will be appreciated, thanks :)</p>
","['python', 'regex', 'python-3.x']",54779037,"<p>You don't need so complex regex for just negating <code>//</code> in your input and capturing the Chinese characters that appear in sequence together. For discarding the lines containing <code>//</code> just this <code>(?!.*//)</code> negative look ahead is enough and for capturing the Chinese text, you can capture with this regex <code>[^\u4e00-\u9fff]*([\u4e00-\u9fff]+)</code> and your overall regex becomes this,</p>

<pre><code>^(?!.*//)[^\u4e00-\u9fff]*([\u4e00-\u9fff]+)
</code></pre>

<p>Where you can extract Chinese characters from first grouping pattern.</p>

<p><strong>Explanation of above regex:</strong></p>

<ul>
<li><code>^</code> - Start of string</li>
<li><code>(?!.*//)</code> - Negative look ahead that will discard the match if <code>//</code> is present in the line anywhere ahead</li>
<li><code>[^\u4e00-\u9fff]*</code> - Optionally matches zero or more non-Chinese characters</li>
<li><code>([\u4e00-\u9fff]+)</code> - Captures Chinese characters one or more and puts then in first grouping pattern.</li>
</ul>

<p><strong><a href=""https://regex101.com/r/kKjTbR/1"" rel=""nofollow noreferrer"">Demo</a></strong></p>

<p><strong>Edit: Here are sample codes showing how to capture text from group1</strong></p>

<pre><code>import re

s = '           showToastByText(""è¯¥åè½ææªå¼æ¾"");'

m = re.search(r'^(?!.*//)[^\u4e00-\u9fff]*([\u4e00-\u9fff]+)',s)
if (m):
 print(m.group(1))
</code></pre>

<p>Prints,</p>

<pre><code>è¯¥åè½ææªå¼æ¾
</code></pre>

<p><a href=""https://rextester.com/XOJ51658"" rel=""nofollow noreferrer"">Online Python Demo</a></p>

<p><strong>Edit: For extracting multiple occurrence of Chinese characters as mentioned in comment</strong></p>

<p>As you want to extract multiple occurrence of Chinese characters, you can check if the string does not contain <code>//</code> and then use <code>findall</code> to extract all the Chinese text. Here is a sample code demonstrating same,</p>

<pre><code>import re

arr = ['showToastByText(""è¯¥åè½ææªå¼æ¾"");','//showToastByText(""è¯¥åè½ææªå¼æ¾"");','showToastByText(""æªå¼æ¾"");','showToastByText(""è¯¥åè½æxxxxxxæªå¼æ¾"");']

for s in arr:
 if (re.match(r'\/\/', s)):
  print(s, ' --&gt; contains // hence not finding')
 else:
  print(s, ' --&gt; ', re.findall(r'[\u4e00-\u9fff]+',s))
</code></pre>

<p>Prints,</p>

<pre><code>showToastByText(""è¯¥åè½ææªå¼æ¾"");  --&gt;  ['è¯¥åè½ææªå¼æ¾']
//showToastByText(""è¯¥åè½ææªå¼æ¾"");  --&gt; contains // hence not finding
showToastByText(""æªå¼æ¾"");  --&gt;  ['æªå¼æ¾']
showToastByText(""è¯¥åè½æxxxxxxæªå¼æ¾"");  --&gt;  ['è¯¥åè½æ', 'æªå¼æ¾']
</code></pre>

<p><a href=""https://rextester.com/LSARS91027"" rel=""nofollow noreferrer"">Online Python demo</a></p>
",python regex combine patterns AND group I trying use regex match something meets following conditions contain string contain Chinese characters pick Chinese characters I read line line file f open test js r lines f readlines line lines matches regex findall line matches print matches First I tried match Chinese characters using following pattern compile r u e u fff works give output Then I tried exclude following pattern combine pattern compile r u e u fff gives output showToastByText almost right I want Chinese characters part I tried add pick part I want Any advice appreciated thanks,"startoftags, python, regex, python3x, endoftags",python arrays numpy endoftags,python regex python3x,python arrays numpy,0.33
54831653,2019-02-22,2019,5,Insert cells in empty Pandas DataFrame,"<p>I need to store some values in an empty Pandas Dataframe. Something like that :</p>

<pre><code>      | col1  col2  col3
------------------------
row1  |  v1    v2    v3
row2  |  v4    v5    v6
row3  |  v7    v8    v9
</code></pre>

<p>I get cells from other sources <code>ex: (row2, col3, v6)</code> and I don't know in advance how many rows and how many columns I will have.</p>

<p>I tried to fill my DataFrame this way, but it's not working :</p>

<pre><code>import pandas as pd

df = pd.DataFrame()

df[""col1""] = """"
df[""col2""] = """"
df[""col3""] = """"

df[""col1""].loc[""row1""] = ""v1""
df[""col2""].loc[""row2""] = ""v4""
df[""col3""].loc[""row3""] = ""v9""
# ...
</code></pre>

<p>When I want to display DataFrame,</p>

<pre><code>print(df)
</code></pre>

<p>it shows as an empty DataFrame.</p>

<pre><code>Empty DataFrame
Columns: [col1, col2, col3]
Index: []
</code></pre>

<p>Out of curiosity, I tried</p>

<pre><code>print(df[""col1""])
</code></pre>

<p>and I get :</p>

<pre><code>row1    v1
row2    v4
row3    v7
Name: col1, dtype: object
</code></pre>

<p>And</p>

<pre><code>print(df.loc[""row1""])
</code></pre>

<p>returns a <code>KeyError</code> exception.</p>

<p>Well, I guess I have an index problem but I don't know how to address it and <strong>I cannot use <code>df.set_index</code> after first insertion</strong> since I have other constraints.</p>

<p>I tried this too:</p>

<pre><code>df = pd.DataFrame(columns=(""some_name"",))
df.set_index(""some_name"", inplace=True)
</code></pre>

<p>but it failed too.</p>

<p>Any idea ? I think I just need to set an empty index before starting to insert data, but I don't know how.</p>
","['python', 'python-3.x', 'pandas']",54831735,"<p>This way you can add values using the <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer"">pd.loc()</a> method:
    import pandas as pd
    df = pd.DataFrame()</p>

<pre><code>df[""col1""] = """"
df[""col2""] = """"
df[""col3""] = """"

df.loc[""row1"", ""col1""] = ""v1""
df.loc[""row2"", ""col2""] = ""v4""
df.loc[""row3"", ""col3""] = ""v9""
</code></pre>

<p>Producing the following output:</p>

<p><a href=""https://i.stack.imgur.com/DZPJq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DZPJq.png"" alt=""enter image description here""></a></p>
",Insert cells empty Pandas DataFrame I need store values empty Pandas Dataframe Something like col col col row v v v row v v v row v v v I get cells sources ex row col v I know advance many rows many columns I I tried fill DataFrame way working import pandas pd df pd DataFrame df col df col df col df col loc row v df col loc row v df col loc row v When I want display DataFrame print df shows empty DataFrame Empty DataFrame Columns col col col Index Out curiosity I tried print df col I get row v row v row v Name col dtype object And print df loc row returns KeyError exception Well I guess I index problem I know address I cannot use df set index first insertion since I constraints I tried df pd DataFrame columns name df,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
54943957,2019-03-01,2019,2,Get difference between two rows in Pandas,"<p>Hi I've read a lot of question here on stackoverflow about this problem, but I have a little different task. </p>

<p>I have this DF: </p>

<pre><code>#    DateTime       Close   
1    2000-01-04    1460
2    2000-01-05    1470 
3    2000-01-06    1480
4    2000-01-07    1450  
</code></pre>

<p>I want to get the difference between each row for Close column, but storing a [1-0] value if the difference is positive or negative. I want this result: </p>

<pre><code>#    DateTime       Close  label 
1    2000-01-04    1460    1
2    2000-01-05    1470    1
3    2000-01-06    1480    1
4    2000-01-07    1450    0
</code></pre>

<p>I've done this: </p>

<pre><code>df = pd.read_csv(DATASET_path)
df['Label'] = 0
df['Label'] = (df['Close'] - df['Close'].shift(1) &gt; 1)
</code></pre>

<p>The problem is that the result is shifted by one row, so I get the difference starting by the second rows instead the first. (Also I got a boolean values [True, False] instead of 1 or 0).</p>

<p>This is what I get: </p>

<pre><code>#    DateTime       Close  label 
1    2000-01-04    1460    
2    2000-01-05    1470    True
3    2000-01-06    1480    True
4    2000-01-07    1450    True
</code></pre>

<p>Any solution? </p>

<p>Thanks</p>
","['python', 'pandas', 'dataframe']",54944026,"<p>I think you need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.diff.html"" rel=""nofollow noreferrer""><code>diff</code></a> with <code>bfill</code> for repalce first missing values by previous one, last convert mask to integers for <code>True/False</code> to <code>1/0</code> mapping:</p>

<pre><code>df['Label'] = (df['Close'].diff().bfill() &gt; 0).astype(int)
</code></pre>

<p>Verify solution:</p>

<pre><code>print (df)
     DateTime  Close
1  2000-01-04   1460
2  2000-01-05   1440 &lt;-changed value
3  2000-01-06   1480
4  2000-01-07   1450

df['Label'] = (df['Close'].diff().bfill() &gt; 0).astype(int)
print (df)
     DateTime  Close  Label
1  2000-01-04   1460      0
2  2000-01-05   1440      0
3  2000-01-06   1480      1
4  2000-01-07   1450      0
</code></pre>
",Get difference two rows Pandas Hi I read lot question stackoverflow problem I little different task I DF DateTime Close I want get difference row Close column storing value difference positive negative I want result DateTime Close label I done df pd read csv DATASET path df Label df Label df Close df Close shift gt The problem result shifted one row I get difference starting second rows instead first Also I got boolean values True False instead This I get DateTime Close label True True True Any solution Thanks,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
55009648,2019-03-05,2019,2,Beautifulsoup4 does not show the table content,"<p>I am using Beautifulsoup4 in order to scrape info in github. However, whenever I try to get the data inside a table the program just returns table tags of opening and close.</p>

<pre><code>from bs4 import BeautifulSoup as bs
import requests
import lxml

source = requests.get(""https://github.com/bitcoin-dot-org/bitcoin.org/find/master"").text
soup = bs(source, ""lxml"")
tbody = soup.find(""tbody"", class_= ""js-tree-finder-results js-navigation-container js-active-navigation-container"")
print(tbody)
</code></pre>

<p>This is what it returns:</p>

<pre><code>&lt;tbody class=""js-tree-finder-results js-navigation-container js-active-navigation-container""&gt;
&lt;/tbody&gt;
</code></pre>

<p>And here is the source code from the github link (It is just the part that concerns the issue):      
        </p>

<pre><code> &lt;tbody class=""js-tree-finder-results js-navigation-container js-active-navigation-container""&gt;&lt;tr class=""js-navigation-item tree-browser-result"" aria-selected=""false""&gt;
              &lt;td class=""icon""&gt;&lt;svg class=""octicon octicon-chevron-right"" viewBox=""0 0 8 16"" version=""1.1"" width=""8"" height=""16"" aria-hidden=""true""&gt;&lt;path fill-rule=""evenodd"" d=""M7.5 8l-5 5L1 11.5 4.75 8 1 4.5 2.5 3l5 5z""&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/td&gt;
              &lt;td class=""icon""&gt;&lt;svg class=""octicon octicon-file"" viewBox=""0 0 12 16"" version=""1.1"" width=""12"" height=""16"" aria-hidden=""true""&gt;&lt;path fill-rule=""evenodd"" d=""M6 5H2V4h4v1zM2 8h7V7H2v1zm0 2h7V9H2v1zm0 2h7v-1H2v1zm10-7.5V14c0 .55-.45 1-1 1H1c-.55 0-1-.45-1-1V2c0-.55.45-1 1-1h7.5L12 4.5zM11 5L8 2H1v12h10V5z""&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/td&gt;
              &lt;td&gt;
                &lt;a class=""css-truncate-target js-navigation-open js-tree-finder-path"" href=""https://github.com/bitcoin-dot-org/bitcoin.org/blob/master/.gitattributes""&gt;.gitattributes&lt;/a&gt;
              &lt;/td&gt;
            &lt;/tr&gt;&lt;tr class=""js-navigation-item tree-browser-result"" aria-selected=""false""&gt;
              &lt;td class=""icon""&gt;&lt;svg class=""octicon octicon-chevron-right"" viewBox=""0 0 8 16"" version=""1.1"" width=""8"" height=""16"" aria-hidden=""true""&gt;&lt;path fill-rule=""evenodd"" d=""M7.5 8l-5 5L1 11.5 4.75 8 1 4.5 2.5 3l5 5z""&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/td&gt;
              &lt;td class=""icon""&gt;&lt;svg class=""octicon octicon-file"" viewBox=""0 0 12 16"" version=""1.1"" width=""12"" height=""16"" aria-hidden=""true""&gt;&lt;path fill-rule=""evenodd"" d=""M6 5H2V4h4v1zM2 8h7V7H2v1zm0 2h7V9H2v1zm0 2h7v-1H2v1zm10-7.5V14c0 .55-.45 1-1 1H1c-.55 0-1-.45-1-1V2c0-.55.45-1 1-1h7.5L12 4.5zM11 5L8 2H1v12h10V5z""&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/td&gt;
              &lt;td&gt;
                &lt;a class=""css-truncate-target js-navigation-open js-tree-finder-path"" href=""https://github.com/bitcoin-dot-org/bitcoin.org/blob/master/.gitignore""&gt;.gitignore&lt;/a&gt;
              &lt;/td&gt;
            &lt;/tr&gt;&lt;/tbody&gt;
</code></pre>

<p>I have already tried to use different parsers and also I tried to use urblib3 instead of requests to get the source code but either ways give me the same result.</p>
","['python', 'web-scraping', 'beautifulsoup']",55013336,"<p>Probably You have search wrong <code>class</code> attribute value.Try the below <code>class</code> attribute value.</p>

<pre><code>from bs4 import BeautifulSoup as bs
import requests
import lxml

source = requests.get(""https://github.com/bitcoin-dot-org/bitcoin.org/find/master"").text
soup = bs(source, ""lxml"")
tbody = soup.find(""tbody"", class_= ""js-tree-browser-result-template"")
print(tbody)
</code></pre>

<p>Output:</p>

<pre><code>    &lt;tbody class=""js-tree-browser-result-template"" hidden=""""&gt;
    &lt;tr class=""js-navigation-item tree-browser-result""&gt;
    &lt;td class=""icon""&gt;&lt;svg aria-hidden=""true"" class=""octicon octicon-chevron-right"" height=""16"" version=""1.1"" viewbox=""0 0 8 16"" width=""8""&gt;&lt;path d=""M7.5 8l-5 5L1 11.5 4.75 8 1 4.5 2.5 3l5 5z"" fill-rule=""evenodd""&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/td&gt;
    &lt;td class=""icon""&gt;&lt;svg aria-hidden=""true"" class=""octicon octicon-file"" height=""16"" version=""1.1"" viewbox=""0 0 12 16"" width=""12""&gt;&lt;path d=""M6 5H2V4h4v1zM2 8h7V7H2v1zm0 2h7V9H2v1zm0 2h7v-1H2v1zm10-7.5V14c0 .55-.45 1-1 1H1c-.55 0-1-.45-1-1V2c0-.55.45-1 1-1h7.5L12 4.5zM11 5L8 2H1v12h10V5z"" fill-rule=""evenodd""&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/td&gt;
    &lt;td&gt;

&lt;a class=""css-truncate-target js-navigation-open js-tree-finder-path"" href=""/bitcoin-dot-org/bitcoin.org/blob/master""&gt;&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
</code></pre>
",Beautifulsoup show table content I using Beautifulsoup order scrape info github However whenever I try get data inside table program returns table tags opening close bs import BeautifulSoup bs import requests import lxml source requests get https github com bitcoin dot org bitcoin org find master text soup bs source lxml tbody soup find tbody class js tree finder results js navigation container js active navigation container print tbody This returns lt tbody class js tree finder results js navigation container js active navigation container gt lt tbody gt And source code github link It part concerns issue lt tbody class js tree finder results js navigation container js active navigation container gt lt tr class js navigation item tree browser result aria selected false gt lt td class icon gt lt svg class octicon octicon chevron right viewBox version width height aria hidden true gt lt path fill rule,"startoftags, python, webscraping, beautifulsoup, endoftags",python webscraping beautifulsoup endoftags,python webscraping beautifulsoup,python webscraping beautifulsoup,1.0
55032810,2019-03-06,2019,3,Avoid iterating through every row in Pandas for custom aggregation,"<p>Let's say I have a pandas DataFrame with two columns: <code>salary</code> and <code>food_perc</code> (the percentage of salary you spend on food). Each row corresponds to a different person.</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

# Set seed
np.random.seed(1)

# Create dataframe
df = pd.DataFrame({'salary': np.round(np.random.uniform(10000, 100000, 100), 2),
                   'food_perc': np.round(np.random.uniform(0.1, 0.9, 100), 2)})

</code></pre>

<p>I want a new column called <code>food_compare</code> where for each person, I see how their <code>food_perc</code> compares to people with similar incomes (+/- 10%). </p>

<p>Because the +/- 10% cohort will be different for every person, I don't see a way to avoid iterating through each row and creating the cohort every time, as below. </p>

<pre class=""lang-py prettyprint-override""><code>for i in df.index:

    # Isolate the cohort
    df_sub = df[(df.loc[:, 'salary'] * 0.9 &lt; df.loc[i, 'salary']) &amp;
                (df.loc[:, 'salary'] * 1.1 &gt; df.loc[i, 'salary'])]

    # Make the comparison
    df.loc[i, 'food_compare'] = np.divide(df.loc[i, 'food_perc'],
                                          np.mean(df_sub['food_perc']))
</code></pre>

<p>Subsetting the dataframe for every iteration is really not a scalable solution. <strong>Unfortunately, I can't preemptively create static bins (e.g. $10,000-$20,000, $20,001-$30,000, etc.) for the problem I'm working on.</strong></p>

<p>Is there a way to do some sort of <code>.groupby</code> when you don't have a discrete key? Otherwise I'm not sure what to do besides maybe sorting the rows by <code>salary</code> beforehand and modifying the subsetting step so it doesn't search through the whole dataframe when constructing the cohort. Thanks!</p>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",55048554,"<p>To get a count of each peer group,  you could do use this:</p>

<pre><code>data['sal_peer_group_count'] = \
     data['salary'].apply(lambda x: len(data.loc[(data['salary']&gt;.9*x) &amp; \
                                       (data['salary']&lt;1.1*x)]))
</code></pre>

<p>To get the peer group average of <code>sal_perc</code></p>

<pre><code>data['peer_group_food_perc_mean'] = \
     data['salary'].apply(lambda x: data.loc[(data['salary'] &gt;.9*x) &amp; \
                                             (data['salary'] &lt; 1.1*x), 'food_perc'].mean())
</code></pre>

<p>Keep in mind that if you have any items of <code>salary</code> that are equal to zero and you would like them to be in the same group, you will need to modify the statement to be: </p>

<pre><code>data['peer_group_food_perc_mean'] = \
     data['salary'].apply(lambda x: data.loc[(data['salary'] &gt;.9*x) &amp; \
                                             (data['salary'] &lt; 1.1*x) \
                                              if x != 0 else \
                                              (data['salary'] == 0), 'food_perc'].mean())
</code></pre>
",Avoid iterating every row Pandas custom aggregation Let say I pandas DataFrame two columns salary food perc percentage salary spend food Each row corresponds different person import pandas pd import numpy np Set seed np random seed Create dataframe df pd DataFrame salary np round np random uniform food perc np round np random uniform I want new column called food compare person I see food perc compares people similar incomes Because cohort different every person I see way avoid iterating row creating cohort every time df index Isolate cohort df sub df df loc salary lt df loc salary amp df loc salary gt df loc salary Make comparison df loc food compare np divide df loc food perc np mean df sub food perc Subsetting dataframe every iteration really scalable solution Unfortunately I preemptively create static bins e g etc problem I working Is way sort groupby discrete,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas dataframe pandasgroupby,python pandas dataframe,0.87
55038612,2019-03-07,2019,6,Creating a DataFrame from RDD while specifying DateType() in schema,"<p>I am creating a DataFrame from RDD and one of the value is a <code>date</code>. I don't know how to specify <code>DateType()</code> in schema.</p>

<p>Let me illustrate the problem at hand - </p>

<p>One way we can load the <code>date</code> into the DataFrame is by first specifying it as string and converting it to proper <code>date</code> using <a href=""http://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.functions.to_date"" rel=""noreferrer"">to_date()</a> function.</p>

<pre><code>from pyspark.sql.types import Row, StructType, StructField, StringType, IntegerType, DateType
from pyspark.sql.functions import col, to_date
values=sc.parallelize([(3,'2012-02-02'),(5,'2018-08-08')])
rdd= values.map(lambda t: Row(A=t[0],date=t[1]))

# Importing date as String in Schema
schema = StructType([StructField('A', IntegerType(), True), StructField('date', StringType(), True)])
df = sqlContext.createDataFrame(rdd, schema)

# Finally converting the string into date using to_date() function.
df = df.withColumn('date',to_date(col('date'), 'yyyy-MM-dd'))
df.show()
+---+----------+
|  A|      date|
+---+----------+
|  3|2012-02-02|
|  5|2018-08-08|
+---+----------+

df.printSchema()
root
 |-- A: integer (nullable = true)
 |-- date: date (nullable = true)
</code></pre>

<p>Is there a way, where we could use <code>DateType()</code> in the <code>schema</code> and avoid having to convert <code>string</code> to <code>date</code> explicitly?</p>

<p>Something like this -</p>

<pre><code>values=sc.parallelize([(3,'2012-02-02'),(5,'2018-08-08')])
rdd= values.map(lambda t: Row(A=t[0],date=t[1]))
# Somewhere we would need to specify date format 'yyyy-MM-dd' too, don't know where though.
schema = StructType([StructField('A', DateType(), True), StructField('date', DateType(), True)])
</code></pre>

<p><strong>UPDATE:</strong> As suggested by <em>@user10465355</em>, following code works - </p>

<pre><code>import datetime
schema = StructType([
  StructField('A', IntegerType(), True),
  StructField('date', DateType(), True)
])
rdd= values.map(lambda t: Row(A=t[0],date=datetime.datetime.strptime(t[1], ""%Y-%m-%d"")))
df = sqlContext.createDataFrame(rdd, schema)
df.show()
+---+----------+
|  A|      date|
+---+----------+
|  3|2012-02-02|
|  5|2018-08-08|
+---+----------+
df.printSchema()
root
 |-- A: integer (nullable = true)
 |-- date: date (nullable = true)
</code></pre>
","['python', 'apache-spark', 'pyspark']",55041976,"<p>Long story short, schema used with <code>RDD</code> of external object is not intended to be used that way - declared types should reflect the actual state of the data, not the desired one.</p>

<p>In other words to allow:</p>

<pre><code>schema = StructType([
  StructField('A', IntegerType(), True),
  StructField('date', DateType(), True)
])
</code></pre>

<p>the data corresponding to <code>date</code> field <a href=""https://spark.apache.org/docs/2.4.0/sql-reference.html#data-types"" rel=""noreferrer"">should use <code>datetime.date</code></a>. So for example with your <code>RDD[Tuple[int, str]]</code>:</p>

<pre><code>import datetime

spark.createDataFrame(
    # Since values from the question are just two element tuples
    # we can use mapValues to transform the ""value""
    # but in general case you'll need map
    values.mapValues(datetime.date.fromisoformat),
    schema
)
</code></pre>

<p>The closest you can get to desired behavior is to convert data (<code>RDD[Row]</code>) with JSON  reader, using <code>dicts</code></p>

<pre><code>from pyspark.sql import Row

spark.read.schema(schema).json(rdd.map(Row.asDict))
</code></pre>

<p>or better explicit JSON dumps: </p>

<pre><code>import json
spark.read.schema(schema).json(rdd.map(Row.asDict).map(json.dumps))
</code></pre>

<p>but that's of course much more expensive than explicit casting, which BTW, is easy to automate in simple cases like the one you describe:</p>

<pre><code>from pyspark.sql.functions import col

(spark
    .createDataFrame(values, (""a"", ""date""))
    .select([col(f.name).cast(f.dataType) for f in schema]))
</code></pre>
",Creating DataFrame RDD specifying DateType schema I creating DataFrame RDD one value date I know specify DateType schema Let illustrate problem hand One way load date DataFrame first specifying string converting proper date using date function pyspark sql types import Row StructType StructField StringType IntegerType DateType pyspark sql functions import col date values sc parallelize rdd values map lambda Row A date Importing date String Schema schema StructType StructField A IntegerType True StructField date StringType True df sqlContext createDataFrame rdd schema Finally converting string date using date function df df withColumn date date col date yyyy MM dd df show A date df printSchema root A integer nullable true date date nullable true Is way could use DateType schema avoid convert string date explicitly Something like values sc parallelize rdd values map lambda Row A date Somewhere would need specify date format yyyy MM dd know though schema StructType StructField,"startoftags, python, apachespark, pyspark, endoftags",python discord discordpy endoftags,python apachespark pyspark,python discord discordpy,0.33
55222420,2019-03-18,2019,4,Converting &quot;year&quot; and &quot;week of year&quot; columns to &quot;date&quot; in Pandas,"<h3>Desired way</h3>

<p>In order to convert two columns with year and week of year into date I would expect to do something like:</p>

<pre><code>df['formatted_date'] = df.year*100+df.weekofyear
df['date'] = pd.to_datetime(df['formatted_date'], format='%Y%w')
</code></pre>

<p>However, it does not work, giving <strong>ValueError</strong>:</p>

<pre><code>ValueError: unconverted data remains: 01
</code></pre>

<h3>Workaround</h3>

<p>The workaround, I have found is converting week of year into a day of year and working with year-dayofyear <code>%Y%j</code> format:</p>

<pre><code>df['formatted_date'] = df.year*1000+df.weekofyear*7-6 
df['date'] = pd.to_datetime(df['formatted_date'], format='%Y%j')
</code></pre>

<p>The first line becomes ugly, but this works fine. Week of year is in the range (00,53). 
Any ideas, why is the elegant way not working?</p>
","['python', 'pandas', 'datetime']",55222510,"<p>You need combine <code>%w</code> for day of week - <a href=""https://stackoverflow.com/a/17087427"">explanation</a> with <code>%W</code> for week:</p>

<p><a href=""http://strftime.org/"" rel=""noreferrer"">http://strftime.org/</a> for <code>%W</code>:</p>

<blockquote>
  <p>Week number of the year (Monday as the first day of the week) as a decimal number. All days in a new year preceding the first Monday are considered to be in week 0.</p>
</blockquote>

<p>And for <code>%w</code>:</p>

<blockquote>
  <p>Weekday as a decimal number, where 0 is Sunday and 6 is Saturday.</p>
</blockquote>

<hr>

<pre><code>df = pd.DataFrame({'year':[2015, 2018],
                   'weekofyear':[10,12]})

dates = df.year*100+df.weekofyear
@adde
df['date'] = pd.to_datetime(dates.astype(str) + '0', format='%Y%W%w')
print (df)

   year  weekofyear  formatted_date       date
0  2015  10          201510         2015-03-15
1  2018  12          201812         2018-03-25
</code></pre>

<p>Another solution:</p>

<pre><code>#added 0 only for demontration, you can remove it
df['formatted_date'] = df.year * 1000 + df.weekofyear * 10 + 0
df['date'] = pd.to_datetime(df['formatted_date'], format='%Y%W%w')
print (df)

   year  weekofyear  formatted_date       date
0  2015  10          2015100        2015-03-15
1  2018  12          2018120        2018-03-25
</code></pre>
",Converting quot year quot quot week year quot columns quot date quot Pandas Desired way In order convert two columns year week year date I would expect something like df formatted date df year df weekofyear df date pd datetime df formatted date format Y w However work giving ValueError ValueError unconverted data remains Workaround The workaround I found converting week year day year working year dayofyear Y j format df formatted date df year df weekofyear df date pd datetime df formatted date format Y j The first line becomes ugly works fine Week year range Any ideas elegant way working,"startoftags, python, pandas, datetime, endoftags",python pandas numpy endoftags,python pandas datetime,python pandas numpy,0.67
55281410,2019-03-21,2019,2,How to solve the problem that `TypeError: &#39;Tensor&#39; object does not support item assignment ` in Keras,"<pre><code>from keras import backend as K
from keras.optimizers import Adam
from keras.models import Model
from keras.layers.core import Dense, Activation, Flatten
from keras.layers import Input,Concatenate
from keras.layers.normalization import BatchNormalization
from keras.layers import LSTM
class MyLoss(object):
    def __init__(self, classes, filter_outlier= True ):
        self.filter_outlier = filter_outlier
        self.classes = classes

    def getMyLoss(self, y_true, y_pred):
        # number of classes
        c = self.classes
        T = np.empty((c, c))
        # predict probability on the fresh sample
        eta_corr =self.output

        # Get Matrix T
        for i in np.arange(c):
            if not self.filter_outlier:
                idx_best = np.argmax(eta_corr[:, i])
            else:
                eta_thresh = np.percentile(eta_corr[:, i], 97,
                                           interpolation='higher')
                robust_eta = eta_corr[:, i]
                robust_eta[robust_eta &gt;= eta_thresh] = 0.0
                idx_best = np.argmax(robust_eta)
            for j in np.arange(c):
                T[i, j] = eta_corr[idx_best, j]

        T_inv = K.constant(np.linalg.inv(T))
        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)
        y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())
        return -K.sum(K.dot(y_true, T_inv) * K.log(y_pred), axis=-1)


class MyModel(object):
    '''
    BiLstm ç½ç»
    '''
    def __init__(self, config):
        self.max_len = config[""max_len""]
        self.hidden_size = config[""hidden_size""]
        self.vocab_size = config[""vocab_size""]
        self.embedding_size = config[""embedding_size""]
        self.n_class = config[""n_class""]
        self.learning_rate = config[""learning_rate""]

    def build_model(self,):
        print(""building model"")
        input = Input(shape = (self.max_len, self.embedding_size))
        rnn_outputs, forward_h, forward_c, backward_h, backward_c = \
        Bidirectional(LSTM(self.hidden_size, return_sequences = True, 
                           return_state = True))(input)

        h_total = Concatenate()([forward_h, backward_h])

        # Fully connected layerï¼dense layer)
        output = Dense(self.n_class, kernel_initializer = 'he_normal')(h_total)

        # Add softmax
        output = Activation('softmax')(output)

        model = Model(inputs = input, outputs = output)        
        # My own Loss Function
        loss_fn = MyLoss(classes = self.n_class)
        self.loss = loss_fn.getLoss
        model.compile(loss = self.loss, optimizer = Adam(
            lr = self.learning_rate))
</code></pre>

<p>Error:</p>

<pre><code>---&gt; 37                 robust_eta[robust_eta &gt;= eta_thresh] = 0.0
TypeError: 'Tensor' object does not support item assignment
</code></pre>

<p>Now I don't know how to change the numpy dtype into tensor in assigning the values.</p>
","['python', 'tensorflow', 'keras']",55284098,"<p>This expression is not valid for tensors:</p>

<pre><code>robust_eta[robust_eta &gt;= eta_thresh] = 0.0
</code></pre>

<p>First, this fancy indexing syntax is not supported for Tensors. Second, Tensors are read-only objects. If you want read-write abilities, you should use <code>tf.Variable</code>.</p>

<p>But it is more practical in this case to create another Tensor. The TensorFlow equivalent of this code would be:</p>

<pre><code>robust_eta = tf.where(tf.greater(robust_eta, eta_thresh), tf.zeros_like(robust_eta), robust_eta)
</code></pre>

<p>However, this will not help you in writing a working loss function, as the next line:</p>

<pre><code>np.argmax(robust_eta)
</code></pre>

<p>Will fail expecting a ndarray. You have there a mixture of numpy and TensorFlow code. You need to stick to either Tensors or NumPy arrays. 
I think the easiest way would be to get the value of eta_corr as NumPy array at the beginning:</p>

<pre><code>eta_corr = K.eval(self.output)
</code></pre>
",How solve problem TypeError Tensor object support item assignment Keras keras import backend K keras optimizers import Adam keras models import Model keras layers core import Dense Activation Flatten keras layers import Input Concatenate keras layers normalization import BatchNormalization keras layers import LSTM class MyLoss object def init self classes filter outlier True self filter outlier filter outlier self classes classes def getMyLoss self true pred number classes c self classes T np empty c c predict probability fresh sample eta corr self output Get Matrix T np arange c self filter outlier idx best np argmax eta corr else eta thresh np percentile eta corr interpolation higher robust eta eta corr robust eta robust eta gt eta thresh idx best np argmax robust eta j np arange c T j eta corr idx best j T inv K constant np linalg inv T pred K sum pred axis keepdims,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
55317251,2019-03-23,2019,2,How to remove duplicates from list of dicts?,"<p>I have a list of dictionaries in python as follows:</p>

<pre><code>[{'category': 'software', 'name': 'irssi', 'version': '1.2.0'},
 {'category': 'software', 'name': 'irssi', 'version': '1.1.2'},
 {'category': 'software', 'name': 'hexchat', 'version': '2.14.2'}]
</code></pre>

<p>(parsing some data txt file)</p>

<p>What I wanna do:</p>

<p>If category and name are the same I wanna leave the first appearance of a package entry and remove the rest, so the final output would look like:</p>

<pre><code>[{'category': 'software', 'name': 'irssi', 'version': '1.2.0'},
{'category': 'software', 'name': 'hexchat', 'version': '2.14.2'}]
</code></pre>

<p>How should I achieve this? I tried converting the list of dictionaries to a dictionary and then iterate over it with <code>dict.items()</code> but with no luck.</p>
","['python', 'list', 'dictionary']",55317300,"<p>Use <code>itertools.groupby</code>, and take first of each group:</p>

<pre><code>def uniq(lst):
    for _, grp in itertools.groupby(lst, lambda d: (d['category'], d['name'])):
        yield list(grp)[0]
lst = [{'category': 'software', 'name': 'irssi', 'version': '1.2.0'},
       {'category': 'software', 'name': 'irssi', 'version': '1.1.2'},
       {'category': 'software', 'name': 'hexchat', 'version': '2.14.2'}]
print(list(uniq(lst))
</code></pre>
",How remove duplicates list dicts I list dictionaries python follows category software name irssi version category software name irssi version category software name hexchat version parsing data txt file What I wanna If category name I wanna leave first appearance package entry remove rest final output would look like category software name irssi version category software name hexchat version How I achieve I tried converting list dictionaries dictionary iterate dict items luck,"startoftags, python, list, dictionary, endoftags",python python3x list endoftags,python list dictionary,python python3x list,0.67
55667777,2019-04-13,2019,2,Django - how to create auth system with custom models,"<p>django default auth system uses own auth models, but I have created my own model and i want, the auth process happens there</p>

<p>My model is :</p>

<pre><code>class User(models.Model):
Nome = models.CharField(max_length=12)
Sobrenome = models.CharField(max_length=12)
Nickname = models.CharField(max_length=21,unique=True)
Email = models.CharField(max_length=40)
Password = models.CharField(max_length=20)
Created_Account_Date = models.DateTimeField(auto_now=False,auto_now_add=True)
Avatar = models.FileField(blank=True)
Number_Following = models.IntegerField(default=0,null=False)
Number_Followers = models.IntegerField(default=0,null=False)

def __str__(self):
    return ""%s%s"" %(""@"",self.Nickname)
</code></pre>

<h1>I want use it to make auth process , and not django's default</h1>
","['python', 'django', 'django-models']",55669687,"<p>You should definitely take a look at <a href=""https://docs.djangoproject.com/en/2.2/topics/auth/customizing/#using-a-custom-user-model-when-starting-a-project"" rel=""nofollow noreferrer"">Customizing Authentication</a> in the Django docs.</p>
<p>But I will try to break it down for you. You can follow along and add your specific fields:</p>
<p>First, to make a custom <code>User</code> model you need to inherit from <code>AbstractUser</code> or <code>AbstractBaseUser</code>. Keep in mind that the former provides more functionality than the latter and also requires less customization. So If you are not too familiar with Django's authentication system then I advise you inherit from <code>AbstractUser</code> to avoid unnecessary customization.</p>
<p><strong>Example:</strong></p>
<p>So let's say you want to extend the standard user model and add an <code>EmailField</code>.</p>
<pre><code>#models.py 'users' app
from django.contrib.auth.models import AbstractUser
class CustomUser(AbstractUser):
    email = models.EmailField(unique=True)
</code></pre>
<p>Now we need to let Django know to use our brand new CustomUser model.
so at our settings file we can go ahead and add:</p>
<pre><code>#settings.py
AUTH_USER_MODEL = 'users.CustomUser'
</code></pre>
<p>You may want to have users provide an E-mail address when they register. For that you also need to customize your <code>UserCreationForm</code> so in our forms file:</p>
<pre><code>#forms.py
from users.models import CustomUser
from django.contrib.auth.forms import UserCreationForm
class CustomUserCreationForm(UserCreationForm):
    email = forms.EmailField(max_length=200)
    
    class Meta:
        model = CustomUser
        fields = ('username', 'email', 'password1', 'password2')
</code></pre>
<p>Afterwards you need to go ahead and register your newly created <code>CustomUser</code> model in your <code>admin.py</code>.</p>
<p>You will most likely need to make a custom <code>UserChangeForm</code>. And possibly a custom authentication backend(If you want users to login with e-mail for example.) But that probably deserves a new question by its own.</p>
<p>I strongly suggest you read the Django authentication docs(The link at the top). It will give you a better guidance to do the proper customization you for your specific needs.</p>
<p>Alternatively, If you feel this is too complicated at the moment you can also consider creating a separate model with the custom fields you need and then declaring a <code>OneToOneField</code> to link between the standard <code>User</code> model and your new model.</p>
<p>Best of luck!</p>
",Django create auth system custom models django default auth system uses auth models I created model want auth process happens My model class User models Model Nome models CharField max length Sobrenome models CharField max length Nickname models CharField max length unique True Email models CharField max length Password models CharField max length Created Account Date models DateTimeField auto False auto add True Avatar models FileField blank True Number Following models IntegerField default null False Number Followers models IntegerField default null False def str self return self Nickname I want use make auth process django default,"startoftags, python, django, djangomodels, endoftags",python django djangorestframework endoftags,python django djangomodels,python django djangorestframework,0.67
55700991,2019-04-16,2019,3,pandas add rows to multi-index at certain level,"<p>I am attempting to get a reasonably performant solution to the following transformation on a dataframe:</p>

<p>given this dataframe:</p>

<p><a href=""https://i.stack.imgur.com/qrtTU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qrtTU.png"" alt=""enter image description here""></a></p>

<p>produce:</p>

<p><a href=""https://i.stack.imgur.com/AYej3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AYej3.png"" alt=""enter image description here""></a></p>

<p>i.e. padding a grouped level of a multi-index to a standardized length (number of rows)</p>

<p>Is there a reasonably fast way to do this on a somewhat large multi-index dataframe (~ several thousand columns and ~million rows)?</p>

<hr>

<p>Here is the given dataframe dictionary for quick reference:</p>

<pre><code>d = {'region': {0: 'intro',
  1: 'intro',
  2: 'intro',
  3: 'mid',
  4: 'mid',
  5: 'start',
  6: 'start',
  7: 'start',
  8: 'title',
  9: 'title'},
 'feat_index': {0: 9, 1: 3, 2: 0, 3: 7, 4: 8, 5: 2, 6: 4, 7: 1, 8: 6, 9: 5},
 'position_in_region': {0: 422,
  1: 5834,
  2: 8813,
  3: 3187,
  4: 9407,
  5: 997,
  6: 3154,
  7: 8416,
  8: 5408,
  9: 8421},
 'document_0': {0: 0.39,
  1: 0.79,
  2: 0.01,
  3: 0.55,
  4: 0.99,
  5: 0.67,
  6: 0.61,
  7: 0.84,
  8: 0.15,
  9: 0.23},
 'document_1': {0: 0.8,
  1: 0.06,
  2: 0.92,
  3: 0.74,
  4: 0.06,
  5: 0.96,
  6: 0.57,
  7: 0.19,
  8: 0.29,
  9: 0.24},
 'document_2': {0: 0.81,
  1: 0.15,
  2: 0.19,
  3: 0.17,
  4: 0.11,
  5: 0.34,
  6: 0.8,
  7: 0.03,
  8: 0.67,
  9: 0.46}}
df = pd.DataFrame(d).set_index(['region', 'feat_index', 'position_in_region'])
</code></pre>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",55701194,"<p>You can use <code>merge</code> with left join by helper DataFrame created by <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html"" rel=""nofollow noreferrer""><code>numpy.repeat</code></a> and <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html"" rel=""nofollow noreferrer""><code>numpy.tile</code></a>:</p>

<pre><code>#get number of new rows by Counter.most_common(1)
from collections import Counter
no_vals = Counter(df.index.labels[0]).most_common(1)[0][1]
print(no_vals)
3

df1 = pd.DataFrame({'region':np.repeat(df.index.levels[0], no_vals),
                    'id':    np.tile(np.arange(no_vals), len(np.unique(df.index.labels[0])))})
print (df1)
   region  id
0   intro   0
1   intro   1
2   intro   2
3     mid   0
4     mid   1
5     mid   2
6   start   0
7   start   1
8   start   2
9   title   0
10  title   1
11  title   2
</code></pre>

<hr>

<pre><code>#MultiIndex to columns
df = df.reset_index()

#new could with counter of regions
df.insert(1, 'id', df.groupby('region').cumcount())
#merge, remove helper id columns and create MultiIndex
df = (df1.merge(df, how='left')
         .drop('id', 1)
         .set_index(['region', 'feat_index', 'position_in_region']))
print (df)
                                      document_0  document_1  document_2
region feat_index position_in_region                                    
intro  9.0        422.0                     0.39        0.80        0.81
       3.0        5834.0                    0.79        0.06        0.15
       0.0        8813.0                    0.01        0.92        0.19
mid    7.0        3187.0                    0.55        0.74        0.17
       8.0        9407.0                    0.99        0.06        0.11
       NaN        NaN                        NaN         NaN         NaN
start  2.0        997.0                     0.67        0.96        0.34
       4.0        3154.0                    0.61        0.57        0.80
       1.0        8416.0                    0.84        0.19        0.03
title  6.0        5408.0                    0.15        0.29        0.67
       5.0        8421.0                    0.23        0.24        0.46
       NaN        NaN                        NaN         NaN         NaN
</code></pre>

<p>Another solution with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>DataFrame.reindex</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.MultiIndex.from_product.html"" rel=""nofollow noreferrer""><code>MultiIndex.from_product</code></a>:</p>

<pre><code>from collections import Counter
no_vals = Counter(df.index.labels[0]).most_common(1)[0][1]
print(no_vals)
3

mux = pd.MultiIndex.from_product([df.index.levels[0],
                                  np.arange(no_vals)], names=['region','id'])
print (mux)
MultiIndex(levels=[['intro', 'mid', 'start', 'title'], [0, 1, 2]],
           codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3], 
                  [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]],
           names=['region', 'id'])

df = df.reset_index(level=[1, 2]).set_index(df.groupby(level=0).cumcount(), append=True)
df = (df.reindex(mux).reset_index(level=1, drop=True)
        .set_index(['feat_index', 'position_in_region'], append=True))
print (df)
                                      document_0  document_1  document_2
region feat_index position_in_region                                    
intro  9.0        422.0                     0.39        0.80        0.81
       3.0        5834.0                    0.79        0.06        0.15
       0.0        8813.0                    0.01        0.92        0.19
mid    7.0        3187.0                    0.55        0.74        0.17
       8.0        9407.0                    0.99        0.06        0.11
       NaN        NaN                        NaN         NaN         NaN
start  2.0        997.0                     0.67        0.96        0.34
       4.0        3154.0                    0.61        0.57        0.80
       1.0        8416.0                    0.84        0.19        0.03
title  6.0        5408.0                    0.15        0.29        0.67
       5.0        8421.0                    0.23        0.24        0.46
       NaN        NaN                        NaN         NaN         NaN
</code></pre>
",pandas add rows multi index certain level I attempting get reasonably performant solution following transformation dataframe given dataframe produce e padding grouped level multi index standardized length number rows Is reasonably fast way somewhat large multi index dataframe several thousand columns million rows Here given dataframe dictionary quick reference region intro intro intro mid mid start start start title title feat index position region document document document df pd DataFrame set index region feat index position region,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas dataframe pandasgroupby,python pandas dataframe,0.87
55745235,2019-04-18,2019,9,How to check if all the elements in list are present in pandas column,"<p>I have a dataframe and a list:</p>

<pre><code>df = pd.DataFrame({'id':[1,2,3,4,5,6,7,8], 
    'char':[['a','b'],['a','b','c'],['a','c'],['b','c'],[],['c','a','d'],['c','d'],['a']]})

names = ['a','c']
</code></pre>

<p>I want to get rows only if both <code>a</code> and <code>c</code> both are present in <code>char</code> column.(order doesn't matter here)</p>

<p><strong>Expected Output:</strong></p>

<pre><code>       char  id                                                                                                                      
1  [a, b, c]   2                                                                                                                      
2     [a, c]   3                                                                                                                      
5  [c, a, d]   6   
</code></pre>

<p><strong>My Efforts</strong></p>

<pre><code>true_indices = []
for idx, row in df.iterrows():
    if all(name in row['char'] for name in names):
        true_indices.append(idx)


ids = df[df.index.isin(true_indices)]
</code></pre>

<p>Which is giving me correct output but it is too slow for large dataset so I am looking for more efficient solution.</p>
","['python', 'python-3.x', 'pandas']",55745363,"<p>You can build a set from the list of names for a faster lookup, and use <a href=""https://docs.python.org/2/library/stdtypes.html#set"" rel=""nofollow noreferrer""><code>set.issubset</code></a> to check if all elements in the set are contained in the column lists:</p>

<pre><code>names = set(['a','c'])
df[df['char'].map(names.issubset)]

   id       char
1   2  [a, b, c]
2   3     [a, c]
5   6  [c, a, d]
</code></pre>
",How check elements list present pandas column I dataframe list df pd DataFrame id char b b c c b c c c names c I want get rows c present char column order matter Expected Output char id b c c c My Efforts true indices idx row df iterrows name row char name names true indices append idx ids df df index isin true indices Which giving correct output slow large dataset I looking efficient solution,"startoftags, python, python3x, pandas, endoftags",python python3x pandas endoftags,python python3x pandas,python python3x pandas,1.0
55756126,2019-04-19,2019,4,Choose a value from a set of columns based on value and create new column with the value?,"<p>so if I have a pandas Dataframe like:</p>

<pre><code>   A  B  C  D
0  1  2  3  a 
1  2  4  6  a
2  4  8  8  b
3  2  3  5  c
</code></pre>

<p>and want to insert row 'E' by choosing from columns 'A', 'B', or 'C' based on conditions in column 'D', how would I go about doing this? For example: if D == a, choose 'A', else choose 'B', outputting:</p>

<pre><code>   A  B  C  D  E
0  1  2  3  a  1
1  2  4  6  a  2
2  4  8  8  b  8
3  2  3  5  c  3
</code></pre>

<p>Thanks in advance!</p>
","['python', 'pandas', 'dataframe']",55756206,"<p>This is <code>lookup</code> </p>

<pre><code>df.lookup(df.index,df.D.str.upper())
Out[749]: array([1, 2, 8, 5], dtype=int64)

df['E']=df.lookup(df.index,df.D.str.upper())
</code></pre>
",Choose value set columns based value create new column value I pandas Dataframe like A B C D b c want insert row E choosing columns A B C based conditions column D would I go For example D choose A else choose B outputting A B C D E b c Thanks advance,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
55781609,2019-04-21,2019,2,How to fill out missing time data by using pandas,"<p>I have a pd data series like below. For some reasons, it doesn't have data through 2018-07-26 13:30:00 ~ 2018-08-03 15:45:00</p>

<pre><code>13     2018-03-13 16:40:00      12   12.07          0       
14     2018-03-13 16:41:00      13   12.07          0       
15     2018-03-13 16:42:00      12   12.07          0       
 â¦
230000 2018-07-26 13:30:00      45   12.07          0
230001 2018-08-03 15:45:00      30   12.07          0
230002 ....
â¦
</code></pre>

<p>I wanted to fill these blank out with 0 and tried ""pandas.Series.asfreq"" like this</p>

<pre><code>df1= df.asfreq(""T"",fill_value=0)
print(df1)
</code></pre>

<p>but it gave me a weird response like below.</p>

<pre><code>1970-01-01       0    0    0
</code></pre>

<p>Could you teach me how to fill out those blanks?</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",55781668,"<p>IIUC, I believe you need to use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>DataFrame.set_index</code></a> first, setting the index to your <code>datetime</code> column.</p>

<p>Here is a basic example, but you would substitute <code>'datetime_col'</code> for the actual name of your own datetime column:</p>

<pre><code># If necessary, cast datetime column to correct dtype
# df['datetime_col'] = pd.to_datetime(df['datetime_col']) 

df_new = df.set_index('datetime_col').asfreq('T', fill_value=0).reset_index()
</code></pre>
",How fill missing time data using pandas I pd data series like For reasons data I wanted fill blank tried pandas Series asfreq like df df asfreq T fill value print df gave weird response like Could teach fill blanks,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
55823283,2019-04-24,2019,6,Why does a PySpark UDF that operates on a column generated by rand() fail?,"<p>Given the following Python function:</p>

<pre><code>def f(col):
    return col
</code></pre>

<p>If I turn it into a UDF and apply it to a column object, it works...</p>

<pre><code>from pyspark.sql import functions as F
from pyspark.sql.types import DoubleType

df = spark.range(10)
udf = F.udf(f, returnType=DoubleType()).asNondeterministic()

df.withColumn('new', udf(F.lit(0))).show()
</code></pre>

<p>...Except if the column is generated by <code>rand</code>:</p>

<pre><code>df.withColumn('new', udf(F.rand())).show()  # fails
</code></pre>

<p><em>However</em>, the following two work:</p>

<pre><code>df.withColumn('new', F.rand()).show()
df.withColumn('new', F.rand()).withColumn('new2', udf(F.col('new'))).show()
</code></pre>

<p>The error:</p>

<pre><code>Py4JJavaError: An error occurred while calling o469.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 34, localhost, executor driver): java.lang.NullPointerException
</code></pre>

<p><strong>Why does this happen, and how can I use a <code>rand</code> column expression created within a UDF?</strong></p>
","['python', 'apache-spark', 'pyspark']",56056443,"<p>The core issue is that the rand() function on the JVM side depends on a transient rng variable that doesn't survive serialization/deserialization coupled with an <code>eval</code> implementation that is null unsafe (defined in RDG class and Rand subclass <a href=""https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/randomExpressions.scala"" rel=""noreferrer"">here</a>). As far as I can tell, <code>rand()</code> and <code>randn()</code> are the only functions with these specific properties in spark</p>

<p>When you write <code>udf(F.rand())</code> spark evaluates this as a single PythonUDF expression and thus serialize the rand() invocation in the command_pickle, losing the initialized transient along the way. This can be observed with the execution plan:</p>

<pre><code>df.withColumn('new', udf(F.rand())).explain()

== Physical Plan ==
*(2) Project [id#0L, pythonUDF0#95 AS new#92]
+- BatchEvalPython [f(rand(-6878806567622466209))], [id#0L, pythonUDF0#95]
   +- *(1) Range (0, 10, step=1, splits=8)
</code></pre>

<p>Unfortunately, you're unlikely to overcome this issue without a fix in spark to make the Rand class null safe, however if you just need to generate random numbers you can trivially build your own rand() udf around Python random generator:</p>

<pre><code>from pyspark.sql import functions as F
from pyspark.sql.types import DoubleType
from random import random

def f(col):
    return col

df = spark.range(10)
udf = F.udf(f, returnType=DoubleType()).asNondeterministic()
rand = F.udf(random, returnType=DoubleType()).asNondeterministic()

df.withColumn('new', udf(rand())).show()

+---+-------------------+
| id|                new|
+---+-------------------+
|  0| 0.4384090392727712|
|  1| 0.5827392568376621|
|  2| 0.4249312702725516|
|  3| 0.8423409231783007|
|  4|0.39533981334524604|
|  5| 0.7073194901736066|
|  6|0.19176164335919255|
|  7| 0.7296698171715453|
|  8|  0.799510901886918|
|  9|0.12662129139761658|
+---+-------------------+
</code></pre>
",Why PySpark UDF operates column generated rand fail Given following Python function def f col return col If I turn UDF apply column object works pyspark sql import functions F pyspark sql types import DoubleType df spark range udf F udf f returnType DoubleType asNondeterministic df withColumn new udf F lit show Except column generated rand df withColumn new udf F rand show fails However following two work df withColumn new F rand show df withColumn new F rand withColumn new udf F col new show The error Py JJavaError An error occurred calling showString org apache spark SparkException Job aborted due stage failure Task stage failed times recent failure Lost task stage TID localhost executor driver java lang Why happen I use rand column expression created within UDF,"startoftags, python, apachespark, pyspark, endoftags",python discord discordpy endoftags,python apachespark pyspark,python discord discordpy,0.33
55862019,2019-04-26,2019,2,Extract domain name from URL using python&#39;s re regex,"<p>I want to input a URL and extract the domain name which is the string that comes after http:// or https:// and contains strings, numbers, dots, underscores, or dashes. </p>

<p>I wrote the regex and used the python's <code>re</code> module as follows:</p>

<pre><code>import re
m = re.search('https?://([A-Za-z_0-9.-]+).*', 'https://google.co.uk?link=something')
m.group(1)
print(m)
</code></pre>

<p>My understanding is that <code>m.group(1)</code> will extract the part between () in the re.search. </p>

<p>The output that I expect is: <code>google.co.uk</code>
But I am getting this:</p>

<pre><code>&lt;_sre.SRE_Match object; span=(0, 35), match='https://google.co.uk?link=something'&gt;
</code></pre>

<p>Can you point to me how to use <code>re</code> to achieve my requirement?</p>
","['python', 'regex', 'python-3.x']",55862033,"<p>You need to write</p>

<pre><code>print(m.group(1))
</code></pre>

<p>Even better yet - have a condition before:</p>

<pre><code>m = re.search('https?://([A-Za-z_0-9.-]+).*', 'https://google.co.uk?link=something')
if m:
    print(m.group(1))
</code></pre>
",Extract domain name URL using python regex I want input URL extract domain name string comes http https contains strings numbers dots underscores dashes I wrote regex used python module follows import search https A Za z https google co uk link something group print My understanding group extract part search The output I expect google co uk But I getting lt sre SRE Match object span match https google co uk link something gt Can point use achieve requirement,"startoftags, python, regex, python3x, endoftags",python arrays numpy endoftags,python regex python3x,python arrays numpy,0.33
55908329,2019-04-29,2019,4,Tensorflow - ValueError: Output tensors to a Model must be the output of a TensorFlow `Layer`,"<p>I have created an RNN with the Keras functional API in TensorFlow 2.0 where the following piece of code workes</p>

<pre class=""lang-py prettyprint-override""><code>sum_input = keras.Input(shape=(UNIT_SIZE, 256,), name='sum')
x         = tf.unstack(sum_input,axis=2, num=256)
t_sum     = x[0]
for i in range(len(x) - 1):
    t_sum = keras.layers.Add()([t_sum, x[i+1]])
sum_m     = keras.Model(inputs=sum_input, outputs=t_sum, name='sum_model')
</code></pre>

<p>I then had to changed to Tensorflow 1.13 which gives me the following error</p>

<pre><code>ValueError: Output tensors to a Model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: Tensor(""add_254/add:0"", shape=(?, 40), dtype=float32)
</code></pre>

<p>I don't understand why the output tensor is not from a Tensorflow layer, since t_sum is the output from keras.layers.Add.</p>

<p>I have tried to wrap parts of the code into keras.layers.Lambda as suggested in <a href=""https://stackoverflow.com/questions/50715928/valueerror-output-tensors-to-a-model-must-be-the-output-of-a-tensorflow-layer"">ValueError: Output tensors to a Model must be the output of a TensorFlow Layer
</a>, but it doesn't seem to work for me. </p>
","['python', 'tensorflow', 'keras']",55909446,"<p>The problem is not with <code>Add()</code> layer but with <code>tf.unstack()</code> - it is not an instance of <code>keras.layers.Layer()</code>. You can just wrap it up as custom layer:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

class Unstack(tf.keras.layers.Layer):
    def __init__(self):
        super(Unstack, self).__init__()
    def call(self, inputs, num=256):
        return tf.unstack(inputs, axis=2, num=num)

x = Unstack()(sum_input)
</code></pre>

<p>or, instead of subclassing, you can do it using <code>Lambda</code> layer:</p>

<pre class=""lang-py prettyprint-override""><code>x = tf.keras.layers.Lambda(lambda t: tf.unstack(t, axis=2, num=256))(sum_input)
</code></pre>
",Tensorflow ValueError Output tensors Model must output TensorFlow Layer I created RNN Keras functional API TensorFlow following piece code workes sum input keras Input shape UNIT SIZE name sum x tf unstack sum input axis num sum x range len x sum keras layers Add sum x sum keras Model inputs sum input outputs sum name sum model I changed Tensorflow gives following error ValueError Output tensors Model must output TensorFlow Layer thus holding past layer metadata Found Tensor add add shape dtype float I understand output tensor Tensorflow layer since sum output keras layers Add I tried wrap parts code keras layers Lambda suggested ValueError Output tensors Model must output TensorFlow Layer seem work,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
56127045,2019-05-14,2019,2,Convert list of identical dictionaries to Dataframe,"<p>I have a list like this:</p>

<pre><code>[{'FirstOfficer': '1'}, {'SecondOfficer': '2'}, {'ThirdOfficer': '3'},{'FirstOfficer': '4'}, {'SecondOfficer': '5'}, {'ThirdOfficer': '6'},{'FirstOfficer': '7'}, {'SecondOfficer': '8'}, {'ThirdOfficer': '9'},{'FirstOfficer': '10'}, {'SecondOfficer': '11'}, {'ThirdOfficer': '12'}]
</code></pre>

<p>I wanted to convert this into a dataframe but i got the dataframe like this:</p>

<pre><code>   FirstOfficer SecondOfficer ThirdOfficer
0             1           NaN          NaN
1           NaN             2          NaN
2           NaN           NaN            3
3             4           NaN          NaN
4           NaN             5          NaN
5           NaN           NaN            6
6             7           NaN          NaN
7           NaN             8          NaN
8           NaN           NaN            9
9            10           NaN          NaN
10          NaN            11          NaN
11          NaN           NaN           12
</code></pre>

<p>the columns name can be anything, so I am not able to hard code it.</p>

<p>Expected dataframe is:</p>

<pre><code>   FirstOfficer SecondOfficer ThirdOfficer
0             1           2          3
1             4           5          6
2             7           8          9
3            10          11         12
</code></pre>

<p>Can anybody suggest me a solution for it?</p>

<p>Any help would be appreciated.</p>
","['python', 'pandas', 'dataframe']",56127145,"<p>Use <code>defaultdict</code> for store values to list by keys of dictionaries:</p>

<pre><code>from collections import defaultdict

d = defaultdict(list)
for x in L:
    a, b = tuple(x.items())[0]
    d[a].append(b)
print (d)


df = pd.DataFrame(d)
print (df)
  FirstOfficer SecondOfficer ThirdOfficer
0            1             2            3
1            4             5            6
2            7             8            9
3           10            11           12
</code></pre>
",Convert list identical dictionaries Dataframe I list like FirstOfficer SecondOfficer ThirdOfficer FirstOfficer SecondOfficer ThirdOfficer FirstOfficer SecondOfficer ThirdOfficer FirstOfficer SecondOfficer ThirdOfficer I wanted convert dataframe got dataframe like FirstOfficer SecondOfficer ThirdOfficer NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN columns name anything I able hard code Expected dataframe FirstOfficer SecondOfficer ThirdOfficer Can anybody suggest solution Any help would appreciated,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
56289270,2019-05-24,2019,7,Mask-RCNN with Keras : Tried to convert &#39;shape&#39; to a tensor and failed. Error: None values not supported,"<p>I am trying to run the <a href=""https://github.com/matterport/Mask_RCNN/"" rel=""noreferrer"">Keras implemention of Mask_RCNN</a> in inference mode. It is basically running this code <a href=""https://github.com/matterport/Mask_RCNN/blob/master/samples/demo.ipynb"" rel=""noreferrer"">demo.ipynb</a>  </p>

<p>But when I run it, I get the following error on model creation : </p>

<pre><code>ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.
</code></pre>

<p>And here is the stacktrace : </p>

<pre><code>Traceback (most recent call last):
  File ""/snap/pycharm-community/128/helpers/pydev/pydevd.py"", line 1758, in &lt;module&gt;
    main()
  File ""/snap/pycharm-community/128/helpers/pydev/pydevd.py"", line 1752, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/snap/pycharm-community/128/helpers/pydev/pydevd.py"", line 1147, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/snap/pycharm-community/128/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""[PATH/TO/Mask_RCNN/]/Own_code/Test.py"", line 44, in &lt;module&gt;
    model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)
  File ""[PATH/TO/Mask_RCNN/]/Mask_RCNN/mrcnn/model.py"", line 1833, in __init__
    self.keras_model = self.build(mode=mode, config=config)
  File ""[PATH/TO/Mask_RCNN/]/Mask_RCNN/mrcnn/model.py"", line 2035, in build
    fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)
  File ""[PATH/TO/Mask_RCNN/]/Mask_RCNN/mrcnn/model.py"", line 947, in fpn_classifier_graph
    mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=""mrcnn_bbox"")(x)
  File ""[PATH/TO/venv/]lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 554, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""[PATH/TO/venv/]lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py"", line 439, in call
    (array_ops.shape(inputs)[0],) + self.target_shape)
  File ""[PATH/TO/venv/]lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 7179, in reshape
    ""Reshape"", tensor=tensor, shape=shape, name=name)
  File ""[PATH/TO/venv/]lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 529, in _apply_op_helper
    (input_name, err))
</code></pre>

<p>It look like my problem is the same as described <a href=""https://github.com/matterport/Mask_RCNN/issues/1070"" rel=""noreferrer"">here</a>, but there is no real answer to it.</p>

<p>Notes : </p>

<ul>
<li>I am running it with Python 3.6 and Tensorflow 1.13.1</li>
<li>I have edited the Keras imports to use the Keras version embedded in Tensorflow</li>
<li>The code that I run to have this error correspond to the 3 first blocks of the demo notebook. </li>
</ul>

<p>EDIT </p>

<p>Here is the code I am running : </p>

<pre><code># Root directory of the project
ROOT_DIR = os.path.abspath(""../"")

# Import Mask RCNN
sys.path.append(ROOT_DIR)  # To find local version of the library

# Import COCO config
sys.path.append(os.path.join(ROOT_DIR, ""samples/coco/""))  # To find local version

# Directory to save logs and trained model
MODEL_DIR = os.path.join(ROOT_DIR, ""logs"")

# Local path to trained weights file
COCO_MODEL_PATH = os.path.join(ROOT_DIR, ""mask_rcnn_coco.h5"")
# Download COCO trained weights from Releases if needed
if not os.path.exists(COCO_MODEL_PATH):
    utils.download_trained_weights(COCO_MODEL_PATH)

# Directory of images to run detection on
IMAGE_DIR = os.path.join(ROOT_DIR, ""images"")

class InferenceConfig(coco.CocoConfig):
    # Set batch size to 1 since we'll be running inference on
    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1

config = InferenceConfig()

# Create model object in inference mode.
model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)
</code></pre>

<p>This is really a copy/paste of the <a href=""https://github.com/matterport/Mask_RCNN/blob/master/samples/demo.ipynb"" rel=""noreferrer"">demo.ipynb</a> from the maskRCNN repo. The last line is where the error occur.</p>
","['python', 'tensorflow', 'keras']",56536441,"<p>I got the same error, until :</p>

<ol>
<li>Using <code>keras.__version__ == 2.2.4</code></li>
<li><code>Tensorflow-gpu version == 1.12.0</code></li>
<li><code>Skimage.__version__ == 0.14.2</code></li>
</ol>

<p>Not sure what is the error with rest versions, probably something does not fit with <code>pycocotools</code> or </p>

<pre><code>optimizer = keras.optimizers.SGD(
            lr=learning_rate, momentum=momentum,
            clipnorm=self.config.GRADIENT_CLIP_NORM) #this portion.
</code></pre>
",Mask RCNN Keras Tried convert shape tensor failed Error None values supported I trying run Keras implemention Mask RCNN inference mode It basically running code demo ipynb But I run I get following error model creation ValueError Tried convert shape tensor failed Error None values supported And stacktrace Traceback recent call last File snap pycharm community helpers pydev pydevd py line lt module gt main File snap pycharm community helpers pydev pydevd py line main globals debugger run setup file None None module File snap pycharm community helpers pydev pydevd py line run pydev imports execfile file globals locals execute script File snap pycharm community helpers pydev pydev imps pydev execfile py line execfile exec compile contents n file exec glob loc File PATH TO Mask RCNN Own code Test py line lt module gt model modellib MaskRCNN mode inference model dir MODEL DIR config config File PATH TO Mask,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
56356111,2019-05-29,2019,2,Getting values from time indexed pandas dataframe for a specific time within the two timestamps,"<p>I have the following pandas dataframe df:</p>

<pre><code>                     C1  C2   C3
Date                             
2000-01-01 00:00:00   2  175  160
2000-01-01 01:00:00   4  192  164
2000-01-01 02:00:00   6  210  189
2000-01-01 03:00:00   8  217  199
2000-01-01 04:00:00  10  176  158
</code></pre>

<p>from which I need to get the value of C1, C2 and C3 for a specific datetime:</p>

<pre><code>import datetime
my_specific_time = str(datetime.datetime(2000, 1, 1, 1, 0, 0))
print(df['C1'].loc[mytime]) # prints 4
</code></pre>

<p>The problem is that I can only get values for the dates stored in the df. For example, getting the value of <code>C1</code> for time <code>2000-01-01 01:30:00</code> is not possible unless I resample my dataframe:</p>

<pre><code>upsampled = df.resample('30min').ffill()
my_specific_time = str(datetime.datetime(2000, 1, 1, 1, 30, 0))
print(upsampled['C1'].loc[mytime]) # again prints 4
</code></pre>

<p>Please note that all the value of <code>C1</code> between timespan of <code>2000-01-01 01:00:00</code> and <code>2000-01-01 02:00:00</code> is <code>4</code>. Now the problem is that <code>my_specific_time</code> can be any random time and I would need to resample df using small enough values to be able to get the value for. I think this is not the best solution for this problem. </p>

<p>While looking for possible solutions I only came across <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html"" rel=""nofollow noreferrer"">time spans</a> in pandas but I did not quite understand how possibly I can use it in my problem. </p>
","['python', 'pandas', 'datetime']",56356323,"<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.asof.html"" rel=""nofollow noreferrer""><code>DataFrame.asof</code></a> method:</p>

<pre><code>print(df['C1'].asof(my_specific_time))

4
</code></pre>
",Getting values time indexed pandas dataframe specific time within two timestamps I following pandas dataframe df C C C Date I need get value C C C specific datetime import datetime specific time str datetime datetime print df C loc mytime prints The problem I get values dates stored df For example getting value C time possible unless I resample dataframe upsampled df resample min ffill specific time str datetime datetime print upsampled C loc mytime prints Please note value C timespan Now problem specific time random time I would need resample df using small enough values able get value I think best solution problem While looking possible solutions I came across time spans pandas I quite understand possibly I use problem,"startoftags, python, pandas, datetime, endoftags",python pandas dataframe endoftags,python pandas datetime,python pandas dataframe,0.67
56690465,2019-06-20,2019,3,Is keras based on closures in python?,"<p>While working with keras and tensorflow, I found the following lines of code confusing.</p>

<pre><code>w_init = tf.random_normal_initializer()
self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units),
                                          dtype='float32'),trainable=True)
</code></pre>

<p>Also, I have seen something like:</p>

<pre><code> Dense(64, activation='relu')(x)
</code></pre>

<p>Therefore, if <code>Dense(...)</code> will create the object for me, then how can I follow that with with <code>(x)</code>?</p>

<p>Likewise for <code>w_init</code> above. How can I say such thing: </p>

<pre><code>tf.random_normal_initializer()(shape=(input_dim, units), dtype='float32'),trainable=True)
</code></pre>

<p>Do we have such thing in python <code>""ClassName()"" followed by ""()""</code> while creating an object such as a layer?</p>

<p>While I was looking into <a href=""https://en.wikibooks.org/wiki/Python_Programming/Functions"" rel=""nofollow noreferrer"">Closures</a> in python, I found that a function can return another function. Hence, is this what really happens in Keras?</p>

<p>Any help is much appreciated!!</p>
","['python', 'tensorflow', 'keras']",56691467,"<p>These are two totally different ways to define models.</p>

<h3>Keras</h3>

<p>Keras works with the concept of layers. Each line defines a full layer of your network. What you are referring to in specific is keras' functional API. The concept is to combine layers like this:</p>

<pre class=""lang-py prettyprint-override""><code>
inp = Input(shape=(28, 28, 1))
x = Conv2D((6,6), strides=(1,1), activation='relu')(inp)
# ... etc ...
x = Flatten()(x)
x = Dense(10, activation='softmax')(x)


model = Model(inputs=[inp], outputs=[x])
</code></pre>

<p>This way you've created a full CNN in just a few lines. Note that you <strong>never</strong> had to manually input the shape of the weight vectors or the operations that are performed. These are inferred automatically by keras.</p>

<p>Now, this just needs to be <em>compiled</em> through <code>model.compile(...)</code> and then you can train it through <code>model.fit(...)</code>.</p>

<h3>Tensorflow</h3>

<p>On the other hand TensorFlow is a bit more low-level. This means that you have do define the variables and operations by hand. So in order to write a fully-connected layer you'd have to do the following:</p>

<pre class=""lang-py prettyprint-override""><code># Input placeholders
x = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))
y = tf.placeholder(tf.float32, shape=(None, 10))

# Convolution layer
W1 = tf.Variable(tf.truncated_normal([6, 6, 1, 32], stddev=0.1)) 
b1 = tf.Variable(tf.constant(0.1, tf.float32, [32]))
z1 = tf.nn.conv2d(x_2d, W1, strides=[1, 1, 1, 1], padding='SAME') + b1 
c1 = tf.nn.relu(z1)

# ... etc ...

# Flatten
flat = tf.reshape(p2, [-1, ...]) # need to calculate the ... by ourselves

# Dense
W3 = tf.Variable(tf.truncated_normal([..., 10], stddev=0.1))  # same size as before
b3 = tf.Variable(tf.constant(0.1, tf.float32, [10]))
fc1 = tf.nn.relu(tf.matmul(flat, W3) + b3)
</code></pre>

<p>Two things to note here. There is no explicit definition of a <code>model</code> here and this has to be trained through a <code>tf.Session</code> with a <code>feed_dict</code> feeding the data to the placeholders. If you're interested you'll find several guides online.</p>

<h3>Closing notes...</h3>

<p>TensorFlow has a much friendlier and easier way to define and train models through <strong>eager execution</strong>, which will be default in TF 2.0! So the code you posted is in a sense the <em>old way</em> of doing things in tensorflow. It's worth taking a look into TF 2.0, which actually recommends doing things the keras way!</p>

<hr>

<p>Edit (after comment by OP):</p>

<p>No a layer is <strong>not a clojure</strong>. A keras layer is a class that implements a <code>__call__</code> method which also makes it callable. The way they did it was so that it is a wrapper to the <code>call</code> method that users typically write.</p>

<p>You can take a look at the implementation <a href=""https://github.com/keras-team/keras/blob/master/keras/engine/base_layer.py#L376"" rel=""nofollow noreferrer"">here</a></p>

<p>Basically how this works is:</p>

<pre class=""lang-py prettyprint-override""><code>class MyClass:
    def __init__(self, param):
        self.p = param
    def call(self, x):
        print(x)
</code></pre>

<p>If you try to write <code>c = MyClass(1)(3)</code>, you'll get a TypeError saying that MyClass is not callable. But if you write it like this:</p>

<pre class=""lang-py prettyprint-override""><code>class MyClass:
    def __init__(self, param):
        self.p = param
    def __call__(self, x):
        print(x)
</code></pre>

<p>It works now. Essentially keras does it like this:</p>

<pre class=""lang-py prettyprint-override""><code>class MyClass:
    def __init__(self, param):
        self.p = param
    def call(self, x):
        print(x)
    def __call__(self, x):
        self.call(x)
</code></pre>

<p>So that when you write your own layer you can implement your own <code>call</code> method and the <code>__call__</code> method that wraps your one will get inherited from keras' base Layer class.</p>
",Is keras based closures python While working keras tensorflow I found following lines code confusing w init tf random normal initializer self w tf Variable initial value w init shape input dim units dtype float trainable True Also I seen something like Dense activation relu x Therefore Dense create object I follow x Likewise w init How I say thing tf random normal initializer shape input dim units dtype float trainable True Do thing python ClassName followed creating object layer While I looking Closures python I found function return another function Hence really happens Keras Any help much appreciated,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
56717184,2019-06-22,2019,2,pygame.event.get() not returning any events when inside a thread,"<p>So I have this code that looks after the user inputs for a pac-man style game.</p>

<pre><code>def receiving_inputs(self):
    while True:
        events = pg.event.get()
        for event in events:
            if event.type == pg.KEYDOWN:
                if event.key == pg.K_UP:
                    self.move = 'n'
                elif event.key == pg.K_RIGHT:
                    self.move = 'e'
                elif event.key == pg.K_DOWN:
                    self.move = 's'
                elif event.key == pg.K_LEFT:
                    self.move = 'w'
        time.sleep(1/60)

threading.Thread(target=self.receiving_inputs).start()
</code></pre>

<p>When I press any keys on my keyboard I do not get any events, however, moving the mouse around will return an event using this code.</p>

<p>The annoying thing is that this exact code works perfectly when not in a thread. i.e when in the program's main loop. </p>

<p>Just fyi I want to use a thread here to minimize the number of times pygame doesn't register a key press (which I'm assuming is due to other things in the mainloop). </p>

<p>Thanks in advance.</p>
","['python', 'python-3.x', 'pygame']",56717299,"<p>You don't get any events at all, because you have to get the events in the main thread.<br>
See the documentation of <a href=""https://www.pygame.org/docs/ref/event.html"" rel=""nofollow noreferrer""><code>pygame.event</code></a>:  </p>

<blockquote>
  <p>[...] The event subsystem should be called from the main thread.</p>
</blockquote>

<p>It is only possible to post events from other thread, but the event queue has to be handled in the main thread. </p>
",pygame event get returning events inside thread So I code looks user inputs pac man style game def receiving inputs self True events pg event get event events event type pg KEYDOWN event key pg K UP self move n elif event key pg K RIGHT self move e elif event key pg K DOWN self move elif event key pg K LEFT self move w time sleep threading Thread target self receiving inputs start When I press keys keyboard I get events however moving mouse around return event using code The annoying thing exact code works perfectly thread e program main loop Just fyi I want use thread minimize number times pygame register key press I assuming due things mainloop Thanks advance,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
56798051,2019-06-27,2019,2,Pandas vectorized way to get counts using conditional statement between two dataframes,"<p>I have two dataframes (with unequal rows, but the same columns) such as the following.</p>

<p>DataFrame A:</p>

<pre><code>dummy | probability
-------------------
  0   |    .1
-------------------
  0   |    .2
</code></pre>

<p>DataFrame B:</p>

<pre><code>dummy | probability
-------------------  
  1   |    .05
-------------------
  1   |    .2
</code></pre>

<p>What I would like to do is implement a vectorized conditional check for pairwise elements.</p>

<p>My actual dataset has a few hundred thousand elements. So if I check element wise, using a double for loop would require at least 100000^2 iterations which I do not want at all.</p>

<p>I believe there is probably a way to do this using numpy and pandas that I am currently unaware of.</p>

<p>pseudocode should look something like this:</p>

<pre><code>def vectorized_counts():

    A = 0
    B = 0
    tie = 0

    if element in dfA second column &gt; element in dfB second column:
        A += 1
    elif element in dfA second column &lt; element in dfB second column:
        B += 1
    else:
        tie += 1

    return list(A,B,tie)
</code></pre>

<p>For my test example above, we have:</p>

<pre><code>A
.1 &gt; .05
.2 &gt; .05

B
.1 &lt; .2

tie
.2 = .2
</code></pre>

<p>Hence:</p>

<pre><code>A = 2
B = 1
tie = 1
</code></pre>

<p>What way can I go about this? It is simple enough to compare one element of a dataframe with the corresponding element of another dataframe. What is confusing me is how to compare every element of a dataframe with every element of another dataframe.</p>
","['python', 'pandas', 'numpy']",56800155,"<p>Here is an iteration-less function that (hopefully) does what you need:</p>

<pre><code>def compare_probabilities(A, B):
    df = pd.concat([A] * B.shape[0], axis=0).reset_index(drop=True)
    df['Ap'] = df.probability
    df['Bp'] = B.probability.repeat(A.shape[0]).values
    AgtB = (df.Ap &gt; df.Bp).sum()
    BgtA = (df.Ap &lt; df.Bp).sum()
    #AeqB = (df.Ap == df.Bp).sum()
    AeqB = df.shape[0] - (AgtB + BgtA)
    return AgtB, BgtA, AeqB

A = pd.DataFrame({'dummy':[0,0], 'probability':[0.1,0.2]})
B = pd.DataFrame({'dummy':[1,1], 'probability':[0.05,0.2]})
print (""compare_probabilities: A&gt;B is %d; B&gt;A is %d;  A==B is %d""%compare_probabilities(A, B))
</code></pre>

<p>This should display:</p>

<p><code>compare_probabilities: A&gt;B is 2; B&gt;A is 1;  A==B is 1</code></p>

<p>And it should work for unequal row sizes in A and B.</p>
",Pandas vectorized way get counts using conditional statement two dataframes I two dataframes unequal rows columns following DataFrame A dummy probability DataFrame B dummy probability What I would like implement vectorized conditional check pairwise elements My actual dataset hundred thousand elements So I check element wise using double loop would require least iterations I want I believe probably way using numpy pandas I currently unaware pseudocode look something like def vectorized counts A B tie element dfA second column gt element dfB second column A elif element dfA second column lt element dfB second column B else tie return list A B tie For test example A gt gt B lt tie Hence A B tie What way I go It simple enough compare one element dataframe corresponding element another dataframe What confusing compare every element dataframe every element another dataframe,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
56907065,2019-07-05,2019,2,Can&#39;t seem to strip numbers from a string,"<p>I have a data frame that looks like this.</p>

<pre><code>0                                             1.144921                     
1                                             1.000000                     
2                                             1.119507                     
3                                                  inf                     
4                                             0.000000                     
5                                                  inf                     
6                                             0.000000                     
7                                             0.000000                     
8                                             1.000000                     
9                                             0.000000                     
10                                            0.000000                     
11                                            0.000000                     
12                                            1.793687                     
13                                                 inf    
</code></pre>

<p>I am trying to get rid of the '<code>inf</code>' string.  Basically, I just want to strip out all strings and keep only the numbers in the dataframe.  </p>

<p>I tried the following code below.</p>

<pre><code>kepler = re.sub(""\D"", """", kepler)
kepler = re.sub('[^0-9]','0', kepler)
</code></pre>

<p>When I run either of these lines of code I get the following error.</p>

<pre><code>TypeError: expected string or bytes-like object
</code></pre>

<p>If I have a very simple string, it actually does work.  So, this will work.</p>

<pre><code>s = '83jjdmi239450  19dkd'
s = re.sub(""\D"", """", s)
</code></pre>

<p>Unfortunately, the code doesn't work on my dataframe.  Any thoughts?  Thanks.</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",56907185,"<p>With <a href=""https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.isfinite.html"" rel=""nofollow noreferrer""><code>numpy.isfinite</code></a> routine on sample dataframe:</p>

<pre><code>In [176]: df
Out[176]: 
           a
0   1.000000
1   1.119507
2        inf
3   0.000000
4        inf
5   0.000000
6   0.000000
7   1.000000
8   0.000000
9   0.000000
10  0.000000
11  1.793687
12       inf

In [177]: df = df[~np.isinf(df['a'])]

In [178]: df
Out[178]: 
           a
0   1.000000
1   1.119507
3   0.000000
5   0.000000
6   0.000000
7   1.000000
8   0.000000
9   0.000000
10  0.000000
11  1.793687
</code></pre>
",Can seem strip numbers string I data frame looks like inf inf inf I trying get rid inf string Basically I want strip strings keep numbers dataframe I tried following code kepler sub D kepler kepler sub kepler When I run either lines code I get following error TypeError expected string bytes like object If I simple string actually work So work jjdmi dkd sub D Unfortunately code work dataframe Any thoughts Thanks,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas numpy endoftags,python python3x pandas dataframe,python pandas numpy,0.58
57123198,2019-07-20,2019,5,Setting initial state of LSTM layer,"<p>I have the following code: </p>

<pre><code>units = 1024
lstm_layer = tf.keras.layers.LSTM(units)

dim = tf.zeros([64,1024])

output, hidden = lstm_layer(embedded_data, initial_state = dim)
</code></pre>

<p>I get the following error message:</p>

<pre><code>ValueError: An `initial_state` was passed that is 
not compatible with `cell.state_size`.
 Received `state_spec`=
ListWrapper([InputSpec(shape=(64, 1024), ndim=2)]);
 however `cell.state_size` is [1024, 1024]
</code></pre>

<p>When I do it with a GRU cell instead of a LSTM cell, it works fine. But for a LSTM cell, this code does not work. I realize that LSTM takes two parameters, hence the code asking for a cell state of [1024,1024], but I don't know how to set the initial state. I tried </p>

<pre><code>initial_state = [dim, dim] 
</code></pre>

<p>and that doesn't work either, as it gives me </p>

<pre><code>ValueError: too many values to unpack (expected 2).
</code></pre>

<p>I referenced <a href=""https://stackoverflow.com/questions/48233400/lstm-initial-state-from-dense-layer"">LSTM Initial state from Dense layer</a> but doesn't seem to resolve my issue...</p>
","['python', 'tensorflow', 'keras']",57125352,"<p>For future reference if anybody needs solving on this problem:</p>
<p>The problem is the output. Just using <code>tf.zeros([64,1024])</code> works fine, you just need to have three outputs:</p>
<pre><code>output, hidden_h,hidden_c = lstm_layer(embedded_data, initial_state = [dim,dim])

</code></pre>
",Setting initial state LSTM layer I following code units lstm layer tf keras layers LSTM units dim tf zeros output hidden lstm layer embedded data initial state dim I get following error message ValueError An initial state passed compatible cell state size Received state spec ListWrapper InputSpec shape ndim however cell state size When I GRU cell instead LSTM cell works fine But LSTM cell code work I realize LSTM takes two parameters hence code asking cell state I know set initial state I tried initial state dim dim work either gives ValueError many values unpack expected I referenced LSTM Initial state Dense layer seem resolve issue,"startoftags, python, tensorflow, keras, endoftags",python django djangorestframework endoftags,python tensorflow keras,python django djangorestframework,0.33
57154329,2019-07-22,2019,2,Creating pairwise relationship between columns of a dataframe,"<p>I have a dataframe as follows :</p>

<pre><code>Neighborhood, City, State, Country       

Westside, Boston, MA,USA
South District, New York,NY,USA
Business Town,,OR,USA
Shopping District,,Wellington,New Zealand
Big Mountain,,,Australia
</code></pre>

<p>Now I want to go over <code>pairs of NON Empty Columns</code> <code>C0,C1  C1,C2  C2,C3</code> and create a dataframe that looks like below. However <code>if C1 is empty or null then pair C0 with C2 and so on</code> </p>

<pre><code>Root               Child
 OR                  Business Town
 USA                 OR
 New Zealand         Wellington
 Wellington.         Shopping District 
 Boston              Westside
 MA                  Boston
 USA                 MA
 New York            South District
 NY                  New York
 USA                 NY
 Australia          Big Mountain
</code></pre>
","['python', 'python-3.x', 'pandas', 'dataframe']",57154449,"<p>Here is one way using <code>shift</code> after <code>stack</code> </p>

<pre><code>s=df.stack().iloc[::-1]
yourdf=pd.DataFrame({'Root':s.groupby(level=0).shift().values,'Child':s.values}).dropna()
yourdf
Out[62]: 
           Root              Child
1     Australia       Big Mountain
3   New Zealand         Wellington
4    Wellington  Shopping District
6           USA                 OR
7            OR      Business Town
9           USA                 NY
10           NY           New York
11     New York     South District
13          USA                 MA
14           MA             Boston
15       Boston           Westside
</code></pre>
",Creating pairwise relationship columns dataframe I dataframe follows Neighborhood City State Country Westside Boston MA USA South District New York NY USA Business Town OR USA Shopping District Wellington New Zealand Big Mountain Australia Now I want go pairs NON Empty Columns C C C C C C create dataframe looks like However C empty null pair C C Root Child OR Business Town USA OR New Zealand Wellington Wellington Shopping District Boston Westside MA Boston USA MA New York South District NY New York USA NY Australia Big Mountain,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
57211584,2019-07-26,2019,3,How to extract pairs from a dictionary into a new smaller one?,"<p>I have a big dictionary but i only need to use some of the data from it. How can i extract only the pairs that i want and build a new one with the selected data?</p>

<pre class=""lang-py prettyprint-override""><code>{'index': 325, 'name': 'Zombie', 'size': 'Medium', 'type': 'undead' ... }
</code></pre>

<p>Let's say i only wanted <code>name</code> and <code>type</code> from this dictionary like so:</p>

<pre class=""lang-py prettyprint-override""><code>{'name': 'Zombie', 'type': 'undead'}
</code></pre>

<p>How would i go about doing this? I could remove the elements i don't need using <code>del d[key]</code> but that seems very inefficient, and won't be reliable if more data gets added later.</p>

<p>The reason i want to make a smaller dictionary is so i can load the data directly into my class like this</p>

<pre class=""lang-py prettyprint-override""><code>Entity(**index[enemy_id])
</code></pre>

<p>But it won't work if there is more data than the class can hold.</p>
","['python', 'python-3.x', 'dictionary']",57211798,"<p>No need to iterate the entire dictionary, just filter out the keys you want:</p>

<pre><code>&gt;&gt;&gt; d = {'index': 325, 'name': 'Zombie', 'size': 'Medium', 'type': 'undead'}
&gt;&gt;&gt; keys = ['name', 'type']
&gt;&gt;&gt; {key: d.get(key) for key in keys}
{'name': 'Zombie', 'type': 'undead'}
</code></pre>

<p>Using <code>dict.get()</code> here also prevents <code>KeyError</code> from being raised if a key doesn't exist, and just returns <code>None</code> instead. </p>

<p>If you want to <em>ignore</em> keys that don't exist in <code>d</code>, then you can do this:</p>

<pre><code>{key: d[key] for key in keys if key in d}
</code></pre>
",How extract pairs dictionary new smaller one I big dictionary need use data How extract pairs want build new one selected data index name Zombie size Medium type undead Let say wanted name type dictionary like name Zombie type undead How would go I could remove elements need using del key seems inefficient reliable data gets added later The reason want make smaller dictionary load data directly class like Entity index enemy id But work data class hold,"startoftags, python, python3x, dictionary, endoftags",python arrays numpy endoftags,python python3x dictionary,python arrays numpy,0.33
57252299,2019-07-29,2019,2,How can I get the records based on condition on date and time of created_time column of sample dataframe,"<p>sample dataframe(df) having following columns:</p>

<pre><code>     id     created_time     faid                                            
0    21 2019-06-17 07:06:45  FF1854155    
1    54 2019-04-12 08:06:03  FF30232     
2    88 2019-04-20 05:36:03  FF1855531251     
3   154 2019-04-26 07:09:22  FF8145292   
4   218 2019-07-25 13:20:51  FF0143154   
5   219 2019-04-30 18:50:24  FF04211 
6   235 2019-04-30 20:37:37  FF0671380   
7   266 2019-05-02 08:38:56  FF08070   
8   268 2019-05-02 11:08:21  FF591087   
</code></pre>

<p>How can i get the records whose created_time is between 2019-04-01 00:00:00 and 2019-06-30 00:00:00</p>
","['python', 'python-3.x', 'pandas']",57252331,"<ol>
<li>convert datetime into datetime format using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html"" rel=""nofollow noreferrer""><code>pd.to_datetime()</code></a></li>
<li>Apply simply less than greator than to fetch data between two dates</li>
</ol>
<pre><code>
df['created_time'] = pd.to_datetime(df['created_time'])

res = df[(df['created_time']&gt;= '2019-04-01')&amp;(df['created_time']&lt;='2019-06-30')]
</code></pre>
<h1>Second Solution</h1>
<p>by @anky_91</p>
<pre><code>df['created_time'] = pd.to_datetime(df['created_time'])
df[df.created_time.between('2019-04-01 00:00:00','2019-06-30 00:00:00')]
</code></pre>
",How I get records based condition date time created time column sample dataframe sample dataframe df following columns id created time faid FF FF FF FF FF FF FF FF FF How get records whose created time,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
57304478,2019-08-01,2019,2,isin() method in pandas ignoring the duplicate values. how can we prevent that?,"<p>I am fetching all the values from the dataframes I cant show it here but all the column names that mentioned in the code exist in this dataframe. Since, I am using isin() method to fetch the values from the column. isin() method does not gives you duplicate values but I want the duplicate values as well. In the above code i shows that how I used isin() method to fetch the multiple values from multiple column. In the voltage_values variable i used isin() method which not fetching the duplicate values.What Can i do that does not remove duplicate values.</p>

<pre class=""lang-py prettyprint-override""><code>start_values = [1,2,3]
load_value_name = [f""^I__ND_LD({n})"" for n in start_values]
load_values=df[df['I__ND_LD'].isin(load_value_name)]['I__ND_LD_Values'].values.astype(np.int)
print(load_values)
bus_names = [f""^I__BS_ND({n})"" for n in load_values]
print(bus_names)
bus_values = df[df['I__BS_ND'].isin(bus_names)]['I__BS_ND_Values'].values.astype(np.int)
print(bus_values)
voltage_bus_value = [f""^VMEAS_BS({n})"" for n in bus_values]
print(voltage_bus_value)
voltage_values = df[df['VMEAS_BS'].isin(voltage_bus_value)]['VMEAS_BS_Values'].reindex().values
print(voltage_values)
</code></pre>

<p>The above shows the corresponding output</p>

<pre><code>load_values=[10 45 44]
bus_names=['^I__BS_ND(10)', '^I__BS_ND(45)', '^I__BS_ND(44)']
bus_values=[ 5 17 17]
voltage_bus_value=['^VMEAS_BS(5)', '^VMEAS_BS(17)', '^VMEAS_BS(17)']
voltage_values=[0.9908185  0.99612296]
</code></pre>

<p>As we can see that ""^VMEAS_BS(17)"" came two times but in the array I got only only one value which is 0.99612296 but I want this value two times. What could be the possible solution for that.</p>
","['python', 'pandas', 'dataframe']",57305328,"<p>I don't have VMEAS_BS in my dataframe, so I'll show you the result with load_values.</p>

<p>This is the quickest I can think of:</p>

<p>Substitute</p>

<pre><code>voltage_values = df[df['VMEAS_BS'].isin(voltage_bus_value)]['VMEAS_BS_Values'].reindex().values
</code></pre>

<p>with all these lines (sorry!!)</p>

<pre><code>voltage_values = []
for _,value in enumerate(voltage_values):
  voltage_values.extend(df[df['VMEAS_BS'] == value]['VMEAS_BS_Values'].reindex().values)
</code></pre>

<p>Here is an example with load_values</p>

<pre><code>load_values = []
for _,value in enumerate(load_value_name):
  load_values.extend(df[df['I__ND_LD']== value]['I__ND_LD_Values'].values.astype(np.int))
#output
[10, 45, 44]
</code></pre>
",isin method pandas ignoring duplicate values prevent I fetching values dataframes I cant show column names mentioned code exist dataframe Since I using isin method fetch values column isin method gives duplicate values I want duplicate values well In code shows I used isin method fetch multiple values multiple column In voltage values variable used isin method fetching duplicate values What Can remove duplicate values start values load value name f I ND LD n n start values load values df df I ND LD isin load value name I ND LD Values values astype np int print load values bus names f I BS ND n n load values print bus names bus values df df I BS ND isin bus names I BS ND Values values astype np int print bus values voltage bus value f VMEAS BS n n bus values print voltage bus value voltage values,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
57588604,2019-08-21,2019,3,Multiplying Pandas series rows containing NaN,"<p>given this Dataframe :</p>

<pre><code>import pandas as pd
import numpy as np

data = {'column1': [True,False, False, True, True],
        'column2' : [np.nan,0.21, np.nan, 0.2222, np.nan],
        'column3': [1000, 0, 0, 0, 0 ]}


df = pd.DataFrame.from_dict(data)

print(df)
</code></pre>

<hr>

<pre><code>   column1  column2  column3
0     True      NaN     1000
1    False   0.2100        0
2    False      NaN        0
3     True   0.2222        0
4     True      NaN        0
</code></pre>

<p>How can I multiply the result from <strong>column2</strong> with the previous value of <strong>column3</strong> when the <strong>column2</strong> row isn't a NaN otherwise just return the previous value of <strong>column3</strong> ?</p>

<p>The results should be something like this :</p>

<pre><code>   column1  column2  column3
0     True      NaN     1000
1    False   0.2100        210
2    False      NaN        210
3     True   0.2222        46.662
4     True      NaN        46.662
</code></pre>

<p>I've been browsing through similar questions but I just can't get my head around it ..</p>

<p>I'd appreciate your input :)</p>
","['python', 'python-3.x', 'pandas']",57589164,"<p>I would define a dummy class to accumulate the last value of column3 and then iterate over rows to do the computation. If you do it this way, you avoid writing a for loop and you concentrate the computation in a map call, which can be for example run in parallel easily</p>

<pre><code>class AccumMult:
    def __init__(self):
        self.last_val = None

    def mult(self, c2, c3):
        self.last_val = c3 if self.last_val is None else self.last_val
        if not np.isnan(c2):
            self.last_val = self.last_val * c2
        return self.last_val

m = AccumMult()

df[""column3""] = list(map(lambda x: m.mult(x[0], x[1]), df[[""column2"", ""column3""]].values.tolist()))
</code></pre>
",Multiplying Pandas series rows containing NaN given Dataframe import pandas pd import numpy np data column True False False True True column np nan np nan np nan column df pd DataFrame dict data print df column column column True NaN False False NaN True True NaN How I multiply result column previous value column column row NaN otherwise return previous value column The results something like column column column True NaN False False NaN True True NaN I browsing similar questions I get head around I appreciate input,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
57603538,2019-08-22,2019,2,Remove single quotes in list in pyspark,"<p>I have a DataFrame like this. <code>colA</code> contains list of items, however it's stored under quotes like this</p>

<pre><code>data = [((""ID1"", ""['valA', 'valB']"")), ((""ID1"", ""[]"")), ((""ID1"", ""['valC']"")), ((""ID1"", """"))]
df = spark.createDataFrame(data, [""ID"", ""colA""])
df.show()

+---+----------------+
| ID|            colA|
+---+----------------+
|ID1|['valA', 'valB']|
|ID2|              []|
|ID3|        ['valC']|
|ID4|                |
+---+----------------+
</code></pre>

<p><code>colA</code> has blank and empty list values. </p>

<p>I want to clean this column, such that I have the following DataFrame</p>

<pre><code>+---+------------+
| ID|        colA|
+---+------------+
|ID1|[valA, valB]|
|ID2|        null|
|ID3|      [valC]|
|ID4|        null|
+---+------------+
</code></pre>
","['python', 'apache-spark', 'pyspark']",57604175,"<p>Steps:</p>

<ol>
<li>Remove starting <code>^['</code> or <code>|</code> ending <code>']$</code> brackets. <code>\</code> is for escaping, <code>^</code> for start of string, <code>$</code> for end of string</li>
<li>Turn empty lists <code>[]</code> into empty strings, again escaping with <code>\</code></li>
<li>Remove empty strings</li>
<li>Split by <code>', '</code> or <code>','</code> to seperate elements, <code>?</code> means optional space</li>
</ol>

<p><strong>Code</strong></p>

<pre><code>from pyspark.sql import functions as f

data = [((""ID1"", ""['valA', 'valB']"")), ((""ID1"", ""[]"")), ((""ID1"", ""['valC']"")), ((""ID1"", """"))]
df = spark.createDataFrame(data, [""ID"", ""colA""])

df_2 \
  .withColumn('colA_2', f.regexp_replace('colA', ""^\['|'\]$"", '')) \
  .withColumn('colA_2', f.regexp_replace('colA_2', ""\[\]"", '')) \
  .withColumn('colA_2', f.when(f.col('colA_2') == """", None).otherwise(f.col('colA_2'))) \
  .withColumn('colA_2', f.split('colA_2', ""', ?'""))
</code></pre>

<p><strong>Output</strong></p>

<pre><code>df_2.show()

+---+----------------+------------+
| ID|            colA|      colA_2|
+---+----------------+------------+
|ID1|['valA', 'valB']|[valA, valB]|
|ID1|              []|        null|
|ID1|        ['valC']|      [valC]|
|ID1|                |        null|
+---+----------------+------------+
</code></pre>

<pre><code>df_2.printSchema()

root
 |-- ID: string (nullable = true)
 |-- colA: string (nullable = true)
 |-- colA_2: array (nullable = true)
 |    |-- element: string (containsNull = true)
</code></pre>
",Remove single quotes list pyspark I DataFrame like colA contains list items however stored quotes like data ID valA valB ID ID valC ID df spark createDataFrame data ID colA df show ID colA ID valA valB ID ID valC ID colA blank empty list values I want clean column I following DataFrame ID colA ID valA valB ID null ID valC ID null,"startoftags, python, apachespark, pyspark, endoftags",python discord discordpy endoftags,python apachespark pyspark,python discord discordpy,0.33
57656404,2019-08-26,2019,2,how to join two dataframe by picking couple of column from each if one of the column has same data,"<p>there are two dataframes <code>df_one</code> and <code>df_two</code> I want to create a new data frame by with selective column from each of the dataframes </p>

<pre><code>df_one
e b c d 
1 2 3 4 
5 6 7 8 
6 2 4 8 
9 2 5 6
</code></pre>

<p>and </p>

<pre><code>df_two

e f g h
1 8 7 6 
5 6 6 4 
6 6 2 4 
9 5 3 2 
</code></pre>

<p>I want to create a new dataframe new_df</p>

<pre><code>e b g h d
1 6 7 6 4
5 2 6 4 8
6 2 2 4 8
9 2 3 2 6
</code></pre>

<p><a href=""https://i.stack.imgur.com/MnZY1.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","['python', 'python-3.x', 'pandas']",57656698,"<pre><code> result = pd.merge(df_one, df_two, on='e')
 result=result.loc[:,[""e"",""b"",""g"",""h"",""d""]]
</code></pre>
",join two dataframe picking couple column one column data two dataframes df one df two I want create new data frame selective column dataframes df one e b c df two e f g h I want create new dataframe new df e b g h enter image description,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
57890861,2019-09-11,2019,2,Pandas Groupby Count Partial Strings,"<p>I am wanting to try to get a count of how many rows within a column contain a partial string based on an imported dataframe. In the sample data below, I want to groupby Trans_type and then get a count of how many rows contain a value.</p>

<p><a href=""https://i.stack.imgur.com/V7nxo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V7nxo.png"" alt=""Table of Sample Data""></a></p>

<p>So I would expect to see:</p>

<p><a href=""https://i.stack.imgur.com/GAolV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GAolV.png"" alt=""Grouped_Counts""></a></p>

<p>First, is this possible generically without passing a link to get each types expected brand? If not, how could I pass say Car a list of .str.contains['Audi','BMW'].</p>

<p>Thanks for any help!</p>
","['python', 'pandas', 'pandas-groupby']",57890984,"<p>Try this one:</p>

<pre class=""lang-py prettyprint-override""><code>df.groupby(df[""Trans_type""], df[""Brand""].str.extract(""([a-zA-Z])+"", expand=False)).count()
</code></pre>
",Pandas Groupby Count Partial Strings I wanting try get count many rows within column contain partial string based imported dataframe In sample data I want groupby Trans type get count many rows contain value So I would expect see First possible generically without passing link get types expected brand If could I pass say Car list str contains Audi BMW Thanks help,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
58245145,2019-10-05,2019,2,Pandas to_datetime error &#39;unconverted data remains&#39;,"<p>I'm trying to convert date column in my Pandas DataFrame to datetime format. If I don't specify date format, it works fine, but then further along in the code I get issues because of different time formats.</p>

<p>The original dates looks like this <code>10/10/2019 6:00</code> in european date format.</p>

<p>I tried specifying format like so:</p>

<pre><code>df['PeriodStartDate'] = pd.to_datetime(df['PeriodStartDate'], 
          format=""%d/%m/%Y"")
</code></pre>

<p>which results in an error: <code>unconverted data remains 6:00</code></p>

<p>I then tried to update format directive to <code>format=""%d/%m/%Y %-I/%H""</code> which comes up with another error: <code>'-' is a bad directive in format '%d/%m/%Y %-I/%H'</code> even though I thought that <code>to_datetime</code> uses the same directives and <code>strftime</code> and in the latter <code>%-I</code> is allowed.</p>

<p>In frustration I then decided to chop off the end of the string since I don't really need hours and minutes:</p>

<pre><code>    df['PeriodStartDate'] = df['PeriodStartDate'].str[:10]
    df['PeriodStartDate'] = pd.to_datetime(df['PeriodStartDate'], 
          format=""%d/%m/%Y"")
</code></pre>

<p>But this once again results in an error: <code>ValueError: unconverted data remains:</code> which of course comes from the fact that some dates have 9 digits like <code>3/10/2019 6:00</code></p>

<p>Not quite sure where to go from here.</p>
","['python', 'pandas', 'datetime']",58245209,"<p>format <code>%H:%M</code> would work(<em>don't forget the <code>:</code> in between</em>)</p>

<pre><code>pd.to_datetime('10/10/2019 6:00', format=""%m/%d/%Y %H:%M"")

Out[1049]: Timestamp('2019-10-10 06:00:00')

pd.to_datetime('3/10/2019 18:00', format=""%d/%m/%Y %H:%M"")

Out[1064]: Timestamp('2019-10-03 18:00:00')
</code></pre>
",Pandas datetime error unconverted data remains I trying convert date column Pandas DataFrame datetime format If I specify date format works fine along code I get issues different time formats The original dates looks like european date format I tried specifying format like df PeriodStartDate pd datetime df PeriodStartDate format Y results error unconverted data remains I tried update format directive format Y I H comes another error bad directive format Y I H even though I thought datetime uses directives strftime latter I allowed In frustration I decided chop end string since I really need hours minutes df PeriodStartDate df PeriodStartDate str df PeriodStartDate pd datetime df PeriodStartDate format Y But results error ValueError unconverted data remains course comes fact dates digits like Not quite sure go,"startoftags, python, pandas, datetime, endoftags",python pandas matplotlib endoftags,python pandas datetime,python pandas matplotlib,0.67
58404368,2019-10-16,2019,5,Dataframe fillna conditional based on Index &amp; Column Name,"<p>I want to be able to use the <code>df.fillna()</code> function on a Dataframe, but apply a conditional to it based on the Index &amp; Column name of that particular cell.</p>

<p>I am trying to create a heatmap of hockey linemate data based on the following dataset (apologies for the large dictionary below) - </p>

<pre class=""lang-py prettyprint-override""><code>linemates_toi = {
    'Player 1': {'Player 2': 0.25, 'Player 3': 7.95, 'Player 4': 0.6333, 'Player 5': 9.95, 'Player 6': 0.6333, 'Player 7': 0.8, 'Player 8': 4.2667, 'Player 9': 7.8833, 'Player 10': 0.3, 'Player 11': 11.2333, 'Player 12': 3.35, 'Player 13': 0.2167},
    'Player 10': {'Player 14': 2.3, 'Player 18': 1.2667, 'Player 2': 6.8333, 'Player 4': 5.5833, 'Player 5': 0.9, 'Player 16': 6.9167, 'Player 6': 4.9667, 'Player 7': 4.15, 'Player 15': 1.0, 'Player 8': 0.3167, 'Player 17': 5.3167, 'Player 1': 0.3, 'Player 11': 1.6167, 'Player 12': 0.6833, 'Player 13': 12.7167}, 
    'Player 12': {'Player 14': 4.5333, 'Player 18': 4.3333, 'Player 2': 3.1167, 'Player 3': 1.2333, 'Player 4': 5.7333, 'Player 5': 3.5167, 'Player 16': 3.0, 'Player 6': 3.0167, 'Player 7': 2.4, 'Player 15': 2.0167, 'Player 8': 11.6667, 'Player 17': 2.2667, 'Player 9': 0.1167, 'Player 1': 3.35, 'Player 10': 0.6833, 'Player 11': 3.35}, 
    'Player 17': {'Player 14': 4.55, 'Player 18': 1.65, 'Player 2': 0.8833, 'Player 3': 2.85, 'Player 5': 0.0333, 'Player 16': 2.9167, 'Player 6': 7.8167, 'Player 7': 6.0833, 'Player 8': 3.8, 'Player 9': 2.25, 'Player 10': 5.3167, 'Player 12': 2.2667, 'Player 13': 5.7833},
    'Player 7': {'Player 18': 0.3667, 'Player 2': 0.6667, 'Player 3': 1.55, 'Player 4': 0.3333, 'Player 5': 0.15, 'Player 16': 1.2167, 'Player 6': 6.8333, 'Player 15': 0.3333, 'Player 8': 3.0667, 'Player 17': 6.0833, 'Player 9': 1.8833, 'Player 1': 0.8, 'Player 10': 4.15, 'Player 11': 1.0, 'Player 12': 2.4, 'Player 13': 4.4333}, 
    'Player 16': {'Player 14': 2.2833, 'Player 2': 8.5333, 'Player 3': 2.7, 'Player 4': 8.0167, 'Player 5': 0.45, 'Player 6': 0.4, 'Player 7': 1.2167, 'Player 8': 2.3, 'Player 17': 2.9167, 'Player 9': 2.15, 'Player 10': 6.9167, 'Player 11': 0.1333, 'Player 12': 3.0, 'Player 13': 6.5833},
    'Player 18': {'Player 14': 10.05, 'Player 2': 0.75, 'Player 3': 5.0, 'Player 4': 3.45, 'Player 5': 0.3333, 'Player 6': 0.8333, 'Player 7': 0.3667, 'Player 15': 5.2, 'Player 8': 5.8167, 'Player 17': 1.65, 'Player 9': 4.3833, 'Player 10': 1.2667, 'Player 11': 1.5, 'Player 12': 4.3333, 'Player 13': 1.5333},
    'Player 13': {'Player 14': 3.0333, 'Player 18': 1.5333, 'Player 2': 5.9167, 'Player 3': 0.7333, 'Player 4': 4.95, 'Player 5': 0.8167, 'Player 16': 6.5833, 'Player 6': 5.1333, 'Player 7': 4.4333, 'Player 15': 1.2667, 'Player 8': 0.2833, 'Player 17': 5.7833, 'Player 1': 0.2167, 'Player 10': 12.7167, 'Player 11': 1.5333},
    'Player 5': {'Player 18': 0.3333, 'Player 2': 0.8333, 'Player 3': 8.0333, 'Player 16': 0.45, 'Player 6': 0.3333, 'Player 7': 0.15, 'Player 8': 3.0167, 'Player 17': 0.0333, 'Player 9': 6.7333, 'Player 1': 9.95, 'Player 10': 0.9, 'Player 11': 11.2333, 'Player 12': 3.5167, 'Player 13': 0.8167},
    'Player 15': {'Player 14': 4.5667, 'Player 18': 5.2, 'Player 2': 0.4667, 'Player 3': 2.35, 'Player 6': 0.1667, 'Player 7': 0.3333, 'Player 8': 2.0167, 'Player 9': 2.0833, 'Player 10': 1.0, 'Player 12': 2.0167, 'Player 13': 1.2667},
    'Player 2': {'Player 18': 0.75, 'Player 3': 2.65, 'Player 4': 8.6, 'Player 5': 0.8333, 'Player 16': 8.5333, 'Player 6': 0.8333, 'Player 7': 0.6667, 'Player 15': 0.4667, 'Player 8': 2.3333, 'Player 17': 0.8833, 'Player 9': 1.9167, 'Player 1': 0.25, 'Player 10': 6.8333, 'Player 11': 1.6167, 'Player 12': 3.1167, 'Player 13': 5.9167},
    'Player 8': {'Player 14': 5.8333, 'Player 18': 5.8167, 'Player 2': 2.3333, 'Player 3': 1.1167, 'Player 4': 5.6833, 'Player 5': 3.0167, 'Player 16': 2.3, 'Player 6': 4.2667, 'Player 7': 3.0667, 'Player 15': 2.0167, 'Player 17': 3.8, 'Player 9': 1.1333, 'Player 1': 4.2667, 'Player 10': 0.3167, 'Player 11': 3.8167, 'Player 12': 11.6667, 'Player 13': 0.2833},
    'Player 4': {'Player 14': 3.2833, 'Player 18': 3.45, 'Player 2': 8.6, 'Player 3': 2.0667, 'Player 16': 8.0167, 'Player 6': 0.8333, 'Player 7': 0.3333, 'Player 8': 5.6833, 'Player 9': 1.85, 'Player 1': 0.6333, 'Player 10': 5.5833, 'Player 11': 0.85, 'Player 12': 5.7333, 'Player 13': 4.95},
    'Player 9': {'Player 14': 4.5167, 'Player 18': 4.3833, 'Player 2': 1.9167, 'Player 3': 14.35, 'Player 4': 1.85, 'Player 5': 6.7333, 'Player 16': 2.15, 'Player 6': 0.8833, 'Player 7': 1.8833, 'Player 15': 2.0833, 'Player 8': 1.1333, 'Player 17': 2.25, 'Player 1': 7.8833, 'Player 11': 9.0667, 'Player 12': 0.1167},
    'Player 14': {'Player 18': 10.05, 'Player 3': 5.7167, 'Player 4': 3.2833, 'Player 16': 2.2833, 'Player 6': 1.8833, 'Player 15': 4.5667, 'Player 8': 5.8333, 'Player 17': 4.55, 'Player 9': 4.5167, 'Player 10': 2.3, 'Player 11': 0.9833, 'Player 12': 4.5333, 'Player 13': 3.0333},
    'Player 11': {'Player 14': 0.9833, 'Player 18': 1.5, 'Player 2': 1.6167, 'Player 3': 9.7667, 'Player 4': 0.85, 'Player 5': 11.2333, 'Player 16': 0.1333, 'Player 6': 0.5, 'Player 7': 1.0, 'Player 8': 3.8167, 'Player 9': 9.0667, 'Player 1': 11.2333, 'Player 10': 1.6167, 'Player 12': 3.35, 'Player 13': 1.5333},
    'Player 6': {'Player 14': 1.8833, 'Player 18': 0.8333, 'Player 2': 0.8333, 'Player 3': 1.1333, 'Player 4': 0.8333, 'Player 5': 0.3333, 'Player 16': 0.4, 'Player 7': 6.8333, 'Player 15': 0.1667, 'Player 8': 4.2667, 'Player 17': 7.8167, 'Player 9': 0.8833, 'Player 1': 0.6333, 'Player 10': 4.9667, 'Player 11': 0.5, 'Player 12': 3.0167, 'Player 13': 5.1333},
    'Player 3': {'Player 14': 5.7167, 'Player 18': 5.0, 'Player 2': 2.65, 'Player 4': 2.0667, 'Player 5': 8.0333, 'Player 16': 2.7, 'Player 6': 1.1333, 'Player 7': 1.55, 'Player 15': 2.35, 'Player 8': 1.1167, 'Player 17': 2.85, 'Player 9': 14.35, 'Player 1': 7.95, 'Player 11': 9.7667, 'Player 12': 1.2333, 'Player 13': 0.7333}
}

df = pd.DataFrame(linemates_toi)
</code></pre>

<p>What I am trying to achieve now is to use <code>df.fillna(0)</code> and apply a conditional so the only <code>NaN</code> that are replaced is when the Index and Column name don't match because I want those cells to remain <code>NaN</code> so that when I plot them into a Heatmap they don't have any color in the <code>cmap</code> applied from Matplotlib.</p>

<p>If I were writing pseudo code, it would look like this - </p>

<pre class=""lang-py prettyprint-override""><code>df.fillna(0, df.cell.Index.Name != df.cell.Column.Name)
</code></pre>

<p>Thanks in advance!</p>
","['python', 'pandas', 'dataframe']",58404460,"<p>Use <code>df.apply</code> to map a lambda over each column:</p>

<pre><code>df = df.apply(lambda col: col.where((col.name == col.index) | col.notnull(), 0))
</code></pre>

<p><a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.combine.html"" rel=""nofollow noreferrer""><code>col.where(condition, value_if_false)</code></a> returns the original value in <code>col</code> if <code>condition</code> is true. Otherwise it returns <code>value_if_false</code> </p>
",Dataframe fillna conditional based Index amp Column Name I want able use df fillna function Dataframe apply conditional based Index amp Column name particular cell I trying create heatmap hockey linemate data based following dataset apologies large dictionary linemates toi Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player Player,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
58435994,2019-10-17,2019,2,How to rotate an object from the center in pycharm?,"<p>I am trying to rotate an image in pycharm, for some reason when I rotate, the object wobbles and doesn't rotate in place. </p>

<pre><code>class rocket(object):
def __init__(self, x, y):
    self.original_image = pygame.image.load('sprites/spaceship-off.png')
    self.image = self.original_image
    self.x = x
    self.y = y
    self.width = 50
    self.height = 50
    self.speed = 15
    self.angle = 0
    self.direction = [0, -1]
    self.position = [400, 400]

def draw(self, win):
    win.blit(self.image, (self.position))
    self.image = pygame.transform.rotate(self.original_image, self.angle)
</code></pre>

<p>-</p>

<pre><code>if keys[pygame.K_a] and player.x &gt; player.speed:
    player.angle += 10 % 360
if keys[pygame.K_d] and player.x &lt; 800 - player.width - player.speed:
    player.angle -= 10 % 360
</code></pre>
","['python', 'python-3.x', 'pygame']",58436469,"<p>When you want to rotate an image around its center, then you've to:</p>

<ul>
<li>get the pivot (which is the center point of the not rotated image) </li>
<li>rotate the image and get the (axis aligned bounding) rectangle of the rotated image</li>
<li>center the rectangle to the pivot</li>
<li><code>blit</code> the image at the top left of the new rectangle </li>
</ul>

<pre class=""lang-py prettyprint-override""><code>def draw(self, win):

    image_rect = self.original_image.get_rect(topleft = self.position)
    pivot = image_rect.center

    self.image = pygame.transform.rotate(self.original_image, self.angle)
    rotated_rect = self.image.get_rect(center = pivot)

    win.blit(self.image, rotated_rect.topleft)
</code></pre>

<p>See also <a href=""https://stackoverflow.com/questions/4183208/how-do-i-rotate-an-image-around-its-center-using-pygame/54714144#54714144"">How do I rotate an image around its center using Pygame?</a></p>
",How rotate object center pycharm I trying rotate image pycharm reason I rotate object wobbles rotate place class rocket object def init self x self original image pygame image load sprites spaceship png self image self original image self x x self self width self height self speed self angle self direction self position def draw self win win blit self image self position self image pygame transform rotate self original image self angle keys pygame K player x gt player speed player angle keys pygame K player x lt player width player speed player angle,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
58488880,2019-10-21,2019,2,Python Counting Same Values For Specific Columns,"<p>If i have a dataframe;</p>

<pre><code>      A       B       C      D
1     1       2       2      1
2     1       1       2      1
3     3       1       0      1
4     2       4       4      4 
</code></pre>

<p>I want to make addition B and C columns and counting whether or not the same values with D columns. Desired output is;</p>

<pre><code>          A       B       C     B+C      D
    1     1       2       2      4       1
    2     1       1       2      3       1
    3     3       1       0      1       1
    4     2       4       4      8       4 

There are 3 different values compare the ""B+C"" and ""D"".
</code></pre>

<p>Could you please help me about this?</p>
","['python', 'pandas', 'dataframe']",58488928,"<p>You could do something like:</p>

<pre><code>df.B.add(df.C).ne(df.D).sum()
# 3
</code></pre>

<hr>

<p>If you need to add the column:</p>

<pre><code>df['B+C'] = df.B.add(df.C)
diff = df['B+C'].ne(df.D).sum()
print(f'There are {diff} different values compare the ""B+C"" and ""D""')
#There are 3 different values compare the ""B+C"" and ""D""
</code></pre>
",Python Counting Same Values For Specific Columns If dataframe A B C D I want make addition B C columns counting whether values D columns Desired output A B C B C D There different values compare B C D Could please help,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
58543235,2019-10-24,2019,2,Swapping values in columns depending on value type in one of the columns,"<p>Suppose I have the following pandas dataframe:</p>

<pre><code>df = pd.DataFrame([['A','B'],[8,'s'],[5,'w'],['e',1],['n',3]])
print(df)

   0  1
0  A  B
1  8  s
2  5  w
3  e  1
4  n  3
</code></pre>

<p>If there is an integer in column 1, then I want to swap the value with the value from column 0, so in other words I want to produce this dataframe:</p>

<pre><code>   0  1
0  A  B
1  8  s
2  5  w
3  1  e
4  3  n
</code></pre>
","['python', 'pandas', 'dataframe']",58543301,"<p>Replace numbers from second column with mask by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html"" rel=""noreferrer""><code>to_numeric</code></a> with <code>errors='coerce'</code> and <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.notna.html"" rel=""noreferrer""><code>Series.notna</code></a>:</p>

<pre><code>m = pd.to_numeric(df[1], errors='coerce').notna()
</code></pre>

<p>Another solution with convert to strings by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.astype.html"" rel=""noreferrer""><code>Series.astype</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.isnumeric.html"" rel=""noreferrer""><code>Series.str.isnumeric</code></a> - but working only for integers:</p>

<pre><code>m = df[1].astype(str).str.isnumeric()
</code></pre>

<p>And then replace by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html"" rel=""noreferrer""><code>DataFrame.loc</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html"" rel=""noreferrer""><code>DataFrame.values</code></a> for numpy array for avoid columns alignment:</p>

<pre><code>df.loc[m, [0, 1]] = df.loc[m, [1, 0]].values
print(df)
   0  1
0  A  B
1  8  s
2  5  w
3  1  e
4  3  n
</code></pre>

<p>Last if possible better is convert first row to columns names:</p>

<pre><code>df.columns = df.iloc[0]
df = df.iloc[1:].rename_axis(None, axis=1)
print(df)
   A  B
1  8  s
2  5  w
3  1  e
4  3  n
</code></pre>

<p>or possible removing <code>header=None</code> in <code>read_csv</code>.</p>
",Swapping values columns depending value type one columns Suppose I following pandas dataframe df pd DataFrame A B w e n print df A B w e n If integer column I want swap value value column words I want produce dataframe A B w e n,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
58748110,2019-11-07,2019,2,Converting String to Numeric in pandas,"<p>I encountered a Problem after importing JSON-Data. Usually _to_numeric_ is working fine, but not in this Case. I have a column of strings Looking like this:
10.2100
10.2400
...</p>

<p>I am using Jupyter Notebook.
I have tried <em>astype()</em>, _infer_objects(), but still the data keeps behaving like string. I checked this by multiplying the column with 2 and get:
10.210010.2100
10.240010.2400
...</p>

<p>This is the Code:</p>

<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame()
temp = pd.DataFrame()
for file in filelist:
    with open(path+file, ""r"", encoding=""windows-1252"") as f:
        xml_string = f.read()
        if xml_string.find(""&lt;"") &gt;= 0:
            json_string = convert_to_json(xml_string)
            flat = flatten_json.flatten(json.loads(json_string))
            temp = pd.DataFrame.from_dict(flat, orient='index').transpose()
            df = df.append(temp, sort=False)

df = df.reset_index()
pd.to_numeric(df.['Telegram_PRC_VALS_STP_2_VAL_0_NUM'])
df['Telegram_PRC_VALS_STP_2_VAL_0_NUM'] = df['Telegram_PRC_VALS_STP_2_VAL_0_NUM']*2
Â´Â´Â´

Any ideas? As this is my first Question in here I hope I am meeting your expectations on decently asking Questions.
</code></pre>
","['python', 'pandas', 'dataframe']",58748132,"<p>You have to reassign output of <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html"" rel=""nofollow noreferrer""><code>to_numeric</code></a> back, because it is not inplace method:</p>

<pre><code>df['Telegram_PRC_VALS_STP_2_VAL_0_NUM']=pd.to_numeric(df['Telegram_PRC_VALS_STP_2_VAL_0_NUM'])
</code></pre>
",Converting String Numeric pandas I encountered Problem importing JSON Data Usually numeric working fine Case I column strings Looking like I using Jupyter Notebook I tried astype infer objects still data keeps behaving like string I checked multiplying column get This Code df pd DataFrame temp pd DataFrame file filelist open path file r encoding windows f xml string f read xml string find lt gt json string convert json xml string flat flatten json flatten json loads json string temp pd DataFrame dict flat orient index transpose df df append temp sort False df df reset index pd numeric df Telegram PRC VALS STP VAL NUM df Telegram PRC VALS STP VAL NUM df Telegram PRC VALS STP VAL NUM Any ideas As first Question I hope I meeting expectations decently asking Questions,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
58841392,2019-11-13,2019,3,Pandas: moving data from two dataframes to another with tuple index,"<p>I have three dataframes like the following:</p>

<p>final_df</p>

<pre><code>                                other   ref
(2014-12-24 13:20:00-05:00, a)  NaN     NaN
(2014-12-24 13:40:00-05:00, b)  NaN     NaN
(2018-07-03 14:00:00-04:00, d)  NaN     NaN
</code></pre>

<p>ref_df</p>

<pre><code>                                a   b   c   d
2014-12-24 13:20:00-05:00       1   2   3   4
2014-12-24 13:40:00-05:00       2   3   4   5
2017-11-24 13:10:00-05:00       ..............
2018-07-03 13:25:00-04:00       ..............
2018-07-03 14:00:00-04:00       9   10  11  12
2019-07-03 13:10:00-04:00       ..............
</code></pre>

<p>other_df</p>

<pre><code>                                a   b   c   d
2014-12-24 13:20:00-05:00       10  20  30  40
2014-12-24 13:40:00-05:00       20  30  40  50
2017-11-24 13:10:00-05:00       ..............
2018-07-03 13:20:00-04:00       ..............
2018-07-03 13:25:00-04:00       ..............
2018-07-03 14:00:00-04:00       90  100 110 120
2019-07-03 13:10:00-04:00       ..............
</code></pre>

<p>And I need to remplace the NaN values in my final_df with the related dataframe to be like that:</p>

<pre><code>                                other   ref
(2014-12-24 13:20:00-05:00, a)  10      1
(2014-12-24 13:40:00-05:00, b)  30      3
(2018-07-03 14:00:00-04:00, d)  110     11
</code></pre>

<p>How can I get it?</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",58841663,"<h2><code>pandas.DataFrame.lookup</code></h2>

<pre><code>final_df['ref'] = ref_df.lookup(*zip(*final_df.index))
final_df['other'] = other_df.lookup(*zip(*final_df.index))
</code></pre>

<hr>

<h2><code>map</code> and <code>get</code></h2>

<p>For when you have missing bits</p>

<pre><code>final_df['ref'] = list(map(ref_df.stack().get, final_df.index))
final_df['other'] = list(map(other_df.stack().get, final_df.index))
</code></pre>

<h2>Demo</h2>

<h3>Setup</h3>

<pre><code>idx = pd.MultiIndex.from_tuples([(1, 'a'), (2, 'b'), (3, 'd')])
final_df = pd.DataFrame(index=idx, columns=['other', 'ref'])
ref_df = pd.DataFrame([
    [ 1,  2,  3,  4],
    [ 2,  3,  4,  5],
    [ 9, 10, 11, 12]
], [1, 2, 3], ['a', 'b', 'c', 'd'])
other_df = pd.DataFrame([
    [ 10,  20,  30,  40],
    [ 20,  30,  40,  50],
    [ 90, 100, 110, 120]
], [1, 2, 3], ['a', 'b', 'c', 'd'])

print(final_df, ref_df, other_df, sep='\n\n')

    other  ref
1 a   NaN  NaN
2 b   NaN  NaN
3 d   NaN  NaN

   a   b   c   d
1  1   2   3   4
2  2   3   4   5
3  9  10  11  12

    a    b    c    d
1  10   20   30   40
2  20   30   40   50
3  90  100  110  120
</code></pre>

<h3>Result</h3>

<pre><code>final_df['ref'] = ref_df.lookup(*zip(*final_df.index))
final_df['other'] = other_df.lookup(*zip(*final_df.index))

final_df

     other  ref
1 a     10    1
2 b     30    3
3 d    120   12
</code></pre>
",Pandas moving data two dataframes another tuple index I three dataframes like following final df ref NaN NaN b NaN NaN NaN NaN ref df b c df b c And I need remplace NaN values final df related dataframe like ref b How I get,"startoftags, python, python3x, pandas, dataframe, endoftags",python python3x pandas endoftags,python python3x pandas dataframe,python python3x pandas,0.87
59024703,2019-11-25,2019,2,Generate a NumPy 1D array with a pre-specified correlation with an existing 1D array?,"<p>I have a non-generated 1D NumPy array. For now, we will use a generated one.</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np

arr1 = np.random.uniform(0, 100, 1_000)
</code></pre>

<p>I need an array that will be correlated <code>0.3</code> with it:</p>

<pre class=""lang-py prettyprint-override""><code>arr2 = '?'
print(np.corrcoef(arr1, arr2))
</code></pre>

<pre><code>Out[1]: 0.3
</code></pre>
","['python', 'arrays', 'numpy']",59024959,"<p>I've adapted <a href=""https://stats.stackexchange.com/a/313138"">this answer by whuber</a> on stats.SE to NumPy. The idea is to generate a second array <code>noise</code> randomly, and then compute the residuals of a least-squares linear regression of <code>noise</code> on <code>arr1</code>. The residuals necessarily have a correlation of 0 with <code>arr1</code>, and of course <code>arr1</code> has a correlation of 1 with itself, so an appropriate linear combination of <code>a*arr1 + b*residuals</code> will have any desired correlation.</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np

def generate_with_corrcoef(arr1, p):
    n = len(arr1)

    # generate noise
    noise = np.random.uniform(0, 1, n)

    # least squares linear regression for noise = m*arr1 + c
    m, c = np.linalg.lstsq(np.vstack([arr1, np.ones(n)]).T, noise)[0]

    # residuals have 0 correlation with arr1
    residuals = noise - (m*arr1 + c)

    # the right linear combination a*arr1 + b*residuals
    a = p * np.std(residuals)
    b = (1 - p**2)**0.5 * np.std(arr1)

    arr2 = a*arr1 + b*residuals

    # return a scaled/shifted result to have the same mean/sd as arr1
    # this doesn't change the correlation coefficient
    return np.mean(arr1) + (arr2 - np.mean(arr2)) * np.std(arr1) / np.std(arr2)
</code></pre>

<p>The last line scales the result so that the mean and standard deviation are the same as <code>arr1</code>'s. However, <code>arr1</code> and <code>arr2</code> will not be identically distributed.</p>

<p>Usage:</p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; arr1 = np.random.uniform(0, 100, 1000)
&gt;&gt;&gt; arr2 = generate_with_corrcoef(arr1, 0.3)
&gt;&gt;&gt; np.corrcoef(arr1, arr2)
array([[1. , 0.3],
       [0.3, 1. ]])
</code></pre>
",Generate NumPy D array pre specified correlation existing D array I non generated D NumPy array For use generated one import numpy np arr np random uniform I need array correlated arr print np corrcoef arr arr Out,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
59031304,2019-11-25,2019,2,calculate LOG10 for few columns only in dataframe,"<p>I want to convert my given observations to log10.</p>

<p>This is the head of my DF:
<a href=""https://i.stack.imgur.com/Q0pvh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q0pvh.png"" alt=""enter image description here""></a></p>

<p>I want to calculate the log10 only for the fields from HR90 and left. I have tried to choose only the correct columns but I get error all the time and I thin my code is too much complex for this.</p>

<p>This is my code:</p>

<pre><code>for i[9:] in data.columns:
 np.log10(data)
</code></pre>

<p>AttributeError: 'str' object has no attribute 'log10'</p>

<p>It seems like it still grabs also columns that I don't want to run this process on them. 
 I have also tried to import math and then:</p>

<pre><code>import math
for i in data.columns:
    if(data[i].dtype == np.float64 or data[i].dtype == np.int64):
        data.applymap(math.log10)
</code></pre>

<p>but thehn I have gotten:</p>

<blockquote>
  <p>TypeError: ('must be real number, not str', 'occurred at index NAME')</p>
</blockquote>

<p>My end goal is to convert part of my observations to LOG10</p>
","['python', 'pandas', 'numpy']",59034245,"<p>How about simply</p>

<pre><code>data[data.columns[:9]] = np.log10(data[data.columns[:9]])
</code></pre>

<p>This will take log10 for all columns with index 0 to index 8 inclusive.</p>
",calculate LOG columns dataframe I want convert given observations log This head DF I want calculate log fields HR left I tried choose correct columns I get error time I thin code much complex This code data columns np log data AttributeError str object attribute log It seems like still grabs also columns I want run process I also tried import math import math data columns data dtype np float data dtype np int data applymap math log thehn I gotten TypeError must real number str occurred index NAME My end goal convert part observations LOG,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
59278038,2019-12-11,2019,3,Make pair from row/column data of Python DataFrame,"<p>I want to make pairs from below like dataframe from python
What I'd like to do is make pairs with row and column like:
(1,a), (4,c), (6,c), (3,d), (2,f), (4,f), (6,f), (6,g)</p>

<p>Is there any way to do this.
Thanks in advance.</p>

<p><img src=""https://i.stack.imgur.com/1sqrx.png"" alt=""Example""></p>
","['python', 'pandas', 'dataframe']",59278245,"<p>If you have a data frame like this:</p>

<pre><code>     a   b   c    d   e    f    g
1  1.0 NaN NaN  NaN NaN  NaN  NaN
2  NaN NaN NaN  NaN NaN  1.0  NaN
3  NaN NaN NaN  1.0 NaN  NaN  1.0
</code></pre>

<p>You can use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>stack</code> against axis 1</a> to get</p>

<pre><code>s = df.stack()
</code></pre>

<p></p>

<pre><code>1  a    1.0
2  f    1.0
3  d    1.0
   g    1.0
dtype: float64
</code></pre>

<p>And a simple <code>to_list()</code> gives</p>

<pre><code>&gt;&gt;&gt; s.index.to_list()

[(1, 'a'), (2, 'f'), (3, 'd'), (3, 'g')]
</code></pre>
",Make pair row column data Python DataFrame I want make pairs like dataframe python What I like make pairs row column like c c f f f g Is way Thanks advance,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
59290874,2019-12-11,2019,2,Rect value not updating x postition?,"<p>I'm creating a game with cars coming at you. I want the cars to change x_pos every time they go off the screen, but for some reason the value changes but the cars don't actually move. I don't know how to fix this and was wondering if there's something I have to do to make it change. If you want to test my code just remove the background and everything else will work. Thanks.</p>

<pre><code>
import time, pygame, random
import math
pygame.init()
#First Variables
BLACK = (0, 0, 0)
BLUE = (0, 0, 255)
GREEN = (100, 255, 100)
ORANGE = (255, 140, 0)
YELLOW = (155, 135, 12)
white = (255, 255, 255)
GOLD = (255, 215, 0)
screenwidth = 500
screenheight = 500
Lines = True
Space = True
color = random.sample(range(250), 3)
i = []
x = 247
y = - 20
y1 = 40
y2 = 100
y3 = 160
y4 = 220
y5 = 280
y6 = 340
y7 = 400
y8 = 460
#Display and caption
win = pygame.display.set_mode((screenwidth, screenheight))
pygame.display.set_caption('Lil cary')
clock = pygame.time.Clock()
#Load in images
bkg = pygame.image.load('BG.jpg')

#Actual player class
class player():
    def __init__(self):
        self.color = (12, 124, 134)
        self.vel = 5
        self.grassdamage = 0
        self.image = pygame.Surface([50, 100], pygame.SRCALPHA, 32)
        pygame.draw.rect(self.image, (self.color), (0, 0, 50, 100))
        self.rect = self.image.get_rect(center = (200, 390))
        self.damage1 = False
        self.damage2 = False
        self.damage3 = False
        self.damage4 = False
        self.damage5 = False
        self.damage6 = False
        self.damage7 = False
        self.damage8 = False
        self.dead = False
        self.score = 0
    def draw (self, win):
        #pygame.draw.rect(win, (self.color), (self.x, self.y, self.width, self.height))
        win.blit(self.image, self.rect.topleft)
    def moveback(self):
        if self.rect.y &lt; 390 and self.rect.y &gt;= 0:
            self.rect.y += 10
    def grass(self):
        if self.rect.x &gt; -10 and self.rect.x &lt; 150:
            self.grassdamage += 1
            self.vel = 3
        elif self.rect.x &gt; 300 and self.rect.x &lt; 500:
            self.vel =3
            self.grassdamage += 1
        else:
            self.vel = 5
    def grasshealth(self):
        if self.grassdamage == 100:
            self.damage1 = True
        elif self.grassdamage == 200:
            self.damage2 = True
        elif self.grassdamage == 300:
            self.damage3 = True
        elif self.grassdamage == 400:
            self.damage4 = True
        elif self.grassdamage == 500:
            self.damage5 = True
        elif self.grassdamage == 600:
            self.damage6 = True
        elif self.grassdamage == 600:
            self.damage6 = True
        elif self.grassdamage == 700:
            self.damage7 = True
        elif self.grassdamage == 800:
            self.damage8 = True
            self.dead = True
    def death(self):
        if self.dead == True:
            raise SystemExit ('YOU MOFO DIED')
    def scorecount (self):
        if self.score &gt;= 15:
            raise SystemExit ('YOU WIN OMG KIDS')
    def scoredraw(self):
        Text = pygame.font.Font('freesansbold.ttf', 25)
        TextSurf, TextRect1 = text_objects(""Score: "" + str(self.score), Text)
        TextRect1.center = ((50), (450))
        win.blit(TextSurf, TextRect1)

#Text Setup
def text_objects(text, font):
    textSurface = font.render(text, True, BLACK)
    return textSurface, textSurface.get_rect()
#Drawing damage bar
def drawbar ():
    if player.damage1 == True and car.damage1 == False or car.damage1 == True:
        pygame.draw.rect(win, (GOLD), (50, 50, 25, 25))
    if player.damage2 == True and car.damage1 == False or car.damage1 == True:
        pygame.draw.rect(win, (GOLD), (50, 75, 25, 25))
    if car.damage1 == True and player.grassdamage &lt;= 200:
        player.grassdamage = 201
    if player.damage3 == True and car.damage2 == False or car.damage2 == True:
        pygame.draw.rect(win, (GOLD), (50, 100, 25, 25))
    if player.damage4 == True and car.damage2 == False or car.damage2 == True:
        pygame.draw.rect(win, (GOLD), (50, 125, 25, 25))
    if car.damage2 == True and player.grassdamage &lt;= 400:
        player.grassdamage = 401
    if player.damage5 == True and car.damage3 == False or car.damage3 == True:
        pygame.draw.rect(win, (GOLD), (50, 150, 25, 25))
    if player.damage6 == True and car.damage3 == False or car.damage3 == True:
        pygame.draw.rect(win, (GOLD), (50, 175, 25, 25))
    if car.damage3 == True and player.grassdamage &lt;= 600:
        player.grassdamage = 601
    if player.damage7 == True and car.damage4 == False or car.damage4 == True:
        pygame.draw.rect(win, (GOLD), (50, 200, 25, 25))
    if player.damage8 == True and car.damage4 == False or car.damage4 == True:
        pygame.draw.rect(win, (GOLD), (50, 225, 25, 25))
        raise SystemExit ('YOU LOST')
    if car.damage4 == True and player.grassdamage &lt;= 800:
        player.grassdamage = 800
    pygame.draw.rect(win, (0,0,0), (50, 50, 25, 200), 3)
    Text = pygame.font.Font('freesansbold.ttf', 20)
    TextSurf, TextRect = text_objects(""Damage"", Text)
    TextRect.center = ((65), (30))
    win.blit(TextSurf, TextRect)


class car1():
    ROAD_LEFT = 175
    ROAD_RIGHT = 300
    def __init__(self):
        self.x_pos = random.randrange(car1.ROAD_LEFT, car1.ROAD_RIGHT)
        self.color = random.sample(range(250), 3)
        self.image = pygame.Surface([50, 100], pygame.SRCALPHA, 32)
        pygame.draw.rect(self.image, (self.color), (0, 0, 50, 100))
        self.rect = self.image.get_rect(topleft = (self.x_pos, -100))
        self.carvel = random.randrange(8, 10)
        self.damage = 0
        self.damage1 = False
        self.damage2 = False
        self.damage3 = False
        self.damage4 = False
    def draw(self, win):
        #pygame.draw.rect(win,(self.color),self.rect)
        win.blit(self.image, self.rect.topleft)
    def move(self):
        if self.rect.y &lt; 530:
            self.rect.y += self.carvel
        else:
            self.rect.y = -100
            player.score += 1
            self.color = random.sample(range(250), 3)
            self.carvel = random.randrange(6, 8)
            self.x_pos = random.randrange(car1.ROAD_LEFT, car1.ROAD_RIGHT)
    def collision_check(self, another_object):
        if self.rect.colliderect(another_object):
            print('collison')
            self.rect.y = -100
            self.damage += 1
    def cardamage(self):
        if self.damage == 1:
            self.damage1 = True
            player.damage1 = True
            player.damage2 = True
        if self.damage == 2:
            self.damage2 = True
            player.damage3 = True
            player.damage4 = True
        if self.damage == 3:
            self.damage3 = True
            player.damage5 = True
            player.damage6 = True
        if self.damage == 4:
            self.damage4 = True
            player.damage7 = True
            player.damage8 = True


        #Putting variables to the classes
player = player()
car = car1()
car_list = [ car ]  # start with one car
car_add_time = 0
#Main drawing function
def redrawgamewindow():
    win.blit(bkg, (0, 0))
    pygame.draw.rect(win, (0, 0, 0), (150, 0, 200, 500))
    pygame.draw.rect(win, (255, 255, 255), (x, (y), 10, 30))
    pygame.draw.rect(win, (255, 255, 255), (x, (y1), 10, 30))
    pygame.draw.rect(win, (255, 255, 255), (x, (y2), 10, 30))
    pygame.draw.rect(win, (255, 255, 255), (x, (y3), 10, 30))
    pygame.draw.rect(win, (255, 255, 255), (x, (y4), 10, 30))
    pygame.draw.rect(win, (255, 255, 255), (x, (y5), 10, 30))
    pygame.draw.rect(win, (255, 255, 255), (x, (y6), 10, 30))
    pygame.draw.rect(win, (255, 255, 255), (x, (y7), 10, 30))
    pygame.draw.rect(win, (255, 255, 255), (x, (y8), 10, 30))
    player.draw(win)
    for car in car_list:
        car.draw(win)
    player.grasshealth()
    for car in car_list:
        car.cardamage()
    player.grass()
    player.death()
    for car in car_list:
        car.collision_check(player.rect)
        car.move()
    drawbar()
    player.scorecount()
    player.scoredraw()
    pygame.display.update()
#MAINLOOP

run = True
while run:
    #Making background and FPS
    clock.tick(80)
    #Quiting Funciton
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            run = False
        elif event.type == pygame.KEYDOWN:
            if event.key == pygame.K_ESCAPE:
                run = False
    time_now = pygame.time.get_ticks()

    if (time_now - car_add_time &gt; 5000):
        new_car = car1()
        car_list.append(new_car)
        car_add_time = time_now
    #Scrolling the lines
    if Lines:
        y += player.vel
        y1 += player.vel
        y2 += player.vel
        y3 += player.vel
        y4 += player.vel
        y5 += player.vel
        y6 += player.vel
        y7 += player.vel
        y8 += player.vel
    if Lines:
        if y &gt;= 500:
            y = -40
        if y1 &gt;= 500:
            y1 = -40
        if y2 &gt;= 500:
            y2 = -40
        if y3 &gt;= 500:
            y3 = -40
        if y4 &gt;= 500:
            y4 = -40
        if y5 &gt;= 500:
            y5 = -40
        if y6 &gt;= 500:
            y6 = -40
        if y7 &gt;= 500:
            y7 = -40
        if y8 &gt;= 500:
            y8 = -40
    #User input
    keys = pygame.key.get_pressed()
    #Boost controller
    if keys[pygame.K_SPACE]:
        start_time = time.time()
        if player.rect.y &gt; 200:
            player.rect.y -= 9
            Space = True
    else:
        player.moveback()
        end_time = time.time()
    #Left movement
    if  keys[pygame.K_LEFT] and player.rect.x &gt; 150:
        player.rect.x -= 5
    if keys [pygame.K_LEFT] and player.rect.x &lt;= 150:
        if player.rect.x &gt; 0:
            player.rect.x -=5
    #Right movement
    if keys[pygame.K_RIGHT] and player.rect.x &lt; 300:
        player.rect.x += 5
    if keys[pygame.K_RIGHT]and player.rect.x &gt;= 300:
        if player.rect.x &lt; 500 - player.rect.width:
            player.rect.x += 5

    #Grass and grass damage

    #MAIN DRAW RECALL
    redrawgamewindow()


</code></pre>
","['python', 'python-3.x', 'pygame']",59291009,"<p>It is not sufficient to set a random value to <code>self.x_pos</code>. For drawing the ""car"" the position of the <a href=""https://www.pygame.org/docs/ref/rect.html"" rel=""nofollow noreferrer""><code>pygame.Rect</code></a> object <code>self.rect</code> is used:</p>

<blockquote>
  <pre class=""lang-py prettyprint-override""><code>def draw(self, win):
   win.blit(self.image, self.rect.topleft)
</code></pre>
</blockquote>

<p>So you've to change <code>self.rect.x</code>, too:</p>

<pre class=""lang-py prettyprint-override""><code>class car1():
    # [...]

    def move(self):
        if self.rect.y &lt; 530:
            self.rect.y += self.carvel
        else:
            self.rect.y = -100
            player.score += 1
            self.color = random.sample(range(250), 3)
            self.carvel = random.randrange(6, 8)
            self.x_pos = random.randrange(car1.ROAD_LEFT, car1.ROAD_RIGHT)
            self.rect.x = self.x_pos # &lt;---------------------------------------
</code></pre>
",Rect value updating x postition I creating game cars coming I want cars change x pos every time go screen reason value changes cars actually move I know fix wondering something I make change If want test code remove background everything else work Thanks import time pygame random import math pygame init First Variables BLACK BLUE GREEN ORANGE YELLOW white GOLD screenwidth screenheight Lines True Space True color random sample range x Display caption win pygame display set mode screenwidth screenheight pygame display set caption Lil cary clock pygame time Clock Load images bkg pygame image load BG jpg Actual player class class player def init self self color self vel self grassdamage self image pygame Surface pygame SRCALPHA pygame draw rect self image self color self rect self image get rect center self damage False self damage False self damage False self damage False self damage False self damage,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
59353984,2019-12-16,2019,2,Pandas groupby on selected rows and columns only,"<p>For the following dataframe, I need to select rows from <code>2017-03</code> to <code>2017-05</code> for each <code>type</code>, then grouby <code>type</code> and calculate means for <code>v2</code> and <code>v3</code>:</p>

<pre><code>  type     date  v1  v2  v3
0    a  2017-01   8  16  32
1    a  2017-02   3   6  12
2    a  2017-03   6  12  24
3    a  2017-04   6  12  24
4    a  2017-05   5  10  20
5    b  2017-01   4   8  16
6    b  2017-02   3   6  12
7    b  2017-03   5  10  20
8    b  2017-04   9  18  36
9    b  2017-05   4   8  16
</code></pre>

<p>The sliced dataframe will look like this: </p>

<pre><code>  type     date  v2  v3
0    a  2017-03  12  24
1    a  2017-04  12  24
2    a  2017-05  10  20
3    b  2017-03  10  20
4    b  2017-04  18  36
5    b  2017-05   8  16
</code></pre>

<p>My expected will look like this:</p>

<pre><code>  type     v2     v3
0    a  11.33  22.67
1    b  12.00  24.00
</code></pre>

<p>How can I do that? Thank you.</p>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",59354013,"<p>You described in words something that would look like this:</p>

<pre><code>(df.loc[df['date'].between('2017-03', '2017-05')]
   .drop(['v1'], 1) 
   .groupby('type', as_index=False)
   .mean())

  type         v2         v3
0    a  11.333333  22.666667
1    b  12.000000  24.000000
</code></pre>

<p>P.S: ""date"" does not need to be treated like a datetime column since ""YYYY-MM' dates can be compared lexicographically.</p>
",Pandas groupby selected rows columns For following dataframe I need select rows type grouby type calculate means v v type date v v v b b b b b The sliced dataframe look like type date v v b b b My expected look like type v v b How I Thank,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas dataframe pandasgroupby,python pandas dataframe,0.87
59436729,2019-12-21,2019,2,Condionally and Randomly update Pandas values?,"<p>I want to build a scheduling app in python using pandas.</p>

<p>The following DataFrame is initialised where <code>0</code> denotes if a person is busy and <code>1</code> if a person is available.</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df = pd.DataFrame({'01.01.': [1,1,0], '02.01.': [0,1,1], '03.01.': [1,0,1]}, index=['Person A', 'Person B', 'Person C']) 

&gt;&gt;&gt; df
          01.01.  02.01.  03.01.
Person A       1       0       1
Person B       1       1       0
Person C       0       1       1
</code></pre>

<p>I now want to randomly schedule <code>n</code> number of people per day if they are available. In other words, for every day, if people are available (<code>1</code>), randomly set <code>n</code> number of people to scheduled (<code>2</code>).</p>

<p>I tried something as follows:</p>

<pre class=""lang-py prettyprint-override""><code># Required number of people across time / columns
required_number = [0, 1, 2]

# Iterate through time / columns
for col in range(len(df.columns)):

    # Current number of scheduled people
    current_number = (df.iloc[:, [col]].values==2).sum()

    # Iterate through indices / rows / people
    for ind in range(len(df.index)):

        # Check if they are available (1) and
        # if the required number of people has not been met yet
        if (df.iloc[ind, col]==1 and
            current_number&lt;required_number[col]):

            # Change ""free"" / 1 person to ""scheduled"" / 2
            df.iloc[ind, col] = 2

            # Increment scheduled people by one
            current_number += 1

&gt;&gt;&gt; df
          01.01.  02.01.  03.01.
Person A       1       0       2
Person B       1       2       0
Person C       0       1       2
</code></pre>

<p>This works as intended but â because I'm simply looping, I have no way of adding randomness (ie. that <code>Person A / B / C</code>) are randomly selected so long as they are available. Is there a way of directly doing so in pandas?</p>

<p>Thanks. BBQuercus</p>
","['python', 'pandas', 'dataframe']",59440446,"<p>You can randomly choose proper indices in a series and then change values corresponding to the chosen indices:</p>

<pre><code>for i in range(len(df.columns)):


    if sum(df.iloc[:,i] == 1) &gt;= required_number[i]:


        column = df.iloc[:,i].reset_index(drop=True)

        #We are going to store indices in a list 
        a = [j for j in column.index if column[j] == 1]


        random_indexes = np.random.choice(a, required_number[i], replace = False)


        df.iloc[:,i] = [column[j] if j not in random_indexes else 2 for j in column.index]
</code></pre>

<p>Now <code>df</code> is the wanted result.</p>
",Condionally Randomly update Pandas values I want build scheduling app python using pandas The following DataFrame initialised denotes person busy person available import pandas pd df pd DataFrame index Person A Person B Person C gt gt gt df Person A Person B Person C I want randomly schedule n number people per day available In words every day people available randomly set n number people scheduled I tried something follows Required number people across time columns required number Iterate time columns col range len df columns Current number scheduled people current number df iloc col values sum Iterate indices rows people ind range len df index Check available required number people met yet df iloc ind col current number lt required number col Change free person scheduled df iloc ind col Increment scheduled people one current number gt gt gt df Person A Person B Person C This works,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
59460608,2019-12-23,2019,3,How get full length of date for Datetime,"<p>I need a column for the df that will be used to group it by weeks.</p>

<p>The problem is all the reports in Tableau are build using the following format for week: <code>2019-01-01</code> it is like, using the first day of week repetitively  Mon-Sun.</p>

<p>Data:</p>

<pre><code>cw = pd.DataFrame({  ""lead_date"" : [2019-01-01 00:02:16, 2018-08-01 00:02:16 , 2017-07-07 00:02:16,  2015-12-01 00:02:16, 2016-09-01 00:02:16] ,
                    ""name"": [""aa"",""bb"",""cc"", ""dd"", ""EE""]  )}  

</code></pre>

<p>My code:</p>

<pre><code># extracting 
cw[""week""] = cw[""lead_date""].apply(lambda df: df.strftime(""%W"") )
cw[""month""] = cw[""lead_date""].apply(lambda df: df.strftime(""%m"") )
cw[""year""] = cw[""lead_date""].apply(lambda df: df.strftime(""%Y"") )
</code></pre>

<p>Output:</p>

<pre><code>lead_date            year   month  week
2019-01-01 00:02:16, 2019 ,  01  , 00
-
-
-
etc..

</code></pre>

<h2>Desired output:</h2>

<p>having week as date format rather then just <code>00 or 01 etc..</code></p>

<pre><code>lead_date            year   month  week
2019-01-01 00:02:16, 2019 ,  01  , 2019-01-01
2019-01-15 00:02:16, 2019 ,  01  , 2019-01-14
2019-01-25 00:02:16, 2019 ,  01  , 2019-01-21
2019-01-28 00:02:16, 2019 ,  01  , 2019-01-21


</code></pre>
","['python', 'pandas', 'datetime']",59460911,"<p>You can do it as follows with using <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/timedeltas.html#time-deltas"" rel=""nofollow noreferrer"">pandas.DatetimeIndex.dayofweek</a> and <a href=""https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DatetimeIndex.dayofweek.html#pandas-datetimeindex-dayofweek"" rel=""nofollow noreferrer"">pandas.Timedelta()</a> </p>

<p>(Note that the first day of <code>2019.01.01.</code> week is <code>2018.12.31.</code>):</p>

<pre><code>    import pandas as pd

    cw = pd.DataFrame({""lead_date"" : pd.DatetimeIndex([
                    ""2019-01-01 00:02:16"", ""2018-08-01 00:02:16"" , ""2017-07-07 00:02:16"",  
                    ""2015-12-01 00:02:16"", ""2016-09-01 00:02:16""]),
                    ""name"": [""aa"",""bb"",""cc"", ""dd"", ""EE""]})  
    # extracting 
    cw[""month""] = cw[""lead_date""].apply(lambda df: df.strftime(""%m"") )
    cw[""year""] = cw[""lead_date""].apply(lambda df: df.strftime(""%Y"") )
    cw[""week""] = (cw[""lead_date""] - ((cw[""lead_date""].dt.dayofweek) * 
                                     pd.Timedelta(days=1)).values.astype('M8[D]'))

    print(cw[[""lead_date"", ""year"", ""month"", ""week""]])
</code></pre>

<p><strong>Out:</strong></p>

<pre><code>            lead_date  year month       week
0 2019-01-01 00:02:16  2019    01 2018-12-31
1 2018-08-01 00:02:16  2018    08 2018-07-30
2 2017-07-07 00:02:16  2017    07 2017-07-03
3 2015-12-01 00:02:16  2015    12 2015-11-30
4 2016-09-01 00:02:16  2016    09 2016-08-29
</code></pre>
",How get full length date Datetime I need column df used group weeks The problem reports Tableau build using following format week like using first day week repetitively Mon Sun Data cw pd DataFrame lead date name aa bb cc dd EE My code extracting cw week cw lead date apply lambda df df strftime W cw month cw lead date apply lambda df df strftime cw year cw lead date apply lambda df df strftime Y Output lead date year month week etc Desired output week date format rather etc lead date year month week,"startoftags, python, pandas, datetime, endoftags",python pandas numpy endoftags,python pandas datetime,python pandas numpy,0.67
59725127,2020-01-13,2020,2,compare values in two arrays with a margin of variance,"<p>Good afternoon guys, I'm reading about the setdiff1d function of numpy library on python:
(<a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.setdiff1d.html"" rel=""nofollow noreferrer"">https://docs.scipy.org/doc/numpy/reference/generated/numpy.setdiff1d.html</a>).</p>

<p>Its serve basically to express the different values between 2 arrays, e.g. (from the link above):</p>

<pre><code>&gt;&gt;&gt; a = np.array([1, 2, 3, 2, 4, 1])
&gt;&gt;&gt; b = np.array([3, 4, 5, 6])
&gt;&gt;&gt; np.setdiff1d(a, b)
array([1, 2])
</code></pre>

<p>I'm wondering if exists any function that allows inserting a percentage of variation in the compared values.</p>

<p>Explanation: as well as shown in the example, the function setdiff1d will return the values that are exactly different between 2 arrays.</p>

<p>But, if I'm working with floats, and I want to allow a percentage of these values, for example, to considerer 3.35 equals to 3.34, there is any specific function that works with this situation, where I can set as an argument one variation margin between the compared values?</p>

<p>Best,</p>
","['python', 'arrays', 'numpy']",59725383,"<p>You can use something like:</p>

<pre><code>def get_difference(va, vb, margin=0):
    return [a for a in set(va) if not
        all(a &lt; (b - margin) or a &gt; (b + margin)
        for b in set(vb))]
</code></pre>
",compare values two arrays margin variance Good afternoon guys I reading setdiff function numpy library python https docs scipy org doc numpy reference generated numpy setdiff html Its serve basically express different values arrays e g link gt gt gt np array gt gt gt b np array gt gt gt np setdiff b array I wondering exists function allows inserting percentage variation compared values Explanation well shown example function setdiff return values exactly different arrays But I working floats I want allow percentage values example considerer equals specific function works situation I set argument one variation margin compared values Best,"startoftags, python, arrays, numpy, endoftags",python python3x numpy endoftags,python arrays numpy,python python3x numpy,0.67
59746629,2020-01-15,2020,2,How to create a stacked bar plot for a dataframe containing individual open/close states?,"<p>I want to plot a stacked bar plot where the state is on the x-axis and each state's bar show opened and closed proportion. I have data something like this </p>

<pre class=""lang-py prettyprint-override""><code>import matplotlib.pyplot as plt
import pandas as pd

# it's dataframe
state  complaints_status
 a         closed
 b         closed
 c         open
 a         closed
 a         open
 e         open
</code></pre>

<p>I tried to solve it but It doesn't work </p>

<pre class=""lang-py prettyprint-override""><code>states = list(complaints_df.State.unique())
closed = []
opened = []
def status(state):
    if 'Closed' in complaints_df[complaints_df['State'] == state]['Complaints_status'].value_counts():
        closed.append(complaints_df[complaints_df['State'] == state]['Complaints_status'].value_counts()['Closed'])
    if 'Open' in complaints_df[complaints_df['State'] == state]['Complaints_status'].value_counts():
        opened.append(complaints_df[complaints_df['State'] == state]['Complaints_status'].value_counts()['Open'])
for state in states:
    status(state)

# plotting stack bar chart
indx = list(range(len(states)))
plt.figure(figsize=(12,8))
graph_state = plt.bar(x=indx, height=states, width=0.35)
graph_closed = plt.bar(x=indx, height=closed, width=0.35, bottom=states)
graph_opened = plt.bar(x=indx, height=opened, widht = 0.35, bottom=closed+states)
plt.xlabel('States')
plt.ylabel('Complaints')
plt.show()
</code></pre>

<p>I realize that, this is wrong as it not depicts precisely that which state has open and closed complaints so  how should I solve it</p>
","['python', 'pandas', 'matplotlib']",59748174,"<p>This could work. I first created some random data for testing.</p>

<p>Then:</p>

<ul>
<li>groupby(['state', 'complaints_status']).size():

<ul>
<li>groups per state and per status and counts by taking the size</li>
</ul></li>
<li>... .unstack():

<ul>
<li>to convert the two rows for each state into one row with separate columns for opened and closed</li>
</ul></li>
<li>... .fillna(0):

<ul>
<li>when some state has zero opened or closed complaints, .size gave <code>Not a Number</code>, replace those with zeros</li>
</ul></li>
<li>... .plot(kind='bar', stacked=True)

<ul>
<li>create the plot</li>
</ul></li>
</ul>

<pre class=""lang-py prettyprint-override""><code>import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

states = ['Maryland','Georgia','Michigan','California','NewMexico','Indiana','Virginia','Illinois','Pennsylvania','Massachusetts','Oregon','Texas','NewHampshire','Minnesota','Tennessee','Colorado','Florida','Alabama','Washington','NewYork','NewJersey','Maine','Missouri','WestVirginia','Montana','Mississippi','Connecticut','Vermont','Kentucky','SouthCarolina','Ohio','Utah','Delaware','Arkansas','Nevada','Louisiana','Kansas','Arizona','North Carolina','Rhode Island','District of Columbia','Iowa']
status_options = [""open"", ""closed""]
num_states = len(states)

N = 300
# create a random list of states (with repitition)
complaints_states = np.random.choice(states, N)
# generate random status, with a little higher probability for ""closed""
complaints_status = np.random.choice(status_options, N, p=[0.3, 0.7])
# create a dataframe
complaints_df = pd.DataFrame({'state': complaints_states, 'complaints_status': complaints_status})

# create the plot
complaints_df.groupby(['state', 'complaints_status']).size().unstack().fillna(0).plot(kind='bar', stacked=True)
plt.xlabel('')  # remove unnecessary 'state' label
plt.ylabel('Number of Complaints')
plt.tight_layout()
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/IYG02.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IYG02.png"" alt=""generated plot""></a></p>
",How create stacked bar plot dataframe containing individual open close states I want plot stacked bar plot state x axis state bar show opened closed proportion I data something like import matplotlib pyplot plt import pandas pd dataframe state complaints status closed b closed c open closed open e open I tried solve It work states list complaints df State unique closed opened def status state Closed complaints df complaints df State state Complaints status value counts closed append complaints df complaints df State state Complaints status value counts Closed Open complaints df complaints df State state Complaints status value counts opened append complaints df complaints df State state Complaints status value counts Open state states status state plotting stack bar chart indx list range len states plt figure figsize graph state plt bar x indx height states width graph closed plt bar x indx height closed width bottom states,"startoftags, python, pandas, matplotlib, endoftags",python pandas matplotlib endoftags,python pandas matplotlib,python pandas matplotlib,1.0
60056611,2020-02-04,2020,2,Get the value of a column where one column is max and another is min,"<p>From a dataframe like this</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({
            'col1': ['a'] * 20 + ['b'] * 20,
            'col2': (['c'] * 10 + ['d'] * 10)*2,
            'col3': ( ['x'] * 3 + ['y'] * 7)*4,
            'col4': np.random.rand(40),
            'col5': np.random.rand(40),
            }
            )
</code></pre>

<p>I want to obtain the value of 'col3' where 'col4' is maximum and, if there are two or more rows with the same 'col4' value, choose the one where 'col5' is minimum.</p>

<pre><code>dg = df.groupby(('col1', 'col2'), sort=False)['col4'].max()
dh = df.groupby(('col1', 'col2'), sort=False)['col5'].min()
</code></pre>

<p>So far I have come up with this, </p>

<pre><code>dg = df.groupby(('col1', 'col2'), sort=False)\
        .agg({'col4':'max', 'col5':'min'})
</code></pre>

<p>but it gives me the maximum 'col4' and minimum 'col5' independently, whereas I want the 'col5' value to be the one corresponding to the maximum 'col4' (if there is only one 'col4' maximum row).</p>

<p>Additionally, I would want to have the 'col3' value correspondinig to the max 'col4'. I have this now:</p>

<pre><code>df.loc[df.groupby(('col1', 'col2'), sort=False)['col4'].idxmax()]['col3'].reset_index()['col3']
</code></pre>

<p>which gives me the column I want, but when I put it in the new dataframe, I don't get what I expect, which would be the max 'col4' and min 'col5' and their corresponding 'col3' value:</p>

<pre><code>dg['col3'] = df.loc[df.groupby(('col1', 'col2'), sort=False)['col4'].idxmax()]['col3'].reset_index()['col3']
</code></pre>

<p>Example:</p>

<pre><code>import numpy as np
import pandas as pd

np.random.seed(2020)
df = pd.DataFrame({ 'col1': ['a'] * 10 + ['b'] * 10, 'col2': (['c'] * 5 + ['d'] * 5)*2, 'col3': ( ['x'] * 5 + ['y'] * 5)*2, 'col4': np.random.randint(5, size=20), 'col5': np.random.randint(5, size=20), } )
</code></pre>

<pre><code>   col1 col2 col3  col4  col5
0     a    c    x     0     4
1     a    c    x     0     1
2     a    c    x     3     1
3     a    c    x     3     2
4     a    c    x     3     1
5     a    d    y     3     2
6     a    d    y     0     4
7     a    d    y     0     4
8     a    d    y     0     2
9     a    d    y     0     3
10    b    c    x     2     4
11    b    c    x     1     1
12    b    c    x     3     4
13    b    c    x     3     1
14    b    c    x     2     3
15    b    d    y     3     2
16    b    d    y     0     0
17    b    d    y     4     1
18    b    d    y     4     1
19    b    d    y     0     2
</code></pre>

<p>The expected output would be:</p>

<pre><code>   col1 col2 col3  col4  col5
     a    c    x     3     1
     a    c    x     3     1
     a    d    y     3     2
     b    c    x     3     1
     b    d    y     4     1
     b    d    y     4     1

</code></pre>
","['python', 'pandas', 'dataframe']",60056681,"<p>Use:</p>

<pre><code>np.random.seed(2020)
df = pd.DataFrame({ 'col1': ['a'] * 10 + ['b'] * 10, '
                   col2': (['c'] * 5 + ['d'] * 5)*2, 
                   'col3': ( ['x'] * 5 + ['y'] * 5)*2, 
                   'col4': np.random.randint(5, size=20), 
                   'col5': np.random.randint(5, size=20), } )
print (df)
   col1 col2 col3  col4  col5
0     a    c    x     0     4
1     a    c    x     0     1
2     a    c    x     3     1
3     a    c    x     3     2
4     a    c    x     3     1
5     a    d    y     3     2
6     a    d    y     0     4
7     a    d    y     0     4
8     a    d    y     0     2
9     a    d    y     0     3
10    b    c    x     2     4
11    b    c    x     1     1
12    b    c    x     3     4
13    b    c    x     3     1
14    b    c    x     2     3
15    b    d    y     3     2
16    b    d    y     0     0
17    b    d    y     4     1
18    b    d    y     4     1
19    b    d    y     0     2
</code></pre>

<p>First are filtered all rows with <a href=""http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer""><code>boolean indexing</code></a> by maximal <code>col4</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.transform.html"" rel=""nofollow noreferrer""><code>GroupBy.transform</code></a> and comparing with <code>col4</code> with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.eq.html"" rel=""nofollow noreferrer""><code>Series.eq</code></a></p>

<hr>

<pre><code>df1 = df[df.groupby(['col1', 'col2'])['col4'].transform('max').eq(df['col4'])]
print (df1)
   col1 col2 col3  col4  col5
2     a    c    x     3     1
3     a    c    x     3     2
4     a    c    x     3     1
5     a    d    y     3     2
12    b    c    x     3     4
13    b    c    x     3     1
17    b    d    y     4     1
18    b    d    y     4     1

#if need only some columns filter by list
cols = ['col1','col2','col3', 'col4','col5']
mask = df1.groupby(['col1', 'col2'], sort=False)['col5'].transform('min').eq(df1['col5'])
df2 = df1.loc[mask, cols]
print (df2)
   col1 col2 col3  col4  col5
2     a    c    x     3     1
4     a    c    x     3     1
5     a    d    y     3     2
13    b    c    x     3     1
17    b    d    y     4     1
18    b    d    y     4     1
</code></pre>
",Get value column one column max another min From dataframe like import pandas pd import numpy np df pd DataFrame col b col c col x col np random rand col np random rand I want obtain value col col maximum two rows col value choose one col minimum dg df groupby col col sort False col max dh df groupby col col sort False col min So far I come dg df groupby col col sort False agg col max col min gives maximum col minimum col independently whereas I want col value one corresponding maximum col one col maximum row Additionally I would want col value correspondinig max col I df loc df groupby col col sort False col idxmax col reset index col gives column I want I put new dataframe I get I expect would max col min col corresponding col value dg col df loc,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
60189328,2020-02-12,2020,2,Arbitrary number of different groupby levels in one go,"<p>Is there a way to compute arbitrary number of different groupby levels in one go with some pre-built Pandas function?
Below is a simple example with two columns.</p>

<pre><code>import pandas as pd

df1 = pd.DataFrame( { 
    ""name"" : [""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""], 
    ""city"" : [""Seattle"", ""Seattle"", ""Portland"", ""Seattle"", ""Seattle"", ""Portland""],  
    ""dollars"":[1, 1, 1, 1, 1, 1] })

group1 = df1.groupby(""city"").dollars.sum().reset_index()
group1['name']='All'

group2 = df1.groupby(""name"").dollars.sum().reset_index()
group2['city']='All'

group3 = df1.groupby([""name"", ""city""]).dollars.sum().reset_index()

total = df1.dollars.sum()
total_df=pd.DataFrame({ 
    ""name"" : [""All""], 
    ""city"" : [""All""],  
    ""dollars"": [total] })

all_groups = group3.append([group1, group2, total_df], sort=False)


    name    city    dollars
0   Alice   Seattle     1
1   Bob     Seattle     2
2   Mallory Portland    2
3   Mallory Seattle     1
0   All     Portland    2
1   All     Seattle     4
0   Alice   All         1
1   Bob     All         2
2   Mallory All         3
0   All     All         6
</code></pre>

<p>So I took Ben. T example and rebuilt it from sum() to agg(). The next step for me is to build an option to pass a specific list of groupby combinations, in case not all of them are needed.</p>

<pre><code>from itertools import combinations
import pandas as pd

df1 = pd.DataFrame( { 
    ""name"" : [""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""], 
    ""city"" : [""Seattle"", ""Seattle"", ""Portland"", ""Seattle"", ""Seattle"", ""Portland""],  
    ""dollars"":[1, 2, 6, 5, 3, 4],
    ""qty"":[2, 3, 4, 1, 5, 6] ,
    ""id"":[1, 1, 2, 2, 3, 3] 
})

col_gr = ['name', 'city']
agg_func={'dollars': ['sum', 'max', 'count'], 'qty': ['sum'], ""id"":['nunique']}

def multi_groupby(in_df, col_gr, agg_func, all_value=""ALL""):
    tmp1 = pd.DataFrame({**{col: all_value for col in col_gr}}, index=[0])
    tmp2 = in_df.agg(agg_func)\
                .unstack()\
                .to_frame()\
                .transpose()\
                .dropna(axis=1)
    tmp2.columns = ['_'.join(col).strip() for col in tmp2.columns.values]
    total = tmp1.join(tmp2)

    for r in range(len(col_gr), 0, -1):
        for cols in combinations(col_gr, r):
            tmp_grp = in_df.groupby(by=list(cols))\
                .agg(agg_func)\
                .reset_index()\
                .assign(**{col: all_value for col in col_gr if col not in cols})
            tmp_grp.columns = ['_'.join(col).rstrip('_') for col in tmp_grp.columns.values]
            total = pd.concat([total]+[tmp_grp], axis=0, ignore_index=True)
    return total

multi_groupby(df1, col_gr, agg_func)


</code></pre>
","['python', 'pandas', 'pandas-groupby']",60191300,"<p>Assuming you look for a general way to create all the combinations in the <code>groupby</code>, you can use <a href=""https://docs.python.org/3.8/library/itertools.html#itertools.combinations"" rel=""nofollow noreferrer"">itertools.combinations</a>:</p>

<pre><code>from itertools import combinations

col_gr = ['name', 'city']
col_sum = ['dollars']

all_groups = pd.concat( [ df1.groupby(by=list(cols))[col_sum].sum().reset_index()\
                             .assign(**{col:'all' for col in col_gr if col not in cols})
                         for r in range(len(col_gr), 0, -1) for cols in combinations(col_gr, r) ] 
                      + [ pd.DataFrame({**{col:'all' for col in col_gr}, 
                                        **{col: df1[col].sum() for col in col_sum},}, index=[0])], 
                        axis=0, ignore_index=True)
print (all_groups)

      name      city  dollars
0    Alice   Seattle        1
1      Bob   Seattle        2
2  Mallory  Portland        2
3  Mallory   Seattle        1
4    Alice       all        1
5      Bob       all        2
6  Mallory       all        3
7      all  Portland        2
8      all   Seattle        4
9      all       all        6
</code></pre>
",Arbitrary number different groupby levels one go Is way compute arbitrary number different groupby levels one go pre built Pandas function Below simple example two columns import pandas pd df pd DataFrame name Alice Bob Mallory Mallory Bob Mallory city Seattle Seattle Portland Seattle Seattle Portland dollars group df groupby city dollars sum reset index group name All group df groupby name dollars sum reset index group city All group df groupby name city dollars sum reset index total df dollars sum total df pd DataFrame name All city All dollars total groups group append group group total df sort False name city dollars Alice Seattle Bob Seattle Mallory Portland Mallory Seattle All Portland All Seattle Alice All Bob All Mallory All All All So I took Ben T example rebuilt sum agg The next step build option pass specific list groupby combinations case needed itertools import combinations import pandas,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas numpy endoftags,python pandas pandasgroupby,python pandas numpy,0.67
60553561,2020-03-05,2020,5,How to optimize such codes as follows in python?,"<p>I have a user-own metric to implement as follows:</p>

<pre><code>def metric(pred:pd.DataFrame(), valid:pd.DataFrame()):
    date_begin = valid.dt.min()
    date_end = valid.dt.max()
    x = valid[valid.label == 1].dt.min()

    # p
    p_n_tpp_df = valid[(valid.dt &gt;= x) &amp;\
                       (valid.dt &lt;= x + timedelta(days=30)) &amp;\
                       (p_n_tpp_df.label == 1)]
    p_n_pp_df =  valid[(valid.dt &gt;= date_begin + timedelta(days=30)) &amp;\ 
                       (valid.dt &lt;= date_end + timedelta(days=30)) &amp;\
                       (p_n_tpp_df.label == 1)]


    p_n_tpp = len([x for x in pred.serial_number.values\ 
                     if x in p_n_tpp_df.serial_number.unique()])
    p_n_pp = len([x for x in pred.serial_number.values\ 
                    if x in p_n_pp_df.serial_number.unique()])

    p = p_n_tpp / p_n_pp
    print('p: ', p)

    # r
    p_n_tpr_df = valid[(valid.dt &gt;= date_begin - timedelta(days=30)) &amp;\ 
                      (valid.dt &lt;= date_end - timedelta(days=30)) &amp;\
                      (p_n_tpr_df.label == 1)]
    p_n_pr_df = valid[(valid.dt &gt;= date_begin) &amp;\ 
                      (valid.dt &lt;= date_end) &amp;\ 
                      (p_n_pr_df.label == 1)]


    p_n_tpr = len([x for x in pred.serial_number.values\
                     if x in p_n_tpr_df.serial_number.unique()])
    p_n_pr = len([x for x in pred.serial_number.values\
                    if x in p_n_pr_df.serial_number.unique()])

    r = p_n_tpr / p_n_pr
    print('p: ', r)

    m = 2 * p * r / (p + r)

    return m
</code></pre>

<p>The <code>pd.DataFrame()</code> of <code>pred</code> and <code>valid</code> have the same columns and <code>dt</code> has no intersections. <br>
And the all the values of <code>serial_number</code> in <code>valid</code> is a subset of all the values of <code>serial_number</code> in <code>pred</code>.<br>
The <code>label</code> column only has 2 values: 0 or 1.<br>
Here is the sample of <code>pred</code> and <code>valid</code> is as follows:</p>

<pre><code>
print(pred.head(3))
    serial_number  dt          label  
0   123            2011-03-21  1
1   52             2011-03-22  0
2   12             2011-03-01  1
..., ...


print(pred.info())
Int64Index: 10000000 entries,
Data columns (total 3 columns):
serial_number  int32
dt             datetimes64[ns]
label          int8
..., ...

print(valid.head(3))
    serial_number  dt          label  
0   324            2011-04-22  1
1   52             2011-04-22  0
2   14             2011-04-01  1
..., ...


print(valid.info())
Int64Index: 10000000 entries,
Data columns (total 3 columns):
serial_number  int32
dt             datetimes64[ns]
label          int8
</code></pre>

<p>And the size of input <code>pd.DataFrame</code> is about 10, 000, 000 samples and 3 features.<br>
When I try to use it to calculate this metric, it is really slow and time spending is more than 2 hours on Intel 9600KF.<br>
So I am wondering how to optimize such code on time cost.<br>
Thanks in advance.</p>
","['python', 'pandas', 'numpy']",60658197,"<p>Here is the biggest performance win in the code that you have:</p>

<h3>Numpy set logic</h3>

<pre><code>len([x for x in pred.serial_number.values\
                     if x in p_n_tpr_df.serial_number.unique()])
</code></pre>

<p>Any line that looks like this is getting the size of the set intersection of <code>pred.serial_number</code> and <code>p_n_tpr_df.serial_number</code>. Using numpy rather than the list comprehension and the <code>unique</code> call will save substantial compute time:</p>

<pre><code>intersect_size = np.intersect1d(pred.serial_number.values,
                                p_n_tpr_df.serial_number.values).shape[0]
</code></pre>
",How optimize codes follows python I user metric implement follows def metric pred pd DataFrame valid pd DataFrame date begin valid dt min date end valid dt max x valid valid label dt min p p n tpp df valid valid dt gt x amp valid dt lt x timedelta days amp p n tpp df label p n pp df valid valid dt gt date begin timedelta days amp valid dt lt date end timedelta days amp p n tpp df label p n tpp len x x pred serial number values x p n tpp df serial number unique p n pp len x x pred serial number values x p n pp df serial number unique p p n tpp p n pp print p p r p n tpr df valid valid dt gt date begin timedelta days amp valid dt lt date end timedelta days,"startoftags, python, pandas, numpy, endoftags",python python3x list endoftags,python pandas numpy,python python3x list,0.33
60618191,2020-03-10,2020,2,Django Foreignkey reverse access when parent model has multiple Foreignkeys,"<p>I am using the standard Django User-model and wrote this cutom Model:</p>

<pre><code>class Messages(models.Model):
    sender = models.ForeignKey(User, related_name=""sender"", on_delete=models.CASCADE)
    receiver = models.ForeignKey(User, related_name=""receiver"", on_delete=models.CASCADE)
    content = models.TextField()
    date = models.DateTimeField(default=timezone.now)
</code></pre>

<p>Now, given a User-object <code>user</code> i want to access all the Messages he either sent or received. I tried: <code>user.messages_set.all()</code> but i am getting the following Error: <code>'User' object has no attribute 'messages_set'</code>.
How do i fix this?</p>

<p>Thanks for your Answers!</p>
","['python', 'django', 'django-models']",60618240,"<p>Cause that you defined <code>related_name='sender'</code>. If you use related name, you must use it for backward queries. You can use this code for access your use messages:</p>

<pre><code>user.sender.all()
</code></pre>

<p>You can use more detail about related_name <a href=""https://docs.djangoproject.com/en/3.0/ref/models/fields/#django.db.models.ForeignKey.related_name"" rel=""nofollow noreferrer"">here</a></p>
",Django Foreignkey reverse access parent model multiple Foreignkeys I using standard Django User model wrote cutom Model class Messages models Model sender models ForeignKey User related name sender delete models CASCADE receiver models ForeignKey User related name receiver delete models CASCADE content models TextField date models DateTimeField default timezone Now given User object user want access Messages either sent received I tried user messages set getting following Error User object attribute messages set How fix Thanks Answers,"startoftags, python, django, djangomodels, endoftags",python django djangorestframework endoftags,python django djangomodels,python django djangorestframework,0.67
60780433,2020-03-20,2020,2,How can I pass a list of columns to select in pyspark dataframe?,"<p>I have list column names.</p>

<pre class=""lang-py prettyprint-override""><code>columns = ['home','house','office','work']
</code></pre>

<p>and I would like to pass that list values as columns name in ""select"" dataframe.</p>

<p>I have tried it...</p>

<pre class=""lang-py prettyprint-override""><code>df_tables_full = df_tables_full.select('time_event','kind','schema','table',columns)
</code></pre>

<p>but I have received error below..</p>

<pre class=""lang-py prettyprint-override""><code>TypeError: Invalid argument, not a string or column: ['home', 'house', 'office',
'work'] of type &lt;class 'list'&gt;. For column literals, use 'lit', 'array', 'struct' 
or 'create_map' function.
</code></pre>

<p>Can you have any ideia?
Thank you guys!</p>
","['python', 'apache-spark', 'pyspark']",60780484,"<p>Use <code>*</code> before  <code>columns</code> to unnest columns list and use in <code>.select</code>.</p>

<pre><code>columns = ['home','house','office','work']

#select the list of columns
df_tables_full.select('time_event','kind','schema','table',*columns).show()

df_tables_full = df_tables_full.select('time_event','kind','schema','table',*columns)
</code></pre>
",How I pass list columns select pyspark dataframe I list column names columns home house office work I would like pass list values columns name select dataframe I tried df tables full df tables full select time event kind schema table columns I received error TypeError Invalid argument string column home house office work type lt class list gt For column literals use lit array struct create map function Can ideia Thank guys,"startoftags, python, apachespark, pyspark, endoftags",python arrays numpy endoftags,python apachespark pyspark,python arrays numpy,0.33
60941224,2020-03-30,2020,2,How can I efficiently and idiomatically filter rows of PandasDF based on multiple StringMethods on a single column?,"<p>I have a Pandas DataFrame <code>df</code> with many columns, of which one is:</p>

<pre><code>col
---
abc:kk__LL-z12-1234-5678-kk__z
def:kk_A_LL-z12-1234-5678-kk_ss_z
abc:kk_AAA_LL-z12-5678-5678-keek_st_z
abc:kk_AA_LL-xx-xxs-4rt-z12-2345-5678-ek__x
...
</code></pre>

<p>I am trying to fetch all records where <code>col</code> starts with <code>abc:</code> and has the first <code>-num-</code> between <code>'1234'</code> and <code>'2345'</code> (inclusive using a string search; the <code>-num-</code> parts are exactly 4 digits each). </p>

<p>In the case above, I'd return</p>

<pre><code>col
---
abc:kk__LL-z12-1234-5678-kk__z
abc:kk_AA_LL-z12-2345-5678-ek__x
...
</code></pre>

<p>My current (working, I think) solution looks like:</p>

<pre><code>df = df[df['col'].str.startswith('abc:')]
df = df[df['col'].str.extract('.*-(\d+)-(\d+)-.*')[0].ge('1234')]
df = df[df['col'].str.extract('.*-(\d+)-(\d+)-.*')[0].le('2345')]
</code></pre>

<p>What is a more idiomatic and efficient way to do this in Pandas? </p>
","['python', 'pandas', 'dataframe']",60941314,"<p>Complex string operations are not as efficient as numeric calculations. So the following approach might be more efficient:</p>

<pre><code>m1 = df['col'].str.startswith('abc')
m2 = pd.to_numeric(df['col'].str.split('-').str[2]).between(1234, 2345)

dfn = df[m1&amp;m2]

                                col
0    abc:kk__LL-z12-1234-5678-kk__z
3  abc:kk_AA_LL-z12-2345-5678-ek__x
</code></pre>
",How I efficiently idiomatically filter rows PandasDF based multiple StringMethods single column I Pandas DataFrame df many columns one col abc kk LL z kk z def kk A LL z kk ss z abc kk AAA LL z keek st z abc kk AA LL xx xxs rt z ek x I trying fetch records col starts abc first num inclusive using string search num parts exactly digits In case I return col abc kk LL z kk z abc kk AA LL z ek x My current working I think solution looks like df df df col str startswith abc df df df col str extract ge df df df col str extract le What idiomatic efficient way Pandas,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
61409702,2020-04-24,2020,3,Trying to delay a specific function for spawning enemy after a certain amount of time,"<p>I am making a mole shooter game using pygame. I want my mole to spawn at a random position after every 1 second. I have tried using time.sleep(1.0) but that delays my whole code and thus the game doesn't function properly because of delayed responses. I am moving an aim using the mouse(which also gets affected because of time.sleep) to which i will be adding a click to shoot. I need help with delaying and spawning my mole. I would also like some opinions on how to organize my code to provide various levels of difficulty and a main menu later on.</p>

<pre><code>import pygame
import random
import time
from threading import Timer

pygame.font.init()



win_width = 1000
win_height = 710

FPS = 60


screen = pygame.display.set_mode((win_width, win_height))

pygame.display.set_caption(""Mole Shooter"")



white = (255,255,255)
red = (255, 0, 0)



counter, text = 30, 'Time Left: 30'.rjust(3)
pygame.time.set_timer(pygame.USEREVENT, 1000)

font = pygame.font.Font('freesansbold.ttf', 32)



run = True
clock = pygame.time.Clock()
background = pygame.transform.scale(pygame.image.load('back_land.png'), (win_width, win_height))

aim = pygame.image.load(""aim.png"")
mole = pygame.image.load(""mole.png"")


def mole_spawn_easy():

    molex = random.randint(50, 950)
    moley = random.randint(450, 682)

    screen.blit(mole, (molex, moley))


while run:
    screen.blit(background, [0,0])
    ax, ay = pygame.mouse.get_pos()

    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            run = False

        if event.type == pygame.USEREVENT:
            counter -= 1
            text = (""Time Left: "" + str(counter)).rjust(3)
            if counter &gt; 0:
                time.sleep(1.0);mole_spawn_easy()

            else:
                print(""game over"")
                break



    screen.blit(aim, ((ax - 32 ),(ay - 32)))



    screen.blit(font.render(text, True, (0, 0, 0)), (32, 48))

    clock.tick(FPS)

    pygame.display.flip()
</code></pre>
","['python', 'python-3.x', 'pygame']",61410788,"<p>In pygame exists a timer event. Use <a href=""https://www.pygame.org/docs/ref/time.html#pygame.time.set_timer"" rel=""nofollow noreferrer""><code>pygame.time.set_timer()</code></a> to repeatedly create a <a href=""https://www.pygame.org/docs/ref/event.html"" rel=""nofollow noreferrer""><code>USEREVENT</code></a> in the event queue.. The time has to be set in milliseconds:</p>
<pre class=""lang-py prettyprint-override""><code>pygame.time.set_timer(pygame.USEREVENT, 1000) # 1 second
</code></pre>
<p>Note, in pygame customer events can be defined. Each event needs a unique id. The ids for the user events have to be between <code>pygame.USEREVENT</code> (24) and <code>pygame.NUMEVENTS</code> (32). In this case the value of <code>pygame.USEREVENT</code> is the event id for the timer event.</p>
<p>Receive the event in the event loop:</p>
<pre class=""lang-py prettyprint-override""><code>running = True
while run:

    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            run = False

         elif event.type == pygame.USEREVENT:
             # [...]
</code></pre>
<p>The timer event can be stopped by passing 0 to the <em>time</em> argument of <code>pygame.time.set_timer</code>.</p>
<p>See also <a href=""https://stackoverflow.com/questions/62112754/spawning-multiple-instances-of-the-same-object-concurrently-in-python/62112894#62112894"">Spawning multiple instances of the same object concurrently in python</a>.</p>
<hr />
<p>Create a list of <code>moles</code> and add a random position to the list in <code>mole_spawn_easy</code>:</p>
<pre class=""lang-py prettyprint-override""><code>moles = []

def mole_spawn_easy():
    molex = random.randint(50, 950)
    moley = random.randint(450, 682)
    moles.append((molex, moley))
</code></pre>
<p>Draw the <code>moles</code> in the main application loop:</p>
<pre class=""lang-py prettyprint-override""><code>while run:
    # [...]

    for pos in moles:
        screen.blit(mole, pos)
</code></pre>
<hr />
<p>See the example:</p>
<pre class=""lang-py prettyprint-override""><code>moles = []

def mole_spawn_easy():
    molex = random.randint(50, 950)
    moley = random.randint(450, 682)
    moles.append((molex, moley))

pygame.time.set_timer(pygame.USEREVENT, 1000)

while run:
    
    ax, ay = pygame.mouse.get_pos()
    
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            run = False

        if event.type == pygame.USEREVENT:
            counter -= 1
            text = (&quot;Time Left: &quot; + str(counter)).rjust(3)
            if counter &gt; 0:
                mole_spawn_easy()
            else:
                print(&quot;game over&quot;)

    screen.blit(background, [0,0])
    
    for pos in moles:
        screen.blit(mole, pos)
    screen.blit(aim, ((ax - 32 ),(ay - 32)))
    screen.blit(font.render(text, True, (0, 0, 0)), (32, 48))
    
    pygame.display.flip()
    clock.tick(FPS)
</code></pre>
",Trying delay specific function spawning enemy certain amount time I making mole shooter game using pygame I want mole spawn random position every second I tried using time sleep delays whole code thus game function properly delayed responses I moving aim using mouse also gets affected time sleep adding click shoot I need help delaying spawning mole I would also like opinions organize code provide various levels difficulty main menu later import pygame import random import time threading import Timer pygame font init win width win height FPS screen pygame display set mode win width win height pygame display set caption Mole Shooter white red counter text Time Left rjust pygame time set timer pygame USEREVENT font pygame font Font freesansbold ttf run True clock pygame time Clock background pygame transform scale pygame image load back land png win width win height aim pygame image load aim png mole pygame,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
61466507,2020-04-27,2020,2,How to add Gaussian noise with varying std during training?,"<p>I am training a CNN using keras and tensorflow. I would like to add Gaussian noise to my input data during training and reduce the percentage of the noise in further steps. What I do right now, I use:</p>
<pre><code>from tensorflow.python.keras.layers import Input, GaussianNoise, BatchNormalization
inputs = Input(shape=x_train_n.shape[1:])
bn0 = BatchNormalization(axis=1, scale=True)(inputs)
g0 = GaussianNoise(0.5)(bn0) 
</code></pre>
<p>The variable that GaussianNoise takes is the standard deviation of the noise distribution and I couldn't assign a dynamic value to it, how can I add for example a noise, and then decrease this value based on the epoch that I am in?</p>
","['python', 'tensorflow', 'keras']",61467929,"<p>You can simply design a custom <code>callback</code> which changes the <code>stddev</code> before training for a epoch.</p>

<p>Reference: </p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/GaussianNoise"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/GaussianNoise</a></p>

<p><a href=""https://www.tensorflow.org/guide/keras/custom_callback"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/keras/custom_callback</a></p>

<pre><code>from tensorflow.keras.layers import Input, Dense, Add, Activation
from tensorflow.keras.models import Model
import tensorflow as tf
import numpy as np
import random


from tensorflow.python.keras.layers import Input, GaussianNoise, BatchNormalization
inputs = Input(shape=100)
bn0 = BatchNormalization(axis=1, scale=True)(inputs)
g0 = GaussianNoise(0.5)(bn0) 
d0 = Dense(10)(g0)
model = Model(inputs, d0)

model.compile('adam', 'mse')
model.summary()


class MyCustomCallback(tf.keras.callbacks.Callback):

  def on_epoch_begin(self, epoch, logs=None):
    self.model.layers[2].stddev = random.uniform(0, 1)
    print('updating sttdev in training')
    print(self.model.layers[2].stddev)


X_train = np.zeros((10,100))
y_train = np.zeros((10,10))

noise_change = MyCustomCallback()
model.fit(X_train, 
          y_train, 
          batch_size=32, 
          epochs=5, 
          callbacks = [noise_change])

</code></pre>

<pre><code>Model: ""model_5""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_6 (InputLayer)         [(None, 100)]             0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 100)               400       
_________________________________________________________________
gaussian_noise_5 (GaussianNo (None, 100)               0         
_________________________________________________________________
dense_5 (Dense)              (None, 10)                1010      
=================================================================
Total params: 1,410
Trainable params: 1,210
Non-trainable params: 200
_________________________________________________________________
Epoch 1/5
updating sttdev in training
0.984045691131548
1/1 [==============================] - 0s 1ms/step - loss: 1.6031
Epoch 2/5
updating sttdev in training
0.02821459469022025
1/1 [==============================] - 0s 742us/step - loss: 1.5966
Epoch 3/5
updating sttdev in training
0.6102984511769268
1/1 [==============================] - 0s 1ms/step - loss: 1.8818
Epoch 4/5
updating sttdev in training
0.021155188690323512
1/1 [==============================] - 0s 1ms/step - loss: 1.2032
Epoch 5/5
updating sttdev in training
0.35950227285165115
1/1 [==============================] - 0s 2ms/step - loss: 1.8817

&lt;tensorflow.python.keras.callbacks.History at 0x7fc67ce9e668&gt;
</code></pre>
",How add Gaussian noise varying std training I training CNN using keras tensorflow I would like add Gaussian noise input data training reduce percentage noise steps What I right I use tensorflow python keras layers import Input GaussianNoise BatchNormalization inputs Input shape x train n shape bn BatchNormalization axis scale True inputs g GaussianNoise bn The variable GaussianNoise takes standard deviation noise distribution I assign dynamic value I add example noise decrease value based epoch I,"startoftags, python, tensorflow, keras, endoftags",python django djangorestframework endoftags,python tensorflow keras,python django djangorestframework,0.33
61784255,2020-05-13,2020,4,Split a pandas dataframe into two dataframes efficiently based on some condition,"<p>So, I want to split a given dataframe into two dataframes based on an if condition for a particular column.
I am currently achieving this by iterating over the whole dataframe two times. Please suggest some ways to improve this.</p>

<pre><code> player   score
 dan       10
 dmitri    45
 darren    15
 xae12     40
</code></pre>

<p>Like in the above dataframe, I want to split the df into two such that one df contains rows with the players scoring less than 15 and the other df contains the remaining rows. I want to this with one iteration only. (Also, if the answer can be generic for n dfs, it will help me a lot) 
Thanks in advance.</p>
","['python', 'pandas', 'dataframe']",61784347,"<p>IICU</p>

<p>Use boolean select</p>

<pre><code>m=df.score&gt;15

Lessthan15=df[~m]
Morethan15=df[m]
</code></pre>

<p>Morethan15</p>

<p><a href=""https://i.stack.imgur.com/tl9vf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tl9vf.png"" alt=""enter image description here""></a></p>

<p>LessThan15</p>

<p><a href=""https://i.stack.imgur.com/s7MC2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s7MC2.png"" alt=""enter image description here""></a></p>
",Split pandas dataframe two dataframes efficiently based condition So I want split given dataframe two dataframes based condition particular column I currently achieving iterating whole dataframe two times Please suggest ways improve player score dan dmitri darren xae Like dataframe I want split df two one df contains rows players scoring less df contains remaining rows I want one iteration Also answer generic n dfs help lot Thanks advance,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
61836614,2020-05-16,2020,2,How to gather all entries for a specific value with Pandas?,"<p>I'm looking for a simple way to get all available date for a specific ID. I used to use a heavy process based on unique IDs and transpose in Google Sheet. It's not performant at all as there is about 10.000 entries, it's taking about 5min to render in Google Sheet. So trying to get something similar using Python and Pandas with my basic knowledge... ;)</p>

<p>Here is a sample dataframe example and what I wanting to achieve :</p>

<pre><code>    date        email
0   2001-01-01  email1@email.com
1   2002-01-01  email1@email.com
2   2003-01-01  email2@email.com
4   2004-01-01  email2@email.com
5   2005-01-01  email1@email.com
... ... ...
</code></pre>

<p>I need to get something like </p>

<pre><code>email1@email.com    2001-01-01, 2002-01-01, 2005-01-01
email2@email.com    2003-01-01, 2004-01-01
</code></pre>

<p>I've tested several Panda's functions without success.</p>

<p>Any idea?</p>

<p>Thanks</p>
","['python', 'python-3.x', 'pandas']",61896791,"<p>Thanks all for your answers!</p>

<p>I finally achieved it using : </p>

<pre><code>df= df.groupby(""email"").agg("","".join)
df = df['date'].str.split(',', expand=True).rename(columns = lambda x: ""date""+str(x+1))
</code></pre>
",How gather entries specific value Pandas I looking simple way get available date specific ID I used use heavy process based unique IDs transpose Google Sheet It performant entries taking min render Google Sheet So trying get something similar using Python Pandas basic knowledge Here sample dataframe example I wanting achieve date email email email com email email com email email com email email com email email com I need get something like email email com email email com I tested several Panda functions without success Any idea Thanks,"startoftags, python, python3x, pandas, endoftags",python python3x pandas endoftags,python python3x pandas,python python3x pandas,1.0
61840120,2020-05-16,2020,3,Get N minimum distance pairs from pandas dataframe,"<p>Consider the following code, which generates a distance matrix from a list of labeled coordinates:</p>

<pre><code>import numpy as np
import pandas as pd
from scipy.spatial.distance import pdist, squareform

coord_data = [
    [1, 2],
    [4, 3],
    [5, 8],
    [6, 7],
]

df = pd.DataFrame(coord_data, index=list('ABCD'))

dist_matrix = squareform(pdist(df, metric='euclidean'))
dist_df = pd.DataFrame(dist_matrix, index=df.index, columns=df.index)

print(dist_df)
</code></pre>

<pre><code>          A         B         C         D
A  0.000000  3.162278  7.211103  7.071068
B  3.162278  0.000000  5.099020  4.472136
C  7.211103  5.099020  0.000000  1.414214
D  7.071068  4.472136  1.414214  0.000000
</code></pre>

<p>Is there an efficient way (using numpy, pandas, etc.) of getting the N minimum distance pairs from this distance matrix?</p>

<p>For instance, if N=2, an output similar to the following is desired for the given example:</p>

<pre><code>[['C', 'D'], ['A', 'B']] # corresponding to minimum distances [1.414214, 3.162278]
</code></pre>
","['python', 'pandas', 'numpy', 'dataframe']",61840368,"<p>Here's one with <a href=""https://numpy.org/doc/stable/reference/generated/numpy.argpartition.html"" rel=""nofollow noreferrer""><code>np.argpartition</code></a> for perf. efficiency -</p>

<pre><code>def topN_index_columns_from_symmmdist(df, N):
    a = dist_df.to_numpy(copy=True)
    a[np.tri(len(a), dtype=bool)] = np.inf
    idx = np.argpartition(a.ravel(),range(N))[:N]
    r,c = np.unravel_index(idx, a.shape)
    return list(zip(dist_df.index[r], dist_df.columns[c]))
</code></pre>

<p>Sample runs -</p>

<pre><code>In [43]: dist_df
Out[43]: 
          A         B         C         D
A  0.000000  3.162278  7.211103  7.071068
B  3.162278  0.000000  5.099020  4.472136
C  7.211103  5.099020  0.000000  1.414214
D  7.071068  4.472136  1.414214  0.000000

In [44]: topN_index_columns_from_symmmdist(df, N=2)
Out[44]: [('C', 'D'), ('A', 'B')]

In [45]: topN_index_columns_from_symmmdist(df, N=4)
Out[45]: [('C', 'D'), ('A', 'B'), ('B', 'D'), ('B', 'C')]
</code></pre>
",Get N minimum distance pairs pandas dataframe Consider following code generates distance matrix list labeled coordinates import numpy np import pandas pd scipy spatial distance import pdist squareform coord data df pd DataFrame coord data index list ABCD dist matrix squareform pdist df metric euclidean dist df pd DataFrame dist matrix index df index columns df index print dist df A B C D A B C D Is efficient way using numpy pandas etc getting N minimum distance pairs distance matrix For instance N output similar following desired given example C D A B corresponding minimum distances,"startoftags, python, pandas, numpy, dataframe, endoftags",python pandas dataframe endoftags,python pandas numpy dataframe,python pandas dataframe,0.87
61916648,2020-05-20,2020,4,Pandas Dataframe Series : check if specific value exists,"<p>I need to iterate over a list and perform a specific operation if the value from the list exists in one of the pandas dataframe column. I tried to do as below, but getting below error </p>

<p>'<strong>Error</strong>: #The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().'</p>

<pre><code>import pandas as pd

people = {
    'fname':['Alex','Jane','John'],
    'age':[20,15,25],
    'sal':[100,200,300]
}

df=pd.DataFrame(people)

check_list=['Alex','John']

for column in check_list:
    if (column == df['fname']):
        df['new_column']=df['sal']/df['age']
    else:
        df['new_column']=df['sal']

df
</code></pre>

<p><strong>Required output</strong>:</p>

<pre><code>fname   age sal new_column
Alex    20  100  5      &lt;&lt;-- sal/age
Jane    15  200  200    &lt;&lt;-- sal as it is
John    25  300  12     &lt;&lt;-- sal/age
</code></pre>
","['python', 'pandas', 'pandas-groupby']",61916728,"<p>use <code>np.where</code> with <code>.isin</code> to check if a column contains particular values.</p>

<pre><code>df['new_column'] = np.where(
        df['fname'].isin(['Alex','John']),
        df['sal']/df['age'],
        df['sal']
)

print(df)

  fname  age  sal  new_column
0  Alex   20  100         5.0
1  Jane   15  200       200.0
2  John   25  300        12.0
</code></pre>

<p>pure pandas version.</p>

<pre><code>df['new_column'] = (df['sal']/df['age']).where(
                            df['fname'].isin(['Alex','John']),other=df['sal'])
</code></pre>

<hr>

<pre><code>print(df)
 fname  age  sal  new_col
0  Alex   20  100      5.0
1  Jane   15  200    200.0
2  John   25  300     12.0
</code></pre>
",Pandas Dataframe Series check specific value exists I need iterate list perform specific operation value list exists one pandas dataframe column I tried getting error Error The truth value Series ambiguous Use empty bool item import pandas pd people fname Alex Jane John age sal df pd DataFrame people check list Alex John column check list column df fname df new column df sal df age else df new column df sal df Required output fname age sal new column Alex lt lt sal age Jane lt lt sal John lt lt sal age,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
62166941,2020-06-03,2020,2,Create single pandas dataframe from a list of dataframes,"<p>I have a list of about 25 dfs and all of the columns are the same. Although the row count is different, I am only interested in the first row of each df. 
How can I iterate through the list of dfs, copy the first row from each and concatenate them all into a single df?</p>
","['python', 'python-3.x', 'pandas']",62166967,"<p>Select first row by position with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html"" rel=""nofollow noreferrer""><code>DataFrame.iloc</code></a> and <code>[[0]]</code> for one row DataFrames and join together by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a>:</p>

<pre><code>df = pd.concat([x.iloc[[0]] for x in dfs], ignore_index=True)
</code></pre>

<p>Or use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html"" rel=""nofollow noreferrer""><code>DataFrame.head</code></a> for one row <code>DataFrame</code>s:</p>

<pre><code>df = pd.concat([x.head(1) for x in dfs], ignore_index=True)
</code></pre>
",Create single pandas dataframe list dataframes I list dfs columns Although row count different I interested first row df How I iterate list dfs copy first row concatenate single df,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
62397553,2020-06-15,2020,2,Calling button only ticks for one second and can&#39;t display text?,"<p>I am making a game and I want when I am in the garage if I buy more speed and I am already max, a text to pop up saying ""Max speed reached!"". But when I click on the button and have max speed, it doesn't stay but goes away after 1 tick. I can't figure out how to make it stay for longer because of how the button works. Any help is appreciated.
Here is the code for the button:</p>

<pre><code>            elif event.type == pygame.MOUSEBUTTONDOWN:
                # 1 is the left mouse button, 2 is middle, 3 is right.
                if event.button == 1:
                    # `event.pos` is the mouse position.
                    if button2.collidepoint(event.pos):
                        print('we been clicked')
                        current_time = time.strftime(""%S"")
                        # Increment the number
                        if player.vel &lt; 20:
                            if player.coins &gt;= 50:
                                player.vel += 5
                                player.coins -= 50
                                print('speed =' + str(player.vel))
                                player.vel1 = player.vel
                                player.grassvel = player.vel // 2
                                player.save()
                            else:
                                print(""ur poor"")
                        else:
                            MaxSpeedReached()
</code></pre>

<p>And here is the code for the function it calls:</p>

<pre><code>def MaxSpeedReached():
    display_crash_text = False
    print(""Max speed reached"")
    if display_crash_text == False:
        start_time = time.strftime(""%S"")
        display_crash_text = True
    maxspeedtext = pygame.font.Font(""freesansbold.ttf"", 20)
    maxspedd, maxspeedr = text_objects(""Max Speed  Reached!"", maxspeedtext, BLACK)
    maxspeedr.center = ((205, 270))
    print(current_time, start_time)
    if display_crash_text == True:
        win.blit(maxspedd, maxspeedr)
        if int(start_time) - int(current_time) &lt; 3:
            display_crash_text = False
            print(""yo we checking the time difference"")
</code></pre>
","['python', 'python-3.x', 'pygame']",62398876,"<p>Obviously you're seeing this because the message is drawn once when <code>MaxSpeedReached()</code> is called, but the only possible path to reach this function is on <code>pygame.MOUSEBUTTONDOWN</code> event.  </p>

<p>You need to paint the message in the main loop, whenever the conditions for max-speed are occurring. </p>

<pre><code>MAX_VELOCITY = 20

maxspeedtext = pygame.font.Font(""freesansbold.ttf"", 20)
maxspedd, maxspeedr = text_objects(""Max Speed  Reached!"", maxspeedtext, BLACK)
maxspeedr.center = ((205, 270))

[ ... ]

# Main loop
while not finished:

    [...]

    if ( player.vel &gt;= MAX_VELOCITY ):
        win.blit( maxspedd, maxspeedr )
</code></pre>
",Calling button ticks one second display text I making game I want I garage I buy speed I already max text pop saying Max speed reached But I click button max speed stay goes away tick I figure make stay longer button works Any help appreciated Here code button elif event type pygame MOUSEBUTTONDOWN left mouse button middle right event button event pos mouse position button collidepoint event pos print clicked current time time strftime S Increment number player vel lt player coins gt player vel player coins print speed str player vel player vel player vel player grassvel player vel player save else print ur poor else MaxSpeedReached And code function calls def MaxSpeedReached display crash text False print Max speed reached display crash text False start time time strftime S display crash text True maxspeedtext pygame font Font freesansbold ttf maxspedd maxspeedr text objects Max Speed Reached maxspeedtext,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
62489297,2020-06-20,2020,2,Separating a pandas data frame based on whether the string-valued entry in a specified column contains a substring,"<p>Suppose I have the following data frame:</p>
<pre><code>df = pd.DataFrame({'name':['a', 'b', 'c','d','e'], 'description':['vim2tests','vim2trial','vim3tests','vim3zip', 'vim4trial'], 'count':[4,5,6,7,8]})
</code></pre>
<p>And I am trying to separate into 3 data frames containing the rows where the 'description' entry contains the 'vim2', 'vim3', 'vim4' substring.</p>
<p>Is there an efficient way to do this? I could implement a for loop to find the indexes for the rows I want, but that is not efficient at all and I am struggling to see how to do this a better way.</p>
","['python', 'pandas', 'dataframe']",62489414,"<p>IIUC, just create a conditional column to <code>groupby</code> using <code>str.extract</code></p>
<p>we can hold the dataframes in a dictionary.</p>
<pre><code>dfs = {group : data.drop('key',1) for group,data in 
                 df.assign(key=df['description'].str.extract('(vim\d+)'))\
                           .groupby('key')
}
</code></pre>
<hr />
<pre><code>print(dfs['vim3'])
  name description  count
2    c   vim3tests      6
3    d     vim3zip      7
</code></pre>
<hr />
<pre><code>print(dfs.keys())

dict_keys(['vim2', 'vim3', 'vim4'])
</code></pre>
<p>or a much simplier solution by anky -</p>
<pre><code>dfs = dict(tuple(
           df.groupby(df['description'].str.extract('(vim\d+)'
                                        ,expand=False))
         ))
</code></pre>
<p>Or:</p>
<pre><code>dict(iter(df.groupby(df['description'].str.extract('(vim\d+)',expand=False)))
</code></pre>
<hr />
<pre><code>print(dfs)

{'vim2':   name description  count
 0    a   vim2tests      4
 1    b   vim2trial      5,
 'vim3':   name description  count
 2    c   vim3tests      6
 3    d     vim3zip      7,
 'vim4':   name description  count
 4    e   vim4trial      8}
</code></pre>
",Separating pandas data frame based whether string valued entry specified column contains substring Suppose I following data frame df pd DataFrame name b c e description vim tests vim trial vim tests vim zip vim trial count And I trying separate data frames containing rows description entry contains vim vim vim substring Is efficient way I could implement loop find indexes rows I want efficient I struggling see better way,"startoftags, python, pandas, dataframe, endoftags",python pandas datetime endoftags,python pandas dataframe,python pandas datetime,0.67
62570129,2020-06-25,2020,2,Find years in string like (90-),"<p>I don't know how to create IF with like two numbers and with &quot;-&quot;
for example</p>
<pre><code>IQ 09-

70-serie

70 84-89

80-serie

80 90-97

90-serie

90 96-99

90 99-02

but without

90-serie 80-serie
</code></pre>
<p>I created function like:</p>
<pre><code>import time
from termcolor import colored
start_time = time.time()
url=&quot;https://www.frontlykter.no/&quot;
def scrape(page):
    url = page
    soup = BeautifulSoup(requests.get(url).content, 'html.parser')
    count=0
    urls=[]

    menu=soup.select('.ls-categories-ajax')
    for a in menu[0].find_all('a', href=True):
        if &quot;-&quot; in a.text:
            object = {
                &quot;URL&quot;: a['href'],
                &quot;Name&quot;:a.text
            }

            # print(a['href'])
            print(a.text)
            count += 1
            urls.append(object)
    print(colored(&quot;Liczba linkow: &quot;, &quot;yellow&quot;), count)
    return urls
</code></pre>
<p>I need to modify my &quot;if&quot; What should I do?</p>
<p>Summarizing.
I am looking for the last category links in the menu. That is, those with vintage car models.
I don't know if it's the best idea with this &quot;if&quot;.</p>
<p>I just don't want to read links like:</p>
<p>Citroen-&gt; AX
Only those that are in AX (they have years in the name)</p>
<p>Edit or maybe find all Third UL or Last child UL?</p>
","['python', 'web-scraping', 'beautifulsoup']",62570509,"<p>I propose you this behavior :</p>
<p>So you want only model with this scheme : &quot;NUM-NUM&quot; or &quot;NUM-&quot;. That what you can code in your if function :</p>
<p>Check if '-' is the last character, so you accept it. Or, you have to check if what is after '-' is a digit.</p>
<p>To do this, you can use find which will return you the position of your find research. If what you are searching for does not exist, find return -1.</p>
<pre><code>import time
from termcolor import colored
from bs4 import BeautifulSoup
import requests

start_time = time.time()
url=&quot;https://www.frontlykter.no/&quot;

def scrape(page):
    
    def add_object(a, urls):
        object = {
                &quot;URL&quot;: a['href'],
                &quot;Name&quot;:a.text
        }
        urls.append(object)
        return urls
            
    url = page
    soup = BeautifulSoup(requests.get(url).content, 'html.parser')
    urls=[]

    menu=soup.select('.ls-categories-ajax')
    
    for a in menu[0].find_all('a', href=True):
        
        pos = a.text.find('-')
        length = len(a.text)
        
        # So there is a '-'
        if pos != -1 :
            
            # '-' is at last position so OK
            if pos == length-1 :
                print(a.text)
                urls = add_object(a, urls)
            
            # '-' is not last position
            if pos &lt; length-1 :
                # check if what is after '-' is a number and not a caracter
                if a.text[pos+1].isdigit() or a.text[pos+1]==' ':
                    print(a.text)
                    urls = add_object(a, urls)
                
    count = len(urls)
            
    print(colored(&quot;Liczba linkow: &quot;, &quot;yellow&quot;), count)
    return urls

scrape(url)
</code></pre>
",Find years string like I know create IF like two numbers quot quot example IQ serie serie serie without serie serie I created function like import time termcolor import colored start time time time url quot https www frontlykter quot def scrape page url page soup BeautifulSoup requests get url content html parser count urls menu soup select ls categories ajax menu find href True quot quot text object quot URL quot href quot Name quot text print href print text count urls append object print colored quot Liczba linkow quot quot yellow quot count return urls I need modify quot quot What I Summarizing I looking last category links menu That vintage car models I know best idea quot quot I want read links like Citroen gt AX Only AX years name Edit maybe find Third UL Last child UL,"startoftags, python, webscraping, beautifulsoup, endoftags",python arrays numpy endoftags,python webscraping beautifulsoup,python arrays numpy,0.33
62571969,2020-06-25,2020,2,Deleting colums of dataFrame where row value is constant for all rows,"<p>Starting with a pandas DataFrame such as</p>
<pre><code>import pandas as pd

df = pd.DataFrame(
    [[0, 3, 1.4, 3], [0, 3, 1.3, 1], [0, 3, 0.5, 3]]
)
</code></pre>
<p>or visually:</p>
<pre><code>   0  1  2    3  
0[[0, 3, 1.4, 3] 
1 [0, 3, 1.3, 1]
1 [0, 3, 0.5, 3]]
</code></pre>
<p>and given a special value <code>x_1=3</code></p>
<p>What would be a smart and scaling way to come up with a DataFrame that deletes all columns in df with a constant value x in EACH row?</p>
<p>The result in this example would be the dataFrame df without column 1.</p>
<p>df_altered =</p>
<pre><code>   0  1    2 
0[[0, 1.4, 3] 
1 [0, 1.3, 1]
2 [0, 0.5, 3]]
</code></pre>
<p>In a small DataFrame I could itterate over all rows for each column but that would not scale and work with large DataFrames.</p>
","['python', 'pandas', 'dataframe']",62572077,"<p>You can use <code>pd.drop()</code>:</p>
<pre><code>df.drop(columns=df.columns[(df == 3).all()])
</code></pre>
<p>Output:</p>
<pre><code>    0   2   3
0   0   1.4 3
1   0   1.3 1
2   0   0.5 3
</code></pre>
",Deleting colums dataFrame row value constant rows Starting pandas DataFrame import pandas pd df pd DataFrame visually given special value x What would smart scaling way come DataFrame deletes columns df constant value x EACH row The result example would dataFrame df without column df altered In small DataFrame I could itterate rows column would scale work large DataFrames,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
62645239,2020-06-29,2020,4,Python datetime and pandas give different timestamps for the same date,"<pre><code>from datetime import datetime
import pandas as pd

date=&quot;2020-02-07T16:05:16.000000000&quot;

#Convert using datetime
t1=datetime.strptime(date[:-3],'%Y-%m-%dT%H:%M:%S.%f')

#Convert using Pandas
t2=pd.to_datetime(date)

#Subtract the dates
print(t1-t2)

#subtract the date timestamps
print(t1.timestamp()-t2.timestamp())
</code></pre>
<p>In this example, my understanding is that both datetime and pandas should use timezone naive dates. Can anyone explain why the difference between the dates is zero, but the difference between the timestamps is not zero? It's off by 5 hours for me, which is my time zone offset from GMT.</p>
","['python', 'pandas', 'datetime']",62651796,"<p>Naive datetime objects of Python's <code>datetime.datetime</code> class represent local time. This is kind of obvious from <a href=""https://docs.python.org/3/library/datetime.html#aware-and-naive-objects"" rel=""nofollow noreferrer"">the docs</a> but can be a brain-teaser to work with nevertheless. If you call the <code>timestamp</code> method on it, the returned POSIX timestamp refers to UTC (seconds since the epoch) as it should.</p>
<p>Coming from the Python datetime object, the behavior of a naive <code>pandas.Timestamp</code> can be counter-intuitive (and I think it's not so obvious). Derived the same way from a tz-naive string, it doesn't represent local time but UTC. You can verify that by localizing the <code>datetime</code> object to UTC:</p>
<pre><code>from datetime import datetime, timezone
import pandas as pd

date = &quot;2020-02-07T16:05:16.000000000&quot;

t1 = datetime.strptime(date[:-3], '%Y-%m-%dT%H:%M:%S.%f')
t2 = pd.to_datetime(date)

print(t1.replace(tzinfo=timezone.utc).timestamp() - t2.timestamp())
# 0.0
</code></pre>
<p>The other way around you can make the <code>pandas.Timestamp</code> timezone-aware, e.g.</p>
<pre><code>t3 = pd.to_datetime(t1.astimezone())
# e.g. Timestamp('2020-02-07 16:05:16+0100', tz='MitteleuropÃ¤ische Zeit')

# now both t1 and t3 represent my local time:
print(t1.timestamp() - t3.timestamp())
# 0.0
</code></pre>
<hr />
<p>My bottom line is that if you <em>know</em> that the timestamps you have represent a certain timezone, work with timezone-aware datetime, e.g. for UTC</p>
<pre><code>import pytz # need to use pytz here since pandas uses that internally

t1 = datetime.strptime(date[:-3], '%Y-%m-%dT%H:%M:%S.%f').replace(tzinfo=pytz.UTC)
t2 = pd.to_datetime(date, utc=True)

print(t1 == t2)
# True
print(t1-t2)
# 0 days 00:00:00
print(t1.timestamp()-t2.timestamp())
# 0.0
</code></pre>
",Python datetime pandas give different timestamps date datetime import datetime import pandas pd date quot T quot Convert using datetime datetime strptime date Y dT H M S f Convert using Pandas pd datetime date Subtract dates print subtract date timestamps print timestamp timestamp In example understanding datetime pandas use timezone naive dates Can anyone explain difference dates zero difference timestamps zero It hours time zone offset GMT,"startoftags, python, pandas, datetime, endoftags",python pandas numpy endoftags,python pandas datetime,python pandas numpy,0.67
62689809,2020-07-02,2020,2,Generate fenced Matrix with m and n integers,"<p>How to generate a matrix with border 1's and 0's at the core when we give m and n positive integers.</p>
<pre><code>Input:
4,5


Output
    [1, 1, 1, 1, 1]
    [1, 0, 0, 0, 1]
    [1, 0, 0, 0, 1]
    [1, 1, 1, 1, 1]
</code></pre>
<p>I used this code.Is there any other way to get the output</p>
<pre><code>import numpy as np
a=np.ones((m,n),dtype=&quot;int&quot;)
a[1:-1,1:-1]=0
</code></pre>
","['python', 'arrays', 'numpy']",62689891,"<p>Another similar solution, however I prefer the suggested solution in the question:</p>
<pre><code>a=np.zeros((m,n),dtype=&quot;int&quot;)
a[[0,-1]] = 1
a[:,[0,-1]] = 1
</code></pre>
<p>Or per @Paul's suggestion in comments:</p>
<pre><code>a[::m-1] = a[:,::n-1] = 1
</code></pre>
<p><em><strong>EDIT</strong></em>: Per OP's comment below:</p>
<p>list of arrays:</p>
<pre><code> a = [x for x in a]
</code></pre>
<p>array of lists:</p>
<pre><code>b = np.empty(m,dtype=object)
b[:] = a.tolist()
</code></pre>
",Generate fenced Matrix n integers How generate matrix border core give n positive integers Input Output I used code Is way get output import numpy np np ones n dtype quot int quot,"startoftags, python, arrays, numpy, endoftags",python python3x list endoftags,python arrays numpy,python python3x list,0.33
62888385,2020-07-14,2020,4,How can I add filename of imported txt files to dataframe in python,"<p>I have imported a few thousand txt files from a folder into <code>pandas dataframe</code>. Is there any way I can create a column adding a sub-string from the filenames of the imported txt files in it? This is to identify each text file in the dataframe by a unique name.</p>
<p>Text files are named as <code>1001example.txt, 1002example.txt, 1003example.txt</code> and son on. I want something like this:</p>
<pre><code>filename        text
1001            this is an example text
1002            this is another example text
1003            this is the last example text
....
</code></pre>
<p>The code I have used to import the data is below. However, I do not know how to create a column by a sub-string of filenames. Any help would be appreciated. Thanks.</p>
<pre><code>import glob
import os
import pandas as pd

file_list = glob.glob(os.path.join(os.getcwd(), &quot;K:\\text_all&quot;, &quot;*.txt&quot;))

corpus = []

for file_path in file_list:
    with open(file_path, encoding=&quot;latin-1&quot;) as f_input:
        corpus.append(f_input.read())

df = pd.DataFrame({'text':corpus})
</code></pre>
","['python', 'python-3.x', 'pandas', 'dataframe']",62888470,"<p>This should work. It takes numbers from file name.</p>
<pre><code>import glob
import os
import pandas as pd

file_list = glob.glob(os.path.join(os.getcwd(), &quot;K:\\text_all&quot;, &quot;*.txt&quot;))

corpus = []
files = []

for file_path in file_list:
    with open(file_path, encoding=&quot;latin-1&quot;) as f_input:
        corpus.append(f_input.read())
        files.append(''.join([n for n in os.path.basename(file_path) if n.isdigit()]))

df = pd.DataFrame({'file':files, 'text':corpus})
</code></pre>
",How I add filename imported txt files dataframe python I imported thousand txt files folder pandas dataframe Is way I create column adding sub string filenames imported txt files This identify text file dataframe unique name Text files named example txt example txt example txt son I want something like filename text example text another example text last example text The code I used import data However I know create column sub string filenames Any help would appreciated Thanks import glob import os import pandas pd file list glob glob os path join os getcwd quot K text quot quot txt quot corpus file path file list open file path encoding quot latin quot f input corpus append f input read df pd DataFrame text corpus,"startoftags, python, python3x, pandas, dataframe, endoftags",python python3x pandas endoftags,python python3x pandas dataframe,python python3x pandas,0.87
62888747,2020-07-14,2020,4,Find the start and end date of consecutive days in a column of the same index using pandas,"<p><strong>I have a data frame <code>df</code>:</strong></p>
<pre><code>df =

index  date        hats
A1     01-01-2020  5
A1     02-01-2020  10
A1     03-01-2020  16
A1     04-01-2020  16
A1     21-01-2020  9
A1     22-01-2020  8
A1     23-01-2020  7
A6     20-03-2020  5
A6     21-03-2020  5
A8     30-07-2020  12
</code></pre>
<p>Here, the first four rows are consecutive days. I want to know the start date and end date of all such consecutive days in the data frame. If there is only one day in a series like wise <code>A8</code> index in the <code>df</code> then the start date and and end will be same. Moreover, I am also interested in knowing the highest value in <code>df['hats']</code> column in the series of consecutive days and return its date in a seperate column <code>high_hat</code> along with its date <code>high_hat_date</code>. If there are two or more equal high values in a series of consecutive days then write the number of occurrence of high value in a new column <code>num_hat</code>, and write the first occurrence date in <code>high_hat_date</code>.</p>
<p><strong>The example output for the above data frame is as follows:</strong></p>
<pre><code>index   start_date    end_date    high_hat    high_hat_date   num_hat
A1      01-01-2020    04-01-2020  16          03-01-2020      2
A1      21-01-2020    23-01-2020  9           21-01-2020      1
A6      20-03-2020    21-03-2020  5           20-03-2020      2
A8      30-07-2020    30-07-2020  12          30-07-2020      1     
</code></pre>
<p>Any help in this regard is highly appreciated.</p>
","['python', 'pandas', 'dataframe']",62889681,"<p>First using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html"" rel=""nofollow noreferrer""><code>pd.to_datetime</code></a> convert the <code>date</code> column to pandas <code>datetime</code> series:</p>
<pre><code>df['date'] = pd.to_datetime(df['date'], dayfirst=True)
</code></pre>
<p><strong>Then use:</strong></p>
<pre><code>g = df.groupby('index')['date'].diff().dt.days.ne(1).cumsum() # STEP A
m = df.groupby(['index', g])['hats'].transform('max').eq(df['hats']) # STEP B

df = df.assign(high_hats=df['hats'].mask(~m), high_date=df['date'].mask(~m)) # STEP C

dct = {'start_date': ('date', 'first'), 'end_date': ('date', 'last'), 'high_hat': ('hats', 'max'),
       'high_hat_date': ('high_date', 'first'), 'num_hats': ('high_hats', 'count')}
df1 = df.groupby(['index', g]).agg(**dct).reset_index().drop('date', 1) # STEP D
</code></pre>
<p><strong>Details:</strong></p>
<p>STEP A: Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>DataFrame.groupby</code></a> on <code>index</code> and use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.diff.html"" rel=""nofollow noreferrer""><code>groupby.diff</code></a> on <code>date</code> to calculate the the days elapsed between successive dates then use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.days.html"" rel=""nofollow noreferrer""><code>Series.dt.days</code></a> + <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cumsum.html"" rel=""nofollow noreferrer""><code>Series.ne</code></a> along with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cumsum.html"" rel=""nofollow noreferrer""><code>Series.cumsum</code></a> to create a grouping series <code>g</code> which will be needed to group the dataframe on consecutive dates.</p>
<pre><code># print(g)
0    1
1    1
2    1
3    1
4    2
5    2
6    2
7    3
8    3
9    4
Name: date, dtype: int64
</code></pre>
<p>STEP B: Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>DataFrame.groupby</code></a> on <code>index</code> and <code>g</code> and use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.transform.html"" rel=""nofollow noreferrer""><code>groupby.transform</code></a> to transform the column <code>hats</code> using <code>max</code> then using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.eq.html"" rel=""nofollow noreferrer""><code>Series.eq</code></a> equate it with <code>hats</code> column to create a boolean mask <code>m</code>.</p>
<pre><code># print(m)
0    False
1    False
2     True
3     True
4     True
5    False
6    False
7     True
8     True
9     True
Name: hats, dtype: bool
</code></pre>
<p>STEP C: Next use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html"" rel=""nofollow noreferrer""><code>DataFrame.assign</code></a> to assign two new columns <code>high_hats</code> and <code>high_date</code> which will be used in <code>STEP D</code> for calculation of <code>high_hat_date</code> and <code>num_hats</code>.</p>
<pre><code># print(df)    
  index       date  hats  high_hats  high_date
0    A1 2020-01-01     5        NaN        NaT
1    A1 2020-01-02    10        NaN        NaT
2    A1 2020-01-03    16       16.0 2020-01-03
3    A1 2020-01-04    16       16.0 2020-01-04
4    A1 2020-01-21     9        9.0 2020-01-21
5    A1 2020-01-22     8        NaN        NaT
6    A1 2020-01-23     7        NaN        NaT
7    A6 2020-03-20     5        5.0 2020-03-20
8    A6 2020-03-21     5        5.0 2020-03-21
9    A8 2020-07-30    12       12.0 2020-07-30
</code></pre>
<p>STEP D: Using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>DataFrame.groupby</code></a> on <code>index</code> and <code>g</code> and aggregate the dataframe using the aggregation dictionary <code>dct</code> which contains all the columns and their corresponding <code>agg</code> functions to be applied.</p>
<pre><code># print(df1)
  index start_date   end_date  high_hat high_hat_date  num_hats
0    A1 2020-01-01 2020-01-04        16    2020-01-03         2
1    A1 2020-01-21 2020-01-23         9    2020-01-21         1
2    A6 2020-03-20 2020-03-21         5    2020-03-20         2
3    A8 2020-07-30 2020-07-30        12    2020-07-30         1
</code></pre>
",Find start end date consecutive days column index using pandas I data frame df df index date hats A A A A A A A A A A Here first four rows consecutive days I want know start date end date consecutive days data frame If one day series like wise A index df start date end Moreover I also interested knowing highest value df hats column series consecutive days return date seperate column high hat along date high hat date If two equal high values series consecutive days write number occurrence high value new column num hat write first occurrence date high hat date The example output data frame follows index start date end date high hat high hat date num hat A A A A Any help regard highly appreciated,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
62951509,2020-07-17,2020,2,Python: create new column conditionally on values from two other columns,"<p>I would like to combine two columns in a new column.</p>
<p>Lets suppose I have:</p>
<pre><code>Index A B
0     1 0
1     1 0
2     1 0
3     1 0
4     1 0
5     1 2
6     1 2
7     1 2
8     1 2
9     1 2
10    1 2
</code></pre>
<p>Now I would like to create a column C with the entries from A from Index 0 to 4 and from column B from Index 5 to 10. It should look like this:</p>
<pre><code>Index A B C
0     1 0 1
1     1 0 1
2     1 0 1
3     1 0 1
4     1 0 1
5     1 2 2
6     1 2 2
7     1 2 2
8     1 2 2
9     1 2 2
10    1 2 2
</code></pre>
<p>Is there a python code how I can get this? Thanks in advance!</p>
","['python', 'pandas', 'numpy', 'dataframe']",62951549,"<p>If <code>Index</code> is an actual column you can use <a href=""https://numpy.org/doc/stable/reference/generated/numpy.where.html"" rel=""noreferrer""><code>numpy.where</code></a> and specify your condition</p>
<pre><code>import numpy as np

df['C'] = np.where(df['Index'] &lt;= 4, df['A'], df['B'])

    Index  A  B  C
0       0  1  0  1
1       1  1  0  1
2       2  1  0  1
3       3  1  0  1
4       4  1  0  1
5       5  1  2  2
6       6  1  2  2
7       7  1  2  2
8       8  1  2  2
9       9  1  2  2
10     10  1  2  2
</code></pre>
",Python create new column conditionally values two columns I would like combine two columns new column Lets suppose I Index A B Now I would like create column C entries A Index column B Index It look like Index A B C Is python code I get Thanks advance,"startoftags, python, pandas, numpy, dataframe, endoftags",python pandas dataframe endoftags,python pandas numpy dataframe,python pandas dataframe,0.87
62954672,2020-07-17,2020,2,Organize columns in dataframe based on condition,"<p>I have a dataframe of products that looks like this</p>
<pre><code>category,number of products
Apple pc,3
Lenovo pc,7
HP pc,4
Apple chargher,6
Lenovo charger,9
</code></pre>
<p>I want to group categories if they contain the same string (for example pc or charger) and send them to another dataframe like this</p>
<pre><code>category,number of products
pc,14
charger,15
</code></pre>
<p>Can i do this using pandas?</p>
","['python', 'pandas', 'dataframe']",62955019,"<p><strong>Try this</strong></p>
<pre><code> df['Category'] = df[&quot;Category&quot;].apply(lambda x: x.split(&quot; &quot;)[1])
 df1 = df.groupby(&quot;Category&quot;).sum()
</code></pre>
<p><strong>Output</strong></p>
<pre><code> Category   num_of_product
 charger    15
 pc         14
</code></pre>
",Organize columns dataframe based condition I dataframe products looks like category number products Apple pc Lenovo pc HP pc Apple chargher Lenovo charger I want group categories contain string example pc charger send another dataframe like category number products pc charger Can using pandas,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
62996351,2020-07-20,2020,2,Understanding NumPy&#39;s copy behaviour when using advanced indexing,"<p>I am currently struggling writing tidy code in NumPy using advanced indexing.</p>
<pre><code>arr = np.arange(100).reshape(10,10) # array I want to manipulate
sl1 = arr[:,-1] # basic indexing
# Do stuff with sl1...
sl1[:] = -1
# arr has been changed as well
sl2 = arr[arr &gt;= 50] # advanced indexing
# Do stuff with sl2...
sl2[:] = -2
# arr has not been changed, 
# changes must be written back into it
arr[arr &gt;= 50] = sl2 # What I'd like to avoid
</code></pre>
<p>I'd like to avoid this &quot;write back&quot; operation because it feels superfluous and I often forget it. Is there a more elegant way to accomplish the same thing?</p>
","['python', 'arrays', 'numpy']",62996457,"<p>Both boolean and integer array indexing, fall under the category of <a href=""https://numpy.org/doc/stable/reference/arrays.indexing.html#advanced-indexing"" rel=""nofollow noreferrer"">advanced indexing methods</a>. In the second example (boolean indexing), you'll see that the original array is not updated, this is because <code>advanced indexing</code> <em>always returns a copy of the data</em> (see second paragraph in the advanced indexing section of the <a href=""https://numpy.org/doc/stable/reference/arrays.indexing.html"" rel=""nofollow noreferrer"">docs</a>). This means that once you do <code>arr[arr &gt;= 50]</code> this is already a copy of <code>arr</code>, and whatever changes you apply over it they won't affect <code>arr</code>.</p>
<p>The reason why it does not return a view is that advanced indexing cannot be expressed as a slice, and hence cannot be addressed with offsets, strides, and counts, which is required to be able to take a view of the array's elements.</p>
<p>We can easily verify that we are viewing different objects in the case of advanced indexing with:</p>
<pre><code>np.shares_memory(arr, arr[arr&gt;50])
# False
np.shares_memory(arr, arr[:,-1])
# True
</code></pre>
<p>Views are only returned when performing basic slicing operations. So you'll have to assign back as you're doing in the last example. In reference to the question in the comments, when assigning back in a same expression:</p>
<pre><code>arr[arr &gt;= 50] = -2
</code></pre>
<p>This is translated by the python interpreter as:</p>
<pre><code>arr.__setitem__(arr &gt;= 50, -2)
</code></pre>
<p>Here the thing to understand is that the expression can be evaluated <em>in-place</em>, hence there's no new object creation involved since there is no need for it.</p>
",Understanding NumPy copy behaviour using advanced indexing I currently struggling writing tidy code NumPy using advanced indexing arr np arange reshape array I want manipulate sl arr basic indexing Do stuff sl sl arr changed well sl arr arr gt advanced indexing Do stuff sl sl arr changed changes must written back arr arr gt sl What I like avoid I like avoid quot write back quot operation feels superfluous I often forget Is elegant way accomplish thing,"startoftags, python, arrays, numpy, endoftags",python pandas numpy endoftags,python arrays numpy,python pandas numpy,0.67
63101261,2020-07-26,2020,3,Find unique values for all the columns of a dataframe,"<p>How can i get the unique values of all the column in a dataframe ?
I am trying to do something like below as of now.</p>
<pre><code>for col in train_features_df.columns:
    print(train_features_df.col.unique())
</code></pre>
<p>But this gives me the error <code>AttributeError: 'DataFrame' object has no attribute 'col'</code></p>
<p>For e.g for below dataframe i want to the below output</p>
<pre><code> df = pd.DataFrame({'A':[1,1,3],
               'B':[4,5,6],
               'C':[7,7,7]})
</code></pre>
<p>I want a output of 1,3 for A and 4,5,6 for B and 7 for C .</p>
","['python', 'pandas', 'dataframe']",63101377,"<p>You can apply <code>unique</code> on each series by transposing like,</p>
<pre><code>&gt;&gt;&gt; df
   A  B  C
0  1  4  7
1  1  5  7
2  3  6  7
&gt;&gt;&gt; df.T.apply(lambda x: x.unique(), axis=1)
A       [1, 3]
B    [4, 5, 6]
C          [7]
dtype: object
&gt;&gt;&gt; 
</code></pre>
",Find unique values columns dataframe How get unique values column dataframe I trying something like col train features df columns print train features df col unique But gives error AttributeError DataFrame object attribute col For e g dataframe want output df pd DataFrame A B C I want output A B C,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
63112465,2020-07-27,2020,3,Python/Pandas: Convert multiple CSV files to have union and ordered header and fill the missing data,"<p>I have 36 <code>CSV</code> files in a folder that contain uneven numbers of columns (no. of columns range from 90 to 255 in each file). In a single <code>CSV</code> file, the maximum number of rows is 300, however, the columns can have zero to 300 rows. An example of a <code>CSV</code> file is as follows:</p>
<pre><code>Row  col1 col2 col3 ........................ col200
 1     2      3   4   ......................... 25
 2     1          8   .......................... 0.2
 3     5          2   ........................... 5
 .     .          .   ..........................  .
 .     .          .   ........................... .
 .     .          .   ........................... .
 .     .          .   ........................... .
 .     .          .   ........................... .
 .     .          .   ........................... .
 300   3          12   ..........................  1 
</code></pre>
<p>I want to convert these <code>CSV</code> files to get the following properties using <code>Python</code>:</p>
<ol>
<li><p>All the converted <code>CSV</code> files must have the same columns (same order and size). The columns are the union of columns (non-repeated) in all original <code>CSV</code> files.</p>
</li>
<li><p>The columns of the converted <code>CSV</code> file must be in order. i.e. <code>3rd column</code> of <code>c1.csv</code> must be the <code>3rd column</code> of other remaining <code>CSV</code> files too.</p>
</li>
<li><p>If any of the union columns is absent in any original <code>CSV</code> file, it's converted <code>CSV</code> file will have the missing columns added with default value in the rows (a fixed value in all 300 rows).</p>
</li>
<li><p>If any column is presented in both the union columns and an original <code>CSV</code> file:</p>
<p>(a) If the numbers of rows in this column are 300, copy as it is.</p>
<p>(b) If the numbers of rows in this column are less than 300, fill the remaining rows by an average of available values in this row.</p>
</li>
</ol>
<p>To achieve the above-mentioned properties, I have written the following code in python:</p>
<pre><code>import pandas as pd
import csv
import glob
import os

path = r'FILE PATH TO ORIGINAL FILES' # file path to original files
all_files = glob.glob(path + &quot;/*.csv&quot;)
combined_csv = pd.concat([pd.read_csv(f) for f in all_files]) #To get common columns
master_set =list(combined_csv.columns)

for file in all_files:
    filtered_df = pd.read_csv(file)
    for cols in master_set:
        if(cols in filtered_df):
            if(filtered_df[cols].count()&gt;300): pass
            elif (filtered_df[cols].count()&lt;300):
                total = sum(value for value in filtered_df[cols])
                avg = total/filtered_df[cols].count()
                i = filtered_df[cols].count()
                while i&lt;301:
                    filtered_df.at[i,cols] = avg
                    i+=1
        else:
         filtered_df[cols] = 10
         
    file_name = os.path.split(file)[-1] #Select individual file (eg. c1.csv)
    file_name_path = os.path.join('FILE PATH TO CONVERTED FILES' + file_name) 
    filtered_df.to_csv(file_name_path)
</code></pre>
<p>After running the above code, I get 36 converted <code>CSV</code> files with a common set of columns. The rows in added columns (columns that are in union columns but not in individual files) are filled with the default values. However, the following properties are still not fulfilled with the above code.</p>
<ol>
<li>The order of columns in newly created <code>CSV</code> files does not match.</li>
<li>The above-mentioned property 4(b) is not achieved. i.e. any column of an original file that is present in union columns but has less than 300 values (rows &lt;300) is not filled by the interpolated value.</li>
</ol>
<p>I will update/edit my question for any further clarity.</p>
<p>Any help, please!</p>
","['python', 'pandas', 'csv']",63127501,"<p>I solved my problem with the following approach:</p>
<pre><code>import pandas as pd
import csv
import glob
import os

path = r'FILE PATH TO ORIGINAL FILES' # file path to original files
all_files = glob.glob(path + &quot;/*.csv&quot;)
combined_csv = pd.concat([pd.read_csv(f) for f in all_files]) #To get common columns
master_set =list(combined_csv.columns)

for file in all_files:
    filtered_df = pd.read_csv(file)
    for cols in master_set:
        if(cols in filtered_df):
            total = 0
            if(filtered_df[cols].count()&gt;300): pass
            elif (filtered_df[cols].count()&lt;300):
                for value in filtered_df[cols]:
                    if(math.isnan(value) == False): 
                        total = total + value
                avg = total/filtered_df[cols].count()
                i = filtered_df[cols].count()
                while i&lt;301:
                    filtered_df.at[i,cols] = avg
                    i+=1
        else:
         filtered_df[cols] = 10
         
    filtered_df = filtered_df[master_set]     
    file_name = os.path.split(file)[-1] #Select individual file (eg. c1.csv)
    file_name_path = os.path.join('FILE PATH TO CONVERTED FILES' + file_name) 
    filtered_df.to_csv(file_name_path)
</code></pre>
<p>To achieve (1), I updated the code with <code>filtered_df = filtered_df[master_set]</code>. For 4(b), I updated the original code as <code>if(math.isnan(value) == False)</code>.</p>
",Python Pandas Convert multiple CSV files union ordered header fill missing data I CSV files folder contain uneven numbers columns columns range file In single CSV file maximum number rows however columns zero rows An example CSV file follows Row col col col col I want convert CSV files get following properties using Python All converted CSV files must columns order size The columns union columns non repeated original CSV files The columns converted CSV file must order e rd column c csv must rd column remaining CSV files If union columns absent original CSV file converted CSV file missing columns added default value rows fixed value rows If column presented union columns original CSV file If numbers rows column copy b If numbers rows column less fill remaining rows average available values row To achieve mentioned properties I written following code python import pandas pd import csv import glob,"startoftags, python, pandas, csv, endoftags",python pandas numpy endoftags,python pandas csv,python pandas numpy,0.67
63237728,2020-08-03,2020,3,Finding row in pandas df and performing a diff relative to that row location,"<p>I have a database object that returns my query outputs as a pandas df.</p>
<p>One of my queries generates a list of dates (<strong>df1</strong>):</p>
<pre><code>      data_interestDate
0  2020-07-15T00:00:00
1  2020-06-11T00:00:00
2  2020-05-14T00:00:00
3  2020-04-14T00:00:00
</code></pre>
<p>The other query returns list of values corresponding to several dates (<strong>df2</strong>):</p>
<pre><code>              data_date value
0   2020-07-21T00:00:00  47.0
1   2020-07-20T00:00:00  46.0
2   2020-07-17T00:00:00  50.0
3   2020-07-16T00:00:00  46.0
4   2020-07-15T00:00:00  48.0
5   2020-07-14T00:00:00  49.0
6   2020-07-13T00:00:00  48.0
7   2020-07-10T00:00:00  49.0
8   2020-07-09T00:00:00  46.0
9   2020-07-08T00:00:00  51.0
10  2020-07-07T00:00:00  49.0
11  2020-07-06T00:00:00  53.0
</code></pre>
<p>I want to iterate through df1 and find the matching dates in df2. Once I have that, in df2 I want to take the difference between the value corresponding to that date and the value x number of rows before that.
For example, for 2020-07-15T00:00:00 in df1, I would find that date in df2 and then do something like:</p>
<pre><code>(df2['value']-df2['value'].shift(-5)).iloc()[0] 
</code></pre>
<p>which should return 3.0, and then overall output of</p>
<pre><code>2020-07-15T00:00:00  -5  3.0
2020-06-11T00:00:00  -5  ...
2020-05-14T00:00:00  -5  ...
2020-04-14T00:00:00  -5  ...
</code></pre>
","['python', 'pandas', 'dataframe']",63238819,"<p>I found myself doing some formatting to get you to where you wanted your format to be, but:</p>
<p>Query DF 1 (df1):</p>
<pre><code>df1.head()

data_interestDate
0   2020-07-15T00:00:00
1   2020-06-11T00:00:00
2   2020-05-14T00:00:00
3   2020-04-14T00:00:00
</code></pre>
<p>Query DF 2 (df2):</p>
<pre><code>df2.head()

    data_date   value
0   2020-07-21T00:00:00 47.0
1   2020-07-20T00:00:00 46.0
2   2020-07-17T00:00:00 50.0
3   2020-07-16T00:00:00 46.0
4   2020-07-15T00:00:00 48.0
</code></pre>
<p>Identify your shift value:</p>
<pre><code>shift_val = -5
</code></pre>
<p>Set your literal column (since the output you desired had it):</p>
<pre><code>df2['shift'] = shift_val
</code></pre>
<p>Generate a df2 with a 'diff' column, giving the absolute value difference of value and value.shift(shift_value):</p>
<pre><code>df2.loc[(df2['data_date'].isin(df1['data_interestDate'])), 'diff'] = abs(pd.to_numeric(df2['value'])-pd.to_numeric(df2['value'].shift(shift_val)))
</code></pre>
<p>Your question seems to say you want to return df2, but your examples shows a df1 return.  Here's both:</p>
<p>Returning df1 (as df3):</p>
<pre><code>df3 = df1.merge(df2, left_on='data_interestDate', right_on='data_date', how='left').drop(['data_date','value'], axis=1)
df3['shift'] = shift_val
</code></pre>
<p>There we do lose the shift value on the join, so it's added back (no idea on this one).
Outputs:</p>
<pre><code>df3.head()

    data_interestDate   shift   diff
0   2020-07-15T00:00:00 -5  3.0
1   2020-06-11T00:00:00 -5  NaN
2   2020-05-14T00:00:00 -5  NaN
3   2020-04-14T00:00:00 -5  NaN
</code></pre>
<p>Returning df2 (as df3):</p>
<pre><code>df3 = df2[df2['data_date'].isin(df1['data_interestDate'])].drop(['value'],axis=1)
</code></pre>
<p>Outputs:</p>
<pre><code>df3.head()

data_date   shift   diff
4   2020-07-15T00:00:00 -5  3.0
</code></pre>
",Finding row pandas df performing diff relative row location I database object returns query outputs pandas df One queries generates list dates df data interestDate T T T T The query returns list values corresponding several dates df data date value T T T T T T T T T T T T I want iterate df find matching dates df Once I df I want take difference value corresponding date value x number rows For example T df I would find date df something like df value df value shift iloc return overall output T T T T,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
63385813,2020-08-12,2020,2,All the bullets are moving when I move the mouse instead of just the one being shot,"<p>I am trying to make a basic top-down shooter game using Pygame. I just figured out how to make bullets shoot at where my mouse is, but my issue now is they will shift direction whenever I move my mouse even after being shot. Additionally, when the bullets collide with the left edge of the screen they do not disappear like with the other edges.</p>
<p>main.py:</p>
<pre><code>import pygame
import time
import math
import os
import sys
import random
from classes import *

pygame.init()

width, height = (1440, 900)
win = pygame.display.set_mode((width, height), pygame.FULLSCREEN)
pygame.display.set_caption(&quot;Dungeon&quot;)
win.fill((255, 255, 255))

# Variables to do with player shooting
shotTimer = 0
bullets = []
bulletVel = 9

# Initialize classes
player = Player(50, 800, 50, 50, PLAYERSPRITE)

def main():
    run = True
    FPS = 60
    clock = pygame.time.Clock()



    while run:
        clock.tick(FPS)

        now = pygame.time.get_ticks()

        mousex, mousey = pygame.mouse.get_pos()

        # Creates the player's bullets
        def shoot():
            global shotTimer

            if now - shotTimer &gt;= player.cooldown:
                bullets.append(Projectile(round(player.x + player.width // 2), round(player.y + player.height // 2), 6))
                shotTimer = now

        def playerBulletUpdates():
            for bullet in bullets:
                if bullet.x &gt;= 0 and bullet.x &lt;= width:
                    if bullet.y &gt;= 0 and bullet.y &lt;= height:
                        global bulletVel

                        xDiff = mousex - player.x
                        yDiff = mousey - player.y

                        angle = math.atan2(yDiff, xDiff)

                        changeX = math.cos(angle) * bulletVel
                        changeY = math.sin(angle) * bulletVel

                        bullet.x += changeX
                        bullet.y += changeY
                    else:
                        bullets.pop(bullets.index(bullet))

        def bulletAngle():
            global bulletVel

            xDiff = mousex - player.x
            yDiff = mousey - player.y

            angle = math.atan2(yDiff, xDiff)

            changeX = int(math.cos(angle) * bulletVel)
            changeY = int(math.sin(angle) * bulletVel)

            bullet.x += changeX
            bullet.y += changeY

        def updateScreen():
            win.fill((255, 255, 255))
            player.draw(win)

            for bullet in bullets:
                bullet.draw(win)

            playerBulletUpdates()

            pygame.display.update()

        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                run = False

        keys = pygame.key.get_pressed()

        if keys[pygame.K_w] and player.y  &gt;= 0: # UP
            player.y += -player.walkSpeed

        if keys[pygame.K_s] and player.y + player.walkSpeed + player.height &lt;= height: # DOWN
            player.y += player.walkSpeed

        if keys[pygame.K_a] and player.x &gt;= 0: #LEFT
            player.x += -player.walkSpeed

        if keys[pygame.K_d] and player.x + player.walkSpeed + player.width &lt;= width: # RIGHT
            player.x += player.walkSpeed

        if pygame.mouse.get_pressed()[0]:
            shoot()

        updateScreen()

main()
</code></pre>
<p>classes.py:</p>
<pre><code>import pygame
import math
import os

pygame.init()

width, height = (1440, 900)
win = pygame.display.set_mode((width, height), pygame.FULLSCREEN)

# Import Sprites
PLAYERSPRITE = pygame.image.load(os.path.join(&quot;assets&quot;, &quot;gunner_class.png&quot;))
BULLETSPRITE = pygame.image.load(os.path.join(&quot;assets&quot;, &quot;bullet.png&quot;))

class Player:
    def __init__(self, x, y, width, height, img):
        self.x = x
        self.y = y
        self.width = width
        self.height = height
        self.img = img
        self.walkSpeed = 5
        self.cooldown = 100

    def draw(self, window):
        window.blit(PLAYERSPRITE, (self.x, self.y))

class Projectile:
    def __init__(self, x, y, radius):
        self.x = x
        self.y = y
        self.radius = radius

    def draw(self, window):
            # pygame.draw.circle(window, self.color, (self.x, self.y), self.radius)
            window.blit(BULLETSPRITE, (self.x, self.y))
</code></pre>
","['python', 'python-3.x', 'pygame']",63385991,"<p>The bullets not being deleted when they pass <code>x=0</code>, simply because you don't cull them on &quot;bad-x&quot;, as the &quot;else pop&quot; clause is only called on &quot;bad-y&quot;.  It's easy to fix:</p>
<pre><code>    def playerBulletUpdates():
        for bullet in bullets:
            if bullet.x &gt;= 0 and bullet.x &lt;= width and \
               bullet.y &gt;= 0 and bullet.y &lt;= height:
                    ...
            else:
                # Bullet has gone off-screen
                bullets.pop(bullets.index(bullet))  
</code></pre>
<p>The mouse-movement of existing bullets is because, during this same loop, you re-compute the direction vectors for <em>every</em> bullet, not just the new one.</p>
<p>Your <code>Projectile</code> class needs to store its direction vector during initialisation - one for every bullet, as it is probably different for every bullet.  So I would re-work the constructor here, then add an <code>update()</code> function to handle the continued movement.</p>
<pre><code>class Projectile:
    def __init__(self, x, y, dx, dy, radius):
        self.x      = x
        self.y      = y
        self.radius = radius
        self.dx     = dx
        self.dy     = dy

    def draw(self, window):
        # pygame.draw.circle(window, self.color, (self.x, self.y), self.radius)
        window.blit(BULLETSPRITE, (self.x, self.y))

    def update( self ):
        &quot;&quot;&quot; Move the projectile along its path &quot;&quot;&quot;
        self.x += self.dx
        self.y += self.dy
</code></pre>
<p><code>shoot()</code> now calculates the direction vecotor ~</p>
<pre><code>def shoot( mousex, mousey, player ):
    &quot;&quot;&quot; Creates a player's bullets &quot;&quot;&quot;
    global shotTimer

    if now - shotTimer &gt;= player.cooldown:
        global bulletVel

        xDiff = mousex - player.x
        yDiff = mousey - player.y
        angle = math.atan2(yDiff, xDiff)

        # Direction of travel
        changeX = math.cos(angle) * bulletVel
        changeY = math.sin(angle) * bulletVel

        # Start position
        new_bullet_x = round(player.x + player.width / 2)
        new_bulley_y = round(player.y + player.height / 2)

        bullets.append( Projectile( new_bullet_x, new_bulley_y, changeX, changeY, 6))
        shotTimer = now
</code></pre>
<p>Making the bullet update function much simpler too:</p>
<pre><code>def playerBulletUpdates():
    for bullet in bullets:
        if bullet.x &gt;= 0 and bullet.x &lt;= width and bullet.y &gt;= 0 and bullet.y &lt;= height:
            # Move the bullet
            bullet.update()
        else:
            # Bullet has gone off-screen
            bullets.pop(bullets.index(bullet))
</code></pre>
<p>While you're changing these things around.  It might be <a href=""https://www.pygame.org/docs/ref/rect.html"" rel=""nofollow noreferrer"">worthwhile investigating PyGame</a> <code>Rect</code> objects using their positioning rather than discrete <code>x</code> and <code>y</code>, as that would give access to the excellent collision functions.</p>
",All bullets moving I move mouse instead one shot I trying make basic top shooter game using Pygame I figured make bullets shoot mouse issue shift direction whenever I move mouse even shot Additionally bullets collide left edge screen disappear like edges main py import pygame import time import math import os import sys import random classes import pygame init width height win pygame display set mode width height pygame FULLSCREEN pygame display set caption quot Dungeon quot win fill Variables player shooting shotTimer bullets bulletVel Initialize classes player Player PLAYERSPRITE def main run True FPS clock pygame time Clock run clock tick FPS pygame time get ticks mousex mousey pygame mouse get pos Creates player bullets def shoot global shotTimer shotTimer gt player cooldown bullets append Projectile round player x player width round player player height shotTimer def playerBulletUpdates bullet bullets bullet x gt bullet x lt width bullet,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
63509387,2020-08-20,2020,3,Delete column and make next row column?,"<p>If I have a column that I want to delete entirely and make the very next row the columns how can I do that?</p>
<p>For example if I have</p>
<pre><code>unnamed column 0 | unnamed column 1 | unnamed column 2 
    Year            Month               Day
    1900             04                  11
</code></pre>
<p>New df</p>
<pre><code>Year |  Month | Day
1900     04      11
</code></pre>
<p>Thanks</p>
","['python', 'python-3.x', 'pandas']",63509607,"<p>In case you are using pandas, would this work?</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

old_df = pd.DataFrame([[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], [&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;], [1990, 4, 11]])
new_df = old_df.loc[2:, :]
new_df.columns = old_df.loc[1, :]

print(old_df)
print(new_df)
</code></pre>
",Delete column make next row column If I column I want delete entirely make next row columns I For example I unnamed column unnamed column unnamed column Year Month Day New df Year Month Day Thanks,"startoftags, python, python3x, pandas, endoftags",python python3x pandas endoftags,python python3x pandas,python python3x pandas,1.0
63591027,2020-08-26,2020,2,Pandas apply a function to a column while the condition is valid - deeper version,"<p>There is a dataframe let's say:</p>
<pre><code>  Product NewNetProfitMargin  Certain Value   Cost    Price
0       A       50               10              10      20
1       B       12               40              5       17
2       C       13               20              6       12
</code></pre>
<p>I would like to apply a function to the price column. Something like this :</p>
<pre><code>def update_price(df):
  while df[&quot;New Net Profit Margin&quot;] &lt; df[&quot;Certain Value&quot;]:
     df[&quot;New Price&quot;] = df[&quot;New Total Cost&quot;] + df[&quot;Certain Value&quot;] * df[&quot;Purchase Price&quot;]
     update_columns() # This function updates the profit margin and cost depended on new price.


df[&quot;Price&quot;] = df.apply(update_price,axis=1)
</code></pre>
<p>So basically, if <code>new net profit margin</code> of a product is lower than a certain value, the function has to update the <code>price</code> until net profit margin is greater than the certain value.</p>
<hr />
<p>The problem that I am facing is an infinite loop. There seems to be <code>df[&quot;Price&quot;]</code> is not updating for each iteration. Could be because of not returning the value after calculation but I have no idea how to do it.</p>
<p>The actual dataset is complex but I tried to simplified. Hope it is easy to understand.</p>
<hr />
<p>Here some additional details:</p>
<pre><code>def update_columns():
  df[&quot;New Comission Amount&quot;] = df.apply(new_commission_amount,axis=1).astype(float)
  df[&quot;New Total Cost&quot;] = df.apply(new_total_cost,axis=1).astype(float)
  df[&quot;New Net Profit&quot;] = df.apply(new_net_profit,axis=1).astype(float)
  df[&quot;New Net Profit Margin&quot;] = df.apply(new_net_profit_margin,axis=1).astype(float)
  print(&quot;Columns updated succesfully!&quot;)




def new_commission_amount(df):
  
   return df['New Price'] * df['Comission Rate'] 



def new_total_cost(df):
  
  return df['Purchase Price'] + df['New Comission Amount'] + df['Shipping Cost']



def new_net_profit(df):
  return df[&quot;New Price&quot;] - df[&quot;New Total Cost&quot;]


def new_net_profit_margin(df):
  
  return df[&quot;New Net Profit&quot;] / df[&quot;Purchase Price&quot;]
</code></pre>
<p>Note: update_columns() function uses current columns and adds new results to the end of the dataframe as new columns.</p>
<p><strong>Most of the time many retailers put their prices by hand. I am trying to prevent if someone puts very low numbers, I will calculate a new price according to a rate and correct it. So they don't lose money</strong></p>
","['python', 'pandas', 'dataframe']",63591091,"<p>Because working with arrays, instead <code>while</code> set new values by mask, also for apply function for all <code>DataFrame</code> is used <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pipe.html"" rel=""nofollow noreferrer""><code>DataFrame.pipe</code></a>:</p>
<p>EDIT:</p>
<pre><code>def new_func(df):
     return df['Price'] * df['Cost'] 


def update_columns(df):
  df[&quot;New Amount&quot;] = df.apply(new_func,axis=1).astype(float)
  print(&quot;Columns updated succesfully!&quot;)
  return df


def update_price(df):
   
    df['Price'] = df['Price']*df['Certain Value'] + df['Cost']
    # This function updates the profit margin and cost depended on new price.
    df = df.pipe(update_columns) 
    return df
</code></pre>
<p>Returned updated columns and also new columns:</p>
<pre><code>mask =  df[&quot;New Net Profit Margin&quot;] &lt; df[&quot;Certain Value&quot;]
df1 = df[mask].copy().pipe(update_price)
print (df1)
  Product  New Net Profit Margin  Certain Value  Cost  Price  New Amount
1       B                     12             40     5    685      3425.0
2       C                     13             20     6    246      1476.0
</code></pre>
<p>Add new columns filled by missing values by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>DataFrame.reindex</code></a>:</p>
<pre><code>df = df.reindex(df1.columns, axis=1)
print (df)
  Product  New Net Profit Margin  Certain Value  Cost  Price  New Amount
0       A                     50             10    10     20         NaN
1       B                     12             40     5     17         NaN
2       C                     13             20     6     12         NaN
</code></pre>
<p>Last update by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.update.html"" rel=""nofollow noreferrer""><code>DataFrame.update</code></a>:</p>
<pre><code>df.update(df1)
print (df)
  Product  New Net Profit Margin  Certain Value  Cost  Price  New Amount
0       A                   50.0           10.0  10.0   20.0         NaN
1       B                   12.0           40.0   5.0  685.0      3425.0
2       C                   13.0           20.0   6.0  246.0      1476.0
</code></pre>
",Pandas apply function column condition valid deeper version There dataframe let say Product NewNetProfitMargin Certain Value Cost Price A B C I would like apply function price column Something like def update price df df quot New Net Profit Margin quot lt df quot Certain Value quot df quot New Price quot df quot New Total Cost quot df quot Certain Value quot df quot Purchase Price quot update columns This function updates profit margin cost depended new price df quot Price quot df apply update price axis So basically new net profit margin product lower certain value function update price net profit margin greater certain value The problem I facing infinite loop There seems df quot Price quot updating iteration Could returning value calculation I idea The actual dataset complex I tried simplified Hope easy understand Here additional details def update columns df quot New Comission Amount quot df,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
63601707,2020-08-26,2020,3,"Calculating the share of each code, by ID","<p>I have this data-frame:</p>
<pre><code>ID  code   X  X_total
 A   456  40       40
 A   789   0       40
 B   123  75      100
 B   987  25      100
 C   789  13       91
 C   987   0       91
 C   123  35       91
 C   456  43       91
</code></pre>
<p>I want the calculate the <em>share</em> of each code (from <code>[123, 465, 789, 987]</code>), by dividing <code>X</code> by <code>X_total</code>, for each <code>ID</code>.</p>
<p>Expected result:</p>
<pre><code>ID  share_123  share_456  share_789  share_987
 A       0.00       1.00       0.00       0.00
 B       0.75       0.00       0.00       0.25
 C       0.38       0.47       0.14       0.00
</code></pre>
","['python', 'pandas', 'numpy']",63601748,"<p>Let us do <code>crosstab</code></p>
<pre><code>s = pd.crosstab(df.ID, df.code, df.X ,aggfunc='sum', normalize='index').add_prefix(&quot;share_&quot;)
Out[70]: 
code       123       456       789   987
ID                                      
A     0.000000  1.000000  0.000000  0.00
B     0.750000  0.000000  0.000000  0.25
C     0.384615  0.472527  0.142857  0.00
</code></pre>
",Calculating share code ID I data frame ID code X X total A A B B C C C C I want calculate share code dividing X X total ID Expected result ID share share share share A B C,"startoftags, python, pandas, numpy, endoftags",python python3x pandas endoftags,python pandas numpy,python python3x pandas,0.67
64113002,2020-09-29,2020,6,How I can aggregate employee based on their department and show average salary in each department using groupby pandas?,"<p>This is my code which has data in which I want to perform the task using <strong>pandas.DataFrame.groupby</strong></p>
<pre><code>import pandas as pd
data = {'employees_no':  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],
        'employees_name': ['Jugal Sompura', 'Maya Rajput', 'Chaitya Panchal', 'Sweta Rampariya', 'Prakshal Patel', 'Dhruv Panchal', 'Prachi Desai', 'Krunal Gosai', 'Hemil Soni', 'Gopal Pithadia', 'Jatin Shah', 'Raj Patel', 'Shreya Desai'],
        'department_name': ['HR', 'Administrative Assistant', 'Production', 'Accountant', 'Production', 'Engineer', 'Finance', 'Engineer', 'Quality Assurance', 'Engineer', 'Engineer', 'Customer Service', 'CEO'],
        'salary': [130000.0, 65000.0, 45000.0, 65000.0, 47000.0, 40000.0, 90000.0, 45000.0, 35000.0, 45000.0, 30000.0, 40000.0, 250000.0]
        }
</code></pre>
<pre><code>df = pd.DataFrame (data, columns = ['employees_no', 'employees_name', 'department_name', 'salary'])
print(df)
---------------------------------------------------------------------
employees_no   employees_name           department_name      salary
0              1    Jugal Sompura                        HR  130000.0
1              2      Maya Rajput  Administrative Assistant   65000.0
2              3  Chaitya Panchal                Production   45000.0
3              4  Sweta Rampariya                Accountant   65000.0
4              5   Prakshal Patel                Production   47000.0
5              6    Dhruv Panchal                  Engineer   40000.0
6              7     Prachi Desai                   Finance   90000.0
7              8     Krunal Gosai                  Engineer   45000.0
8              9       Hemil Soni         Quality Assurance   35000.0
9             10   Gopal Pithadia                  Engineer   45000.0
10            11       Jatin Shah                  Engineer   30000.0
11            12        Raj Patel          Customer Service   40000.0
12            13     Shreya Desai                       CEO  250000.0
---------------------------------------------------------------------
</code></pre>
<p>I tried this and could only get this output.</p>
<pre><code>print(df.groupby('department_name').agg({'salary':'mean'}))
---------------------------------------------------------------------
department_name             salary
Accountant                 65000.0
Administrative Assistant   65000.0
CEO                       250000.0
Customer Service           40000.0
Engineer                   40000.0
Finance                    90000.0
HR                        130000.0
Production                 46000.0
Quality Assurance          35000.0
---------------------------------------------------------------------
</code></pre>
<p>I'm not able to get output like this...</p>
<pre><code>department_name          employees_name     avg_salary        
Accountant               Sweta Rampariya     65000.0
Administrative Assistant Maya Rajput         65000.0
CEO                      Shreya Desai       250000.0
Customer Service         Raj Patel           40000.0
Engineer                 Dhruv Panchal       40000.0
                         Gopal Pithadia      
                         Krunal Gosai        
                         Jatin Shah     
Finance                  Prachi Desai        90000.0
HR                       Jugal Sompura      130000.0
Production               Chaitya Panchal     46000.0
                         Prakshal Patel      
Quality Assurance        Hemil Soni          35000.0
</code></pre>
<p>Can you help me with this?</p>
","['python', 'pandas', 'pandas-groupby']",64113528,"<p>Extending what @Chris did and adding the part of remove average salary values if department_name is same.</p>
<p>Here's the full code:</p>
<pre><code>import pandas as pd
data = {'employees_no':  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],
        'employees_name': ['Jugal Sompura', 'Maya Rajput', 'Chaitya Panchal', 'Sweta Rampariya', 'Prakshal Patel', 'Dhruv Panchal', 'Prachi Desai', 'Krunal Gosai', 'Hemil Soni', 'Gopal Pithadia', 'Jatin Shah', 'Raj Patel', 'Shreya Desai'],
        'department_name': ['HR', 'Administrative Assistant', 'Production', 'Accountant', 'Production', 'Engineer', 'Finance', 'Engineer', 'Quality Assurance', 'Engineer', 'Engineer', 'Customer Service', 'CEO'],
        'salary': [130000.0, 65000.0, 45000.0, 65000.0, 47000.0, 40000.0, 90000.0, 45000.0, 35000.0, 45000.0, 30000.0, 40000.0, 250000.0]
        }

df = pd.DataFrame (data)
df['avg_sal'] = df.groupby('department_name')['salary'].transform('mean')
new_df = df.set_index([&quot;department_name&quot;, &quot;employees_name&quot;]).sort_index()
new_df.loc[new_df.index.get_level_values(0).duplicated()==True,'avg_sal']=''
print (new_df['avg_sal'])
</code></pre>
<p>This will print as follows:</p>
<pre><code>department_name           employees_name 
Accountant                Sweta Rampariya     65000
Administrative Assistant  Maya Rajput         65000
CEO                       Shreya Desai       250000
Customer Service          Raj Patel           40000
Engineer                  Dhruv Panchal       40000
                          Gopal Pithadia           
                          Jatin Shah               
                          Krunal Gosai             
Finance                   Prachi Desai        90000
HR                        Jugal Sompura      130000
Production                Chaitya Panchal     46000
                          Prakshal Patel           
Quality Assurance         Hemil Soni          35000
</code></pre>
",How I aggregate employee based department show average salary department using groupby pandas This code data I want perform task using pandas DataFrame groupby import pandas pd data employees employees name Jugal Sompura Maya Rajput Chaitya Panchal Sweta Rampariya Prakshal Patel Dhruv Panchal Prachi Desai Krunal Gosai Hemil Soni Gopal Pithadia Jatin Shah Raj Patel Shreya Desai department name HR Administrative Assistant Production Accountant Production Engineer Finance Engineer Quality Assurance Engineer Engineer Customer Service CEO salary df pd DataFrame data columns employees employees name department name salary print df employees employees name department name salary Jugal Sompura HR Maya Rajput Administrative Assistant Chaitya Panchal Production Sweta Rampariya Accountant Prakshal Patel Production Dhruv Panchal Engineer Prachi Desai Finance Krunal Gosai Engineer Hemil Soni Quality Assurance Gopal Pithadia Engineer Jatin Shah Engineer Raj Patel Customer Service Shreya Desai CEO I tried could get output print df groupby department name agg salary mean,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas numpy endoftags,python pandas pandasgroupby,python pandas numpy,0.67
64292297,2020-10-10,2020,3,GroupBy and plot with pandas,"<p>I have data.
There are some groups of people who participate in the meetings. Meetings are divided into speeches. Each meeting and speeches combination has a number of participants. The number of participants in one meeting does not change. In other words, the number of participants only changes from meeting to meeting.</p>
<pre><code>data = [
 ['group_1', 1, 1, 68],
 ['group_2', 1, 1, 35],
 ['group_1', 1, 2, 68],
 ['group_2', 1, 2, 35],
 ['group_1', 2, 1, 78],
 ['group_2', 2, 1, 25],
 ['group_1', 2, 2, 78], 
 ['group_2', 2, 2, 25],
 ['group_1', 3, 1, 73], 
 ['group_2', 3, 1, 30],
 ['group_1', 3, 2, 73], 
 ['group_2', 3, 2, 30]]
df = pd.DataFrame(data, columns=['group_name', 'meeting', 'present', 'members'])
</code></pre>
<p>X is meeting, y is number of participants. I want to plot something like this.</p>
<pre><code>df.groupby(['group_name']).plot(
         x='meeting', y='members',
         color='#4b0082', linewidth=3,
         marker='h', markerfacecolor='lightgreen', markeredgewidth=1, markersize=9, markevery=1);
</code></pre>
<p><a href=""https://i.stack.imgur.com/rwpYD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rwpYD.png"" alt=""enter image description here"" /></a></p>
<p>However, I would like to add a title as a group name and sign the y-axis. and I also have a problem when I run this code on all data, for some reason I have extra points on the plot.
<a href=""https://i.stack.imgur.com/holYd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/holYd.png"" alt=""enter image description here"" /></a></p>
<p>On the first graph the count should start from meeting 27 and there is an anomaly in meeting area 40. On the second graph there are anomalies in a 27 meeting area.</p>
","['python', 'pandas', 'matplotlib']",64292497,"<p>Since <code>pandas &gt;= 1.1.0</code> we have the <code>ylabel</code> argument in <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html"" rel=""nofollow noreferrer""><code>DataFrame.plot</code></a>. Also we will rewrite your groupby a bit so we can access the group name:</p>
<pre><code>for grp, d in df.groupby('group_name'):
    d.plot(
        x='meeting',
        y='members',
        color='#4b0082',
        ylabel='members',
        title=grp,
        linewidth=3,
        marker='h',
        markerfacecolor='lightgreen',
        markeredgewidth=1,
        markersize=9,
        markevery=1
    )
</code></pre>
<p><a href=""https://i.stack.imgur.com/vSfCV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vSfCV.png"" alt=""plots"" /></a></p>
",GroupBy plot pandas I data There groups people participate meetings Meetings divided speeches Each meeting speeches combination number participants The number participants one meeting change In words number participants changes meeting meeting data group group group group group group group group group group group group df pd DataFrame data columns group name meeting present members X meeting number participants I want plot something like df groupby group name plot x meeting members color b linewidth marker h markerfacecolor lightgreen markeredgewidth markersize markevery However I would like add title group name sign axis I also problem I run code data reason I extra points plot On first graph count start meeting anomaly meeting area On second graph anomalies meeting area,"startoftags, python, pandas, matplotlib, endoftags",python pandas numpy endoftags,python pandas matplotlib,python pandas numpy,0.67
64345704,2020-10-14,2020,4,How can I merge a Pandas dataframes based on a substring from one of the columns?,"<p>I have 2 dataframes: df1 and df2</p>
<pre><code>df1
                  School Conference
0              Air Force   Mt. West
1                  Akron        MAC
2  Alabama at Birmingham      C-USA
3                 Auburn   Sun Belt

df2
                           SCHOOL_NAME           RATE
0                    Auburn University           93.0
1                    Air Force Academy           53.0
2                           Birmingham           75.0
3                  University of Akron           77.0

I would like to get the output below, basically binding the `RATE` column from df2 into df1 based on substring from School column
                  School Conference  RATE
0              Air Force   Mt. West  53.0
1                  Akron        MAC  77.0
2  Alabama at Birmingham      C-USA  75.0
3                 Auburn   Sun Belt  93.0
</code></pre>
<p>I tried the code below but it's not working. When I run it, it seems to execute successfully, but nothing happens</p>
<pre><code>for i in range(1, len(df1)):
    if df1['School'][i] in df2['SCHOOL_NAME']:
       pd.merge(df1, df2, how = 'left', left_on = 'School', right_on = 'SCHOOL_NAME')
</code></pre>
","['python', 'pandas', 'dataframe']",64345815,"<p>You can use list comprehension to check if the columns from each dataframe are <code>in</code> each other (you also compare case-insensitively) and then merge:</p>
<pre><code>df1['SCHOOL_NAME'] = df1['School'].apply(lambda x: [y for y in df2['SCHOOL_NAME']
                                                    if x in y or y in x]).str[0]
df1 = df1.merge(df2, how='left').drop('SCHOOL_NAME', axis=1) #can pass on='SCHOOL_NAME' to merge.
df1
Out[1]: 
                  School Conference  RATE
0              Air Force   Mt. West  53.0
1                  Akron        MAC  77.0
2  Alabama at Birmingham      C-USA  75.0
3                 Auburn   Sun Belt  93.0
</code></pre>
<p>You could also search case-insensitively by adding <code>.lower()</code> to <code>x</code> and <code>y</code>:</p>
<pre><code>df1['SCHOOL_NAME'] = df1['School'].apply(lambda x: [y for y in df2['SCHOOL_NAME']
                                                    if x.lower() in y.lower()
                                                    or y.lower() in x.lower()]).str[0]
df1 = df1.merge(df2, how='left').drop('SCHOOL_NAME', axis=1) #can pass on='SCHOOL_NAME' to merge.
df1
Out[2]:
                  School Conference  RATE
0              Air Force   Mt. West  53.0
1                  Akron        MAC  77.0
2  Alabama at Birmingham      C-USA  75.0
3                 Auburn   Sun Belt  93.0
</code></pre>
<p>Single line of code per comment:</p>
<pre><code>df1 = (df1.assign(SCHOOL_NAME = df1['School'].apply(lambda x: [y for y in df2['SCHOOL_NAME']
                                                    if x.lower() in y.lower()
                                                    or y.lower() in x.lower()]).str[0])
          .merge(df2, how='left').drop('SCHOOL_NAME', axis=1))
df1
Out[3]: 
                  School Conference  RATE
0              Air Force   Mt. West  53.0
1                  Akron        MAC  77.0
2  Alabama at Birmingham      C-USA  75.0
3                 Auburn   Sun Belt  93.0
</code></pre>
",How I merge Pandas dataframes based substring one columns I dataframes df df df School Conference Air Force Mt West Akron MAC Alabama Birmingham C USA Auburn Sun Belt df SCHOOL NAME RATE Auburn University Air Force Academy Birmingham University Akron I would like get output basically binding RATE column df df based substring School column School Conference RATE Air Force Mt West Akron MAC Alabama Birmingham C USA Auburn Sun Belt I tried code working When I run seems execute successfully nothing happens range len df df School df SCHOOL NAME pd merge df df left left School right SCHOOL NAME,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
64370758,2020-10-15,2020,2,Automatically Update Profile Picture in Django,"<p>I have written a command which reads a csv file and creates 50+ users at once. Also, I have downloaded a bunch of avatars that I plan to assign randomly to each user. This is where the problem occurs. I have written the code below which assigns a random image as the default image to each user.</p>
<p><strong>models.py</strong></p>
<pre><code>def random_image():
    directory = os.path.join(settings.BASE_DIR, 'media')
    files = os.listdir(directory)
    images = [file for file in files if os.path.isfile(os.path.join(directory, file))]
    rand = choice(images)
    return rand


class UserProfile(models.Model):
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    avatar = models.ImageField(upload_to='avatars', default=random_image)

    def save(self, *args, **kwargs):
        # check model is new and avatar is empty
        if self.pk is None and self.avatar is not None:
            self.avatar = random_image
        super(UserProfile, self).save(*args, **kwargs)
</code></pre>
<p>However, the code seems to run only when the user uploads a picture which is not what I desire. What I want is once the user is logged it, the model should check if it has an avatar and if not, to assign a random image. So, my questions are these:</p>
<ol>
<li>How can I implement this?</li>
<li>Because the system keeps checking this after each log in, it seems to me that it is a little inefficient. To replace this strategy, what would be a better alternative?</li>
</ol>
<p>**Edit:
<strong>ProjectName/management/commands/CreateUsers.py</strong></p>
<pre><code>class Command(BaseCommand):
    help = 'Creates bulk users given a csv file.'

    def handle(self, *args, **options):

        ## Get the Groups
        TopManagement = Group.objects.get(name='TopManagement')
        Finance = Group.objects.get(name='Finance')

        ## Read the csv file
        with open(os.path.join(settings.FILES_DIR,'bulkusers.csv'), newline='') as csvfile:
            ausers = csv.reader(csvfile, delimiter = ',')
            next(ausers, None)  # skip the headers
            
            print('Creating Users...')
            
            # Create users
            for row in ausers:
                newuser = User.objects.create_user(
                    username = row[0],
                    password = row[1],
                    email = row[2],
                    is_superuser = 0,
                    is_staff = 0,
                    is_active = 1,
                    date_joined = datetime.datetime.now(tz=timezone.utc)
                )
                
                # Assign user to group
                if row[3] == 'TopManagement':
                    TopManagement.user_set.add(newuser)
                    newuser.groups.add(TopManagement)
                elif row[3] == 'Finance':
                    newuser.groups.add(Finance)

            print('Users created successfully!')
</code></pre>
","['python', 'django', 'django-models']",64388372,"<p>If someone faces the same problem in the future, here is a solution that worked for me.</p>
<pre><code>from django.db.models.signals import post_save
from django.dispatch import receiver


def random_image():
    directory = os.path.join(settings.BASE_DIR, 'media')
    files = os.listdir(directory)
    images = [file for file in files if os.path.isfile(os.path.join(directory, file))]
    rand = choice(images)
    return rand


class UserProfile(models.Model):
    user = models.OneToOneField(User, on_delete=models.CASCADE)
    avatar = models.ImageField(upload_to='avatars', default=random_image)

    def __str__(self):
        return self.user.username


@receiver(post_save, sender=User)
def create_or_update_user_profile(sender, instance, created, **kwargs):
    if created:
        UserProfile.objects.create(user=instance)
    instance.userprofile.save()
</code></pre>
<p>I do not know why the <code>save</code> function was not invoked but after a couple of indirect tips from @GProst I managed to change it with a <code>receiver</code> tag. Now, whenever I call the custom command <code>python manage.py CreateUsers</code> the UserProfile model is immediately activated and a random image is assigned to each user.</p>
",Automatically Update Profile Picture Django I written command reads csv file creates users Also I downloaded bunch avatars I plan assign randomly user This problem occurs I written code assigns random image default image user models py def random image directory os path join settings BASE DIR media files os listdir directory images file file files os path isfile os path join directory file rand choice images return rand class UserProfile models Model user models ForeignKey User delete models CASCADE avatar models ImageField upload avatars default random image def save self args kwargs check model new avatar empty self pk None self avatar None self avatar random image super UserProfile self save args kwargs However code seems run user uploads picture I desire What I want user logged model check avatar assign random image So questions How I implement Because system keeps checking log seems little inefficient To replace strategy,"startoftags, python, django, djangomodels, endoftags",python pandas matplotlib endoftags,python django djangomodels,python pandas matplotlib,0.33
64381401,2020-10-16,2020,3,Pandas how to extract mix of ints and floats in dataframe columns,"<p>I tried these: <a href=""https://stackoverflow.com/a/37683738/13865853"">https://stackoverflow.com/a/37683738/13865853</a>, <a href=""https://stackoverflow.com/a/50830098/13865853"">https://stackoverflow.com/a/50830098/13865853</a>.</p>
<p>My dataframe is all strings but the dtype is object for reasons I read elsewhere on SO.</p>
<p>The columns are units of micronutrients in foods that look like this:</p>
<pre><code>  Life-Stage Group Arsenic Boron (mg/d) Calcium (mg/d) Chromium Copper (Î¼g/d)  \
0         &lt;= 3.0 y   nan g         3 mg        2500 mg    nan g       1000 Î¼g   
1         &lt;= 8.0 y   nan g         6 mg        2500 mg    nan g       3000 Î¼g   

  Fluoride (mg/d) Iodine (Î¼g/d) Iron (mg/d) Magnesium (mg/d) Manganese (mg/d)  \
0          1.3 mg        200 Î¼g       40 mg            65 mg             2 mg   
1          2.2 mg        300 Î¼g       40 mg           110 mg             3 mg   

  Molybdenum (Î¼g/d) Nickel (mg/d) Phosphorus (g/d) Potassium Selenium (Î¼g/d)  \
0            300 Î¼g        0.2 mg              3 g     nan g           90 Î¼g   
1            600 Î¼g        0.3 mg              3 g     nan g          150 Î¼g   

  Silicon Sulfate Vanadium (mg/d) Zinc (mg/d) Sodium Chloride (g/d)  \
0   nan g   nan g          nan mg        7 mg  nan g          2.3 g   
1   nan g   nan g          nan mg       12 mg  nan g          2.9 g   

  Vitamin A (Î¼g/d) Vitamin C (mg/d) Vitamin D (Î¼g/d) Vitamin E (mg/d)  \
0         600.0 Î¼g           400 mg          63.0 Î¼g           200 mg   
1         900.0 Î¼g           650 mg          75.0 Î¼g           300 mg   

  Vitamin K (Î¼g/d) Thiamin (mg/d) Riboflavin (mg/d) Niacin (mg/d)  \
0           nan Î¼g         nan mg            nan mg         10 mg   
1           nan Î¼g         nan mg            nan mg         15 mg   

  Vitamin B6 (mg/d) Folate (Î¼g/d) Vitamin B12 (Î¼g/d) Pantothenic Acid (mg/d)  \
0             30 mg        300 Î¼g             nan Î¼g                  nan mg   
1             40 mg        400 Î¼g             nan Î¼g                  nan mg   

  Biotin (Î¼g/d) Choline (mg/d) Carotenoids  
0        nan Î¼g         1.0 mg       nan g  
1        nan Î¼g         1.0 mg       nan g  
</code></pre>
<p>I want to zero-out <code>nan</code> and just get the numerical values as I want to multiply <code>g</code> by 1000 and divide any <code>ug</code> (<code>\u03BCg</code> in Python for micro) by 1000 so that everything is in <code>mg</code> so I can plot them on a bar graph in Plotly Dash.<br />
But I'm stuck at extracting numbers.
Previously when I was making csv files after downloading the data, this worked but it now does not:</p>
<pre><code># extract numbers
new_df_arr = []
for _,df in df_dict.items():
    df = df.astype(str)
    df_copy = df.copy()
    for i in range(1, len(df.columns)):
        df_copy[df.columns[i]]=df_copy[df.columns[i]].str.extract('(\d+[.]?\d*)', expand=False) #replace(r'[^0-9]+','')
    new_df_arr.append(df_copy)
# check df's
for df in new_df_arr:
    print(df)
</code></pre>
","['python', 'pandas', 'dataframe']",64381512,"<p>I used an input of just the first set of columns. You can:</p>
<ol>
<li>Loop through columns and create a series <code>s</code> that transforms the unit into what you want to multiply by mapping to a dictionary <code>d</code></li>
<li>Extract the digits and multiply by <code>s</code> for each column</li>
</ol>
<hr />
<pre><code>df = pd.DataFrame({'Life-Stage Group': {0: '&lt;= 3.0 y', 1: '&lt;= 8.0 y'},
 'Arsenic': {0: 'nan g', 1: 'nan g'},
 'Boron (mg/d)': {0: '3 mg', 1: '6 mg'},
 'Calcium (mg/d)': {0: '2500 mg', 1: '2500 mg'},
 'Chromium': {0: 'nan g', 1: 'nan g'},
 'Copper (Î¼g/d)': {0: '1000 Î¼g', 1: '3000 Î¼g'}})

d = {'Î¼g' : .001, 'g' : 1000, 'mg' : 1}

for col in df.columns[1:]:
    s = df[col].str.split(' ').str[1].map(d).astype(float)
    df[col] = (df[col].str.extract('(\d+[.]?\d*)').astype(float) * s).fillna(0)
df
Out[1]: 
  Life-Stage Group  Arsenic  Boron (mg/d)  Calcium (mg/d)  Chromium  Copper (Î¼g/d)
0         &lt;= 3.0 y      0.0           3.0          2500.0       0.0            1.0
1         &lt;= 8.0 y      0.0           6.0          2500.0       0.0            3.0     
</code></pre>
",Pandas extract mix ints floats dataframe columns I tried https stackoverflow com https stackoverflow com My dataframe strings dtype object reasons I read elsewhere SO The columns units micronutrients foods look like Life Stage Group Arsenic Boron mg Calcium mg Chromium Copper g lt nan g mg mg nan g g lt nan g mg mg nan g g Fluoride mg Iodine g Iron mg Magnesium mg Manganese mg mg g mg mg mg mg g mg mg mg Molybdenum g Nickel mg Phosphorus g Potassium Selenium g g mg g nan g g g mg g nan g g Silicon Sulfate Vanadium mg Zinc mg Sodium Chloride g nan g nan g nan mg mg nan g g nan g nan g nan mg mg nan g g Vitamin A g Vitamin C mg Vitamin D g Vitamin E mg g mg g mg g mg g mg Vitamin,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
64570173,2020-10-28,2020,2,Sort_value wrong descend,"<p>i'dont know what's wrong with my code</p>
<pre><code>import pandas as pd
import numpy as np
woe = [1.1147295474833758,0.364043491078754,-0.05525053172192353,-0.3950007109750665,-0.6784658191115104,-0.9522135140050229,-1.1441658353033486]
iv = [0.29078213954085946,0.29078213954085946,0.29078213954085946,0.29078213954085946,0.29078213954085946,0.29078213954085946,0.29078213954085946]
lis = ['A', 'B', 'C', 'D', 'E', 'F', 'G']
fin = [lis,woe,iv]
fin = np.array(fin).T  
df_disc = pd.DataFrame(fin,columns=['Label','WoE','IV'])
print(df_disc)
df_disc = df_disc.sort_values(by=['WoE'])
df_disc = df_disc.reset_index(drop=True)
print(df_disc)
</code></pre>
<p>result</p>
<pre><code>  Label                   WoE                   IV
0     A    1.1147295474833758  0.29078213954085946
1     B     0.364043491078754  0.29078213954085946
2     C  -0.05525053172192353  0.29078213954085946
3     D   -0.3950007109750665  0.29078213954085946
4     E   -0.6784658191115104  0.29078213954085946
5     F   -0.9522135140050229  0.29078213954085946
6     G   -1.1441658353033486  0.29078213954085946
  Label                   WoE                   IV
0     C  -0.05525053172192353  0.29078213954085946
1     D   -0.3950007109750665  0.29078213954085946
2     E   -0.6784658191115104  0.29078213954085946
3     F   -0.9522135140050229  0.29078213954085946
4     G   -1.1441658353033486  0.29078213954085946
5     B     0.364043491078754  0.29078213954085946
6     A    1.1147295474833758  0.29078213954085946
</code></pre>
<p>i think the correct ones would be label G,F,E,D,C,B,A but the result seems wrong</p>
","['python', 'pandas', 'dataframe']",64570224,"<p>Problem is in your DataFrame, column is filled by objects, not numeric.</p>
<p>In code if convert strings and numeric values all values are converted to objects:</p>
<pre><code>fin = np.array(fin).T  
</code></pre>
<p>Solution is use dictionary by columns names and pass to <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html"" rel=""nofollow noreferrer""><code>DataFrame.astype</code></a>:</p>
<pre><code>df_disc = (pd.DataFrame(fin,columns=['Label','WoE','IV'])
             .astype({'WoE':'float', 'IV':'float'}))
print(df_disc)

df_disc = df_disc.sort_values(by=['WoE'], ignore_index=True)
print(df_disc)
  Label       WoE        IV
0     G -1.144166  0.290782
1     F -0.952214  0.290782
2     E -0.678466  0.290782
3     D -0.395001  0.290782
4     C -0.055251  0.290782
5     B  0.364043  0.290782
6     A  1.114730  0.290782
</code></pre>
<p>You can prevent it if pass dictionaries to <code>DataFrame</code> constructor:</p>
<pre><code>df_disc = pd.DataFrame({'Label':lis,'WoE':woe,'IV':iv})
print(df_disc)
    
df_disc = df_disc.sort_values(by=['WoE'], ignore_index=True)
print(df_disc)
  Label       WoE        IV
0     G -1.144166  0.290782
1     F -0.952214  0.290782
2     E -0.678466  0.290782
3     D -0.395001  0.290782
4     C -0.055251  0.290782
5     B  0.364043  0.290782
6     A  1.114730  0.290782
</code></pre>
",Sort value wrong descend dont know wrong code import pandas pd import numpy np woe iv lis A B C D E F G fin lis woe iv fin np array fin T df disc pd DataFrame fin columns Label WoE IV print df disc df disc df disc sort values WoE df disc df disc reset index drop True print df disc result Label WoE IV A B C D E F G Label WoE IV C D E F G B A think correct ones would label G F E D C B A result seems wrong,"startoftags, python, pandas, dataframe, endoftags",python pandas datetime endoftags,python pandas dataframe,python pandas datetime,0.67
64949395,2020-11-21,2020,2,How can I make my discord bot respond with a &quot;follow up&quot; message,"<p>I'm a beginner to discord.py and struggling to find an answer to this.
I want to make my discord bot respond to &quot;message2&quot; <em>only</em> after I send &quot;message1&quot;.
For example,</p>
<pre class=""lang-py prettyprint-override""><code>@client.event
async def on_message(message):
    if message.content.lower() == '!rps':
        await message.channel.send(&quot;Input your choice!&quot;)
</code></pre>
<p>Here I'd want the user to enter !rps which then I would wanna check his follow up response of rock/paper/scissors and let the bot reply accordingly. How do I make the bot wait for the user's next message after the user enters &quot;!rps&quot;</p>
","['python', 'discord', 'discord.py']",64950204,"<p>You can make the bot to wait for the user input by <a href=""https://discordpy.readthedocs.io/en/latest/api.html#discord.Client.wait_for"" rel=""nofollow noreferrer"">wait_for</a> function!</p>
<pre><code>async def on_message(message):
    if message.content.lower() == '!rps':
        await message.channel.send(&quot;Input your choice!&quot;)
        def check(m):
            return m.content in ['rock','paper','scissors'] and m.channel == channel and m.author == message.author
        msg = await client.wait_for('message', check=check)
        #msg variable stores the new user input(rock/paper/scissors) [your code down]
        await channel.send(f'User Choosed {msg.content}')
</code></pre>
<p>In The above code i used <code>wait_for</code> function to wait for the user input! and with the help of check function i used to check if the input is valid and the input is made by the correct author n correct channel! if everything is valid, it start to execute the remaining code</p>
",How I make discord bot respond quot follow quot message I beginner discord py struggling find answer I want make discord bot respond quot message quot I send quot message quot For example client event async def message message message content lower rps await message channel send quot Input choice quot Here I want user enter rps I would wanna check follow response rock paper scissors let bot reply accordingly How I make bot wait user next message user enters quot rps quot,"startoftags, python, discord, discordpy, endoftags",python discord discordpy endoftags,python discord discordpy,python discord discordpy,1.0
65078707,2020-11-30,2020,5,Image sequence training with CNN and RNN,"<p>I'm making my first steps learning Deep Learning. I am trying to do Activity Recognition from images sequences (frames) of videos. As a result i am facing a problem with the training procedure.</p>
<p>Firstly i need to determine the architecture of my images folders:</p>
<pre><code>Making Food -&gt; p1 -&gt; rgb_frame1.png,rgb_frame2.png ... rgb_frame200.png
Making Food -&gt; p2 -&gt; rgb_frame1.png,rgb_frame2.png ... rgb_frame280.png
                      ...
                      ...
                      ...
Taking  Medicine -&gt; p1 -&gt; rgb_frame1.png,rgb_frame2.png...rgbframe500.png

                      etc..
      
</code></pre>
<p>So the problem is that each folder can have a <strong>different number of frames</strong> so I get confused both with the input shape of the model and the timesteps which I should use.
I am creating a model (as you see bellow) with time distirbuted CNN(pre trained VGG16) and LSTM that takes an input all the frames of all classes with the coresponding labels (in the above example  making food would be the coresponding label to p1_rgb_frame1 etc.) and the final shape of <code>x_train</code> is <code>(9000,200,200,3)</code> where <code>9000</code> coresponds to all frames from all classes, <code>200</code> is height &amp; width and <code>3</code> the channel of images. I am reshaping this data to <code>(9000,1,200,200,3)</code> in order to be used as input to the model.
I am wondering and worried that I do not pass a proper timestep, as a result a wrong training , i have val_acc ~ 98% but when testing with different dataset is much lower. Can you suggest another way to do it more efficient?</p>
<pre><code>  x = base_model.output
  x = Flatten()(x)
  features = Dense(64, activation='relu')(x)
  conv_model = Model(inputs=base_model.input, outputs=features)    
  for layer in base_model.layers:
      layer.trainable = False
       
  model = Sequential()
  model.add(TimeDistributed(conv_model, input_shape=(None,200,200,3)))
  model.add(LSTM(32, return_sequences=True))
  model.add(LSTM(16))
</code></pre>
","['python', 'tensorflow', 'machine-learning', 'keras', 'deep-learning']",65131007,"<p>The structure of your model isn't obviously bad as far as I can see. As far as the different number of frames issue goes, the solution is to simply not do that. Preprocess your data to take the same number of frames from each action.</p>
<p>The deeper issue here is more likely just simple overfitting. You don't specify, but based off the fact that you are talking about hosting your training data on a single computer, I imagine that you don't have much training data, and your network is not learning the activities, but rather just learning to recognize your training data. Consider that VGG16 had about 1.2 million distinct training examples and was trained for weeks on top-end GPUs, just to distinguish 1000 classes of static images. Arguably, learning temporal aspects and activities should require a similar amount of training data. You had a good idea to start with VGG as a base and add onto it so your network doesn't have to relearn static image recognition features, but the conceptual leap from static images to dynamic videos that your network needs to learn is still a big one!</p>
",Image sequence training CNN RNN I making first steps learning Deep Learning I trying Activity Recognition images sequences frames videos As result facing problem training procedure Firstly need determine architecture images folders Making Food gt p gt rgb frame png rgb frame png rgb frame png Making Food gt p gt rgb frame png rgb frame png rgb frame png Taking Medicine gt p gt rgb frame png rgb frame png rgbframe png etc So problem folder different number frames I get confused input shape model timesteps I use I creating model see bellow time distirbuted CNN pre trained VGG LSTM takes input frames classes coresponding labels example making food would coresponding label p rgb frame etc final shape x train coresponds frames classes height amp width channel images I reshaping data order used input model I wondering worried I pass proper timestep result wrong training val acc testing different,"startoftags, python, tensorflow, machinelearning, keras, deeplearning, endoftags",python django djangorestframework endoftags,python tensorflow machinelearning keras deeplearning,python django djangorestframework,0.26
65368430,2020-12-19,2020,2,How to remove NaNs and squeeze in a DataFrame - pandas,"<p>I was doing some coding and realized something, I think there is an easier way of doing this.</p>
<p>So I have a <code>DataFrame</code> like this:</p>
<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'a': [1, 'A', 2, 'A'], 'b': ['A', 3, 'A', 4]})
   a  b
0  1  A
1  A  3
2  2  A
3  A  4
</code></pre>
<p>And I want to remove all of the <code>A</code>s from the data, but I also want to squeeze in the <code>DataFrame</code>, what I mean by squeezing in the <code>DataFrame</code> is to have a result of this:</p>
<pre><code>   a  b
0  1  3
1  2  4
</code></pre>
<p>I have a solution as follows:</p>
<pre><code>a = df['a'][df['a'] != 'A']
b = df['b'][df['b'] != 'A']
df2 = pd.DataFrame({'a': a.tolist(), 'b': b.tolist()})
print(df2)
</code></pre>
<p>Which works, but I seem to think there is an easier way, I've stopped coding for a while so not so bright anymore...</p>
<p><strong>Note:</strong></p>
<p>All columns have the same amount of <code>A</code>s, there is no problem there.</p>
","['python', 'pandas', 'dataframe']",65368461,"<p>This would do:</p>
<pre><code>In [1513]: df.replace('A', np.nan).apply(lambda x: pd.Series(x.dropna().to_numpy()))
Out[1513]: 
     a    b
0  1.0  3.0
1  2.0  4.0
</code></pre>
",How remove NaNs squeeze DataFrame pandas I coding realized something I think easier way So I DataFrame like gt gt gt df pd DataFrame A A b A A b A A A A And I want remove As data I also want squeeze DataFrame I mean squeezing DataFrame result b I solution follows df df A b df b df b A df pd DataFrame tolist b b tolist print df Which works I seem think easier way I stopped coding bright anymore Note All columns amount As problem,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
65456517,2020-12-26,2020,7,"Join two DataFrames on common columns only if the difference in a separate column is within range [-n, +n]","<p>I have two data frames <code>df1</code> and <code>df2</code> as shown below:</p>
<pre><code>df1

Date        BillNo.     Amount
10/08/2020  ABBCSQ1ZA   878
10/09/2020  AADC9C1Z5   11
10/12/2020  AC928Q1ZS   3998
10/14/2020  AC9268RE3   198
10/16/2020  AA171E1Z0   5490
10/19/2020  BU073C1ZW   3432

df2

Date        BillNo.     Amount
10/08/2020  ABBCSQ1ZA   876
10/11/2020  ATRC95REW   115
10/14/2020  AC9268RE3   212
10/16/2020  AA171E1Z0   5491
10/25/2020  BPO66W2LO   344

</code></pre>
<p>My final answer should be:</p>
<pre><code>final

Date        BillNo.     Amount
10/08/2020  ABBCSQ1ZA   876
10/16/2020  AA171E1Z0   5491
</code></pre>
<p>How do I find common rows from both the data frame using <code>Date BillNo. Amount</code> when the difference in value range is between [-5,5]?</p>
<p>I know how to find common rows by using:</p>
<pre><code>df_all = df1.merge(df2.drop_duplicates(), on=['Date', 'BillNo.', 'Amount'], 
                   how='outer', indicator=True)
</code></pre>
<p>However, this doesn't give the rows which are in range. Anyone who could help?</p>
<p>Edit: We can see in <code>df1: 10/14/2020,AC9268RE3,198</code> and <code>df2: 10/14/2020,AC9268RE3,212</code> the difference is 14, hence this should not be included in common rows</p>
","['python', 'pandas', 'dataframe']",65456601,"<p>We can merge, then perform a query to drop rows not within the range:</p>
<pre><code>(df1.merge(df2, on=['Date', 'BillNo.'])
    .query('abs(Amount_x - Amount_y) &lt;= 5')
    .drop('Amount_x', axis=1))

         Date    BillNo.  Amount_y
0  10/08/2020  ABBCSQ1ZA       876
1  10/16/2020  AA171E1Z0      5491
</code></pre>
<p>This works well as long as there is only one row that corresponds to a specific (Date, BillNo) combination in each frame.</p>
",Join two DataFrames common columns difference separate column within range n n I two data frames df df shown df Date BillNo Amount ABBCSQ ZA AADC C Z AC Q ZS AC RE AA E Z BU C ZW df Date BillNo Amount ABBCSQ ZA ATRC REW AC RE AA E Z BPO W LO My final answer final Date BillNo Amount ABBCSQ ZA AA E Z How I find common rows data frame using Date BillNo Amount difference value range I know find common rows using df df merge df drop duplicates Date BillNo Amount outer indicator True However give rows range Anyone could help Edit We see df AC RE df AC RE difference hence included common rows,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
65506750,2020-12-30,2020,3,DataFrame MultiIndex - find column by value,"<p>I have a multiindex dataframe with two layers of indices and roughly 100 columns. I would like to get groups of values (organized in columns) based on the presence of a certain value, but I am still struggling with the indexing mechanics.</p>
<p>Here is some example data:</p>
<pre><code>import pandas as pd

index_arrays = [np.array([&quot;one&quot;]*5+[&quot;two&quot;]*5), 
                np.array([&quot;aaa&quot;,&quot;bbb&quot;,&quot;ccc&quot;,&quot;ddd&quot;,&quot;eee&quot;]*2)]

df = pd.DataFrame([[1,2,3],[4,5,6],[7,8,9],
                   [10,11,12],[13,14,15],[16,1,17],
                   [18,19,20],[21,22,23],[24,25,26],
                   [27,28,29]], index=index_arrays)
</code></pre>
<p>Gives</p>
<pre><code>          0   1   2
one aaa   1   2   3
    bbb   4   5   6
    ccc   7   8   9
    ddd  10  11  12
    eee  13  14  15
two aaa  16   1  17
    bbb  18  19  20
    ccc  21  22  23
    ddd  24  25  26
    eee  27  28  29
</code></pre>
<p>Now, for each level_0 index (<code>one</code> and <code>two</code>), I want to return the entire column in which the level_1 index of <code>aaa</code> equals to a certain value, for example 1.
What I got so far is this:</p>
<pre><code>df[df.loc[(slice(None), &quot;aaa&quot;),:]==1].any(axis=1)
&gt;
one  aaa     True
     bbb    False
     ccc    False
     ddd    False
     eee    False
two  aaa     True
     bbb    False
     ccc    False
     ddd    False
     eee    False
</code></pre>
<p>Instead of the boolean values, I would like to retrieve the actual values. The expected output would be:</p>
<pre><code>expected:
          0
one aaa   1
    bbb   4
    ccc   7
    ddd  10
    eee  13
two aaa   1
    bbb  19
    ccc  22
    ddd  25
    eee  28
</code></pre>
<p>I would appreciate your help.</p>
<p><strong>Bonus question</strong>: Additionally, it would be great to know which column contains the values in question. For the example above, this would be <code>column 0</code> (for index <code>one</code>)and <code>column 1</code> (for index <code>two</code>). Is there a way to do this?
Thanks!</p>
","['python', 'pandas', 'dataframe']",65507167,"<p>This might be what you're looking for:</p>
<pre><code>df.loc[df.index.get_level_values(0) == 'one', df.loc[('one', 'aaa')] == 1]
</code></pre>
<p>This outputs:</p>
<pre><code>          0
one aaa   1
    bbb   4
    ccc   7
    ddd  10
    eee  13
</code></pre>
<p>To combine the results for all of the different values of the first index, generate these DataFrames and concatenate them:</p>
<pre><code>output_df = pd.DataFrame()
for level_0_val in df.index.get_level_values(0).unique():
    _ = df.loc[df.index.get_level_values(0) == level_0_val, df.loc[(level_0_val, 'aaa')] == 1]
    output_df = output_df.append(_)
</code></pre>
<p>Here is output_df:</p>
<pre><code>            0     1
one aaa   1.0   NaN
    bbb   4.0   NaN
    ccc   7.0   NaN
    ddd  10.0   NaN
    eee  13.0   NaN
two aaa   NaN   1.0
    bbb   NaN  19.0
    ccc   NaN  22.0
    ddd   NaN  25.0
    eee   NaN  28.0
</code></pre>
<p>You can then generate your desired output from this.</p>
",DataFrame MultiIndex find column value I multiindex dataframe two layers indices roughly columns I would like get groups values organized columns based presence certain value I still struggling indexing mechanics Here example data import pandas pd index arrays np array quot one quot quot two quot np array quot aaa quot quot bbb quot quot ccc quot quot ddd quot quot eee quot df pd DataFrame index index arrays Gives one aaa bbb ccc ddd eee two aaa bbb ccc ddd eee Now level index one two I want return entire column level index aaa equals certain value example What I got far df df loc slice None quot aaa quot axis gt one aaa True bbb False ccc False ddd False eee False two aaa True bbb False ccc False ddd False eee False Instead boolean values I would like retrieve actual values The expected output would expected one,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
65527482,2021-01-01,2021,2,How to get value of column while performing groupby function,"<p>Hi I have a dataframe with datetime and high and low during the period. for eg. its 5 minutes data call it df</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;""></th>
<th style=""text-align: center;"">Datetime</th>
<th style=""text-align: center;"">High</th>
<th style=""text-align: right;"">Low</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">0</td>
<td style=""text-align: center;"">2020-12-02 09:15:00</td>
<td style=""text-align: center;"">590.349976</td>
<td style=""text-align: right;"">584.299988</td>
</tr>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: center;"">2020-12-02 09:20:00</td>
<td style=""text-align: center;"">593.900024</td>
<td style=""text-align: right;"">588.750000</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: center;"">2020-12-02 09:25:00</td>
<td style=""text-align: center;"">594.900024</td>
<td style=""text-align: right;"">592.450012</td>
</tr>
<tr>
<td style=""text-align: left;"">3</td>
<td style=""text-align: center;"">2020-12-02 09:30:00</td>
<td style=""text-align: center;"">593.849976</td>
<td style=""text-align: right;"">591.799988</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td style=""text-align: center;"">2020-12-02 09:35:00</td>
<td style=""text-align: center;"">594.400024</td>
<td style=""text-align: right;"">592.200012</td>
</tr>
<tr>
<td style=""text-align: left;"">5</td>
<td style=""text-align: center;"">2020-12-02 09:40:00</td>
<td style=""text-align: center;"">596.250000</td>
<td style=""text-align: right;"">593.099976</td>
</tr>
<tr>
<td style=""text-align: left;"">6</td>
<td style=""text-align: center;"">2020-12-02 09:45:00</td>
<td style=""text-align: center;"">596.849976</td>
<td style=""text-align: right;"">593.102476</td>
</tr>
<tr>
<td style=""text-align: left;"">7</td>
<td style=""text-align: center;"">2020-12-02 09:50:00</td>
<td style=""text-align: center;"">595.400024</td>
<td style=""text-align: right;"">592.950012</td>
</tr>
<tr>
<td style=""text-align: left;"">8</td>
<td style=""text-align: center;"">2020-12-02 09:55:00</td>
<td style=""text-align: center;"">596.500000</td>
<td style=""text-align: right;"">594.500000</td>
</tr>
</tbody>
</table>
</div>
<p>I perform a groupby function to convert the 5 minutes to 15 minutes and get following output.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;""></th>
<th style=""text-align: center;"">Datetime</th>
<th style=""text-align: center;"">High</th>
<th style=""text-align: right;"">Low</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">0</td>
<td style=""text-align: center;"">2020-12-02 09:15:00</td>
<td style=""text-align: center;"">594.900024</td>
<td style=""text-align: right;"">584.299988</td>
</tr>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: center;"">2020-12-02 09:30:00</td>
<td style=""text-align: center;"">596.250000</td>
<td style=""text-align: right;"">591.799988</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: center;"">2020-12-02 09:45:00</td>
<td style=""text-align: center;"">596.849976</td>
<td style=""text-align: right;"">592.950012</td>
</tr>
</tbody>
</table>
</div>
<p>Is it possible while performing groupby function to also get exact datetime of high and low in following desired output.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;""></th>
<th style=""text-align: center;"">Datetime</th>
<th style=""text-align: center;"">High</th>
<th style=""text-align: center;"">Low</th>
<th style=""text-align: center;"">High datetime</th>
<th style=""text-align: right;"">Low Datetime</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">0</td>
<td style=""text-align: center;"">2020-12-02 09:15:00</td>
<td style=""text-align: center;"">594.900024</td>
<td style=""text-align: center;"">584.299988</td>
<td style=""text-align: center;"">2020-12-02 09:25:00</td>
<td style=""text-align: right;"">2020-12-02 09:15:00</td>
</tr>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: center;"">2020-12-02 09:30:00</td>
<td style=""text-align: center;"">596.250000</td>
<td style=""text-align: center;"">591.799988</td>
<td style=""text-align: center;"">2020-12-02 09:40:00</td>
<td style=""text-align: right;"">2020-12-02 09:30:00</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: center;"">2020-12-02 09:45:00</td>
<td style=""text-align: center;"">596.849976</td>
<td style=""text-align: center;"">592.950012</td>
<td style=""text-align: center;"">2020-12-02 09:45:00</td>
<td style=""text-align: right;"">2020-12-02 09:50:00</td>
</tr>
</tbody>
</table>
</div>","['python', 'pandas', 'datetime']",65527703,"<p>You can <code>group</code> the dataframe by <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Grouper.html"" rel=""nofollow noreferrer""><code>pd.Grouper</code></a> with a frequency of <code>15min</code> and aggregate using dictionary containing the aggregation functions:</p>
<pre><code>d = {'High': ('High', 'max'), 'Low': ('Low', 'min'),
     'High date': ('High', 'idxmax'), 'Low date': ('Low', 'idxmin')}

df.set_index('Datetime').groupby(pd.Grouper(freq='15min')).agg(**d)
</code></pre>
<hr />
<pre><code>                     High        Low        High date           Low date
Datetime                                                                           
2020-12-02 09:15:00  594.900024  584.299988 2020-12-02 09:25:00 2020-12-02 09:15:00
2020-12-02 09:30:00  596.250000  591.799988 2020-12-02 09:40:00 2020-12-02 09:30:00
2020-12-02 09:45:00  596.849976  592.950012 2020-12-02 09:45:00 2020-12-02 09:50:00
</code></pre>
",How get value column performing groupby function Hi I dataframe datetime high low period eg minutes data call df Datetime High Low I perform groupby function convert minutes minutes get following output Datetime High Low Is possible performing groupby function also get exact datetime high low following desired output Datetime High Low High datetime Low Datetime,"startoftags, python, pandas, datetime, endoftags",python pandas numpy endoftags,python pandas datetime,python pandas numpy,0.67
65534148,2021-01-01,2021,2,Merge dataframes on multiple columns,"<p>There are two tables like:  <code>first_name</code>, <code>last_name</code>, <code>date of birth</code>, ...</p>
<p>I would like to create a new table with only the people listed that are present in both tables.</p>
<p>If I try to lookup by looping an &quot;isin&quot;-Method, I can look for a match for one column, but I would like to match both columns at the same time.</p>
","['python', 'pandas', 'dataframe']",65535714,"<p>Extending the answer mentioned in the comment (by @Scott Boston), if you have the following dataframes:</p>
<pre><code>df1 = pd.DataFrame(np.array([['Jack', 'Brown', '1980-01-01'], ['Joe', 'Doe', '1990-02-02']
                            , ['John', 'Jones', '2000-03-03']])
                   , columns=['first_name', 'last_name', 'birth_date'])
</code></pre>
<p>df1 would be:</p>
<pre><code>first_name last_name birth_date
Jack       Brown     1980-01-01
Joe        Doe       1990-02-02
John       Jones     2000-03-03
</code></pre>
<pre><code>df2 = pd.DataFrame(np.array([['Jack', 'Brown', '2020-01-29'], ['Joe', 'Smith', '1999-09-09']
                            , ['Sarah', 'Morphy', '2011-11-11']])
                   , columns=['first_name', 'last_name', 'birth_date'])
</code></pre>
<p>df2 would be:</p>
<pre><code>first_name last_name birth_date
Jack       Brown     2010-10-10
Joe        Smith     1999-09-09
Sarah      Morphy    2011-11-11
</code></pre>
<pre><code>result = pd.merge(df1, df2, how=&quot;inner&quot;, on=[&quot;first_name&quot;, &quot;last_name&quot;])
</code></pre>
<p>Your result would be this:</p>
<pre><code>first_name last_name birth_date_x birth_date_y
Jack       Brown     1980-01-01   2010-10-10
</code></pre>
<p>You can change <code>_x</code> and <code>_y</code> suffixes as well by adding <code>suffixes=[&quot;_df1&quot;, &quot;_df2&quot;]</code></p>
",Merge dataframes multiple columns There two tables like first name last name date birth I would like create new table people listed present tables If I try lookup looping quot isin quot Method I look match one column I would like match columns time,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
65557182,2021-01-04,2021,4,Merge values of two Python dictionaries by keys,"<p>I want to merge the values of two dictionaries by their keys. Example:</p>
<pre><code>d1 = {'a':1, 'b':2, 'c':3}
d2 = {'a':2, 'b':[2,3], 'd':3}
</code></pre>
<p>desired output:</p>
<pre><code>{'a': [1, 2], 'b': [2, 2, 3], 'c': [3], 'd': [3]}
</code></pre>
<p>What I have so far is</p>
<pre><code>d12 = {}
for d in (d1, d2):
    for k,v in d.items(): 
        d12.setdefault(k, []).append(v)
</code></pre>
<p>which produces</p>
<pre><code>d12 = {'a': [1, 2], 'b': [2, [2, 3]], 'c': [3], 'd': [3]}
</code></pre>
<p>not desired output.</p>
<p>I searched a bit on SO and found that <a href=""https://stackoverflow.com/a/31093734/7864386"">this post</a> answers my question if only it didn't throw up <code>TypeError: can only concatenate tuple (not &quot;int&quot;) to tuple.</code></p>
","['python', 'python-3.x', 'dictionary']",65557214,"<p>The problem is your values are sometimes <code>int</code>s and sometimes <code>list</code>s. You must check the data type and either <code>append</code> or <code>extend</code> accordingly:</p>
<pre><code>for k, v in d.items():
    if isinstance(v, list):
        d12.setdefault(k, []).extend(v)
    else:
        d12.setdefault(k, []).append(v)
</code></pre>
",Merge values two Python dictionaries keys I want merge values two dictionaries keys Example b c b desired output b c What I far k v items setdefault k append v produces b c desired output I searched bit SO found post answers question throw TypeError concatenate tuple quot int quot tuple,"startoftags, python, python3x, dictionary, endoftags",python django djangorestframework endoftags,python python3x dictionary,python django djangorestframework,0.33
65603352,2021-01-06,2021,2,How do I replace a string from one column using the data from two other columns (pandas),"<p>My <code>df</code> looks something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>col1</th>
<th>col2</th>
<th>col3</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.98</td>
<td>0.01</td>
<td>SP</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>SP</td>
</tr>
<tr>
<td>0.89</td>
<td>SP</td>
<td>0.1</td>
</tr>
<tr>
<td>0.97</td>
<td>SP</td>
<td>0.02</td>
</tr>
<tr>
<td>0.96</td>
<td>0</td>
<td>SP</td>
</tr>
</tbody>
</table>
</div>
<p>I have some idea of how to code this but not quite there,</p>
<p>I want the <code>df</code> to become this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>col1</th>
<th>col2</th>
<th>col3</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.98</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0.89</td>
<td>0.01</td>
<td>0.1</td>
</tr>
<tr>
<td>0.97</td>
<td>0.01</td>
<td>0.02</td>
</tr>
<tr>
<td>0.96</td>
<td>0</td>
<td>0.04</td>
</tr>
</tbody>
</table>
</div>
<p>This is an idea of what I am trying to do but it's not quite right</p>
<p><code>df = df.apply(lambda x: float(1 - x['col1'] - x['col2']) if x['col3'] == &quot;SP&quot; else x, axis=1)</code></p>
<p><code>df = df.apply(lambda x: float(1 - x['col1'] - x['col3']) if x['col2'] == &quot;SP&quot; else x, axis=1)</code></p>
","['python', 'pandas', 'dataframe']",65603526,"<p>You can try fillna:</p>
<pre><code>s = df[['col2','col3']].apply(pd.to_numeric, errors='coerce')
SPs = s.isna()

fill= 1 - (s.mask(SPs,0).astype(float).sum(1) + df['col1'])

df[['col2','col3']] = s.apply(lambda x: x.fillna(fill))
</code></pre>
",How I replace string one column using data two columns pandas My df looks something like col col col SP SP SP SP SP I idea code quite I want df become col col col This idea I trying quite right df df apply lambda x float x col x col x col quot SP quot else x axis df df apply lambda x float x col x col x col quot SP quot else x axis,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
66030104,2021-02-03,2021,2,Pandas: Calculate end-day product volumes,"<p>I Am trying to calculate the volume at the end of each day for each product-buyer from this table:</p>
<pre><code>import pandas as pd
df = pd.DataFrame({'day': [pd.datetime(2020,1,1), pd.datetime(2020,1,2), pd.datetime(2020,1,3), pd.datetime(2020,1,4), pd.datetime(2020,1,5)], 'Product':['Bread', 'Bread', 'Bread', 'Bread', 'Water'],'Buyer': ['John', 'Mat', 'John', 'Bill', 'John'], 'Seller': ['Mat', 'John', 'Bill', 'John', 'Bill'], 'Volume':[1000,2000,3000,2000,1000]})

df
         day Product Buyer Seller  Volume
0 2020-01-01   Bread  John    Mat    1000
1 2020-01-02   Bread   Mat   John    2000
2 2020-01-03   Bread  John   Bill    3000
3 2020-01-04   Bread  Bill   John    2000
4 2020-01-05   Water  John   Bill    1000

df_expected

Out[61]: 
         day Product Buyer  Volume
0 2020-01-01   Bread  Jhon    1000
1 2020-01-02   Bread   Mat    1000
2 2020-01-03   Bread  Jhon    2000
3 2020-01-04   Bread  Bill    2000
4 2020-01-05   Water  Jhon    1000
</code></pre>
<p>For example, at the end of 2020-01-03 John should have 2000â¬ of Bread(1000-2000+3000).
Is there any way that i can achieve this result. I have tried countless ways of group by but nothing worked as intended.</p>
","['python', 'pandas', 'dataframe']",66031020,"<p>Basically using accounting principles,  turn it into a ledger of buy and sell transactions.  Then <code>cumsum()</code> is straight forward.</p>
<pre><code>import datetime as dt
import pandas as pd
df = pd.DataFrame({'day':pd.date_range(dt.date(2020,1,1), dt.date(2020,1,5)), 
                   'Product': ['Bread', 'Water', 'Bread', 'Bread', 'Water'], 
                   'Buyer': ['John', 'Mat', 'John', 'Bill', 'John'], 
                   'Seller': ['Mat', 'John', 'Bill', 'John', 'Bill'], 
                   'Volume':[1000,2000,3000,2000,1000]
                  })

# transform to effectively a double entry ledger
dfde = pd.concat([df.loc[:,[c for c in df.columns if c!=&quot;Buyer&quot;]]
           .rename(columns={&quot;Seller&quot;:&quot;Account&quot;})
                  .assign(Volume=lambda dfa: dfa[&quot;Volume&quot;]*-1, Ind=&quot;S&quot;)
          ,df.loc[:,[c for c in df.columns if c!=&quot;Seller&quot;]]
           .rename(columns={&quot;Buyer&quot;:&quot;Account&quot;})
           .assign( Ind=&quot;B&quot;)
          ]).sort_values([&quot;day&quot;,&quot;Product&quot;,&quot;Account&quot;]).assign(Holding=dfde.groupby([&quot;Product&quot;,&quot;Account&quot;]).cumsum())

</code></pre>
<h2>output</h2>
<pre><code>       day Product Account  Volume Ind  Holding
2020-01-01   Bread    John    1000   B     1000
2020-01-01   Bread     Mat   -1000   S    -1000
2020-01-02   Water    John   -2000   S    -2000
2020-01-02   Water     Mat    2000   B     2000
2020-01-03   Bread    Bill   -3000   S    -3000
2020-01-03   Bread    John    3000   B     4000
2020-01-04   Bread    Bill    2000   B    -1000
2020-01-04   Bread    John   -2000   S     2000
2020-01-05   Water    Bill   -1000   S    -1000
2020-01-05   Water    John    1000   B    -1000

</code></pre>
",Pandas Calculate end day product volumes I Am trying calculate volume end day product buyer table import pandas pd df pd DataFrame day pd datetime pd datetime pd datetime pd datetime pd datetime Product Bread Bread Bread Bread Water Buyer John Mat John Bill John Seller Mat John Bill John Bill Volume df day Product Buyer Seller Volume Bread John Mat Bread Mat John Bread John Bill Bread Bill John Water John Bill df expected Out day Product Buyer Volume Bread Jhon Bread Mat Bread Jhon Bread Bill Water Jhon For example end John Bread Is way achieve result I tried countless ways group nothing worked intended,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
66334500,2021-02-23,2021,2,Pandas group by values in list (in series),"<p>I am trying to group by items in a list in DataFrame Series. The dataset being used is the <a href=""https://insights.stackoverflow.com/survey"" rel=""nofollow noreferrer"">Stack Overflow 2020 Survey</a>.</p>
<p>The layout is roughly as follows:</p>
<pre><code>           ... LanguageWorkedWith ... ConvertedComp ...
Respondent
    1               Python;C              50000
    2                C++;C                70000
</code></pre>
<p>I want to essentially want to use <code>groupby</code> on the unique values in the list of languages worked with, and apply the a mean aggregator function to the ConvertedComp like so...</p>
<pre><code>LanguageWorkedWith
        C++                 70000
        C                   60000
      Python                50000
</code></pre>
<p>I have actually managed to achieve the desired output but my solution seems somewhat janky and being new to Pandas, I believe that there is probably a better way.</p>
<p>My solution is as follows:</p>
<pre><code># read csv
sos = pd.read_csv(&quot;developer_survey_2020/survey_results_public.csv&quot;, index_col='Respondent')

# seperate string into list of strings, disregarding unanswered responses
temp = sos[&quot;LanguageWorkedWith&quot;].dropna().str.split(';')

# create new DataFrame with respondent index and rows populated withknown languages
langs_known = pd.DataFrame(temp.tolist(), index=temp.index)

# stack columns as rows, dropping old column names
stacked_responses = langs_known.stack().reset_index(level=1, drop=True)

# Re-indexing sos DataFrame to match stacked_responses dimension
# Concatenate reindex series to ConvertedComp series columnwise
reindexed_pays = sos[&quot;ConvertedComp&quot;].reindex(stacked_responses.index)
stacked_with_pay = pd.concat([stacked_responses, reindexed_pays], axis='columns')

# Remove rows with no salary data
# Renaming columns
stacked_with_pay.dropna(how='any', inplace=True)
stacked_with_pay.columns = [&quot;LWW&quot;, &quot;Salary&quot;]

# Group by LLW and apply median 
lang_ave_pay = stacked_with_pay.groupby(&quot;LWW&quot;)[&quot;Salary&quot;].median().sort_values(ascending=False).head()
</code></pre>
<p>Output:</p>
<pre><code>LWW
Perl     76131.5
Scala    75669.0
Go       74034.0
Rust     74000.0
Ruby     71093.0
Name: Salary, dtype: float64
</code></pre>
<p>which matches the value calculated when choosing specific language: <code>sos.loc[sos[&quot;LanguageWorkedWith&quot;].str.contains('Perl').fillna(False), &quot;ConvertedComp&quot;].median()</code></p>
<p>Any tips on how to improve/functions that provide this functionality/etc would be appreciated!</p>
","['python', 'pandas', 'pandas-groupby']",66347468,"<p>In the target column only data frame, decompose the language name and combine it with the salary. The next step is to convert the data from horizontal format to vertical format using melt. Then we group the language names together to get the median. <a href=""https://pandas.pydata.org/docs/reference/api/pandas.melt.html"" rel=""nofollow noreferrer"">melt docs</a></p>
<pre><code>lww = sos[[&quot;LanguageWorkedWith&quot;,&quot;ConvertedComp&quot;]]
lwws = pd.concat([lww['ConvertedComp'], lww['LanguageWorkedWith'].str.split(';', expand=True)], axis=1)
lwws.reset_index(drop=True, inplace=True)
df_long = pd.melt(lwws, id_vars='ConvertedComp', value_vars=lwws.columns[1:], var_name='lang', value_name='lang_name')
df_long.groupby('lang_name')['ConvertedComp'].median().sort_values(ascending=False).head()

lang_name
Perl     76131.5
Scala    75669.0
Go       74034.0
Rust     74000.0
Ruby     71093.0
Name: ConvertedComp, dtype: float64
</code></pre>
",Pandas group values list series I trying group items list DataFrame Series The dataset used Stack Overflow Survey The layout roughly follows LanguageWorkedWith ConvertedComp Respondent Python C C C I want essentially want use groupby unique values list languages worked apply mean aggregator function ConvertedComp like LanguageWorkedWith C C Python I actually managed achieve desired output solution seems somewhat janky new Pandas I believe probably better way My solution follows read csv sos pd read csv quot developer survey survey results public csv quot index col Respondent seperate string list strings disregarding unanswered responses temp sos quot LanguageWorkedWith quot dropna str split create new DataFrame respondent index rows populated withknown languages langs known pd DataFrame temp tolist index temp index stack columns rows dropping old column names stacked responses langs known stack reset index level drop True Re indexing sos DataFrame match stacked responses dimension Concatenate reindex series ConvertedComp series,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
66428328,2021-03-01,2021,2,How to selectively create a dictionary from an existing one?,"<p>I have two dictionaries like below which have common keys:</p>
<pre><code>dictionary1 = {1: 'a', 2: 'b' , 3: 'c'}
dictionary2 = {1: 'no', 2: 'yes' ,3:'yes'}
</code></pre>
<p>I want to create a new dictionary with the key and values of <code>dictionary1</code> only if the corresponding values of the <code>dictionary2</code> key has <code>&quot;yes&quot;</code>.</p>
<p>Expected output:</p>
<pre><code>{2: 'b', 3: 'c'}
</code></pre>
<p>What I have tried:</p>
<pre><code>dictionary1 = {1: 'a', 2: 'b' , 3: 'c'}
dictionary2 = {1: 'no', 2: 'yes' ,3:'yes'}
common_pairs = dict()

for key,value in dictionary2.items():
  for key,v in dictionary1.items():
    if(value == &quot;yes&quot;):
      common_pairs[key] =  v
</code></pre>
","['python', 'python-3.x', 'dictionary']",66428359,"<p>You don't need a nested <code>for</code> loop. Just a single iteration over <code>dictionary1</code> items with corresponding O(1) lookup in <code>dictionary2</code>:</p>
<p>With a <code>dict</code> comprehension this would look like:</p>
<pre><code>&gt;&gt;&gt; dictionary1 = {1: 'a', 2: 'b' , 3: 'c'}
&gt;&gt;&gt; dictionary2 = {1: 'no', 2: 'yes' ,3:'yes'}
&gt;&gt;&gt; new = {k: v for k, v in dictionary1.items() if dictionary2[k] == 'yes'}
&gt;&gt;&gt; new
{2: 'b', 3: 'c'}
</code></pre>
<p>With a traditional <code>for</code> loop:</p>
<pre><code>&gt;&gt;&gt; new = {}
&gt;&gt;&gt; for k, v in dictionary1.items():
...     if dictionary2[k] == 'yes':
...         new[k] = v
... 
&gt;&gt;&gt; new
{2: 'b', 3: 'c'}
</code></pre>
",How selectively create dictionary existing one I two dictionaries like common keys dictionary b c dictionary yes yes I want create new dictionary key values dictionary corresponding values dictionary key quot yes quot Expected output b c What I tried dictionary b c dictionary yes yes common pairs dict key value dictionary items key v dictionary items value quot yes quot common pairs key v,"startoftags, python, python3x, dictionary, endoftags",python python3x list endoftags,python python3x dictionary,python python3x list,0.67
66717869,2021-03-20,2021,2,Multiplying pandas columns based on multiple conditions,"<p>I have a df like this</p>
<pre><code>|  count  | people |  A  |  B  |  C  |
|---------|--------|-----|-----|-----|
|  yes    |  siya  | 4   | 2   |  0  |
|  no     |   aish | 4   | 3   |  0  | 
|  total  |        | 4   |     |  0  | 
|  yes    |   dia  | 6   |  4  |  0  | 
|  no     |   dia  | 6   |   2 |  0  |
|  total  |        | 6   |     |  0  |
</code></pre>
<p>I want a output like below</p>
<pre><code>|  count  | people |  A  |  B  |  C  |
|---------|--------|-----|-----|-----|
|  yes    |  siya  | 4   | 2   |  8  |
|  no     |   aish | 4   | 3   |  0  | 
|  total  |        | 4   |     |  0  | 
|  yes    |   dia  | 6   |  4  |  0  | 
|  no     |   dia  | 6   |  2  |  2  |
|  total  |        | 6   |     |  0  |
</code></pre>
<p>The goal is calculate column C by mulytiplying A and B only when the count value is &quot;yes&quot; but if the column People values are same that is yes for dia and no for also dia , then we have to calculate for  the count value &quot;no&quot;</p>
<p>I tried this much so far</p>
<pre><code>df.C= df.groupby(&quot;Host&quot;, as_index=False).apply(lambda dfx : df.A * 
                           df.B if (df['count'] == 'no') else df.A *df.B)
</code></pre>
<p>But not able to achieve the goal, any idea how can I achieve the output</p>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",66717972,"<pre><code>import numpy as np

#Set Condtions

c1=df.groupby('people')['count'].transform('nunique').eq(1)&amp;df['count'].eq('yes')
c2=df.groupby('people')['count'].transform('nunique').gt(1)&amp;df['count'].eq('no')
#Put conditions in list
c=[c1,c2]

#Mke choices corresponding to condition list
choice=[df['A']*df['B'],len(df[df['count'].eq('no')])]

#Apply np select
df['C']= np.select(c,choice,0)

print(df)




    count people  A    B    C
0    yes   siya  4  2.0  8.0
1     no   aish  4  3.0  0.0
2  total    NaN  4  0.0  0.0
3    yes    dia  6  4.0  0.0
4     no    dia  6  2.0  2.0
5  total    NaN  6  NaN  0.0
</code></pre>
",Multiplying pandas columns based multiple conditions I df like count people A B C yes siya aish total yes dia dia total I want output like count people A B C yes siya aish total yes dia dia total The goal calculate column C mulytiplying A B count value quot yes quot column People values yes dia also dia calculate count value quot quot I tried much far df C df groupby quot Host quot index False apply lambda dfx df A df B df count else df A df B But able achieve goal idea I achieve output,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas dataframe pandasgroupby,python pandas dataframe,0.87
66802629,2021-03-25,2021,3,Transform month over month to month and months prior,"<p>Initial table</p>
<pre><code>salesman    training_date   01/20   02/20   03/20   04/20   05/20   06/20   07/20   08/20   09/20   10/20   11/20   12/20
0   John    2020-11-01       100      20     200     250      0       28      80      30     150     100     300    250
1   Ruddy   2020-07-12       90       50      30     225     300     100      95      10      20      0     20      100
</code></pre>
<p>In Python:</p>
<pre><code>t1 = {'salesman': ['John', 'Ruddy'],
     'training_date':['2020-11-30','2020-07-12'],
     '01/20': [100, 90], '02/20':[20,50], '03/20':[200,30],'04/20':[250,225],'05/20':[0,300],'06/20':[28,100],
     '07/20': [80, 95], '08/20':[30,10], '09/20':[150,20],'10/20':[100,0],'11/20':[300,20],'12/20':[250,100],
     }
t1a = pd.DataFrame(data=t1)
t1a
</code></pre>
<p>Dataframe expected:</p>
<pre><code>    salesman    training_date   training_month  1m_prior    2m_prior    3m_prior    4m_prior    5m_prior    6m_prior
0   John          2020-11-30         300           100        150           30         80      28       0
1   Ruddy         2020-07-12          95           100        300          225         30      50      90
</code></pre>
<p>In Python:</p>
<pre><code>t2 = {'salesman': ['John', 'Ruddy'],
     'training_date':['2020-11-30','2020-07-12'],
     'training_month': [300, 95], '1m_prior':[100,100], '2m_prior':[150,300],
     '3m_prior':[30,225],'4m_prior':[80,30],'5m_prior':[28,50], '6m_prior': [0, 90]}
t2a = pd.DataFrame(data=t2)
t2a
</code></pre>
<p>Explanation:<br>
John was trained on November 1st. 1m before November 1st, in October, John generated $100.
2m before November 1st, September, John generated $150.</p>
<p>Ruddy was trained on July 12th. 1m before July 12th, in June, Ruddy generated $100.
2m before July 12th, May, Ruddy generated $300.</p>
<p>In an ideal case, we start calculating 1 full month, always starting on the 1st of each month.
So, if Ruddy was hired on July 12th, 2020, one month before should be 1 June - 30 June.</p>
<p>Up to this point, we transform the data manually in Excel.</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",66802955,"<p>First turn your <code>training_date</code> into the 1st of the month using to_datetime and <code>astype</code> (I label it <code>'anchor_date'</code>). We then set the index and turn your columns into <code>datetime</code> dtypes and stack, setting us up for a simple way to calculate time differences in the next step</p>
<pre><code>import pandas as pd
import numpy as np

# Make datetime and then turn value into first of the month
df['training_date'] = pd.to_datetime(df['training_date'])
df['date_anchor'] = df.training_date.astype('datetime64[M]')

df = df.set_index(['salesman', 'training_date', 'date_anchor'])
df.columns = pd.Index(pd.to_datetime(df.columns, format='%m/%y'), name='date')
df = df.stack().reset_index()

#   salesman training_date date_anchor       date    0
#0      John    2020-11-30  2020-11-01 2020-01-01  100
#1      John    2020-11-30  2020-11-01 2020-02-01   20
#2      John    2020-11-30  2020-11-01 2020-03-01  200
#3      John    2020-11-30  2020-11-01 2020-04-01  250
#...
#19    Ruddy    2020-07-12  2020-07-01 2020-08-01   10
#20    Ruddy    2020-07-12  2020-07-01 2020-09-01   20
#21    Ruddy    2020-07-12  2020-07-01 2020-10-01    0
#22    Ruddy    2020-07-12  2020-07-01 2020-11-01   20
#23    Ruddy    2020-07-12  2020-07-01 2020-12-01  100   
</code></pre>
<p>Now we need to calculate the integer number of months in between, which we can do with some math, and use <code>np.select</code> to NaN months in the future and set our labels. Finally, pivot into your DataFrame.</p>
<pre><code>df['months'] = ((df.date.dt.year - df.date_anchor.dt.year) * 12 
                 + (df.date.dt.month - df.date_anchor.dt.month))

df['months'] = np.select([df.months.eq(0), df.months.lt(0)],
                         ['training_month', df.months.abs().astype(str) + 'm_prior'],
                         df.months.abs().astype(str) + 'm_post')

df = (df.pivot_table(index=['salesman', 'training_date'], columns='months', values=0)
        .rename_axis(columns=None)
        .reset_index())
</code></pre>
<hr />
<pre><code>  salesman training_date  10m_prior  1m_post  1m_prior  2m_post  2m_prior  3m_post  3m_prior  4m_post  4m_prior  5m_post  5m_prior  6m_prior  7m_prior  8m_prior  9m_prior  training_month
0     John    2020-11-30      100.0    250.0     100.0      NaN     150.0      NaN      30.0      NaN      80.0      NaN      28.0       0.0     250.0     200.0      20.0           300.0
1    Ruddy    2020-07-12        NaN     10.0     100.0     20.0     300.0      0.0     225.0     20.0      30.0    100.0      50.0      90.0       NaN       NaN       NaN            95.0
</code></pre>
",Transform month month month months prior Initial table salesman training date John Ruddy In Python salesman John Ruddy training date pd DataFrame data Dataframe expected salesman training date training month prior prior prior prior prior prior John Ruddy In Python salesman John Ruddy training date training month prior prior prior prior prior prior pd DataFrame data Explanation John trained November st November st October John generated November st September John generated Ruddy trained July th July th June Ruddy generated July th May Ruddy generated In ideal case start calculating full month always starting st month So Ruddy hired July th one month June June Up point transform data manually Excel,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
66966529,2021-04-06,2021,2,Pandas: Apply custom function to groups and store result in new columns in each group,"<p>I am trying to apply a custom function to each group in a groupby object and store the result into new columns in each group itself. The function returns 2 values and I want to store these values separately into 2 columns in each group.</p>
<p>I have tried this:</p>
<pre><code># Returns True if all values in Column1 is different.
def is_unique(x):
    status = True
    if len(x) &gt; 1:
        a = x.to_numpy() 
        if (a[0] == a).all():
            status = False
    return status

# Finds difference of the column values and returns the value with a message.
def func(x):
    d  = (x['Column3'].diff()).dropna()).iloc[0]
    return d, &quot;Calculated!&quot;

# is_unique() is another custom function used to filter unique groups.
df[['Difference', 'Message']] = df.filter(lambda x: is_unique(x['Column1'])).groupby(['Column2']).apply(lambda s: func(s))
</code></pre>
<p>But I am getting the error: <code>'DataFrameGroupBy' object does not support item assignment</code></p>
<p>I don't want to reset the index and want to view the result using the <code>get_group</code> function. The final dataframe should look like:</p>
<pre><code>df.get_group('XYZ')


   -----------------------------------------------------------------
   |   Column1 | Column2 | Column3  |  Difference   |    Message   |
   -----------------------------------------------------------------
   | 0   A     |   XYZ   |   100    |               |              |
   ----------------------------------               |              |
   | 1   B     |   XYZ   |    20    |      70       |  Calculated! |
   ----------------------------------               |              |
   | 2   C     |   XYZ   |    10    |               |              |
   -----------------------------------------------------------------
</code></pre>
<p>What is the most efficient way to achieve this result?</p>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",66966623,"<p>I think you need:</p>
<pre><code>def func(x):
    d  = (x['Column3'].diff()).dropna()).iloc[0]
    last = x.index[-1]
    x.loc[last, 'Difference'] = d
    x.loc[last, 'Message'] = &quot;Calculated!&quot;
    return x

df1 = df.filter(lambda x: is_unique(x['Column1']))

df1 = df1.groupby(['Column2']).apply(func)
</code></pre>
",Pandas Apply custom function groups store result new columns group I trying apply custom function group groupby object store result new columns group The function returns values I want store values separately columns group I tried Returns True values Column different def unique x status True len x gt x numpy status False return status Finds difference column values returns value message def func x x Column diff dropna iloc return quot Calculated quot unique another custom function used filter unique groups df Difference Message df filter lambda x unique x Column groupby Column apply lambda func But I getting error DataFrameGroupBy object support item assignment I want reset index want view result using get group function The final dataframe look like df get group XYZ Column Column Column Difference Message A XYZ B XYZ Calculated C XYZ What efficient way achieve result,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas dataframe pandasgroupby,python pandas dataframe,0.87
66971795,2021-04-06,2021,2,"Find large number in a list, where all previous numbers are also in the list","<p>I am trying to implement a Yellowstone Integer calculation which suggests that &quot;E<a href=""https://oeis.org/A064413"" rel=""nofollow noreferrer"">very number appears exactly once: this is a permutation of the positive numbers</a>&quot;. The formula I have implemented to derive the values is as follows:</p>
<pre><code>import math

yellowstone_list = []
item_list = []
i = 0

while i &lt;= 1000:
    if i &lt;= 3:
        yellowstone_list.append(i)
            
    else:
        j = 1
        inList = 1
        
        while inList == 1:
            minus_1 = math.gcd(j, yellowstone_list[i-1])
            minus_2 = math.gcd(j, yellowstone_list[i-2])
            if minus_1 == 1 and minus_2 &gt; 1:
                if j in yellowstone_list:
                    inList = 1
                else:
                    inList = 0

            j += 1
        
        yellowstone_list.append(j - 1)    
               
    item_list.append(i)

    i += 1
</code></pre>
<p>The issue becomes that as i increases, the time taken for the formula to determine the value of j also increases (naturally as i is increasingly further away from the start point of j).</p>
<p>What I would like to do is determine the largest value of j in the <code>yellowstone_list</code>, where all the values of 1 to j are already in the list.</p>
<p>As an example, in the below list, <code>j</code> would be 9, as all the values 0 - 9 are in the list:</p>
<p><code>yellowstone_list = [0, 1, 2, 3, 4, 9, 8, 15, 14, 5, 6, 25, 12, 35, 16, 7]</code></p>
<p>Any suggestions on how to implement this in an efficient manner?</p>
","['python', 'python-3.x', 'list']",66973013,"<p>After some time to sit down and think about it, and using the suggestions to sort the list, I came up with two solutions:</p>
<ul>
<li><em><strong>Sorting</strong></em></li>
</ul>
<p>I implemented @eugebe Sh.'s solution within the while i &lt; 1000 loop as follows:</p>
<pre class=""lang-py prettyprint-override""><code>    while i &lt;= 1000:

        m = sorted(yellowstone_list)
        for n in range(1, len(m)):
            if m[n]!=n:
                break

        if i == 0:
....
</code></pre>
<ul>
<li><em><strong>In List</strong></em></li>
</ul>
<p>I ran an increment to check if the value was in the list using the &quot;in&quot; function, also within the while i &lt; 1000 loop, as follows:</p>
<pre class=""lang-py prettyprint-override""><code>    while i &lt;= 1000:

        while k in yellowstone_list:
            k += 1

        if i == 0:
....
</code></pre>
<p>Running both codes 100 times, I got the following:</p>
<p><em><strong>Sorting</strong></em>: Total: 1:56.403527 seconds, Average: 1.164035 seconds.</p>
<p><em><strong>In List</strong></em>: Total: 1:14.225230 seconds, Average: 0.742252 seconds.</p>
",Find large number list previous numbers also list I trying implement Yellowstone Integer calculation suggests quot Every number appears exactly permutation positive numbers quot The formula I implemented derive values follows import math yellowstone list item list lt lt yellowstone list append else j inList inList minus math gcd j yellowstone list minus math gcd j yellowstone list minus minus gt j yellowstone list inList else inList j yellowstone list append j item list append The issue becomes increases time taken formula determine value j also increases naturally increasingly away start point j What I would like determine largest value j yellowstone list values j already list As example list j would values list yellowstone list Any suggestions implement efficient manner,"startoftags, python, python3x, list, endoftags",python django djangorestframework endoftags,python python3x list,python django djangorestframework,0.33
67000250,2021-04-08,2021,3,web scraping a certain row from a table,"<p>I am struggling in scraping a certain row from <a href=""http://www.centrometeolombardo.com/content.asp?CatId=332&amp;ContentType=Dati"" rel=""nofollow noreferrer"">this</a> website.</p>
<p>First of all the table element has no class in it but I think i got a workaround to that.</p>
<p>My problem is that I want to print (or store in a variable or access the data) of a certain row,
Let's say the row with the first value &quot;Bollate&quot;: <a href=""https://i.stack.imgur.com/FR6Qf.png"" rel=""nofollow noreferrer"">Screenshot of the row in the website</a></p>
<p>So I coded:</p>
<pre><code>import requests
import bs4

URL = &quot;http://www.centrometeolombardo.com/content.asp?CatId=332&amp;ContentType=Dati&quot;

response = requests.get(URL)
soup = bs4.BeautifulSoup(response.text, &quot;lxml&quot;)

table = soup.find(text=&quot;Bollate&quot;).find_parent(&quot;table&quot;)

for a in table:
    if a.text == &quot;Bollate&quot;:
       for val in a.parent-find_next_siblings():
           print(val.text)
</code></pre>
<p>But I get getting:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/pi/Documents/Python/ngu.py&quot;, line 12, in &lt;module&gt;
   if a.text == &quot;Bollate&quot;:
  File &quot;/usr/lib/Python3/dist-packages/bs4/element.py&quot;, line 370, in _getattr_
   self._class_._name_, attr))
AttributeError: 'NavigableString' object has no attribute 'text'
</code></pre>
<p>Which suggests me I am wrong since I get something that is not a text but I do not know how to overcome the problem.</p>
<p>Thanks all</p>
","['python', 'web-scraping', 'beautifulsoup']",67000779,"<p>You can use <code>pandas</code> to grab the <code>HTML</code> and parse the table. Then just select the value you need.</p>
<p>Here's how:</p>
<pre><code>import pandas as pd

url = &quot;http://www.centrometeolombardo.com/content.asp?CatId=332&amp;ContentType=Dati&quot;
df = pd.read_html(url, flavor=&quot;bs4&quot;)[19]
print(df.loc[df[0] == &quot;Bollate&quot;])
</code></pre>
<p>Output:</p>
<pre><code>         0     1     2      3  4  5
2  Bollate  -0.3  12.3  Brina  -  -
</code></pre>
",web scraping certain row table I struggling scraping certain row website First table element class I think got workaround My problem I want print store variable access data certain row Let say row first value quot Bollate quot Screenshot row website So I coded import requests import bs URL quot http www centrometeolombardo com content asp CatId amp ContentType Dati quot response requests get URL soup bs BeautifulSoup response text quot lxml quot table soup find text quot Bollate quot find parent quot table quot table text quot Bollate quot val parent find next siblings print val text But I get getting Traceback recent call last File quot home pi Documents Python ngu py quot line lt module gt text quot Bollate quot File quot usr lib Python dist packages bs element py quot line getattr self class name attr AttributeError NavigableString object attribute text Which suggests I wrong since,"startoftags, python, webscraping, beautifulsoup, endoftags",python webscraping beautifulsoup endoftags,python webscraping beautifulsoup,python webscraping beautifulsoup,1.0
67036970,2021-04-10,2021,5,"Create column based on date conditions, but I get this error AttributeError: &#39;SeriesGroupBy&#39; object has no attribute &#39;sub&#39;?","<p>Hey a python newbie here.</p>
<p>Suppose I have the first two columns of this data dataframe:</p>
<pre><code>df = pd.DataFrame({'group': [&quot;Sun&quot;, &quot;Moon&quot;, &quot;Sun&quot;, &quot;Moon&quot;, &quot;Mars&quot;, &quot;Mars&quot;],
                   'score': [2, 13, 24, 15, 11, 44], 
                   'datetime': [&quot;2017-08-30 07:00:00&quot;, &quot;2017-08-30 08:00:00&quot;, &quot;2017-08-31 07:00:00&quot;, &quot;2017-08-31 08:00:00&quot;, &quot;2017-08-29 21:00:00&quot;, &quot;2017-08-28 21:00:00&quot;],
                   'difference': [2, 13, 22, 2, -33, 44]})
</code></pre>
<p>I want to create a new column named <code>difference</code> (I have put it there as an illustration), such that
it is equal:</p>
<ul>
<li>score value in that row - score value of the day before in the same hour, for that group</li>
</ul>
<p>e.g. difference in row 3 is equal to:
score in that row - score on the day before (30th) at <code>08:00:00</code> for that group (i.e. Moon), i.e. <code>15 - 13 = 2</code>. If the day before and same time do not exist, then the value of the score of that row is taken (e.g. in row 0, for time <code>2017-08-30 07:00:00</code> there is no <code>2017-08-29 07:00:00</code>, hence only the 2 is taken).</p>
<p>I write the following:</p>
<pre><code>df['datetime'] = pd.to_datetime(df['datetime'])
before = df['datetime'] - pd.DateOffset(days=1)

df['difference'] = df.groupby([&quot;group&quot;, &quot;datetime&quot;])['score'].sub(
    before.map(df.set_index('datetime')['score']), fill_value=0)
</code></pre>
<p>but I get the error:
AttributeError: 'SeriesGroupBy' object has no attribute 'sub'</p>
<p>What am I missing? IS there any more elegant solution?</p>
","['python', 'pandas', 'dataframe']",67037494,"<h3><code>MultiIndex.map</code></h3>
<p>We can set the <code>group</code> column along with the <code>before</code> column as the index of the dataframe, then <code>map</code> the multiindex with score values belonging to the same <code>group</code> then subtract the mapped score values from the <code>score</code> column to calculate the difference.</p>
<pre><code>s = df.set_index(['group', before]).index.map(df.set_index(['group', 'datetime'])['score'])
df['difference'] = df['score'].sub(list(s), fill_value=0)
</code></pre>
<hr />
<pre><code>&gt;&gt;&gt; df

  group  score            datetime  difference
0   Sun      2 2017-08-30 07:00:00         2.0
1  Moon     13 2017-08-30 08:00:00        13.0
2   Sun     24 2017-08-31 07:00:00        22.0
3  Moon     15 2017-08-31 08:00:00         2.0
4  Mars     11 2017-08-29 21:00:00       -33.0
5  Mars     44 2017-08-28 21:00:00        44.0
</code></pre>
",Create column based date conditions I get error AttributeError SeriesGroupBy object attribute sub Hey python newbie Suppose I first two columns data dataframe df pd DataFrame group quot Sun quot quot Moon quot quot Sun quot quot Moon quot quot Mars quot quot Mars quot score datetime quot quot quot quot quot quot quot quot quot quot quot quot difference I want create new column named difference I put illustration equal score value row score value day hour group e g difference row equal score row score day th group e Moon e If day time exist value score row taken e g row time hence taken I write following df datetime pd datetime df datetime df datetime pd DateOffset days df difference df groupby quot group quot quot datetime quot score sub map df set index datetime score fill value I get error AttributeError SeriesGroupBy object attribute sub What,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
67126442,2021-04-16,2021,2,pandas.str.findall() returning multiple instance of the same value but with reduce characters,"<p>so I am having trouble with Pandas for a series findall(). currently I am trying to look at a report and retrieving all the electric components. Currently the report is either a line or a paragraph and mention components in a standardize way. I am using this code</p>
<pre><code>failedCoFromReason =rlist['report'].str.findall(r'([CULJRQF]([\dV]{2,4}))',flags=re.IGNORECASE)
</code></pre>
<p>It returns the components but it also returns a repeat value of the number like this [('r919', '919'), ('r920', '920')]</p>
<p>I would like it just to return [('r919'), ('r920')] but I am struggling with getting it to work. Pretty new to pandas and regex and confused how to search. I have tried greedy and non greedy searches but it didn't work.</p>
","['python', 'regex', 'pandas']",67126678,"<p>See the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.str.findall.html"" rel=""nofollow noreferrer"">Series.str.findall</a> reference:</p>
<blockquote>
<p>Equivalent to applying <a href=""https://docs.python.org/3/library/re.html#re.findall"" rel=""nofollow noreferrer"">re.findall()</a> to all the elements in the Series/Index.</p>
</blockquote>
<p>The <a href=""https://docs.python.org/3/library/re.html#re.findall"" rel=""nofollow noreferrer""><code>re.findall</code></a> references says that &quot;if one or more groups are present in the pattern, return a list of groups; this will be a list of tuples if the pattern has more than one group.&quot;</p>
<p>So, all you need to do is actually remove all capturing parentheses in this case, as all you need is to get the whole match:</p>
<pre class=""lang-py prettyprint-override""><code>rlist['report'].str.findall(r'[CULJRQF][\dV]{2,4}', flags=re.I)
</code></pre>
<p>In other cases, when you need to preserve the group (to quantify it, or to use alternatives), you need to change the capturing groups to <a href=""https://stackoverflow.com/questions/3512471/what-is-a-non-capturing-group-in-regular-expressions"">non-capturing</a> ones:</p>
<pre class=""lang-py prettyprint-override""><code>rlist['report'].str.findall(r'(?:[CULJRQF](?:[\dV]{2,4}))', flags=re.I)
</code></pre>
<p>Though, in this case, it is quite redundant.</p>
",pandas str findall returning multiple instance value reduce characters I trouble Pandas series findall currently I trying look report retrieving electric components Currently report either line paragraph mention components standardize way I using code failedCoFromReason rlist report str findall r CULJRQF dV flags IGNORECASE It returns components also returns repeat value number like r r I would like return r r I struggling getting work Pretty new pandas regex confused search I tried greedy non greedy searches work,"startoftags, python, regex, pandas, endoftags",python pandas matplotlib endoftags,python regex pandas,python pandas matplotlib,0.67
67244425,2021-04-24,2021,5,how to continuesly move character when mousebutton is clicked,"<p>I want to create a simple program that when you click on the screen, the box moves forever. It seems very simple, and it probably is, but I can't seem to make it work.</p>
<p>code:</p>
<pre><code>import pygame
import sys
from pygame.locals import *

count = 0

def blast1():
    global bl, blast
    blast.y = bl
    # ran = random.randint(0, 450)

pygame.init()
running = True

clock = pygame.time.Clock()
bl = 10

blast = pygame.Rect(300, 200 ,20,20)
screen = pygame.display.set_mode((500, 500))


while running:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            running = False

    blast1()

    screen.fill((255, 255, 255)) #color

    pygame.draw.ellipse(screen, [0, 255, 0], blast)
    
    keys=pygame.key.get_pressed()

    if event.type == pygame.MOUSEBUTTONDOWN and count != 1:
        blast.x += bl
        count = 1

        

    
    pygame.display.flip()
    
    clock.tick(30)
pygame.quit()
</code></pre>
<p>There are no errors but I want it to move continuously and smoothly without stopping. How can I do this? I have tried to put a <code> for I in range(100)</code> but that only doubles the movement and it doesn't move smoothly. I have also tried to lower the speed and put a <code>pygame. time.sleep(100)</code> but that completely freezes everything</p>
","['python', 'python-3.x', 'pygame']",67244477,"<p>You must handle the event in the event loop. Set a Boolean variable (<code>move</code>) when you click the mouse and move the object depending on the state of the variable:</p>
<pre class=""lang-py prettyprint-override""><code>moving = False
count = 0

blast1()

while running:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            running = False
        if event.type == pygame.MOUSEBUTTONDOWN1:
            moving = True

    if moving and count &lt; 100:
        blast.x += 1
        count += 1  
    
    screen.fill((255, 255, 255)) #color
    pygame.draw.ellipse(screen, [0, 255, 0], blast)
    pygame.display.flip()
    
    clock.tick(30)
</code></pre>
",continuesly move character mousebutton clicked I want create simple program click screen box moves forever It seems simple probably I seem make work code import pygame import sys pygame locals import count def blast global bl blast blast bl ran random randint pygame init running True clock pygame time Clock bl blast pygame Rect screen pygame display set mode running event pygame event get event type pygame QUIT running False blast screen fill color pygame draw ellipse screen blast keys pygame key get pressed event type pygame MOUSEBUTTONDOWN count blast x bl count pygame display flip clock tick pygame quit There errors I want move continuously smoothly without stopping How I I tried put I range doubles movement move smoothly I also tried lower speed put pygame time sleep completely freezes everything,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
67266863,2021-04-26,2021,2,Django: How do I get referenced objects in a symmetric ManyToMany relationship?,"<p>I've created a Many-to-Many relationship for the model <code>UserProfile</code>, to enable users to grant access to a particular feature to one another. The relationship works as expected with the use of <code>symmetrical=False</code> to ensure a user access is one-way.</p>
<p><strong>Model</strong></p>
<pre class=""lang-py prettyprint-override""><code>from django.contrib.auth.models import User

class UserProfile(models.Model):
    user = models.OneToOneField(User, on_delete=models.CASCADE)
    phone = models.IntegerField(blank=True, null=True)
    image = models.ImageField(upload_to='profile_image', default=&quot;default_thumbnail.jpg&quot;)
    department = models.ForeignKey(DepartmentModel, on_delete=models.SET_NULL, null=True)
    allow_booking_access = models.ManyToManyField(&quot;self&quot;, blank=True, symmetrical=False)

    def __str__(self):
        return self.user.username


class UserInline(admin.StackedInline):
    model = UserProfile
    can_delete = False
    verbose_name_plural = 'UserAccounts'


class UserAccount(BaseUserAdmin):
    inlines = (UserInline,)
</code></pre>
<p>I am able to query the users that a particular user wants to grant access to via: <i>(for example id=1)</i></p>
<pre class=""lang-py prettyprint-override""><code>UserProfile.objects.get(id=1).allow_booking_access.all()
</code></pre>
<p>However, I would like to retrieve the users that have granted access to the particular user.</p>
<p>How would I do this?</p>
<p><strong>Additional Information</strong></p>
<p><a href=""https://i.stack.imgur.com/AKBJD.png"" rel=""nofollow noreferrer"">Using Relation</a>
<a href=""https://i.stack.imgur.com/PiYBh.png"" rel=""nofollow noreferrer"">Database Information</a></p>
","['python', 'django', 'django-models']",67266931,"<p>You can filter with:</p>
<pre><code>UserProfile.objects.filter(<b>allow_booking_access=<i>my_user</i></b>)</code></pre>
<p>With your sample data, it will return the <code>UserProfile</code> with <code>id=7</code> for this query.</p>
<p>or if you want to query in reverse:</p>
<pre><code>UserProfile.objects.filter(<b>userprofile=<i>my_user</i></b>)</code></pre>
<p>With your sample data, it will return the <code>UserProfile</code>s with <code>id=7</code>, <code>id=3</code>, <code>user=4</code> and <code>user=7</code> for this query.</p>
",Django How I get referenced objects symmetric ManyToMany relationship I created Many Many relationship model UserProfile enable users grant access particular feature one another The relationship works expected use symmetrical False ensure user access one way Model django contrib auth models import User class UserProfile models Model user models OneToOneField User delete models CASCADE phone models IntegerField blank True null True image models ImageField upload profile image default quot default thumbnail jpg quot department models ForeignKey DepartmentModel delete models SET NULL null True allow booking access models ManyToManyField quot self quot blank True symmetrical False def str self return self user username class UserInline admin StackedInline model UserProfile delete False verbose name plural UserAccounts class UserAccount BaseUserAdmin inlines UserInline I able query users particular user wants grant access via example id UserProfile objects get id allow booking access However I would like retrieve users granted access particular user How would,"startoftags, python, django, djangomodels, endoftags",python django djangorestframework endoftags,python django djangomodels,python django djangorestframework,0.67
67407851,2021-05-05,2021,2,Count the number of values in each dictionary key including one value,"<p>I have a dict with 200e3 integer keys that have either one or two values each (strings).</p>
<p>I need to write logic that checks if each key has more than one value, if so add to a list.</p>
<pre><code>my_dict = defaultdict(list)
my_dict = {1: ['789456', '456123'], 2: '123456', 3: '987654'}
final = []
</code></pre>
<p>This is what my initial solution. This works for two values, but returns the length of characters when only one value exists.</p>
<pre><code>for key, value in my_dict.items():
    if len(value) &gt; 1:
        final.append(key)
    else:
        continue
</code></pre>
<p>I tried enumerate but it only returns the range.</p>
<pre><code>for x in enumerate(my_dict.items()):
    print(x)
</code></pre>
<p>I came up with this solution but I'm not sure if there would be any issues with <code>defaultdict(list)</code> being used. Or if there's a larger issue I might not be seeing.</p>
<pre><code>for key, value in my_dict.items():
    if isinstance(value, list):
       final.append(key)
    else:
       continue
</code></pre>
","['python', 'python-3.x', 'dictionary']",67407998,"<p>Please try this:</p>
<pre class=""lang-py prettyprint-override""><code>final = [k for k,v in my_dict.items() if type(v) == list and len(v) &gt; 1]
</code></pre>
<p>or in your code:</p>
<pre class=""lang-py prettyprint-override""><code>for key, value in my_dict.items():
    # only this additional condition is present before `and`
    if type(value) == list and len(value) &gt; 1:
        final.append(key)
</code></pre>
<p>Here, both <code>'34534'</code> and <code>['534', '456465']</code> are iterables and <code>len()</code> function works for both of them hence, we need to check the type of the value and only check length in case of the <code>list</code>.</p>
",Count number values dictionary key including one value I dict e integer keys either one two values strings I need write logic checks key one value add list dict defaultdict list dict final This initial solution This works two values returns length characters one value exists key value dict items len value gt final append key else continue I tried enumerate returns range x enumerate dict items print x I came solution I sure would issues defaultdict list used Or larger issue I might seeing key value dict items isinstance value list final append key else continue,"startoftags, python, python3x, dictionary, endoftags",python python3x list endoftags,python python3x dictionary,python python3x list,0.67
67487367,2021-05-11,2021,2,Pandas dataframe: Get value pairs from subsets of dataframe,"<p>I have a df:</p>
<pre><code>df = pd.DataFrame({'id': [1, 1, 2, 2, 2, 3, 4, 4, 4], \
                    &quot;name&quot;: [&quot;call&quot;, &quot;response&quot;, &quot;call&quot;, &quot;call&quot;, &quot;response&quot;, &quot;call&quot;, &quot;call&quot;, &quot;response&quot;, &quot;response&quot;]})
</code></pre>
<pre><code>    id  name
0   1   call
1   1   response
2   2   call
3   2   call
4   2   response
5   3   call
6   4   call
7   4   response
8   4   response
</code></pre>
<p>And I'm trying to extract a call - response pair, where the first response after call is the right pattern. Call and responses pairs are in their own subsets with <code>id</code> like so:</p>
<pre><code>    id  name
0   1   call
1   1   response
3   2   call
4   2   response
6   4   call
7   4   response
</code></pre>
<p>Ideally I'd keep the <code>indexes</code> in the dataframe so I can use <code>df.loc</code> with indexes later.</p>
<p>What I have tried is to go through the <code>df</code> in subsets and <code>apply</code> something or use <code>rolling window</code>. But have only succeeded to get errors.</p>
<pre><code>unique_ids = df.id.unique()

for unique_id in unique_ids :
    df.query('id== @unique_id').apply(something))
</code></pre>
<p>I have yet to discover something that could work specifically with <code>subsets</code> of dataframe</p>
","['python', 'pandas', 'dataframe']",67487411,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.shift.html"" rel=""nofollow noreferrer""><code>DataFrameGroupBy.shift</code></a> with compare values by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.eq.html"" rel=""nofollow noreferrer""><code>Series.eq</code></a> for check equality and filter in <a href=""http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer""><code>boolean indexing</code></a>:</p>
<pre><code>m1 = df['name'].eq('call') &amp; df.groupby('id')['name'].shift(-1).eq('response')
m2 = df['name'].eq('response') &amp; df.groupby('id')['name'].shift().eq('call')
df2 = df[m1 | m2]

print (df2)
   id      name
0   1      call
1   1  response
3   2      call
4   2  response
6   4      call
7   4  response
</code></pre>
",Pandas dataframe Get value pairs subsets dataframe I df df pd DataFrame id quot name quot quot call quot quot response quot quot call quot quot call quot quot response quot quot call quot quot call quot quot response quot quot response quot id name call response call call response call call response response And I trying extract call response pair first response call right pattern Call responses pairs subsets id like id name call response call response call response Ideally I keep indexes dataframe I use df loc indexes later What I tried go df subsets apply something use rolling window But succeeded get errors unique ids df id unique unique id unique ids df query id unique id apply something I yet discover something could work specifically subsets dataframe,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
67693733,2021-05-25,2021,2,Pandas trying to transform a dataframe by getting rows between certain strings,"<p>I have a dataset that looks something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">COL1</th>
<th style=""text-align: left;"">COL2</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">name</td>
<td style=""text-align: left;"">name_2</td>
</tr>
<tr>
<td style=""text-align: left;"">a</td>
<td style=""text-align: left;"">1</td>
</tr>
<tr>
<td style=""text-align: left;"">b</td>
<td style=""text-align: left;"">2</td>
</tr>
<tr>
<td style=""text-align: left;"">stop</td>
<td style=""text-align: left;"">stop</td>
</tr>
<tr>
<td style=""text-align: left;"">name2</td>
<td style=""text-align: left;"">name2_2</td>
</tr>
<tr>
<td style=""text-align: left;"">c</td>
<td style=""text-align: left;"">3</td>
</tr>
<tr>
<td style=""text-align: left;"">d</td>
<td style=""text-align: left;"">4</td>
</tr>
<tr>
<td style=""text-align: left;"">e</td>
<td style=""text-align: left;"">5</td>
</tr>
<tr>
<td style=""text-align: left;"">stop</td>
<td style=""text-align: left;"">stop</td>
</tr>
</tbody>
</table>
</div>
<p>I want to transform it to something like:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">COL1</th>
<th style=""text-align: left;"">COL2</th>
<th style=""text-align: left;"">COL3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">name</td>
<td style=""text-align: left;"">a</td>
<td style=""text-align: left;"">1</td>
</tr>
<tr>
<td style=""text-align: left;"">name</td>
<td style=""text-align: left;"">b</td>
<td style=""text-align: left;"">1</td>
</tr>
<tr>
<td style=""text-align: left;"">name2</td>
<td style=""text-align: left;"">c</td>
<td style=""text-align: left;"">3</td>
</tr>
<tr>
<td style=""text-align: left;"">name2</td>
<td style=""text-align: left;"">d</td>
<td style=""text-align: left;"">4</td>
</tr>
<tr>
<td style=""text-align: left;"">name2</td>
<td style=""text-align: left;"">e</td>
<td style=""text-align: left;"">5</td>
</tr>
</tbody>
</table>
</div>
<p>I've been trying to find a way to subset the dataset to get everything between &quot;name&quot; and &quot;Stop&quot;</p>
","['python', 'pandas', 'dataframe']",67693822,"<p>One way to transform:</p>
<pre><code>d = {'COL1': {0: 'name', 1: 'a', 2: 'b', 3: 'stop', 4: 'name2', 5: 'c', 6: 'd', 7: 'e', 8: 'stop'}, 'COL2': {0: 'name_2', 1: '1', 2: '2', 3: 'stop', 4: 'name2_2', 5: '3', 6: '4', 7: '5', 8: 'stop'}}
df = pd.DataFrame(d)
m = df.COL1.str.contains('name')
df.loc[m, 't'] = df.loc[m]['COL1']
df = df.fillna(method= 'ffill')
df = df.groupby('t').apply(lambda x : x.iloc[1:-1]).reset_index(drop=True)
</code></pre>
<p>Rename/sort columns :</p>
<pre><code>df.columns = ['COL2','COL3','COL1']
df = df[sorted(df.columns)]
</code></pre>
<p>OUTPUT:</p>
<pre><code>    COL1 COL2 COL3
0   name    a    1
1   name    b    2
2  name2    c    3
3  name2    d    4
4  name2    e    5
</code></pre>
",Pandas trying transform dataframe getting rows certain strings I dataset looks something like COL COL name name b stop stop name name c e stop stop I want transform something like COL COL COL name name b name c name name e I trying find way subset dataset get everything quot name quot quot Stop quot,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
67771907,2021-05-31,2021,5,Is there a function to write certain values of a dataframe to a .txt file in Python?,"<p>I have a dataframe as follows:</p>
<pre><code>Index A B C D E F
1     0 0 C 0 E 0 
2     A 0 0 0 0 F
3     0 0 0 0 E 0
4     0 0 C D 0 0 
5     A B 0 0 0 0
</code></pre>
<p>Basically I would like to write the dataframe to a txt file, such that every row consists of the index and the subsequent column name only, excluding the zeroes.</p>
<p>For example:</p>
<pre><code>txt file

1 C E 
2 A F 
3 E 
4 C D 
5 A B
</code></pre>
<p>The dataset is quite big, about 1k rows, 16k columns. Is there any way I can do this using a function in Pandas?</p>
","['python', 'pandas', 'dataframe']",67772172,"<p>Take a matrix vector multiplication between the boolean matrix generated by &quot;is this entry <code>&quot;0&quot;</code> or not&quot; and the columns of the dataframe, and write it to a text file with <code>to_csv</code> (thanks to @Andreas' answer!):</p>
<pre><code>df.ne(&quot;0&quot;).dot(df.columns + &quot; &quot;).str.rstrip().to_csv(&quot;text_file.txt&quot;)
</code></pre>
<p>where we right strip the spaces at the end due to the added <code>&quot; &quot;</code> to the last entries.</p>
<p>If you don't want the name <code>Index</code> appearing in the text file, you can chain a <code>rename_axis(index=None)</code> to get rid of it i.e.,</p>
<pre><code>df.ne(&quot;0&quot;).dot(df.columns + &quot; &quot;).str.rstrip().rename_axis(index=None)
</code></pre>
<p>and then <code>to_csv</code> as above.</p>
",Is function write certain values dataframe txt file Python I dataframe follows Index A B C D E F C E A F E C D A B Basically I would like write dataframe txt file every row consists index subsequent column name excluding zeroes For example txt file C E A F E C D A B The dataset quite big k rows k columns Is way I using function Pandas,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
67892088,2021-06-08,2021,2,python panda groupby and eliminate duplicates,"<p>I want to go to as few stores as possible to get my products. How do I do this?
I have a list of stores that carry a specific product.</p>
<pre><code>wanted_Products = pd.DataFrame({'p':[1,2,3,4,5,6,7]})
stores = pd.DataFrame({'Store': np.repeat(np.arange(1,5),4),
                       'Product': [1,2,3,5,0,2,3,4,0,6,7,8,0,1,2,6]})
# return 1 if the Product is wanted
stores['Wanted'] = stores.Product.isin(wanted_Products.p).values.astype(int)

     Store  Product  Wanted
0       1        1       1
1       1        2       1
2       1        3       1
3       1        5       1
4       2        0       0
5       2        2       1
6       2        3       1
7       2        4       1
8       3        0       0
9       3        6       1
10      3        7       1
11      3        8       0
12      4        0       0
13      4        1       1
14      4        2       1
15      4        6       1

# Group products per store and calculate how many wanted products are in a store
w = stores.groupby('Store', as_index=False).agg(list)
w['Number_wanted'] = stores.groupby('Store', as_index=False)['Wanted'].sum().agg(list)['Wanted']

      Store  Product        Wanted         Number_wanted  ?Products_wanted?
0      1  [1, 2, 3, 5]  [1, 1, 1, 1]              4            [1,2,3,5]
1      2  [0, 2, 3, 4]  [0, 1, 1, 1]              3            [2,3,4]
2      3  [0, 6, 7, 8]  [0, 1, 1, 0]              2            [6,7]
3      4  [0, 1, 2, 6]  [0, 1, 1, 1]              3            [1,2,6]
</code></pre>
<p>How do I get the Products I want in a new column (Products_wanted) without the non wanted products? when I use isin() I only get true/false (1/0 if I use astype(int)) not  the actual numbers.</p>
","['python', 'pandas', 'pandas-groupby']",67893553,"<p>A way to do this could be to keep track of all products that are availabe in a store, get them and then mark those products as <strong>&quot;taken&quot;</strong> so that you do not pick the same in next stores.</p>
<p>So initially you have <strong><code>wanted_Products</code></strong><code>= [1,2,3,4,5,6,7]</code><strong>.</strong> Since you are getting <code>[1, 2, 3, 5]</code> from <strong>store 1</strong>, you select and return these as products to get from <strong>store 1</strong>, and then mark all these as <strong>&quot;taken&quot;</strong> simply by replacing these values in <code>wanted_Products</code> to something else like say <strong><code>-1</code></strong> <em>(or some other value you like, to denote they have been taken)</em>.</p>
<p>Now  <strong><code>wanted_Products</code></strong><code>= [-1,-1,-1,4,-1,6,7]</code>. <code>-1</code> ones are taken so you only have <code>[4,6,7]</code> products to get from next stores. Repeating the same logic for all stores will give you the products to get from there, without any duplicates:</p>
<pre><code>def get_products(possible, wanted):
    i = np.where(np.in1d(wanted, possible))
    available = wanted[i]
    wanted[i] = -1
    return available

w = stores.groupby('Store', as_index=False).agg(list)
w['Products to get'] = w.Product.apply(get_products, args=(np.array(wanted_Products),))
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>&gt;&gt;&gt; w
   Store       Product Products to get
0      1  [1, 2, 3, 5]    [1, 2, 3, 5]
1      2  [0, 2, 3, 4]             [4]
2      3  [0, 6, 7, 8]          [6, 7]
3      4  [0, 1, 2, 6]              []
</code></pre>
",python panda groupby eliminate duplicates I want go stores possible get products How I I list stores carry specific product wanted Products pd DataFrame p stores pd DataFrame Store np repeat np arange Product return Product wanted stores Wanted stores Product isin wanted Products p values astype int Store Product Wanted Group products per store calculate many wanted products store w stores groupby Store index False agg list w Number wanted stores groupby Store index False Wanted sum agg list Wanted Store Product Wanted Number wanted Products wanted How I get Products I want new column Products wanted without non wanted products I use isin I get true false I use astype int actual numbers,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
67916181,2021-06-10,2021,3,Change value in pandas after chained loc and iloc,"<p>I have the following problem: in a df, I want to select specific rows and a specific column and in this selection take the first <code>n</code> elements and assign a new value to them. Naively, I thought that the following code should do the job:</p>
<pre class=""lang-py prettyprint-override""><code>import seaborn as sns
import pandas as pd

df = sns.load_dataset('tips')
df.loc[df.day==&quot;Sun&quot;, &quot;smoker&quot;].iloc[:4] = &quot;Yes&quot;
</code></pre>
<p>Both of the <code>loc</code> and <code>iloc</code> should return a view into the df and the value should be overwritten. However, the dataframe does not change. Why?</p>
<p>I know how to go around it -- creating a new df first just with the <code>loc</code>, then changing the value using <code>iloc</code> and updating back the original df (as below).</p>
<p>But a) I do not think it's optimal, and b) I would like to know why the top solution does not work. Why does it return a copy and not a view of a view?</p>
<p>The alternative solution:</p>
<pre class=""lang-py prettyprint-override""><code>df = sns.load_dataset('tips')
tmp = df.loc[df.day==&quot;Sun&quot;, &quot;smoker&quot;]
tmp.iloc[:4] = &quot;Yes&quot;
df.loc[df.day==&quot;Sun&quot;, &quot;smoker&quot;] = tmp
</code></pre>
<hr />
<p>Note: I have read the <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html"" rel=""nofollow noreferrer"">docs</a>, this really great <a href=""https://realpython.com/pandas-settingwithcopywarning/"" rel=""nofollow noreferrer"">post</a> and this <a href=""https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas"">question</a> but they don't explain this. Their concern is the difference between <code>df.loc[mask,&quot;z]</code> and the chained <code>df[&quot;z&quot;][mask]</code>.</p>
","['python', 'pandas', 'dataframe']",67916765,"<p>I believe <code>df.loc[].iloc[]</code> is a chained assignment case and pandas doesn't guarantee that you will get a view at the end. From the <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy"" rel=""nofollow noreferrer"">docs</a>:</p>
<blockquote>
<p>Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided.</p>
</blockquote>
<p>Since you have a filtering condition in <code>loc</code>, pandas will create a new <code>pd.Series</code> and than will apply an assignment to it. For example the following will work because you'll get the same series as <code>df[&quot;smoker&quot;]</code>:</p>
<pre><code>df.loc[:, &quot;smoker&quot;].iloc[:4] = 'Yes'
</code></pre>
<p>But you will get ``SettingWithCopyWarning` warning.</p>
<p>You need to rewrite your code so that pandas handles this as a single <code>loc</code> entity.</p>
<p>Another possible workaround:</p>
<pre><code>df.loc[df[df.day==&quot;Sun&quot;].index[:4], &quot;smoker&quot;] = 'Yes'
</code></pre>
",Change value pandas chained loc iloc I following problem df I want select specific rows specific column selection take first n elements assign new value Naively I thought following code job import seaborn sns import pandas pd df sns load dataset tips df loc df day quot Sun quot quot smoker quot iloc quot Yes quot Both loc iloc return view df value overwritten However dataframe change Why I know go around creating new df first loc changing value using iloc updating back original df But I think optimal b I would like know top solution work Why return copy view view The alternative solution df sns load dataset tips tmp df loc df day quot Sun quot quot smoker quot tmp iloc quot Yes quot df loc df day quot Sun quot quot smoker quot tmp Note I read docs really great post question explain Their concern difference df,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
68003784,2021-06-16,2021,2,pySpark array&lt;string&gt; != string;,"<p>I'm trying to extract from dataframe rows that contains words from list: below I'm pasting my code:</p>
<pre><code>from pyspark.ml.feature import Tokenizer, RegexTokenizer
from pyspark.sql.functions import col, udf
from pyspark.sql.types import IntegerType
from pyspark.sql.types import IntegerType
import findspark
findspark.init()
conf = SparkConf().setAppName(&quot;nlp&quot;).setMaster(&quot;spark://192.168.1.51:7077&quot;)\
                  .set(&quot;spark.jars&quot;,&quot;/home/artur/elasticsearch-spark-20_2.11-7.8.1.jar&quot;)\
                  .set(&quot;spark.sql.repl.eagerEval.enabled&quot;,True)\
                  .set(&quot;spark.cores.max&quot;,&quot;8&quot;)\
                  .set(&quot;spark.executor.memory&quot;, &quot;2g&quot;)\
                  .set(&quot;spark.driver.memory&quot;, &quot;2g&quot;)\
                  .set(&quot;spark.sql.repl.eagerEval.enabled&quot;,True)
sc = SparkContext(conf=conf).getOrCreate()
sqlContext = SQLContext(sc)
spark = SparkSession(sc)
sentenceDataFrame = spark.createDataFrame([
    (0, [&quot;Hi&quot;, &quot;I&quot; ,&quot;heard&quot; ,&quot;about&quot;, &quot;Spark&quot;]),
    (1, [&quot;I&quot;, &quot;wish&quot; ,&quot;Java&quot;, &quot;could&quot;, &quot;use&quot;, &quot;case&quot;, &quot;classes&quot;]),
    (2, [&quot;Logistic&quot;,&quot;regression&quot;,&quot;models&quot;,&quot;are&quot;,&quot;neat&quot;])
], [&quot;id&quot;, &quot;sentence&quot;])
my_list = ['Java','Spark']
selected_df = sentenceDataFrame.filter(sentenceDataFrame.sentence.isin(my_list))
</code></pre>
<p>and im getting following error:</p>
<pre><code>Py4JJavaError: An error occurred while calling o262.filter.
: org.apache.spark.sql.AnalysisException: cannot resolve '(`sentence` IN ('Java', 'Spark'))' due to data type mismatch: Arguments must be same type but were: array&lt;string&gt; != string;;
'Filter sentence#27 IN (Java,Spark)
+- LogicalRDD [id#26L, sentence#27], false
</code></pre>
<p>Please give me advice how to solve this issue</p>
","['python', 'apache-spark', 'pyspark']",68004692,"<p>solutions depend on your spark version :</p>
<p><strong>Spark 2.4+</strong></p>
<pre class=""lang-py prettyprint-override""><code>from pyspark.sql import functions as F

sentenceDataFrame.filter(
    F.size(
        F.array_intersect(
            F.col(&quot;sentence&quot;), F.array(*(F.lit(item) for item in my_list))
        )
    )
    &gt; 0
).show()

+---+--------------------+
| id|            sentence|
+---+--------------------+
|  0|[Hi, I, heard, ab...|
|  1|[I, wish, Java, c...|
+---+--------------------+
</code></pre>
<p><strong>version 2.3 or below</strong></p>
<pre class=""lang-py prettyprint-override""><code># using broadcast join, fast but my_list needs to be small
keyword_df = spark.createDataFrame([(keyword,) for keyword in my_list], [&quot;keyword&quot;])

sentenceDataFrame.join(
    F.broadcast(keyword_df),
    how=&quot;leftsemi&quot;,
    on=F.expr(&quot;array_contains(sentence, keyword)&quot;),
).show()
</code></pre>
<pre class=""lang-py prettyprint-override""><code># Using UDF, slowest solution
from pyspark.sql import functions as F, types as T

def intersect(keyword_list):
    @F.udf(T.BooleanType())
    def udf(sentence):
        intersection = set(sentence).intersection(set(keyword_list))
        return True if intersection else False

    return udf


sentenceDataFrame.filter(intersect(my_list)(sentenceDataFrame.sentence)).show()
</code></pre>
",pySpark array lt string gt string I trying extract dataframe rows contains words list I pasting code pyspark ml feature import Tokenizer RegexTokenizer pyspark sql functions import col udf pyspark sql types import IntegerType pyspark sql types import IntegerType import findspark findspark init conf SparkConf setAppName quot nlp quot setMaster quot spark quot set quot spark jars quot quot home artur elasticsearch spark jar quot set quot spark sql repl eagerEval enabled quot True set quot spark cores max quot quot quot set quot spark executor memory quot quot g quot set quot spark driver memory quot quot g quot set quot spark sql repl eagerEval enabled quot True sc SparkContext conf conf getOrCreate sqlContext SQLContext sc spark SparkSession sc sentenceDataFrame spark createDataFrame quot Hi quot quot I quot quot heard quot quot quot quot Spark quot quot I quot quot wish quot quot Java quot quot could quot quot,"startoftags, python, apachespark, pyspark, endoftags",python discord discordpy endoftags,python apachespark pyspark,python discord discordpy,0.33
68099361,2021-06-23,2021,3,Pandas Data Frame - Sum all the values in a previous column which match a specific condition and add it to a new column,"<p>I'm probably missing something, but I was not able to find a solution for this.
Is there a way in python to add values to a new column which satisfy a certain condition.
In Excel I would apply the following formula in the new column and paste it below</p>
<pre><code>=SUMIF(A1:C1, &quot;&gt;0&quot;)
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">val1</th>
<th style=""text-align: center;"">val2</th>
<th style=""text-align: right;"">val3</th>
<th style=""text-align: right;"">output</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">0.5</td>
<td style=""text-align: center;"">0.7</td>
<td style=""text-align: right;"">-0.9</td>
<td style=""text-align: right;"">1.2</td>
</tr>
<tr>
<td style=""text-align: left;"">0.3</td>
<td style=""text-align: center;"">-0.7</td>
<td style=""text-align: right;""></td>
<td style=""text-align: right;"">0.3</td>
</tr>
<tr>
<td style=""text-align: left;"">-0.5</td>
<td style=""text-align: center;"">-0.7</td>
<td style=""text-align: right;"">-0.9</td>
<td style=""text-align: right;"">0</td>
</tr>
</tbody>
</table>
</div>
<p>Also in my extracts, there are a few blank values. Can you please help me understand what code should be written for this?</p>
<pre><code>df['total'] = df[['A','B']].sum(axis=1).where(df['A'] &gt; 0, 0)
</code></pre>
<p>I came across the above code, but it checks only one condition. What I need is a sum of all of those columns which match the given condition.</p>
<p>Thanks!</p>
","['python', 'pandas', 'dataframe']",68099506,"<p>Another way, somewhat similar to <code>SUMIF</code>:</p>
<pre><code># this is the &quot;IF&quot;
is_positive = df.loc[:, &quot;val1&quot;: &quot;val3&quot;] &gt; 0

# this is selecting the parts where condition holds &amp; sums
df[&quot;output&quot;] = df.loc[:, &quot;val1&quot;: &quot;val3&quot;][is_positive].sum(axis=1)
</code></pre>
<p>where <code>axis=1</code> in last line is to sum along rows,</p>
<p>to get</p>
<pre><code>&gt;&gt;&gt; df

   val1  val2  val3  output
0   0.5   0.7  -0.9     1.2
1   0.3  -0.7   NaN     0.3
2  -0.5  -0.7  -0.9     0.0
</code></pre>
",Pandas Data Frame Sum values previous column match specific condition add new column I probably missing something I able find solution Is way python add values new column satisfy certain condition In Excel I would apply following formula new column paste SUMIF A C quot gt quot val val val output Also extracts blank values Can please help understand code written df total df A B sum axis df A gt I came across code checks one condition What I need sum columns match given condition Thanks,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
48111638,2018-01-05,2018,2,python deprecated pd.convert_objects(convert_numeric=True) works the alternative one malfunctions,"<p>The following warning is thrown but i am getting the actual intended result ,when i try to change the code it fails.</p>

<pre><code>new_table = new_table.convert_objects(convert_numeric=True)
new_table=new_table.replace(np.nan,0)  # This is used to make - to 0 for calc
</code></pre>

<p>Warning (from warnings module):
new_table = new_table.convert_objects(convert_numeric=True)
FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.</p>

<p>the new_table is nothing but a pandas dataframe it contains </p>

<pre><code>A    B   C  D  E
1    -   3  5  6
2    3   5  6  7
-    -   5  5  5
5    4   -  -  -
-    -   4  -  4
9    -   -  10 23
</code></pre>

<p>In this given data frame format since we have the string ""-"" further sum or diff or multiplication logics throws error if i use the below method .</p>

<pre><code>new_table = pd.to_numeric(new_table)
#new_table=new_table.replace(""-"",0)
new_table=new_table.replace(np.nan,0)
</code></pre>

<p>Traceback (most recent call last):
  File  line 107, in 
    new_table = pd.to_numeric(new_table)
  File  line 113, in to_numeric
    raise TypeError('arg must be a list, tuple, 1-d array, or Series')
TypeError: arg must be a list, tuple, 1-d array, or Series</p>

<p>What is the best way of handling this situation the first row shall be index in str format and others rows are numeric so that my arithmatic calculation will not be affected.</p>

<p>Any help ?</p>
","['python', 'pandas', 'dataframe']",48113762,"<p>You can if need replace all non numeric values to <code>NaN</code>s use <code>apply</code> for working with columns in <code>df</code> with function <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"" rel=""nofollow noreferrer""><code>to_numeric</code></a>, then to <code>0</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html"" rel=""nofollow noreferrer""><code>fillna</code></a> and last all values to <code>int</code>s by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html"" rel=""nofollow noreferrer""><code>astype</code></a>:</p>

<pre><code>new_table1 = new_table.apply(pd.to_numeric, errors='coerce').fillna(0).astype(int)
print (new_table1)
   A  B  C   D   E
0  1  0  3   5   6
1  2  3  5   6   7
2  0  0  5   5   5
3  5  4  0   0   0
4  0  0  4   0   4
5  9  0  0  10  23

print (new_table1.dtypes)
A    int32
B    int32
C    int32
D    int32
E    int32
dtype: object
</code></pre>

<p>Anoter solution if all values are integers is <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html"" rel=""nofollow noreferrer""><code>replace</code></a> all non numbers + <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html"" rel=""nofollow noreferrer""><code>astype</code></a>:</p>

<pre><code>new_table2 = new_table.replace('\D+', 0, regex=True).astype(int)
print (new_table2)
   A  B  C   D   E
0  1  0  3   5   6
1  2  3  5   6   7
2  0  0  5   5   5
3  5  4  0   0   0
4  0  0  4   0   4
5  9  0  0  10  23

print (new_table2.dtypes)
A    int32
B    int32
C    int32
D    int32
E    int32
dtype: object
</code></pre>

<p>And if all values are only <code>-</code> then solution is simplifying:</p>

<pre><code>new_table3 = new_table.replace('-', 0, regex=True).astype(int)
print (new_table3)
   A  B  C   D   E
0  1  0  3   5   6
1  2  3  5   6   7
2  0  0  5   5   5
3  5  4  0   0   0
4  0  0  4   0   4
5  9  0  0  10  23

print (new_table3.dtypes)
A    int32
B    int32
C    int32
D    int32
E    int32
dtype: object
</code></pre>
",python deprecated pd convert objects convert numeric True works alternative one malfunctions The following warning thrown getting actual intended result try change code fails new table new table convert objects convert numeric True new table new table replace np nan This used make calc Warning warnings module new table new table convert objects convert numeric True FutureWarning convert objects deprecated Use data type specific converters pd datetime pd timedelta pd numeric new table nothing pandas dataframe contains A B C D E In given data frame format since string sum diff multiplication logics throws error use method new table pd numeric new table new table new table replace new table new table replace np nan Traceback recent call last File line new table pd numeric new table File line numeric raise TypeError arg must list tuple array Series TypeError arg must list tuple array Series What best way handling situation,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
48158309,2018-01-08,2018,3,Change field type in model serializer without changing model,"<p>I have a model with a string field with a corresponding model serializer. I'd like to change the serializer so that it now takes a list of strings for that field, but converts that list to a string internally. </p>

<p>Basically, <code>internal_repr = ','.join(input)</code></p>

<p>I've tried changing the data type in the validate function, but I still get a validation error that it's not a string. Where should this change occur? Is it also necessary to override the serializer on that field to a <code>ListSerializer</code> with <code>child=CharField</code> specified?</p>

<p>Basic representation of the current situation:</p>

<p>Model:</p>

<pre><code>class MyModel(models.Model):
    myfield = models.CharField(max_length=100)

    def save(self, *args, **kwargs):
        self.full_clean()
        return super(MyModel, self).save(*args, **kwargs)
</code></pre>

<p>Serializer:</p>

<pre><code>MyModelSerializer(serializers.ModelSerializer):
     class Meta:
         model = MyModel
         fields = ('myfield')
</code></pre>

<p>I want to have the serializer take a list for myfield instead, and convert that to a string via ','.join()</p>
","['python', 'django', 'django-rest-framework']",48169706,"<p>I was able to add
<code>myfield = serializers.ListField(child=serializers.CharField(), min_length=1)</code>
to the serializer to change the serializer type, as well as overriding the <code>to_internal_value()</code> and <code>to_representation()</code> methods to achieve this.</p>

<p>The code is now:</p>

<p>Model:</p>

<pre><code>class MyModel(models.Model):
    myfield = models.CharField(max_length=100)

    def save(self, *args, **kwargs):
        self.full_clean()
        return super(MyModel, self).save(*args, **kwargs)
</code></pre>

<p>Serializer:</p>

<pre><code>MyModelSerializer(serializers.ModelSerializer):
   myfield = serializers.ListField(child=serializers.CharField(), min_length=1)

   def to_internal_value(self, data):
        myfield_val = data.get('myfield')
        output = super(MyModelSerializer, self).to_internal_value(data)
        output['myfield'] = ','.join(myfield_val)

   def to_representation(self, instance):
        myfield_val = instance.myfield.split(',')
        output = super(MyModelSerializer, self).to_representation(instance)
        output['myfield'] = myfield_val

   class Meta:
         model = MyModel
         fields = ('myfield')
</code></pre>
",Change field type model serializer without changing model I model string field corresponding model serializer I like change serializer takes list strings field converts list string internally Basically internal repr join input I tried changing data type validate function I still get validation error string Where change occur Is also necessary override serializer field ListSerializer child CharField specified Basic representation current situation Model class MyModel models Model myfield models CharField max length def save self args kwargs self full clean return super MyModel self save args kwargs Serializer MyModelSerializer serializers ModelSerializer class Meta model MyModel fields myfield I want serializer take list myfield instead convert string via join,"startoftags, python, django, djangorestframework, endoftags",python django djangorestframework endoftags,python django djangorestframework,python django djangorestframework,1.0
48198021,2018-01-11,2018,17,Filter pandas dataframe with specific column names in python,"<p>I have a pandas dataframe and a list as follows</p>

<pre><code>mylist = ['nnn', 'mmm', 'yyy']
mydata =
   xxx   yyy zzz nnn ddd mmm
0  0  10      5    5   5  5
1  1   9      2    3   4  4
2  2   8      8    7   9  0
</code></pre>

<p>Now, I want to get only the columns mentioned in <code>mylist</code> and save it as a csv file.</p>

<p>i.e.</p>

<pre><code>     yyy  nnn   mmm
0    10     5     5
1    9      3     4
2    8      7     0
</code></pre>

<p>My current code is as follows.</p>

<pre><code>mydata = pd.read_csv( input_file, header=0)

for item in mylist:
    mydata_new = mydata[item]

print(mydata_new)
mydata_new.to_csv(file_name)
</code></pre>

<p>It seems to me that my new dataframe produces wrong results.Where I am making it wrong? Please help me!</p>
","['python', 'pandas', 'dataframe']",48198034,"<p>Just pass a list of column names to index <code>df</code>:</p>

<pre><code>df[['nnn', 'mmm', 'yyy']]

   nnn  mmm  yyy
0    5    5   10
1    3    4    9
2    7    0    8
</code></pre>

<hr>

<p>If you need to handle non-existent column names in your list, try filtering with <code>df.columns.isin</code> - </p>

<pre><code>df.loc[:, df.columns.isin(['nnn', 'mmm', 'yyy', 'zzzzzz'])]

   yyy  nnn  mmm
0   10    5    5
1    9    3    4
2    8    7    0
</code></pre>
",Filter pandas dataframe specific column names python I pandas dataframe list follows mylist nnn mmm yyy mydata xxx yyy zzz nnn ddd mmm Now I want get columns mentioned mylist save csv file e yyy nnn mmm My current code follows mydata pd read csv input file header item mylist mydata new mydata item print mydata new mydata new csv file name It seems new dataframe produces wrong results Where I making wrong Please help,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
48221070,2018-01-12,2018,2,Filter a Pandas dataframe using a list,"<p>I am trying to filter using a list of user_id's and a mask. Here is the input with two user_id's:</p>

<pre><code>data = np.array([['user_id','comment','label'],
            [100,'First comment',0],
            [101,'Buy viagra',1],
            [100,'Buy viagra two',1],
            [101,'Third comment',0],
            [100,'Third comment two',0],
            [101,'Buy drugs',1],
            [100,'Buy drugs two',1],
            [101,'Buy icecream',1],
            [100,'Buy icecream two',1],
            [101,'Buy something',1],
            [100,'Buy something two',1]])
</code></pre>

<p>The desired output is:</p>

<pre><code>0      100      First comment     0
1      101         Buy viagra     1
2      100     Buy viagra two     1
3      101      Third comment     0
4      100  Third comment two     0
5      101          Buy drugs     1
6      100      Buy drugs two     1
7      101       Buy icecream     1
8      100   Buy icecream two     1
</code></pre>

<p>By passing a list of user_id's, I am getting an incorrect output.</p>

<pre><code>m = df.user_id.isin([100,101]) &amp; df.label.eq('1')
i = df[m].head(3)
j = df[~m]
df = pd.concat([i, j]).sort_index()
print (df)
</code></pre>

<p>However, if I pass just one user_id as below, I get correct output. Can you please suggest me what's wrong? Thanks.</p>

<pre><code>m = df.user_id.eq('101') &amp; df.label.eq('1')
</code></pre>
","['python', 'pandas', 'pandas-groupby']",48221147,"<p>There is problem your values are strings in <code>user_id</code> column, so need <code>['100','101']</code> instead <code>[100, 101]</code>:</p>

<pre><code>df = pd.DataFrame(data[1:], columns=data[0])

m = df.user_id.isin(['100','101']) &amp; df.label.eq('1')
i = df[m]
print (i)
   user_id            comment label
1      101         Buy viagra     1
2      100     Buy viagra two     1
5      101          Buy drugs     1
6      100      Buy drugs two     1
7      101       Buy icecream     1
8      100   Buy icecream two     1
9      101      Buy something     1
10     100  Buy something two     1
</code></pre>

<p>You can check <code>type</code>s in one column by:</p>

<pre><code>print (df.user_id.apply(type))

0     &lt;class 'str'&gt;
1     &lt;class 'str'&gt;
2     &lt;class 'str'&gt;
3     &lt;class 'str'&gt;
4     &lt;class 'str'&gt;
5     &lt;class 'str'&gt;
6     &lt;class 'str'&gt;
7     &lt;class 'str'&gt;
8     &lt;class 'str'&gt;
9     &lt;class 'str'&gt;
10    &lt;class 'str'&gt;
Name: user_id, dtype: object
</code></pre>

<p>And if need check all columns:</p>

<pre><code>print (df.applymap(type))

          user_id        comment          label
0   &lt;class 'str'&gt;  &lt;class 'str'&gt;  &lt;class 'str'&gt;
1   &lt;class 'str'&gt;  &lt;class 'str'&gt;  &lt;class 'str'&gt;
2   &lt;class 'str'&gt;  &lt;class 'str'&gt;  &lt;class 'str'&gt;
3   &lt;class 'str'&gt;  &lt;class 'str'&gt;  &lt;class 'str'&gt;
4   &lt;class 'str'&gt;  &lt;class 'str'&gt;  &lt;class 'str'&gt;
5   &lt;class 'str'&gt;  &lt;class 'str'&gt;  &lt;class 'str'&gt;
6   &lt;class 'str'&gt;  &lt;class 'str'&gt;  &lt;class 'str'&gt;
7   &lt;class 'str'&gt;  &lt;class 'str'&gt;  &lt;class 'str'&gt;
8   &lt;class 'str'&gt;  &lt;class 'str'&gt;  &lt;class 'str'&gt;
9   &lt;class 'str'&gt;  &lt;class 'str'&gt;  &lt;class 'str'&gt;
10  &lt;class 'str'&gt;  &lt;class 'str'&gt;  &lt;class 'str'&gt;
</code></pre>
",Filter Pandas dataframe using list I trying filter using list user id mask Here input two user id data np array user id comment label First comment Buy viagra Buy viagra two Third comment Third comment two Buy drugs Buy drugs two Buy icecream Buy icecream two Buy something Buy something two The desired output First comment Buy viagra Buy viagra two Third comment Third comment two Buy drugs Buy drugs two Buy icecream Buy icecream two By passing list user id I getting incorrect output df user id isin amp df label eq df head j df df pd concat j sort index print df However I pass one user id I get correct output Can please suggest wrong Thanks df user id eq amp df label eq,"startoftags, python, pandas, pandasgroupby, endoftags",python python3x pandas endoftags,python pandas pandasgroupby,python python3x pandas,0.67
48431868,2018-01-24,2018,5,Collapse pandas multiindex to a single index,"<p>I have a multi-indexed Pandas dataframe that looks like this:</p>

<p><a href=""https://i.stack.imgur.com/4SYE0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4SYE0.png"" alt=""enter image description here""></a></p>

<p>How can I merge the three-tiered index into one index? Namely, I want to turn (1987, 1, 2) into pd.datetime(1987, 1, 2). I'd prefer a vectorized approach using df.index.map. Here's code that can create the top part of the dataframe:</p>

<pre><code>df = pd.DataFrame(
    {'3 months': [1, 2, 3, 4, 5]},
    index=[
        [1987, 1987, 1987, 1987,1987],
        [1,1,1,1,1],
        [2,5,6,7,8]
    ]
)
</code></pre>
","['python', 'pandas', 'datetime']",48431906,"<p>Simplest solution with <code>pd.Index.map</code>  </p>

<pre><code>df.set_index(df.index.map(lambda t: pd.datetime(*t)))

            3 months
1987-01-02         1
1987-01-05         2
1987-01-06         3
1987-01-07         4
1987-01-08         5
</code></pre>
",Collapse pandas multiindex single index I multi indexed Pandas dataframe looks like How I merge three tiered index one index Namely I want turn pd datetime I prefer vectorized approach using df index map Here code create top part dataframe df pd DataFrame months index,"startoftags, python, pandas, datetime, endoftags",python pandas dataframe endoftags,python pandas datetime,python pandas dataframe,0.67
48447641,2018-01-25,2018,2,Cumulative Sum Up to Threshold,"<p>I have a two column pandas DataFrame, that looks something like the following:</p>

<pre><code>import pandas as pd
df = pd.DataFrame([[5,100],[6,200],[7,250],[8,1000]],columns=['Price','Units'])
</code></pre>

<p>Given a threshold, of say 1500, I'd like to take the cumulative sum of the product of the two columns (until the total reaches the threshold of 1500) and divide by the cumulative sum of the 'Units' column such that the cumulative sum only sums up to 1500.  I can implement this using for loops, but how would a panda do it?  </p>

<p>To spell out the details, a bit:</p>

<pre><code>df['Product'] = df.prod(axis=1)
df['CumSum'] = df['Product'].cumsum()
</code></pre>

<p>At index=1, the cumulative sum exceeds the threshold (1700 > 1500).  We would then like to take only the number of units that gets to the threshold. For example, the result would be df:</p>

<pre><code>Price   Units  Product  CumSum  CumSumWithThreshold
  5       100     500     500          500
  6       200    1200    1700         1500
  7       250    1750    3450            0
  8      1000    8000   11450            0
</code></pre>

<p>Given you have $1500 to spend, what's the average price per unit?   In the above, you can afford 100 units at price $5 (total of $500) and (1000/6) units of price $6 (total of $1000 at this price).  The average price per unit is therefore:  $1500/(100 + (1000/6)) = $5.625...</p>
","['python', 'python-3.x', 'pandas']",48461762,"<p>I've been able to come close, but perhaps not exactly what a panda would do.  Perhaps this answer will percolate some more creativity:</p>

<pre><code>import pandas as pd
import numpy as np

Threshold = 1500
df = pd.DataFrame([[5,100],[6,200],[7,250],[8,1000]],columns=['Price','Units'])

df['Diff'] = df.prod(axis=1).cumsum() - Threshold
df['ThisUnits'] = df.apply(lambda x: x['Units'] if x['Diff']&lt;0 else np.max([0,x['Units'] - x['Diff']/x['Price']]),axis=1)
print('Result: $%.2f' % (Threshold/df['ThisUnits'].sum()))
</code></pre>

<p>Any other ideas?  </p>
",Cumulative Sum Up Threshold I two column pandas DataFrame looks something like following import pandas pd df pd DataFrame columns Price Units Given threshold say I like take cumulative sum product two columns total reaches threshold divide cumulative sum Units column cumulative sum sums I implement using loops would panda To spell details bit df Product df prod axis df CumSum df Product cumsum At index cumulative sum exceeds threshold We would like take number units gets threshold For example result would df Price Units Product CumSum CumSumWithThreshold Given spend average price per unit In afford units price total units price total price The average price per unit therefore,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
48531843,2018-01-30,2018,5,How to remove rows of a DataFrame based off of data from another DataFrame?,"<p>I'm new to pandas and I'm trying to figure this scenario out: 
I have a sample DataFrame with two products. df = </p>

<pre><code>  Product_Num     Date   Description  Price 
          10    1-1-18   Fruit Snacks  2.99
          10    1-2-18   Fruit Snacks  2.99
          10    1-5-18   Fruit Snacks  1.99
          10    1-8-18   Fruit Snacks  1.99
          10    1-10-18  Fruit Snacks  2.99
          45    1-1-18         Apples  2.99 
          45    1-3-18         Apples  2.99
          45    1-5-18         Apples  2.99
          45    1-9-18         Apples  1.49
          45    1-10-18        Apples  1.49
          45    1-13-18        Apples  1.49
          45    1-15-18        Apples  2.99 
</code></pre>

<p>I also have another small DataFrame that looks like this (which shows promotional prices of the same products): df2=</p>

<pre><code>  Product_Num   Price 
          10    1.99
          45    1.49 
</code></pre>

<p>Notice that df2 does not contain columns 'Date' nor 'Description.' What I want to do is to remove all promo prices from df1 (for all dates that are on promo), using the data from df1. What is the best way to do this? </p>

<p>So, I want to see this: </p>

<pre><code>  Product_Num     Date   Description  Price 
          10    1-1-18   Fruit Snacks  2.99
          10    1-2-18   Fruit Snacks  2.99
          10    1-10-18  Fruit Snacks  2.99
          45    1-1-18         Apples  2.99 
          45    1-3-18         Apples  2.99
          45    1-5-18         Apples  2.99
          45    1-15-18        Apples  2.99 
</code></pre>

<p>I was thinking of doing a merge on columns Price and Product_Num, then seeing what I can do from there. But I was getting confused because of the multiple dates. </p>
","['python', 'pandas', 'dataframe']",48532006,"<p><code>isin</code> with <code>&amp;</code></p>

<pre><code>df.loc[~((df.Product_Num.isin(df2['Product_Num']))&amp;(df.Price.isin(df2['Price']))),:]
Out[246]: 
    Product_Num     Date  Description  Price
0            10   1-1-18  FruitSnacks   2.99
1            10   1-2-18  FruitSnacks   2.99
4            10  1-10-18  FruitSnacks   2.99
5            45   1-1-18       Apples   2.99
6            45   1-3-18       Apples   2.99
7            45   1-5-18       Apples   2.99
11           45  1-15-18       Apples   2.99
</code></pre>

<p>Update </p>

<pre><code>df.loc[~df.index.isin(df.merge(df2.assign(a='key'),how='left').dropna().index)]
Out[260]: 
    Product_Num     Date  Description  Price
0            10   1-1-18  FruitSnacks   2.99
1            10   1-2-18  FruitSnacks   2.99
4            10  1-10-18  FruitSnacks   2.99
5            45   1-1-18       Apples   2.99
6            45   1-3-18       Apples   2.99
7            45   1-5-18       Apples   2.99
11           45  1-15-18       Apples   2.99
</code></pre>
",How remove rows DataFrame based data another DataFrame I new pandas I trying figure scenario I sample DataFrame two products df Product Num Date Description Price Fruit Snacks Fruit Snacks Fruit Snacks Fruit Snacks Fruit Snacks Apples Apples Apples Apples Apples Apples Apples I also another small DataFrame looks like shows promotional prices products df Product Num Price Notice df contain columns Date Description What I want remove promo prices df dates promo using data df What best way So I want see Product Num Date Description Price Fruit Snacks Fruit Snacks Fruit Snacks Apples Apples Apples Apples I thinking merge columns Price Product Num seeing I But I getting confused multiple dates,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
48543979,2018-01-31,2018,2,How to get missed dates in the given range of datetime64 values?,"<p>I have the following DataFrame <code>df</code> in Pandas:</p>

<pre><code>dti                  id_n
2016-07-27 13:55:00  1
2016-07-29 13:50:07  1
2016-07-29 14:50:08  1
2016-07-30 23:50:01  2
2016-08-01 12:50:00  3
2016-08-02 12:50:00  3
</code></pre>

<p>The type of <code>dti</code> is <code>datetime64</code>.
I want to get the new DataFrame <code>result</code> with missed dates between the <code>min</code> and <code>max</code> values of <code>dti</code>:</p>

<p>result =</p>

<pre><code>2016-07-28
2016-07-31
</code></pre>

<p>How can I get it?</p>
","['python', 'pandas', 'datetime']",48544112,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.floor.html"" rel=""nofollow noreferrer""><code>floor</code></a> for remove times, then create <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.date_range.html"" rel=""nofollow noreferrer""><code>date_range</code></a> and get <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.difference.html"" rel=""nofollow noreferrer""><code>difference</code></a>:</p>

<pre><code>d = df['dti'].dt.floor('d')
print (d)
0   2016-07-27
1   2016-07-29
2   2016-07-29
3   2016-07-30
4   2016-08-01
5   2016-08-02
Name: dti, dtype: datetime64[ns]

a = pd.date_range(d.min(), d.max(), freq='d')
print (a)
DatetimeIndex(['2016-07-27', '2016-07-28', '2016-07-29', '2016-07-30',
               '2016-07-31', '2016-08-01', '2016-08-02'],
              dtype='datetime64[ns]', freq='D')

b = a.difference(d)
print (b)
DatetimeIndex(['2016-07-28', '2016-07-31'], dtype='datetime64[ns]', freq=None)

df1 = pd.DataFrame({'missing':a.difference(d)})
print (df1)
     missing
0 2016-07-28
1 2016-07-31
</code></pre>

<p>Another solution is downsample by <code>mean</code> and get indices of <code>NaN</code>s values:</p>

<pre><code>a = df.resample('d', on='dti').mean()
print (a)
            id_n
dti             
2016-07-27   1.0
2016-07-28   NaN
2016-07-29   1.0
2016-07-30   2.0
2016-07-31   NaN
2016-08-01   3.0
2016-08-02   3.0

b = a.index[a['id_n'].isnull()]
print (b)
DatetimeIndex(['2016-07-28', '2016-07-31'], dtype='datetime64[ns]', name='dti', freq=None)
</code></pre>
",How get missed dates given range datetime values I following DataFrame df Pandas dti id n The type dti datetime I want get new DataFrame result missed dates min max values dti result How I get,"startoftags, python, pandas, datetime, endoftags",python pandas numpy endoftags,python pandas datetime,python pandas numpy,0.67
48558107,2018-02-01,2018,6,Converting float to string in pandas dataframe,"<p>I have a dataframe in pandas containing datetime and float data.</p>

<pre><code>time                         price1              price2
2018-02-01T00:00:00.000Z     1.4526547885        1.654775563
</code></pre>

<p>I need to convert the columns to string format such that the price1 and price2 columns shows number upto 4 decimal places and the time is displayed as: 01,02,2018  00:00:00</p>

<p>Any leads on this is appreciated. Thanks </p>
","['python', 'python-3.x', 'pandas']",48558213,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.strftime.html"" rel=""noreferrer""><code>dt.strftime</code></a> for formating <code>datetime</code>s and then custom format of <code>float</code>s:</p>

<pre><code>df['time'] = df['time'].dt.strftime('%Y,%m,%d %H:%M:%S')

cols = ['price1','price2']
df[cols] = df[cols].applymap(lambda x: '{0:.4f}'.format(x))
print (df)
                  time  price1  price2
0  2018,02,01 00:00:00  1.4527  1.6548
</code></pre>
",Converting float string pandas dataframe I dataframe pandas containing datetime float data time price price T Z I need convert columns string format price price columns shows number upto decimal places time displayed Any leads appreciated Thanks,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
48610136,2018-02-04,2018,5,How to conditionally select elements in numpy array,"<p>Can someone help me with conditionally selecting elements in a numpy array? I am trying to return elements that are greater than a threshold. My current solution is:</p>

<pre><code>sampleArr = np.array([ 0.725, 0.39, 0.99 ])
condition = (sampleArr &gt; 0.5)`
extracted = np.extract(condition, sampleArr) #returns [0.725 0.99]
</code></pre>

<p>However, this seems roundabout and I suspect there's a way to do it in one line?</p>
","['python', 'arrays', 'numpy']",48610171,"<p>You can index directly like:</p>

<pre><code>sampleArr[sampleArr &gt; 0.5]
</code></pre>

<h3>Test Code:</h3>

<pre><code>sampleArr = np.array([0.725, 0.39, 0.99])

condition = (sampleArr &gt; 0.5)
extracted = np.extract(condition, sampleArr)  # returns [0.725 0.99]

print(sampleArr[sampleArr &gt; 0.5])
print(sampleArr[condition])
print(extracted)
</code></pre>

<h3>Results:</h3>

<pre><code>[ 0.725  0.99 ]
[ 0.725  0.99 ]
[ 0.725  0.99 ]
</code></pre>
",How conditionally select elements numpy array Can someone help conditionally selecting elements numpy array I trying return elements greater threshold My current solution sampleArr np array condition sampleArr gt extracted np extract condition sampleArr returns However seems roundabout I suspect way one line,"startoftags, python, arrays, numpy, endoftags",python python3x numpy endoftags,python arrays numpy,python python3x numpy,0.67
48735367,2018-02-11,2018,2,Numpy array with coordinates,"<p>I have a DataFrame with a column with different coordinates, clustered together in other lists, like this:</p>

<pre><code>    name    OBJECTID    geometry
0    NaN           1    ['-80.304852,-3.489302,0.0','-80.303087,-3.490214,0.0',...]

1    NaN           2    ['-80.27494,-3.496571,0.0',...]

2    NaN           3    ['-80.267987,-3.500003,0.0',...]
</code></pre>

<p>I want to separate the values and remove the '0.0', but keep them inside the lists to add them to a certain key in a dictionary, that looks like this:</p>

<pre><code>    name    OBJECTID    geometry
0    NaN           1    [[-80.304852, -3.489302],[-80.303087, -3.490214],...]

1    NaN           2    [[-80.27494, -3.496571],...]

2    NaN           3    [[-80.267987, -3.500003],...]
</code></pre>

<p>This is my code that didn't work where I tried to separate them in a for loop:</p>

<pre><code>import panda as pd
import numpy as np

r = pd.read_csv('data.csv') 
rloc = np.asarray(r['geometry'])

r['latitude'] = np.zeros(r.shape[0],dtype= r['geometry'].dtype)
r['longitude'] = np.zeros(r.shape[0],dtype= r['geometry'].dtype)

# Separating the latitude and longitude values form each string.
for i in range(0, len(rloc)):
    for j in range(0, len(rloc[i])):
        coord = rloc[i][j].split(',')
        r['longitude'] = coord[0]
        r['latitude'] = coord[1]

r = r[['OBJECTID', 'latitude', 'longitude', 'name']]
</code></pre>

<p>Edit: The result wasn't good because it printed out only one value for each one.</p>

<pre><code>  OBJECTID  latitude    longitude   name
0        1  -3.465566   -80.151633  NaN
1        2  -3.465566   -80.151633  NaN
2        3  -3.465566   -80.151633  NaN
</code></pre>

<p>Bonus question: How cand I add all of these longitude and latitude values inside a tuple to use with geopy? Like this:</p>

<pre><code>r['location'] = (r['latitude], r['longitude'])
</code></pre>

<p>So, instead, the geometry column would look like this:</p>

<pre><code>geometry
[(-80.304852, -3.489302),(-80.303087, -3.490214),...]

[(-80.27494, -3.496571),...]

[(-80.267987, -3.500003),...]
</code></pre>

<p>Edit:</p>

<p>The data looked like this at first(for each row):</p>

<pre><code>&lt;LineString&gt;&lt;coordinates&gt;-80.304852,-3.489302,0.0 -80.303087,-3.490214,0.0 ...&lt;/coordinates&gt;&lt;/LineString&gt;
</code></pre>

<p>I modified it with regex, using this code:</p>

<pre><code>geo = np.asarray(r['geometry']); 
geo = [re.sub(re.compile('&lt;.*?&gt;'), '', string) for string in geo]
</code></pre>

<p>And then I placed it in an array:</p>

<pre><code>rv = [geo[i].split() for i in range(0,len(geo))]
r['geometry'] = np.asarray(rv)
</code></pre>

<p>When I call r['geometry'], the output is:</p>

<pre><code>0    [-80.304852,-3.489302,0.0, -80.303087,-3.49021...
1    [-80.27494,-3.496571,0.0, -80.271963,-3.49266,...
2    [-80.267987,-3.500003,0.0, -80.267845,-3.49789...
Name: geometry, dtype: object
</code></pre>

<p>And <code>r['geometry'][0]</code> is:</p>

<pre><code> ['-80.304852,-3.489302,0.0',
 '-80.303087,-3.490214,0.0',
 '-80.302131,-3.491878,0.0',
 '-80.300763,-3.49213,0.0']
</code></pre>
","['python', 'pandas', 'numpy']",48737196,"<p>A pandas solution with input from a toy data set:</p>

<pre><code>df = pd.read_csv(""test.txt"")
   name  OBJECTID                                           geometry
0   NaN         1  ['-80.3,-3.4,0.0','-80.3,-3.9,0.0','-80.3,-3.9...
1   NaN         2  ['80.2,-4.4,0.0','-81.3,2.9,0.0','-80.7,-3.2,0...
2   NaN         3  ['-80.1,-3.2,0.0','-80.8,-2.9,0.0','-80.1,-1.9...
</code></pre>

<p>Now the transformation into columns of longitude-latitude pairs:</p>

<pre><code>#regex extraction of longitude latitude pairs
pairs = ""(-?\d+.\d+,-?\d+.\d+)""
s = df[""geometry""].str.extractall(pairs)
#splitting string into two parts, creating two columns for longitude latitude
s = s[0].str.split("","", expand = True)  
#converting strings into float numbers - is this even necessary?
s[[0, 1]] = s[[0, 1]].apply(pd.to_numeric)
#creating a tuple from longitude/latitude columns
s[""lat_long""] = list(zip(s[0], s[1]))
#placing the tuples as columns in original dataframe 
df = pd.concat([df, s[""lat_long""].unstack(level = -1)], axis = 1)
</code></pre>

<p>Output from the toy data set:</p>

<pre><code>   name  OBJECTID                                           geometry  \
0   NaN         1  ['-80.3,-3.4,0.0','-80.3,-3.9,0.0','-80.3,-3.9...   
1   NaN         2  ['80.2,-4.4,0.0','-81.3,2.9,0.0','-80.7,-3.2,0...   
2   NaN         3  ['-80.1,-3.2,0.0','-80.8,-2.9,0.0','-80.1,-1.9...   

               0              1              2  
0  (-80.3, -3.4)  (-80.3, -3.9)  (-80.3, -3.9)  
1   (80.2, -4.4)   (-81.3, 2.9)  (-80.7, -3.2)  
2  (-80.1, -3.2)  (-80.8, -2.9)  (-80.1, -1.9)  
</code></pre>

<p>Alternatively, you can combine the tuples in one column as a list:</p>

<pre><code>s[""lat_long""] = list(zip(s[0], s[1]))
#placing the tuples as a list into a column of the original dataframe 
df[""lat_long""] = s.groupby(level=[0])[""lat_long""].apply(list)
</code></pre>

<p>Output now:</p>

<pre><code>   name  OBJECTID                                           geometry  \
0   NaN         1  ['-80.3,-3.4,0.0','-80.3,-3.9,0.0','-80.3,-3.9...   
1   NaN         2  ['80.2,-4.4,0.0','-81.3,2.9,0.0','-80.7,-3.2,0...   
2   NaN         3  ['-80.1,-3.2,0.0','-80.8,-2.9,0.0','-80.1,-1.9...   

                                        lat_long  
0  [(-80.3, -3.4), (-80.3, -3.9), (-80.3, -3.9)]  
1    [(80.2, -4.4), (-81.3, 2.9), (-80.7, -3.2)]  
2  [(-80.1, -3.2), (-80.8, -2.9), (-80.1, -1.9)]  
</code></pre>
",Numpy array coordinates I DataFrame column different coordinates clustered together lists like name OBJECTID geometry NaN NaN NaN I want separate values remove keep inside lists add certain key dictionary looks like name OBJECTID geometry NaN NaN NaN This code work I tried separate loop import panda pd import numpy np r pd read csv data csv rloc np asarray r geometry r latitude np zeros r shape dtype r geometry dtype r longitude np zeros r shape dtype r geometry dtype Separating latitude longitude values form string range len rloc j range len rloc coord rloc j split r longitude coord r latitude coord r r OBJECTID latitude longitude name Edit The result good printed one value one OBJECTID latitude longitude name NaN NaN NaN Bonus question How cand I add longitude latitude values inside tuple use geopy Like r location r latitude r longitude So instead geometry column,"startoftags, python, pandas, numpy, endoftags",python pandas numpy endoftags,python pandas numpy,python pandas numpy,1.0
48938264,2018-02-22,2018,2,Subtracting columns from a numpy array,"<p>This question is a follow-up of a previous post of mine: </p>

<p><a href=""https://stackoverflow.com/questions/48937018/multiply-each-column-of-a-numpy-array-with-each-value-of-another-array"">Multiply each column of a numpy array with each value of another array</a>.</p>

<p>Suppose i have the following arrays:</p>

<pre><code>In [252]: k
Out[252]: 
array([[200, 800, 400, 1600],
       [400, 1000, 800, 2000],
       [600, 1200, 1200,2400]])

In [271]: f = np.array([[100,50],[200,100],[300,200]])

In [272]: f
Out[272]: 
array([[100,  50],
       [200, 100],
       [300, 200]])
</code></pre>

<p>How can i subtract f from k to obtain the following?</p>

<pre><code>In [252]: g
Out[252]: 
array([[100, 750, 300, 1550],
       [200, 900, 600, 1900],
       [300, 1000, 900,2200]])
</code></pre>

<p>Ideally, i would like to make the subtraction in as fewer steps as possible and in concert with the solution provided in my other post, but any solution welcome.</p>
","['python', 'python-3.x', 'numpy']",48938298,"<p>You can use <code>np.tile</code>, like this:</p>

<pre><code>In [1]: k - np.tile(f, (1, 2))
Out[1]: 
array([[ 100,  750,  300, 1550],
       [ 200,  900,  600, 1900],
       [ 300, 1000,  900, 2200]])
</code></pre>

<p>Also, if you happen to know for sure that each dimension of <code>f</code> evenly divides the corresponding dimension of <code>k</code> (which I assume you must, for your desired subtraction to make sense), then you could generalize it slightly:</p>

<pre><code>In [2]: k - np.tile(f, np.array(k.shape) // np.array(f.shape))
Out[2]: 
array([[ 100,  750,  300, 1550],
       [ 200,  900,  600, 1900],
       [ 300, 1000,  900, 2200]])
</code></pre>
",Subtracting columns numpy array This question follow previous post mine Multiply column numpy array value another array Suppose following arrays In k Out array In f np array In f Out array How subtract f k obtain following In g Out array Ideally would like make subtraction fewer steps possible concert solution provided post solution welcome,"startoftags, python, python3x, numpy, endoftags",python arrays numpy endoftags,python python3x numpy,python arrays numpy,0.67
48953875,2018-02-23,2018,2,Copying data from one pandas dataframe to other based on column value and separated by comma,"<p>I have two dataframes viz., df1 and df2.</p>

<p><strong>df1</strong> is like</p>

<pre><code>Index YH   HE  MT  CU  EI
 0    Dot  Sf  Sy  Lc  
 1    Rls  Bd  Sa  Ta  
 2    Fs       Ft  Rg     
</code></pre>

<p>And <strong>df2</strong> is like </p>

<pre><code>Index   Z1   Z2  Z3
 0      YH       HE
 1      HE       EI
 2      MT       CU  
</code></pre>

<p>I want a <strong>df3</strong> which looks like this</p>

<pre><code>Index   Z1           Z2     Z3
 0      YH                  HE
 1      HE                  EI
 2      MT                  CU  
 3      Dot,Rls,Fs          Sf,Bd
 4      Sf,Bd              
 5      Sy,Sa,Ft            Lc,Ta,Rg
</code></pre>

<p>Basically, in one cell only but separated by a comma. The comma separated cell is obtained from df1.</p>

<p>In df3, the rows 3,4,5 correspond to rows 0,1,2</p>

<pre><code>(0,Z1) of df2 is YH and column YH in df1 is Dot,Rls,Fs 
So Dot,Rls,Fs comes at (3,Z1) in df3

(1,Z1) of df2 is HE and HE column has Sf,BD in df1 
So Sf,Bd comes at (4,Z1) in df3 
</code></pre>

<p>Similarly, for all others.</p>

<p>Please let me know if it is still not clear.</p>
","['python', 'pandas', 'dataframe']",48954119,"<p>The answer already contain in the previous question </p>

<pre><code>s=df2.set_index('Index').astype(object).apply(lambda x : x.map(df1.set_index('Index').to_dict('l')))


pd.concat([df2.set_index('Index'),s.fillna('').applymap(','.join)])

Out[1798]: 
               Z1 Z2        Z3
Index                         
0              YH           HE
1              HE           EI
2              MT           CU
0      Dot,Rls,Fs       Sf,Bd,
1          Sf,Bd,           ,,
2        Sy,Sa,Ft     Lc,Ta,Rg
</code></pre>

<p>Update </p>

<pre><code>s=df2.set_index('Index').astype(object).apply(lambda x : x.map(df1.set_index('Index').replace('',np.nan).stack().groupby(level=1).apply(list).to_dict()))
pd.concat([df2.set_index('Index'),s.fillna('').applymap(','.join)])
Out[1815]: 
               Z1 Z2        Z3
Index                         
0              YH           HE
1              HE           EI
2              MT           CU
0      Dot,Rls,Fs        Sf,Bd
1           Sf,Bd             
2        Sy,Sa,Ft     Lc,Ta,Rg
</code></pre>
",Copying data one pandas dataframe based column value separated comma I two dataframes viz df df df like Index YH HE MT CU EI Dot Sf Sy Lc Rls Bd Sa Ta Fs Ft Rg And df like Index Z Z Z YH HE HE EI MT CU I want df looks like Index Z Z Z YH HE HE EI MT CU Dot Rls Fs Sf Bd Sf Bd Sy Sa Ft Lc Ta Rg Basically one cell separated comma The comma separated cell obtained df In df rows correspond rows Z df YH column YH df Dot Rls Fs So Dot Rls Fs comes Z df Z df HE HE column Sf BD df So Sf Bd comes Z df Similarly others Please let know still clear,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
49008010,2018-02-27,2018,4,How to get maximum value based on another series in pandas groupby,"<p>I have a large Data frame, in that  a column called <b>currency</b> and <b>amount_in_euros</b>, Currency column contains data like EUR, GBR etc., and amount_in_euros contains float value. I want to compute sum of each currency(EUR, GBR etc.,) and put maximum value of currency in new series. <strong>I have to compute this operation for each customer</strong>. How to achieve this in pandas.</p>

<p>Input:</p>

<pre><code>Customer  currency   amount_in_euros
1           EUR      10
1           GBR      6
1           GBR      18
1           EUR      2
1           EUR      3
2           IND      12 
.
.
.
</code></pre>

<p>Output:</p>

<pre><code>Customer  currency   amount_in_euros   max
1           EUR      10                GBR
1           GBR      6                 GBR
1           GBR      18                GBR
1           EUR      2                 GBR
1           EUR      3                 GBR 
2           IND      12                IND
. 
. 
.
</code></pre>

<p>so far I tried,</p>

<pre><code>df=pd.read_csv('analysis.csv')
res=pd.DataFrame()
for u,v in df.groupby(['Customer']):
   temp= v[['currency','amount_in_euros']].groupby(['currency'])['amount_in_euros'].sum().reset_index().sort_values('amount_in_euros',ascending=False)
   v['max']=temp['currency'].iloc[0]
   res=res.append(v)
</code></pre>

<p>My above code works fine for me, but due to append operation it takes a long time. please help me to solve this problem.
Thanks in advance.</p>
","['python', 'pandas', 'pandas-groupby']",49008178,"<p>Use:</p>

<ul>
<li>first aggregate <code>sum</code> by <code>Customer</code> and <code>currency</code></li>
<li>sort by columns by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html"" rel=""nofollow noreferrer""><code>sort_values</code></a></li>
<li><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html"" rel=""nofollow noreferrer""><code>drop_duplicates</code></a> for rows with <code>max</code></li>
<li>create <code>Series</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>set_index</code></a></li>
<li>last create new column by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>map</code></a></li>
</ul>

<hr>

<pre><code>df1 = df.groupby(['Customer', 'currency'], as_index=False)['amount_in_euros'].sum()
s = (df1.sort_values(['Customer','amount_in_euros'])
        .drop_duplicates('Customer', keep='last')
        .set_index('Customer')['currency'])

df['max'] = df['Customer'].map(s)
print (df)
   Customer currency  amount_in_euros  max
0         1      EUR               10  GBR
1         1      GBR                6  GBR
2         1      GBR               18  GBR
3         1      EUR                2  GBR
4         1      EUR                3  GBR
5         2      IND               12  IND
</code></pre>

<p>EDIT:</p>

<p>Similar solution for first, second, third values in new columns:</p>

<pre><code>print (df)
   Customer currency  amount_in_euros
0         1      EUR               10
1         1      GBR                6
2         1      GBR               18
3         1      EUR                2
4         1      USD                1
5         1      USD                2
6         1      EUR                3
7         2      IND               12
8         2      USD                2

df1 = df.groupby(['Customer', 'currency'], as_index=False)['amount_in_euros'].sum()
df2 = df1.sort_values(['Customer','amount_in_euros'])
df2 = (df2.set_index(['Customer', 
                      df2.groupby(['Customer']).cumcount(ascending=False)])['currency']
          .unstack()
          .add_prefix('max_'))

print (df2)
         max_0 max_1 max_2
Customer                  
1          GBR   EUR   USD
2          IND   USD  None

df = df.join(df2, on='Customer')
</code></pre>

<hr>

<pre><code>print (df)
   Customer currency  amount_in_euros max_0 max_1 max_2
0         1      EUR               10   GBR   EUR   USD
1         1      GBR                6   GBR   EUR   USD
2         1      GBR               18   GBR   EUR   USD
3         1      EUR                2   GBR   EUR   USD
4         1      USD                1   GBR   EUR   USD
5         1      USD                2   GBR   EUR   USD
6         1      EUR                3   GBR   EUR   USD
7         2      IND               12   IND   USD  None
8         2      USD                2   IND   USD  None
</code></pre>
",How get maximum value based another series pandas groupby I large Data frame column called currency amount euros Currency column contains data like EUR GBR etc amount euros contains float value I want compute sum currency EUR GBR etc put maximum value currency new series I compute operation customer How achieve pandas Input Customer currency amount euros EUR GBR GBR EUR EUR IND Output Customer currency amount euros max EUR GBR GBR GBR GBR GBR EUR GBR EUR GBR IND IND far I tried df pd read csv analysis csv res pd DataFrame u v df groupby Customer temp v currency amount euros groupby currency amount euros sum reset index sort values amount euros ascending False v max temp currency iloc res res append v My code works fine due append operation takes long time please help solve problem Thanks advance,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
49062669,2018-03-02,2018,3,How to group Pandas DataFrame dates into custom date range bins using groupby/cut,"<p>I am trying to group dates with a custom range using <code>groupby</code> and <code>cut</code> with no success so far. From the error message being returned, I wonder if cut is trying to process my dates as a number.</p>

<p>I want to group <code>df1['date']</code> by custom date ranges and then sum the <code>df1['HDD']</code> values. The custom ranges are found in <code>df2</code>:</p>

<pre><code>import pandas as pd
df1 = pd.DataFrame( {'date': ['2/1/2015', '3/2/2015', '3/3/2015', '3/4/2015','4/17/2015','5/12/2015'],
                             'HDD' : ['7.5','8','5','23','11','55']})
    HDD  date
0   7.5 2/1/2015
1   8   3/2/2015
2   5   3/3/2015
3   23  3/4/2015
4   11  4/17/2015
5   55  5/12/2015
</code></pre>

<p><code>df2</code> has the custom date ranges:</p>

<pre><code>df2 = pd.DataFrame( {'Period': ['One','Two','Three','Four'],
                     'Start Dates': ['1/1/2015','2/15/2015','3/14/2015','4/14/2015'],
                     'End Dates' : ['2/14/2015','3/13/2015','4/13/2015','5/10/2015']})

    Period  Start Dates End Dates
0   One     1/1/2015    2/14/2015
1   Two     2/15/2015   3/13/2015
2   Three   3/14/2015   4/13/2015
3   Four    4/14/2015   5/10/2015
</code></pre>

<p>My Desired output is to group <code>df1</code> by the custom date ranges and aggregate the HDD values for each Period. Should output something like this:</p>

<pre><code>   Period    HDD
0  One       7.5
1  Two       36
2  Three     0
3  Four      11
</code></pre>

<p>Here is one example of what I have tried to use custom grouping:</p>

<pre><code>df3 = df1.groupby(pd.cut(df1['date'], df2['Start Dates'])).agg({'HDD': sum})
</code></pre>

<p>...and here is the error I get:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-103-55ea779bcd73&gt; in &lt;module&gt;()
----&gt; 1 df3 = df1.groupby(pd.cut(df1['date'], df2['Start Dates'])).agg({'HDD': sum})

/opt/conda/lib/python3.5/site-packages/pandas/tools/tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest)
    112     else:
    113         bins = np.asarray(bins)
--&gt; 114         if (np.diff(bins) &lt; 0).any():
    115             raise ValueError('bins must increase monotonically.')
    116 

/opt/conda/lib/python3.5/site-packages/numpy/lib/function_base.py in diff(a, n, axis)
   1576         return diff(a[slice1]-a[slice2], n-1, axis=axis)
   1577     else:
-&gt; 1578         return a[slice1]-a[slice2]
   1579 
   1580 

TypeError: unsupported operand type(s) for -: 'str' and 'str'
</code></pre>

<ul>
<li>Is cut trying to process my date ranges as numbers? </li>
<li>Do I need to explicitly convert my dates as datetime objects (tried
this but maybe was going about it correctly)?</li>
</ul>

<p>Thanks for any suggestions offered!</p>
","['python', 'pandas', 'pandas-groupby']",49063052,"<p>This works if you convert all your dates form dtype string to datetime.</p>

<pre><code>df1['date'] = pd.to_datetime(df1['date'])

df2['End Dates'] = pd.to_datetime(df2['End Dates'])

df2['Start Dates'] = pd.to_datetime(df2['Start Dates'])

df1['HDD'] = df1['HDD'].astype(float)

df1.groupby(pd.cut(df1['date'], df2['Start Dates'])).agg({'HDD': sum})
</code></pre>

<p>Output:</p>

<pre><code>                           HDD
date                          
(2015-01-01, 2015-02-15]   7.5
(2015-02-15, 2015-03-14]  36.0
(2015-03-14, 2015-04-14]   NaN
</code></pre>

<p>Adding labels:</p>

<pre><code>df1.groupby(pd.cut(df1['date'], df2['Start Dates'], labels=df2.iloc[:-1,1])).agg({'HDD': sum})
</code></pre>

<p>Output:</p>

<pre><code>        HDD
date       
One     7.5
Two    36.0
Three   NaN
</code></pre>
",How group Pandas DataFrame dates custom date range bins using groupby cut I trying group dates custom range using groupby cut success far From error message returned I wonder cut trying process dates number I want group df date custom date ranges sum df HDD values The custom ranges found df import pandas pd df pd DataFrame date HDD HDD date df custom date ranges df pd DataFrame Period One Two Three Four Start Dates End Dates Period Start Dates End Dates One Two Three Four My Desired output group df custom date ranges aggregate HDD values Period Should output something like Period HDD One Two Three Four Here one example I tried use custom grouping df df groupby pd cut df date df Start Dates agg HDD sum error I get TypeError Traceback recent call last lt ipython input ea bcd gt lt module gt gt df df groupby,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas numpy endoftags,python pandas pandasgroupby,python pandas numpy,0.67
49097944,2018-03-04,2018,2,Subtract from one column of a numpy array,"<p>I have a Numpy array of the form:</p>

<pre><code>40002 1511863545
40000 1511863546
156 1511863547
40005 1511863547
40003 1511863548
40008 1511863549
340 1511863550
40011 1511863550
...
</code></pre>

<p>The first column is data, the second timestamps. The data is in two parts, separated by a gap of 40000.</p>

<p>What I need is two datasets:</p>

<pre><code>156 1511863547
340 1511863550
...
</code></pre>

<p>and:</p>

<pre><code>2 1511863545
0 1511863546
5 1511863547
3 1511863548
8 1511863549
11 1511863550
</code></pre>

<p>How can I,</p>

<ol>
<li>Split the data according to the value in the first column?</li>
<li>Subtract 40000 from the first column of the second new dataset (leaving the timestamps intact)?</li>
</ol>
","['python', 'arrays', 'numpy']",49098062,"<p>You can use a fairly simple test to do that like:</p>

<h3>Code:</h3>

<pre><code>split_data = data[:, 0] &lt; 40000
low_data = data[split_data]
high_data = data[~split_data]
high_data[:, 0] -= 40000
</code></pre>

<h3>Test Code:</h3>

<pre><code>data = np.array([
    [40002, 1511863545],
    [40000, 1511863546],
    [156, 1511863547],
    [40005, 1511863547],
    [40003, 1511863548],
    [40008, 1511863549],
    [340, 1511863550],
    [40011, 1511863550],
])
print(data)

split_data = data[:, 0] &lt; 40000
low_data = data[split_data]
high_data = data[~split_data]
high_data[:, 0] -= 40000

print(split_data)
print(low_data)
print(high_data)
</code></pre>

<h3>Results:</h3>

<pre><code>[[     40002 1511863545]
 [     40000 1511863546]
 [       156 1511863547]
 [     40005 1511863547]
 [     40003 1511863548]
 [     40008 1511863549]
 [       340 1511863550]
 [     40011 1511863550]]

[False False  True False False False  True False]

[[       156 1511863547]
 [       340 1511863550]]

[[         2 1511863545]
 [         0 1511863546]
 [         5 1511863547]
 [         3 1511863548]
 [         8 1511863549]
 [        11 1511863550]]
</code></pre>
",Subtract one column numpy array I Numpy array form The first column data second timestamps The data two parts separated gap What I need two datasets How I Split data according value first column Subtract first column second new dataset leaving timestamps intact,"startoftags, python, arrays, numpy, endoftags",python python3x numpy endoftags,python arrays numpy,python python3x numpy,0.67
49137031,2018-03-06,2018,5,pandas dataframe delete rows with low frequency,"<p>What is the best practice to remove all rows that has a column with low frequency value?</p>

<p>Dataframe:</p>

<pre><code>IN:
foo bar poo
1   a   A
2   a   A
3   a   B
4   b   B
5   b   A
6   b   A
7   c   C
8   d   B
9   e   B
</code></pre>

<p>Example 1:
Remove all rows that have less than 3 in frequency value in column 'poo':</p>

<pre><code>OUT:
foo bar poo
1   a   A
2   a   A
3   a   B
4   b   B
5   b   A
6   b   A
8   d   B
9   e   B
</code></pre>

<p>Example 2:
Remove all rows that have less than 3 in frequency value in column 'bar':</p>

<pre><code>OUT:
foo bar poo
1   a   A
2   a   A
3   a   B
4   b   B
5   b   A
6   b   A
</code></pre>
","['python', 'pandas', 'dataframe']",49137073,"<p>This should generalise pretty easily. You'll need <code>groupby</code> + <code>transform</code> + <code>count</code>, and then filter the result:</p>

<pre><code>col = 'poo'  # 'bar'
n = 3        # 2

df[df.groupby(col)[col].transform('count').ge(n)]

   foo bar poo
0    1   a   A
1    2   a   A
2    3   a   B
3    4   b   B
4    5   b   A
5    6   b   A
7    8   d   B
8    9   e   B
</code></pre>
",pandas dataframe delete rows low frequency What best practice remove rows column low frequency value Dataframe IN foo bar poo A A B b B b A b A c C B e B Example Remove rows less frequency value column poo OUT foo bar poo A A B b B b A b A B e B Example Remove rows less frequency value column bar OUT foo bar poo A A B b B b A b A,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
49240133,2018-03-12,2018,2,Calculate absolute from percent change using pandas dataframe,"<p>I know how to calculate percent change from absolute using a pandas dataframe, using the following:</p>

<pre><code>df_pctChange = df_absolute.pct_change()
</code></pre>

<p>But I can't seem to figure out how to calculate the inverse: using the initial row of <code>df_absolute</code> as the starting point, how do I calculate the absolute number from the percent change located in <code>df_pctChange</code>?</p>

<p>As an example, let's say that the initial row for the two columns in <code>df_absolute</code> are <code>548625</code> and <code>525980</code>, and the the <code>df_pctChange</code> is the following:</p>

<pre><code>NaN         NaN
-0.004522   -0.000812
-0.009018    0.001385
-0.009292   -0.002438
</code></pre>

<p>How can I produce the content of <code>df_absolute</code>? It should look as follows:</p>

<pre><code> 548625      525980 
 546144      525553 
 541219      526281 
 536190      524998 
</code></pre>
","['python', 'pandas', 'dataframe']",49241225,"<p>You should be able to use the formula:</p>

<pre><code>(1 + r).cumprod()
</code></pre>

<p>to get a cumulative growth factor.</p>

<p>Example:</p>

<pre><code>&gt;&gt;&gt; data
        0       1
0  548625  525980
1  546144  525553
2  541219  526281
3  536190  524998

&gt;&gt;&gt; pctchg = data.pct_change()

&gt;&gt;&gt; init = data.iloc[0]  # may want to use `data.iloc[0].copy()`
&gt;&gt;&gt; res = (1 + pctchg).cumprod() * init
&gt;&gt;&gt; res.iloc[0] = init
&gt;&gt;&gt; res
          0         1
0  548625.0  525980.0
1  546144.0  525553.0
2  541219.0  526281.0
3  536190.0  524998.0
</code></pre>

<p>To confirm you worked backwards into the correct absolute figures:</p>

<pre><code>&gt;&gt;&gt; np.allclose(data, res)
True
</code></pre>
",Calculate absolute percent change using pandas dataframe I know calculate percent change absolute using pandas dataframe using following df pctChange df absolute pct change But I seem figure calculate inverse using initial row df absolute starting point I calculate absolute number percent change located df pctChange As example let say initial row two columns df absolute df pctChange following NaN NaN How I produce content df absolute It look follows,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
49484015,2018-03-26,2018,2,How to handle the multiple-line pattern in Python regular expression matching,"<p>I have a list of company names to be replaced by the word 'company'. The list across multiple lines. </p>

<pre><code>cmp=re.compile("""""" A | B |
                   C | D
               """""")
text='A is a great company, so is B'
cmp.sub('company',text)
</code></pre>

<p>But it doesn't work. How should I fix this?</p>

<p>Edit: </p>

<p>The above example given didn't consider the whitespace in company names.</p>

<pre><code>company1=re.compile(r""""""Berkshire Hathaway|Australia &amp; New Zealand Bank
                  |Wells Fargo|AIG
                  |Ind &amp; Comm Bank of China|BNP Paribas"""""")
company2=re.compile(r""""""Berkshire Hathaway|Australia &amp; New Zealand Bank
                  |Wells Fargo|AIG
                  |Ind &amp; Comm Bank of China|BNP Paribas"""""",re.VERBOSE)
text='AIG is a great company, so is Berkshire Hathaway'  
company1.sub('cmp',text) 
&gt;&gt;&gt; 'AIG is a great company, so is cmp'
company2.sub('cmp',text) 
&gt;&gt;&gt; 'cmp is a great company, so is Berkshire Hathaway'
</code></pre>
","['python', 'regex', 'python-3.x']",49484144,"<p>You could treat this as an example of a verbose pattern which allows (and ignores) whitespace like line breaks:</p>

<pre><code>import re

cmp = re.compile(r"""""" A | B |
                   C | D
               """""", re.VERBOSE)
text = 'A is a great company, so is B'
print(cmp.sub('company', text))
</code></pre>

<p><strong>OUTPUT</strong></p>

<pre><code>company is a great company, so is company
</code></pre>

<blockquote>
  <p>Space is contained in the company names. ... Any idea on how to fix
  this?</p>
</blockquote>

<p>We need to do something like a CGI escape of the space characters that appear inside of names.  Here's a regex-based approach that doesn't require decoding of the encoded spaces:</p>

<pre><code>import re

companies = re.compile(re.sub(r""(?&lt;=\S) (?=\S)"", r""[ ]"", """"""Berkshire Hathaway|Australia &amp; New Zealand Bank
                  |Wells Fargo|AIG
                  |Ind &amp; Comm Bank of China|BNP Paribas""""""), re.VERBOSE)

text = 'AIG is a great company, so is Berkshire Hathaway'

print(companies.sub('cmp', text))
</code></pre>

<p><strong>OUTPUT</strong></p>

<pre><code>cmp is a great company, so is cmp
</code></pre>
",How handle multiple line pattern Python regular expression matching I list company names replaced word company The list across multiple lines cmp compile A B C D text A great company B cmp sub company text But work How I fix Edit The example given consider whitespace company names company compile r Berkshire Hathaway Australia amp New Zealand Bank Wells Fargo AIG Ind amp Comm Bank China BNP Paribas company compile r Berkshire Hathaway Australia amp New Zealand Bank Wells Fargo AIG Ind amp Comm Bank China BNP Paribas VERBOSE text AIG great company Berkshire Hathaway company sub cmp text gt gt gt AIG great company cmp company sub cmp text gt gt gt cmp great company Berkshire Hathaway,"startoftags, python, regex, python3x, endoftags",python django djangorestframework endoftags,python regex python3x,python django djangorestframework,0.33
49545758,2018-03-28,2018,5,Flip or reverse columns in numpy array,"<p>I want to flip the first and second values of arrays in an array. A naive solution is to loop through the array. What is the right way of doing this?</p>

<pre><code>import numpy as np
contour = np.array([[1, 4],
                    [3, 2]])

flipped_contour = np.empty((0,2))
for point in contour:
    x_y_fipped = np.array([point[1], point[0]])
    flipped_contour = np.vstack((flipped_contour, x_y_fipped))

print(flipped_contour)

[[4. 1.]
[2. 3.]]
</code></pre>
","['python', 'arrays', 'numpy']",49545765,"<p>Use the aptly named <code>np.flip</code>:</p>

<pre><code>np.flip(contour, axis=1)
</code></pre>

<p>Or,</p>

<pre><code>np.fliplr(contour)
</code></pre>

<p></p>

<pre><code>array([[4, 1],
       [2, 3]])
</code></pre>
",Flip reverse columns numpy array I want flip first second values arrays array A naive solution loop array What right way import numpy np contour np array flipped contour np empty point contour x fipped np array point point flipped contour np vstack flipped contour x fipped print flipped contour,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
49661708,2018-04-04,2018,8,Keras LSTM Multiple Input Multiple Output,"<p>I am trying to train an RNN to predict stock prices in the future.</p>

<p>My goal is to train the model using two datasets: X_train and y_train.</p>

<p>X_train is a 3D array including (number of observations, number of previous candles, attributes of each candle)</p>

<p>y_train is a 3D array including (number of observations, number of observations in the future, price).</p>

<p>So if I have data from 500 candles my X_train will be (430,60,6): for 430 observations (current candle each time) take the 60 observations that came before it and 6 characteristics (close price, volume, etc.) of them and try to use that data (through the RNN) to predict y_train(430, 10,1): for 430 observations predict the close price (corresponds to 1) for the next 10 candles.
I cannot, for the life of me, get the dimensions to enter the model correctly.
I use the following code for the model:</p>

<pre><code>regressor = Sequential()

regressor.add(LSTM(units = 50, return_sequences = True, input_shape = ( 
X_train.shape[1], 6)))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units = 50, return_sequences = True))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units = 50, return_sequences = True))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units = 50, return_sequences = True))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units = 50, return_sequences=True))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units = 1))

regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')
regressor.fit(X_train, y_train, epochs = 20, batch_size = 32)
</code></pre>

<p>I get <code>ValueError: Error when checking target: expected lstm_6 to have 2 dimensions, but got array with shape (430, 10, 1)</code></p>

<p>Thanks a lot.</p>
","['python', 'tensorflow', 'keras']",49662059,"<p>Let's take a step back here and look at what is done and why it does not work.</p>

<p>Firstly, your input data is of the following shape:</p>

<pre><code>(samples, timesteps, features)
</code></pre>

<p>Secondly, you would like your output data to be of the following shape:</p>

<pre><code>(samples, future_timesteps, 1)
</code></pre>

<p>This kind of architecture is known as <a href=""https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"" rel=""noreferrer"">sequence to sequence learning</a> (colloquially referred to as Seq2Seq).</p>

<p>So how do we do a Seq2Seq? There are a few ways you might want to read up on and this is still an area of very active research. Here are some ideas.</p>

<p>Note this is done with the <a href=""https://keras.io/models/model/"" rel=""noreferrer"">keras functional api</a>. It is better.</p>

<p>Read the input sequence from both directions, then predict the next 30 units for pricing by having a final dense layer of 30 units.</p>

<pre><code>input_layer = Input(shape=(600,6,))

lstm = Bidirectional(
    LSTM(250),
    merge_mode='concat'
)(input_layer)

pred = Dense(10)(lstm)
model = Model(inputs=input_layer, outputs=pred)
model.compile(optimizer = 'adam', loss = 'mean_squared_error')

model.fit(X_train, Y_train, epochs = 20, batch_size = 32)
</code></pre>

<p>where <code>Y_train</code> is reshaped to <code>(430, 10)</code> instead of <code>(430, 10, 1)</code>. In response to a comment, this does not alter the labels (<code>Y_train</code>) in any meaningful way. This is because the difference between (x, y, 1) and (x,y) is just as follows:</p>

<pre><code>[[[1],[2],[3]],
 [[4],[5],[6]]]
</code></pre>

<p>instead of </p>

<pre><code>[[1,2,3],
 [4,5,6]]
</code></pre>

<p>So a call like:</p>

<pre><code>Y_train = np.reshape(Y_train, Y_train.shape[:2])
</code></pre>

<p>does not meaningfully affect the training data.</p>

<p>However, this may not be the best architecture to begin with. This is because a single dense layer will be taking in the last hidden states from the forward and backward direction, instead of feeding every hidden state (from both directions) at every timestep. Effectively, the above model is unaware of more information presented in the below model. I suggest the following as an alternative.</p>

<pre><code>input_layer = Input(shape=(600,6,))

encoder = Bidirectional(
    LSTM(250),
    merge_mode='concat',
    return_sequences=True
)(input_layer)
decoder = LSTM(250, return_sequences=True)(encoder)
pred = TimeDistributed(Dense(1))(decoder)
model = Model(inputs=input_layer, outputs=pred)
model.compile(optimizer = 'adam', loss = 'mean_squared_error')

model.fit(X_train, Y_train, epochs = 20, batch_size = 32)
</code></pre>

<p>where <code>Y_train</code> is formatted as <code>(430, 60, 1)</code>. If you only care about the next <code>10</code> entries, pass <code>sample weighting</code> into fit and weight everything after the <code>10th</code> time index as <code>0</code> (you can even populate it with garbage then if you want while training). This would be done as follows:</p>

<pre><code>Y_train = np.hstack([Y_train]*6) 
</code></pre>

<p>Then you would create a sample weight mask like:</p>

<pre><code>W = np.zeros(Y_train.shape)
W[:,np.arange(W.shape[1]) &lt; 10,:] = 1
</code></pre>

<p>That is, a mask where only the first 10 entries along the second axis are 1, and all others are zero. Pass this <code>W</code>, as the <code>sample_weights</code> parameter in <code>model.fit</code></p>

<p>This way, the model can have a true sequence to sequence notion in the encoder/decoder paradigm.</p>

<p>Finally, it is not that additional LSTMS (stacking) is necessaraly bad, but it is regarded to be incremental at best in improving models of this nature, and does add a large amount of complexity as well as severely increasing training time. Get a single model to work with recurrent depths of 1 (no stacking), and then you can work on either stacking your single lstm or stacking encoders/decoders in the second structure I gave you.</p>

<p>Some additional tips for what you are doing:</p>

<ul>
<li>scale your data. StandardScaler, MinMaxScaler, whatever. Do not pass raw price data into an LSTM (or any deep learning model) as the activation functions will smash these values to -1, 1, 0 whatever and you will be subject to the vanishing or exploding gradients problem.</li>
</ul>

<p>I hope this helps!</p>
",Keras LSTM Multiple Input Multiple Output I trying train RNN predict stock prices future My goal train model using two datasets X train train X train D array including number observations number previous candles attributes candle train D array including number observations number observations future price So I data candles X train observations current candle time take observations came characteristics close price volume etc try use data RNN predict train observations predict close price corresponds next candles I cannot life get dimensions enter model correctly I use following code model regressor Sequential regressor add LSTM units return sequences True input shape X train shape regressor add Dropout regressor add LSTM units return sequences True regressor add Dropout regressor add LSTM units return sequences True regressor add Dropout regressor add LSTM units return sequences True regressor add Dropout regressor add LSTM units return sequences True regressor add Dropout regressor add LSTM,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
49753316,2018-04-10,2018,7,Django Rest Framework &quot;A valid integer is required.&quot;?,"<p>I want to default an empty string to a 0 or null during deserialization. </p>

<p>JSON</p>

<pre><code>{
  'injuries': '6',
  'children': '2',
  'civilians': '',
}
</code></pre>

<p>However, I keep getting this error: </p>

<blockquote>
  <p>""A valid integer is required.""</p>
</blockquote>

<p>models.py</p>

<pre><code>from django.db import models    

class Strike(models.Model):
    location = models.ForeignKey('Location', on_delete=models.CASCADE)
    civilians = models.PositiveIntegerField(blank=True, null=True)
    injuries = models.PositiveIntegerField(blank=True, null=True)
    children = models.PositiveIntegerField(blank=True, null=True)
</code></pre>

<p>serializers.py</p>

<pre><code>from rest_framework import serializers
from .models import Strike


class StrikeSerializer(serializers.ModelSerializer):
    civilians = serializers.IntegerField(default=0, required=False)

    class Meta:
        model = Strike
        fields = '__all__'

    def create(self, validated_data):
        return Strike.objects.create(**validated_data)
</code></pre>

<p>main</p>

<pre><code>serializer = StrikeSerializer(data=strike)
</code></pre>

<p>I tried manipulating data in create method, but the error gets raised before that. Where in the DRF structure can I override this, specifically convert '' to 0 or None?</p>
","['python', 'django', 'django-rest-framework']",49753451,"<p>You can use a <code>CharField</code> and then convert to <code>int</code> in the validation method.</p>

<pre><code>class StrikeSerializer(serializers.ModelSerializer):
    civilians = serializers.CharField(
            required=False, allow_null=True, allow_blank=True)

    def validate_civilians(self, value):
        if not value:
            return 0
        try:
            return int(value)
        except ValueError:
            raise serializers.ValidationError('You must supply an integer')
</code></pre>
",Django Rest Framework quot A valid integer required quot I want default empty string null deserialization JSON injuries children civilians However I keep getting error A valid integer required models py django db import models class Strike models Model location models ForeignKey Location delete models CASCADE civilians models blank True null True injuries models blank True null True children models blank True null True serializers py rest framework import serializers models import Strike class StrikeSerializer serializers ModelSerializer civilians serializers IntegerField default required False class Meta model Strike fields def create self validated data return Strike objects create validated data main serializer StrikeSerializer data strike I tried manipulating data create method error gets raised Where DRF structure I override specifically convert None,"startoftags, python, django, djangorestframework, endoftags",python django djangorestframework endoftags,python django djangorestframework,python django djangorestframework,1.0
49830323,2018-04-14,2018,2,How can I set the size of a button in pixels - python,"<p>I'm using Python 3, and I want to set the size of a button in pixels.</p>

<p>I wanted to make it width = 100 pixels, height = 30 pixels, but it didn't work.</p>

<p>It was much bigger than I expected.</p>

<p>Here's My code:</p>

<pre><code>from tkinter import *

def background():
    root = Tk()
    root.geometry('1160x640')

    btn_easy = Button(root, text = 'Easy', width = 100, height = 50)
    btn_easy.place(x = 100, y = 450)

    root.mainloop()

background()
</code></pre>

<p>How can I make it?</p>
","['python', 'python-3.x', 'tkinter']",49830356,"<p><a href=""http://effbot.org/tkinterbook/button.htm"" rel=""noreferrer"">http://effbot.org/tkinterbook/button.htm</a></p>

<blockquote>
  <p>You can also use the height and width options to explicitly set the
  size. <strong>If you display text in the button, these options define the size
  of the button in text units</strong>. If you display bitmaps or images instead,
  they define the size in pixels (or other screen units). <strong>You can
  specify the size in pixels even for text buttons, but that requires
  some magic. Hereâs one way to do it (there are others):</strong></p>

<pre><code>f = Frame(master, height=32, width=32)
f.pack_propagate(0) # don't shrink
f.pack()

b = Button(f, text=""Sure!"")
b.pack(fill=BOTH, expand=1)
</code></pre>
</blockquote>

<hr>

<pre><code>from tkinter import *

def background():
    root = Tk()
    root.geometry('1160x640')

    f = Frame(root, height=50, width=50)
    f.pack_propagate(0) # don't shrink
    f.place(x = 100, y = 450)

    btn_easy = Button(f, text = 'Easy')
    btn_easy.pack(fill=BOTH, expand=1)

    root.mainloop()

background()
</code></pre>

<hr>

<p>Bonus: many buttons (just to get the idea)</p>

<pre><code>from tkinter import *

def sizedButton(root, x,y):

    f = Frame(root, height=50, width=50)
    f.pack_propagate(0) # don't shrink
    f.place(x = x, y = y)

    btn_easy = Button(f, text = 'Easy')
    btn_easy.pack(fill=BOTH, expand=1)


def background():
    root = Tk()
    root.geometry('1160x640')

    for x in range(50,350,100):
        for y in range(50,350,100):
            sizedButton(root, x,y)


    root.mainloop()

background()
</code></pre>
",How I set size button pixels python I using Python I want set size button pixels I wanted make width pixels height pixels work It much bigger I expected Here My code tkinter import def background root Tk root geometry x btn easy Button root text Easy width height btn easy place x root mainloop background How I make,"startoftags, python, python3x, tkinter, endoftags",python python3x list endoftags,python python3x tkinter,python python3x list,0.67
49926279,2018-04-19,2018,3,Pandas GroupBy on column names,"<p>I have a dataframe, we can proxy by</p>

<pre><code>df = pd.DataFrame({'a':[1,0,0], 'b':[0,1,0], 'c':[1,0,0], 'd':[2,3,4]})
</code></pre>

<p>and a category series</p>

<pre><code>category = pd.Series(['A', 'B', 'B', 'A'], ['a', 'b', 'c', 'd'])
</code></pre>

<p>I'd like to get a sum of df's columns grouped into the categories 'A', 'B'.  Maybe something like:</p>

<pre><code>result = df.groupby(??, axis=1).sum()
</code></pre>

<p>returning</p>

<pre><code>result = pd.DataFrame({'A':[3,3,4], 'B':[1,1,0]})
</code></pre>
","['python', 'pandas', 'pandas-groupby']",49926321,"<p>Use <code>groupby</code> + <code>sum</code> on the columns (the <code>axis=1</code> is important here):</p>

<pre><code>df.groupby(df.columns.map(category.get), axis=1).sum()

   A  B
0  3  1
1  3  1
2  4  0
</code></pre>
",Pandas GroupBy column names I dataframe proxy df pd DataFrame b c category series category pd Series A B B A b c I like get sum df columns grouped categories A B Maybe something like result df groupby axis sum returning result pd DataFrame A B,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
50499351,2018-05-24,2018,2,Array size difference Python,"<p>My question is about Python array shape. </p>

<p>What is the difference between array size <code>(2, )</code> and <code>(2, 1)</code>? </p>

<p>I tried to add those two arrays together. However, I got an error as follows:  </p>

<blockquote>
  <p>Non-broadcastable output operant with shape (2, ) doesn't match the broadcast shape (2, 2)</p>
</blockquote>
","['python', 'arrays', 'numpy']",50499453,"<p>There is no difference in the raw memory. But logically, one is a one-dimensional array of two values, the other is a 2D array (where one of the dimensions just happens to be size 1).</p>

<p>The logical distinction is important to <code>numpy</code>; when you try to add them, it wants to make a new 2x2 array where the top row is the sum of the <code>(2, 1)</code> array's top ""row"" with each value in the <code>(2,)</code> array. If you use <code>+=</code> to do that though, you're indicating that you expect to be able to modify the <code>(2,)</code> array in place, which is not possible without resizing (which <code>numpy</code> won't do). If you change your code from:</p>

<pre><code>arr1 += arr2
</code></pre>

<p>to:</p>

<pre><code>arr1 = arr1 + arr2
</code></pre>

<p>it will happily create a new <code>(2, 2)</code> array. Or if the goal was that the 2x1 array should act like a flat 1D array, you can <code>flatten</code> it:</p>

<pre><code>alreadyflatarray += twodarray.flatten()
</code></pre>
",Array size difference Python My question Python array shape What difference array size I tried add two arrays together However I got error follows Non broadcastable output operant shape match broadcast shape,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
50584414,2018-05-29,2018,2,Add a column from one pandas dataframe to another when both are indexed differently,"<p>I'm new to python. I have two pandas dataframes that are indexed differently. I want to copy a column from one to another. 
Dataframe 1: Holds the id and class that each image belongs to</p>

<pre><code>      ID  index  class
0  10472  10472      0
1   7655   7655      0
2   6197   6197      0
3   9741   9741      0
4   9169   9169      0
</code></pre>

<p>Dataframe 2: Holds the id of the image in index and the image data in data columns</p>

<pre><code>                                                    data
index                                                   
5882   [[[255, 255, 255, 0], [255, 255, 255, 0], [255...
360    [[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0...
1906   [[[255, 255, 255, 0], [255, 255, 255, 0], [255...
3598   [[[255, 255, 255, 0], [232, 232, 247, 25], [34...
231    [[[255, 255, 255, 0], [234, 234, 234, 0], [57,...
</code></pre>

<p>I want to iterate through the dataframe1 and pick up the image id and look up the dataframe 2 for the matching id in the index and copy the 'data' column over to dataframe1. How could i do this (in a performance optimal way)?</p>
","['python', 'pandas', 'dataframe']",50584450,"<p>First for match data need same types, so if get different:</p>

<pre><code>print (df1['index'].dtype)    
int64
print (df2.index.dtype)   
object
</code></pre>

<p>there are 2 possible solutions - convert index to integers by:</p>

<pre><code>df2.index = df2.index.astype(int)
</code></pre>

<p>Or column to strings:</p>

<pre><code>df1['index'] = df1['index'].astype(str)
</code></pre>

<hr>

<p>Then use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>map</code></a> by column <code>data</code> in <code>df2</code>:</p>

<pre><code>df1['data'] = df1['index'].map(df2['data']) 
</code></pre>

<p>Or if need add multiple columns from <code>df2</code> (e.g. in real data) use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html"" rel=""nofollow noreferrer""><code>join</code></a>:</p>

<pre><code>df1 = df1.join(df2, on=['index'])
</code></pre>
",Add column one pandas dataframe another indexed differently I new python I two pandas dataframes indexed differently I want copy column one another Dataframe Holds id class image belongs ID index class Dataframe Holds id image index image data data columns data index I want iterate dataframe pick image id look dataframe matching id index copy data column dataframe How could performance optimal way,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
50598524,2018-05-30,2018,2,How to read a csv in scientic notation with capital E in Python?,"<p>I do have a csv file seperated by whitespace which looks like:</p>

<pre><code>5.64E-4   0.1259   3.556E-4   300
2.98E-4   4.7E-3   5.322E-4   270
</code></pre>

<p>I pandas like this</p>

<pre><code>df1 = pandas.read_csv(filepath[0], header=None, delim_whitespace=True, lineterminator='\r')
</code></pre>

<p>But I realized that pandas saves the DataFrame as a String, as it doesn't know what E means.
Can I somehow import the csv file and convert it to numeric writing, so I can plot it?</p>
","['python', 'pandas', 'csv']",50598840,"<p>In my opinion problem should be some not numeric values.</p>

<p>Possible solution is use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"" rel=""nofollow noreferrer""><code>to_numeric</code></a> with <code>errors='coerce'</code> for parse non numeric to <code>NaN</code>s with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer""><code>apply</code></a>, because it working only for one column (<code>Series</code>):</p>

<pre><code>print (df)
         0       1         2    3
0  5.64E-4  0.1259  3.556E-4  300
1  2.98E-4  4.7E-3       AAA  270

df = df.apply(pd.to_numeric, errors='coerce')
print (df)
          0       1         2    3
0  0.000564  0.1259  0.000356  300
1  0.000298  0.0047       NaN  270
</code></pre>
",How read csv scientic notation capital E Python I csv file seperated whitespace looks like E E E E E I pandas like df pandas read csv filepath header None delim whitespace True lineterminator r But I realized pandas saves DataFrame String know E means Can I somehow import csv file convert numeric writing I plot,"startoftags, python, pandas, csv, endoftags",python pandas matplotlib endoftags,python pandas csv,python pandas matplotlib,0.67
50608790,2018-05-30,2018,2,summing rows in multi-index pandas dataframe,"<p>I have a Pandas dataframe with a multiindex</p>

<pre><code>             A         B
year  age  
1895   0     10        12
1895   1     13        14
...
1965   0     34        45
1965   1     41        34
      ...
1965  50     56        22
1966   0     10        34
...
</code></pre>

<p>I would like to get all ages between two values (e.g. 10 and 20) summed for column A (and B). I played around a bit with .xs e.g. </p>

<pre><code>pops.xs(20, level='age')
</code></pre>

<p>gives all the age 20 for each year, but I cannot get this for multiple ages (and summed). </p>

<p>Eg. for 0 and 1 I would like to get</p>

<p>Any suggetions for an elegant (efficient) way to do that? </p>

<pre><code>          A         B
year    
1895      23        26
...
1965      75        79
...
</code></pre>
","['python', 'pandas', 'dataframe']",50608888,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html"" rel=""nofollow noreferrer""><code>query</code></a> for select with <code>sum</code> per first level <code>year</code>s:</p>

<pre><code>print (df)
           A   B
year age        
1895 8    10  12
     12   13  14
1965 0    34  45
     14   41  34
     12   56  22
1966 0    10  34

df = df.query('10 &lt;= age &lt;= 20').sum(level=0)
print (df)
       A   B
year        
1895  13  14
1965  97  56
</code></pre>

<p><strong>Detail</strong>:</p>

<pre><code>print (df.query('10 &lt;= age &lt;= 20'))
           A   B
year age        
1895 12   13  14
1965 14   41  34
     12   56  22
</code></pre>

<p>Another solution is use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.get_level_values.html"" rel=""nofollow noreferrer""><code>Index.get_level_values</code></a> for <code>index</code> and filter by <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""nofollow noreferrer""><code>boolean indexing</code></a>:</p>

<pre><code>i = df.index.get_level_values('age')
print (i)
Int64Index([8, 12, 0, 14, 12, 0], dtype='int64', name='age')

df = df[(i &gt;= 10) &amp; (i &lt;= 20)].sum(level=0)
print (df)
       A   B
year        
1895  13  14
1965  97  56
</code></pre>
",summing rows multi index pandas dataframe I Pandas dataframe multiindex A B year age I would like get ages two values e g summed column A B I played around bit xs e g pops xs level age gives age year I cannot get multiple ages summed Eg I would like get Any suggetions elegant efficient way A B year,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
50653953,2018-06-02,2018,2,Pandas: How to maintain the type of columns with nan?,"<p>For example,I have a <code>df</code> with <code>nan</code> and use the following method to <code>fillna</code>.</p>

<pre><code>import pandas as pd 
a = [[2.0, 10, 4.2], ['b', 70, 0.03], ['x',  ]]
df = pd.DataFrame(a)
print(df)

df.fillna(int(0),inplace=True)
print('fillna df\n',df)
dtype_df = df.dtypes.reset_index()
</code></pre>

<p>OUTPUT:</p>

<pre><code>   0     1     2
0  2  10.0  4.20
1  b  70.0  0.03
2  x   NaN   NaN
fillna df
    0     1     2
0  2  10.0  4.20
1  b  70.0  0.03
2  x   0.0  0.00
   col     type
0    0   object
1    1  float64
2    2  float64
</code></pre>

<p>Actually,I want the <code>column 1</code> maintain the type of <code>int</code> instead of <code>float</code>.</p>

<p>My desired output:</p>

<pre><code>fillna df
    0     1     2
0  2  10  4.20
1  b  70  0.03
2  x   0  0.00

   col     type
0    0   object
1    1  int64
2    2  float64
</code></pre>

<p>So how to do it?</p>
","['python', 'pandas', 'dataframe']",50654032,"<p>Try adding <code>downcast='infer'</code> to downcast any eligible columns:</p>

<pre><code>df.fillna(0, downcast='infer')

   0   1     2
0  2  10  4.20
1  b  70  0.03
2  x   0  0.00
</code></pre>

<p>And the corresponding <code>dtypes</code> are</p>

<pre><code>0     object
1      int64
2    float64
dtype: object
</code></pre>
",Pandas How maintain type columns nan For example I df nan use following method fillna import pandas pd b x df pd DataFrame print df df fillna int inplace True print fillna df n df dtype df df dtypes reset index OUTPUT b x NaN NaN fillna df b x col type object float float Actually I want column maintain type int instead float My desired output fillna df b x col type object int float So,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
50663164,2018-06-03,2018,2,pandas equivalent to .rda/.rdata,"<p>I have a large data frame of around 1000 columns. After doing all the missing value treatment and changing the data types to what I want, I have been able to reduce the size of this data frame to almost half. I did this by changing a lot of int64 to int16 and object to category. </p>

<p>After doing similar operations in R, I can save this new data frame to .rda format and just load it back. This helps me directly get all the variables in the format that I had after doing all the manipulation. Is there a way that I can save it in python and then reload this particular data format so that the data types are maintained?</p>

<p>P.S. - Writing to a csv and loading it back makes me do some work again.</p>
","['python', 'python-3.x', 'pandas']",50663595,"<h2>Setup</h2>

<pre><code>df = pd.DataFrame(dict(A=[1, 2, 3], B=list('XYZ')))
df.A = df.A.astype(np.int16)
df.B = pd.Categorical(df.B)

df

   A  B
0  1  X
1  2  Y
2  3  Z
</code></pre>

<hr>

<pre><code>df.dtypes

A       int16
B    category
dtype: object
</code></pre>

<hr>

<h2>You can use <a href=""http://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.to_hdf.html"" rel=""nofollow noreferrer""><code>pandas.DataFrame.to_hdf</code></a></h2>

<p>Save to <code>hdf</code><br>
Use <code>format='table'</code> because NotImplementedError from categorical</p>

<pre><code>df.to_hdf('small.h5', 'this_df', format='table')
</code></pre>

<p>Read back in</p>

<pre><code>df1 = pd.read_hdf('small.h5', 'this_df')

df1

   A  B
0  1  X
1  2  Y
2  3  Z
</code></pre>

<p>Check <code>dtypes</code></p>

<pre><code>df.dtypes

A       int16
B    category
dtype: object
</code></pre>

<p>Check equvivalence</p>

<pre><code>df1.equals(df)

True
</code></pre>

<hr>

<h2>Use <code>feather</code></h2>

<p>You might need to install the feather-format</p>

<pre><code>conda install feather-format -c conda-forge
</code></pre>

<p>Or </p>

<pre><code>pip install -U feather-format
</code></pre>

<p>Then</p>

<pre><code>df.to_feather('small.feather')

df1 = pd.read_feather('small.feather')

df1.equals(df)

True
</code></pre>

<p>The advantages of <code>feather</code> are that you should also be able to read them in <strong>R</strong> and reading and writing should be very fast.</p>

<hr>

<h2>Crude time comparison</h2>

<pre><code>%timeit pd.read_feather('small.feather')
%timeit pd.read_hdf('small.h5', 'this_df')

842 Âµs Â± 11.1 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
23.2 ms Â± 479 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
</code></pre>
",pandas equivalent rda rdata I large data frame around columns After missing value treatment changing data types I want I able reduce size data frame almost half I changing lot int int object category After similar operations R I save new data frame rda format load back This helps directly get variables format I manipulation Is way I save python reload particular data format data types maintained P S Writing csv loading back makes work,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
50756563,2018-06-08,2018,4,Joining panda dataframes with UK date format,"<p>I'm struggling to join panda dataframes from csv files that contain UK date format of dd/mm/yyyy.</p>

<p>The data contained within the csv file is:</p>

<pre><code>UK Date     Price
30/12/2015  120
31/12/2015  123
...         ... 
11/01/2016  135
12/01/2016  144
</code></pre>

<p>My issue is that the data corresponding to 11/01/2016 is appended to 1st November 2016 on the DataFrame and conversely, the data in the row of 01/11/2016 is appending to 11th January 2016.</p>

<p>Here is my simple code I am using to create a DataFrame within a date range and joining data from a temporary dataframe:</p>

<pre><code># Define the dates
dates = pd.date_range('2015-12-01', '2018-06-07')

# Create an empty DataFrame
df1 = pd.DataFrame(index = dates)

# Read the data into a temp dataframe
dftemp = pd.read_csv(""a.csv"", index_col='UK Date', parse_dates = True,
                        usecols = ['UK Date', 'Price'])

# Join the two DataFrames
df1 = df1.join(dftemp, how='inner')

print df1
</code></pre>

<p>I am not sure if it is best to try and convert the blank dataframe dates to UK format or to change the format when I read the file in? Also, what is the best way to change the format?</p>

<p>Thanks</p>
","['python', 'pandas', 'dataframe']",50756584,"<p>I believe the best is convert to <code>datetime</code>s in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""nofollow noreferrer""><code>read_csv</code></a> by parameter <code>dayfirst</code>:</p>

<pre><code>df = pd.read_csv(""a.csv"",
                 index_col='UK Date', 
                 parse_dates = True, 
                 dayfirst=True,
                 usecols = ['UK Date', 'Price'])
</code></pre>

<p>Another solutions with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html"" rel=""nofollow noreferrer""><code>to_datetime</code></a>:</p>

<pre><code>df['UK Date'] = pd.to_datetime(df['UK Date'], dayfirst=True)
#if need DatetimeIndex 
#df.index = pd.to_datetime(df.index, dayfirst=True)
</code></pre>

<p>Or:</p>

<pre><code>df['UK Date'] = pd.to_datetime(df['UK Date'], format='%d/%m/%Y')
#if need DatetimeIndex 
#df.index = pd.to_datetime(df.index, format='%d/%m/%Y')
</code></pre>
",Joining panda dataframes UK date format I struggling join panda dataframes csv files contain UK date format dd mm yyyy The data contained within csv file UK Date Price My issue data corresponding appended st November DataFrame conversely data row appending th January Here simple code I using create DataFrame within date range joining data temporary dataframe Define dates dates pd date range Create empty DataFrame df pd DataFrame index dates Read data temp dataframe dftemp pd read csv csv index col UK Date parse dates True usecols UK Date Price Join two DataFrames df df join dftemp inner print df I sure best try convert blank dataframe dates UK format change format I read file Also best way change format Thanks,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
50971337,2018-06-21,2018,3,Detecting Arabic characters in regex,"<p>I have a dataset of Arabic sentences, and I want to remove non-Arabic characters or special characters. I used this regex in python:</p>
<pre><code>text = re.sub(r'[^Ø¡-Ù0-9]',' ',text)
</code></pre>
<p>It works perfectly, but in some sentences (4 cases from the whole dataset) the regex also removes the Arabic words!</p>
<p>I read the dataset using Panda (python package) like:</p>
<pre><code>train = pd.read_excel('d.xlsx', encoding='utf-8')
</code></pre>
<p>Just to show you in a picture, I tested on Pythex site:
<a href=""https://i.stack.imgur.com/bYWrl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bYWrl.png"" alt=""enter image description here"" /></a></p>
<p>What is the problem?</p>
<p>------------------ Edited:</p>
<p>The sentences in the example:</p>
<blockquote>
<p>Ø§ÙØ§ Ø¨Ø­ÙÙ Ø±Ø¬Ø¹Ù ÙØ¨Ø§Ø±Ù ÙØ§Ø¹ÙÙÙ Ø­ÙÙØ© ÙØ§Ø­Ø±ÙÙÙØ§ Ø¨Ø§ÙÙØ¹Ø§Ø²ÙÙ ÙÙÙØ§ Ø§ÙØ§Ø®ÙØ§Ù ÙØ±ÙØ­Ù
ÙØ¹Ø²Ù Ø§Ø­Ø±ÙÙ Ø§ÙØ¹Ø²Ø§ -- Ø§Ø­Ø³ÙÙÙÙ ÙØ§ÙÙÙ   #ÙØµØ±</p>
<h1>ïº·ï»ï»´ï» ïºïº­ïº©ï»­ï»ïºï»¥ ï»£ïº¼ïº® ..ïºïº£ï»¨ïº ï»§ïºï»ï»° ï»£ï»´ï»¦ ï»³ïº ïº©ïºïº©ïºØ #ï»£ïº´ïº¨ïº®ïº #ï»ïºïº #EgyPresident #Egypt #ï»£ï»ïºï»ï»ï»®ï»¥ ÙØ§ ÙØ§ Ø­Ø¨ÙØ¨Ù ÙØ§ Ø­Ø²Ø±Øª: Ø¨Ø´Ø§Ø± ØºØ¨Ù Ø¨ÙØ¬ÙØ¯ Ø¨Ø¹Ø«Ø© Ø£ÙØ§Ù Ø­Ø§Ø¨ ÙÙØ¶Ø­ Ø±ÙØ­Ù Ø§ÙÙ ÙØ¬Ø±Ù ÙÙ ÙÙÙ ÙÙØ° Ø§ÙÙØ¬Ø²Ø±Ø© ÙØªØ±Ù Ø§ÙØ¨Ø¹Ø«Ø© Ø§Ø¬Ø±Ø§ÙÙ Ø¨Ø­Ù Ø§ÙØ³ÙØ±ÙÙ</h1>
</blockquote>
","['python', 'regex', 'python-3.x']",50972137,"<p>Those incorrectly included characters are not in the <em>common</em> Unicode range for Arabic (U+0621..U+64A), but are ""hardcoded"" as their initial, medial, and final forms.</p>

<p>Comparable to capitalization in Latin-based languages, but more strict than that, Arabic writing indicates both the start and end of words with a special 'flourish' form. In addition it also allows an ""isolated"" form (to be used when the character is not part of a full word).</p>

<p>This is <em>usually</em> encoded in a file as 'an' Arabic character and the actual rendering in initial, medial, or final form is left to the text renderer, but since all forms also have Unicode codepoints of their own, it is also possible to ""hardcode"" the exact forms. That is what you encountered: a mix of these two systems.</p>

<p>Fortunately, the Unicode ranges for the hardcoded forms are also fixed values:</p>

<blockquote>
  <p>Arabic Presentation Forms-A is a Unicode block encoding contextual forms and ligatures of letter variants needed for Persian, Urdu, Sindhi and Central Asian languages. The presentation forms are present only for compatibility with older standards such as codepage 864 used in DOS, and are typically used in visual and not logical order.<br>
  (<a href=""https://en.wikipedia.org/wiki/Arabic_Presentation_Forms-A"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Arabic_Presentation_Forms-A</a>)</p>
</blockquote>

<p>and their ranges are U+FB50..U+FDFF (Presentation Forms A) and U+FE70..U+FEFC (Presentation Forms B). If you add these ranges to your exclusion set, the regex will no longer delete these texts:</p>

<pre><code>[^Ø¡-Ù0-9ï­-ï·¿ï¹°-ï»¼]
</code></pre>

<p>Depending on your browser and/or editor, you may have problems with selecting this text to copy and paste it. It may be more clear to explicitly use a string specifying the exact characters:</p>

<pre><code>[^0-9\u0621-\u064a\ufb50-\ufdff\ufe70-\ufefc]
</code></pre>
",Detecting Arabic characters regex I dataset Arabic sentences I want remove non Arabic characters special characters I used regex python text sub r text It works perfectly sentences cases whole dataset regex also removes Arabic words I read dataset using Panda python package like train pd read excel xlsx encoding utf Just show picture I tested Pythex site What problem Edited The sentences example EgyPresident Egypt,"startoftags, python, regex, python3x, endoftags",python arrays numpy endoftags,python regex python3x,python arrays numpy,0.33
51037254,2018-06-26,2018,2,Python3 one-liner to verify presence of item in dictionary values,"<p>I am looking for a possible one-liner to return the equivalent of</p>

<pre><code>'A' in ['A', 'B', 'C']
</code></pre>

<p>but in a following case:
Suppose I have a dictionary containing lists as values, like:</p>

<pre><code>dictionary = {'key1': ['A', 'B', 'C', 'D'], 
              'key2': ['E', 'F'], 
              'key3': ['G', 'H', 'I']}
</code></pre>

<p>So far the closest I could get was something like:</p>

<pre><code>r = {v[0] for k, v in dictionary.items() if 'A' in v}
</code></pre>

<p>however this returns a set of length 0 or 1 returning the list element for the presence of which I want to check.</p>

<p>I would want to get only True/False regarding if 'A' is present in any of the lists stored in the dictionary values.</p>
","['python', 'python-3.x', 'dictionary']",51037291,"<p>You need <a href=""https://docs.python.org/3/library/functions.html#any"" rel=""nofollow noreferrer""><code>any()</code></a> built-in function:</p>

<pre><code>any('A' in v for v in dictionary.values())
</code></pre>

<p><code>any()</code> is optimum by itself in terms of returning <code>True</code> as soon as encountering a true proposition but if you wanna boost the performance even more you can use <code>set</code> objects instead of lists for preserving the values for their membership checking is linear. </p>
",Python one liner verify presence item dictionary values I looking possible one liner return equivalent A A B C following case Suppose I dictionary containing lists values like dictionary key A B C D key E F key G H I So far closest I could get something like r v k v dictionary items A v however returns set length returning list element presence I want check I would want get True False regarding A present lists stored dictionary values,"startoftags, python, python3x, dictionary, endoftags",python python3x list endoftags,python python3x dictionary,python python3x list,0.67
51246174,2018-07-09,2018,2,Adding a row at the bottom of a DataFrame for a grand total,"<p>I am grouping information from an Excel file with Pandas, with this information i am summing the values in order to get a summary of the data.</p>

<p>It's pulling certain info from an excel, then grouping columns in order to reflect the summary i want.</p>

<pre><code>summary_df = df.groupby([""NAME"", ""CITY""])[""QUANTITY""].sum().reset_index()
summary_df.loc[""Grand Total""] = summary_df[""QUANTITY""].sum()
</code></pre>

<p>This is returning the information I want AND giving me the summed total but it is showing the quantity summary on each of the columns for the dataframe like this:</p>

<pre><code>NAME         |   CITY   |   QUANTITY
JOHN         |  LONDON  |     50
STEVE        |  PARIS   |     100
GRAND TOTAL  |   150    |     150
</code></pre>

<p>I only want to see the grand total under QUANTITY and no other columns.</p>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",51246274,"<p>You can add your total row to your <code>groupby</code> series and <em>then</em> convert to a dataframe via <code>reset_index</code>. Currently, you are populating the entire row of a dataframe with a scalar, which triggers Pandas to use broadcasting.</p>

<pre><code>df = pd.DataFrame([['A', 'LONDON', 10], ['A', 'LONDON', 20],
                   ['B', 'CHICAGO', 30], ['C', 'CHICAGO', 20]],
                  columns=['NAME', 'CITY', 'QUANTITY'])

df_summary = df.groupby(['NAME', 'CITY'])['QUANTITY'].sum()
df_summary.loc['Grand Total'] = df_summary.sum()

df_summary = df_summary.reset_index()

print(df_summary)

          NAME     CITY  QUANTITY
0            A   LONDON        30
1            B  CHICAGO        30
2            C  CHICAGO        20
3  Grand Total                 80
</code></pre>
",Adding row bottom DataFrame grand total I grouping information Excel file Pandas information summing values order get summary data It pulling certain info excel grouping columns order reflect summary want summary df df groupby NAME CITY QUANTITY sum reset index summary df loc Grand Total summary df QUANTITY sum This returning information I want AND giving summed total showing quantity summary columns dataframe like NAME CITY QUANTITY JOHN LONDON STEVE PARIS GRAND TOTAL I want see grand total QUANTITY columns,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas dataframe pandasgroupby,python pandas dataframe,0.87
51281265,2018-07-11,2018,3,Filling a default value if count is less than threshold in pandas,"<p>I have a dataframe:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame()
df['name'] = ['john','sam','john','john','dean','dean','maggi',
              'ram','maggi','ana','sam','sam']
df['pt'] = [23, 32, 45, 65, 65, 45, 32, 45, 90, 10, 32, 22]
</code></pre>

<p>I want to replace a name by other if name appears only once in name column.</p>

<p>So my output would be:</p>

<pre><code>     name  pt
0    john  23
1     sam  32
2    john  45
3    john  65
4    dean  65
5    dean  45
6   maggi  32
7   other  45
8   maggi  90
9   other  10
10    sam  32
11    sam  22
</code></pre>

<p>In my original data, values are in hundreds so using
<code>df.replace</code> or <code>map</code> is not feasible. </p>

<p><code>np.where</code> works only with binary column. So I'm stuck here.</p>

<p>Thanks,</p>
","['python', 'pandas', 'dataframe']",51281345,"<p>Use <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html"" rel=""nofollow noreferrer""><code>numpy.where</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.duplicated.html"" rel=""nofollow noreferrer""><code>duplicated</code></a> for select all unique rows or use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.transform.html"" rel=""nofollow noreferrer""><code>transform</code></a> with <code>size</code> for more general solution with specifying threshold:</p>

<pre><code>df['name'] = np.where(~df['name'].duplicated(keep=False), 'other', df['name'])
</code></pre>

<p>Or:</p>

<pre><code>df['name'] = np.where(df.groupby('name')['name'].transform('size') == 1, 'other', df['name'])
print (df)
     name  pt
0    john  23
1     sam  32
2    john  45
3    john  65
4    dean  65
5    dean  45
6   maggi  32
7   other  45
8   maggi  90
9   other  10
10    sam  32
11    sam  22
</code></pre>

<p>Another solution, thanks @Jon Clements:</p>

<pre><code>df.name.where(df.groupby('name')['name'].transform('size') &gt; 1, 'other', inplace=True)
</code></pre>
",Filling default value count less threshold pandas I dataframe import pandas pd import numpy np df pd DataFrame df name john sam john john dean dean maggi ram maggi ana sam sam df pt I want replace name name appears name column So output would name pt john sam john john dean dean maggi maggi sam sam In original data values hundreds using df replace map feasible np works binary column So I stuck Thanks,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
51546176,2018-07-26,2018,3,Different behaviour between same implementations of TensorFlow and Keras,"<p>I have TensorFlow 1.9 and Keras 2.0.8 on my machine. When training a neural network with some toy data, the resulting training curves are very different between TensorFlow and Keras, and I do not understand why.</p>

<p>For the Keras implementation, the network learns well and the loss continues to decrease, whereas for the TensorFlow implementation, the network does not learn anything and the loss does not decrease. I have tried to ensure that both implementations use the same hyperparameters. <strong>Why is the behaviour so different?</strong></p>

<p>The network itself has two inputs: and image, and a vector. These are then passed through their own layers, before being concatenated.</p>

<p>Here are my implementations.</p>

<p>Tensorflow:</p>

<pre><code># Create the placeholders
input1 = tf.placeholder(""float"", [None, 64, 64, 3])
input2 = tf.placeholder(""float"", [None, 4])
label = tf.placeholder(""float"", [None, 4])

# Build the TensorFlow network
# Input 1
x1 = tf.layers.conv2d(inputs=input1, filters=30, kernel_size=[5, 5], strides=(2, 2), padding='valid', activation=tf.nn.relu)
x1 = tf.layers.conv2d(inputs=x1, filters=30, kernel_size=[5, 5], strides=(2, 2), padding='valid', activation=tf.nn.relu)
x1 = tf.layers.flatten(x1)
x1 = tf.layers.dense(inputs=x1, units=30)
# Input 2
x2 = tf.layers.dense(inputs=input2, units=30, activation=tf.nn.relu)
# Output
x3 = tf.concat(values=[x1, x2], axis=1)
x3 = tf.layers.dense(inputs=x3, units=30)
prediction = tf.layers.dense(inputs=x3, units=4)

# Define the optimisation
loss = tf.reduce_mean(tf.square(label - prediction))
train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)

# Train the model
sess = tf.Session()
sess.run(tf.global_variables_initializer())
training_feed = {input1: training_input1_data, input2: training_input2_data, label: training_label_data}
validation_feed = {input1: validation_input1_data, input2: validation_input2_data, label: validation_label_data}
for epoch_num in range(30):
    train_loss, _ = sess.run([loss, train_op], feed_dict=training_feed)
    val_loss = sess.run(loss, feed_dict=validation_feed)
</code></pre>

<p>Keras:</p>

<pre><code># Build the keras network
# Input 1
input1 = Input(shape=(64, 64, 3), name='input1')
x1 = Conv2D(filters=30, kernel_size=5, strides=(2, 2), padding='valid', activation='relu')(input1)
x1 = Conv2D(filters=30, kernel_size=5, strides=(2, 2), padding='valid', activation='relu')(x1)
x1 = Flatten()(x1)
x1 = Dense(units=30, activation='relu')(x1)
# Input 2
input2 = Input(shape=(4,), name='input2')
x2 = Dense(units=30, activation='relu')(input2)
# Output
x3 = keras.layers.concatenate([x1, x2])
x3 = Dense(units=30, activation='relu')(x3)
prediction = Dense(units=4, activation='linear', name='output')(x3)

# Define the optimisation
model = Model(inputs=[input1, input2], outputs=[prediction])
adam = optimizers.Adam(lr=0.001)
model.compile(optimizer=adam, loss='mse')

# Train the model
training_inputs = {'input1': training_input1_data, 'input2': training_input2_data}
training_labels = {'output': training_label_data}
validation_inputs = {'input1': validation_images, 'input2': validation_state_diffs}
validation_labels = {'output': validation_label_data}
callback = PlotCallback()
model.fit(x=training_inputs, y=training_labels, validation_data=(validation_inputs, validation_labels), batch_size=len(training_label_data[0]), epochs=30)
</code></pre>

<p>And here are the training curves (two runs for each implementation).</p>

<p>Tensorflow:</p>

<p><a href=""https://i.stack.imgur.com/dRvY6m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dRvY6m.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/vGGgim.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vGGgim.png"" alt=""enter image description here""></a></p>

<p>Keras:</p>

<p><a href=""https://i.stack.imgur.com/Aok0Tm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Aok0Tm.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/JlMjOm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JlMjOm.png"" alt=""enter image description here""></a></p>
","['python', 'tensorflow', 'keras']",51555938,"<p>After carefully examining your implementations, I observed that all the hyperparameters match except for the batch size. I don't agree with the answer from @Ultraviolet, because the default <code>kernel_initializer</code> of <code>tf.layers.conv2d</code> is also Xavier (see the TF implementation of <a href=""https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/layers/convolutional.py#L323"" rel=""nofollow noreferrer"">conv2d</a>).</p>

<p>The learning curves don't match for the following two reasons:</p>

<ol>
<li><p>The parameters from the Keras implementation (version 2) are receiving many more updates than those of the TF implementation (version 1). In version 1, you're feeding the full dataset simultaneously into the network at each epoch. This results in only 30 adam updates. In contrast, version 2 is performing <code>30 * ceil(len(training_label_data)/batch_size)</code> adam updates, with <code>batch_size=4</code>.</p></li>
<li><p>The updates of version 2 are noisier than those of version 1, because the gradients are averaged over less samples. </p></li>
</ol>
",Different behaviour implementations TensorFlow Keras I TensorFlow Keras machine When training neural network toy data resulting training curves different TensorFlow Keras I understand For Keras implementation network learns well loss continues decrease whereas TensorFlow implementation network learn anything loss decrease I tried ensure implementations use hyperparameters Why behaviour different The network two inputs image vector These passed layers concatenated Here implementations Tensorflow Create placeholders input tf placeholder float None input tf placeholder float None label tf placeholder float None Build TensorFlow network Input x tf layers conv inputs input filters kernel size strides padding valid activation tf nn relu x tf layers conv inputs x filters kernel size strides padding valid activation tf nn relu x tf layers flatten x x tf layers dense inputs x units Input x tf layers dense inputs input units activation tf nn relu Output x tf concat values x x axis x tf layers,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
51774634,2018-08-09,2018,2,How to remove index column after Converting xlsx to csv using pandas,"<pre><code>import pandas as pd

data_xls=pd.read_excel('/users/adam/abc.xlsx',index=False) 
data_xls.to_csv('def.csv,encoding='utf-8')
</code></pre>

<p>Also tried: </p>

<pre><code>data_xls=pd.read_excel('/users/adam/abc.xlsx',index_col=False)
data_xls=pd.read_excel('/users/adam/abc.xlsx',index=None)
data_xls=pd.read_excel('/users/adam/abc.xlsx',index_col=None)
</code></pre>

<p>Actual Output:</p>

<pre><code>     Name    Age

0    Adam    24

1    Steve   25

2    Jhon    23
</code></pre>

<p>Expected Output:</p>

<pre><code>Name    Age

Adam    24

Steve   25

Jhon    23 
</code></pre>

<p>Is there a way If I can drop the index column before inserting the data into a hive table? </p>
","['python', 'python-3.x', 'python-2.7']",51774682,"<p>when writing file you can use the following code if you don't want pandas to write the index column in the csv file</p>

<pre><code>pd.to_csv('your.csv', index=False)
</code></pre>

<p>Also if you want to drop the index when reading a file you should be able to do it through:</p>

<pre><code>df = pd.read_csv('some.csv').drop(['Unnamed 0'],axis=1)
</code></pre>
",How remove index column Converting xlsx csv using pandas import pandas pd data xls pd read excel users adam abc xlsx index False data xls csv def csv encoding utf Also tried data xls pd read excel users adam abc xlsx index col False data xls pd read excel users adam abc xlsx index None data xls pd read excel users adam abc xlsx index col None Actual Output Name Age Adam Steve Jhon Expected Output Name Age Adam Steve Jhon Is way If I drop index column inserting data hive table,"startoftags, python, python3x, python27, endoftags",python pandas numpy endoftags,python python3x python27,python pandas numpy,0.33
51774826,2018-08-09,2018,80,append dictionary to data frame,"<p>I have a function, which returns a dictionary like this:</p>

<pre><code>{'truth': 185.179993, 'day1': 197.22307753038834, 'day2': 197.26118010160317, 'day3': 197.19846975345905, 'day4': 197.1490578795196, 'day5': 197.37179265011116}
</code></pre>

<p>I am trying to append this dictionary to a dataframe like so:</p>

<pre><code>output = pd.DataFrame()
output.append(dictionary, ignore_index=True)
print(output.head())
</code></pre>

<p>Unfortunately, the printing of the dataframe results in an empty dataframe. Any ideas?</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",51775051,"<p>You don't assign the value to the result.</p>

<pre><code>output = pd.DataFrame()
output = output.append(dictionary, ignore_index=True)
print(output.head())
</code></pre>
",append dictionary data frame I function returns dictionary like truth day day day day day I trying append dictionary dataframe like output pd DataFrame output append dictionary ignore index True print output head Unfortunately printing dataframe results empty dataframe Any ideas,"startoftags, python, python3x, pandas, dataframe, endoftags",python python3x pandas endoftags,python python3x pandas dataframe,python python3x pandas,0.87
51860691,2018-08-15,2018,4,Rolling average all values of pandas DataFrame,"<p>I have a pandas DataFrame and I want to calculate on a rolling basis the average of all the value: for all the columns, for all the observations in the rolling window.</p>
<p>I have a solution with loops but feels very inefficient. Note that I can have <code>NaNs</code> in my data, so calculating the sum and dividing by the shape of the window would not be safe (as I want a <code>nanmean</code>).</p>
<p>Any better approach?</p>
<p><strong>Setup</strong></p>
<pre><code>import numpy as np
import pandas as pd

np.random.seed(1)

df = pd.DataFrame(np.random.randint(0, 10, size=(10, 2)), columns=['A', 'B'])

df[df&gt;5] = np.nan  # EDIT: add nans
</code></pre>
<p><strong>My Attempt</strong></p>
<pre><code>n_roll = 2

df_stacked = df.values
roll_avg = {}
for idx in range(n_roll, len(df_stacked)+1):
    roll_avg[idx-1] = np.nanmean(df_stacked[idx - n_roll:idx, :].flatten())

roll_avg = pd.Series(roll_avg)
roll_avg.index = df.index[n_roll-1:]
roll_avg = roll_avg.reindex(df.index)
</code></pre>
<p><strong>Desired Result</strong></p>
<pre><code>roll_avg
Out[33]: 
0         NaN
1    5.000000
2    1.666667
3    0.333333
4    1.000000
5    3.000000
6    3.250000
7    3.250000
8    3.333333
9    4.000000
</code></pre>
<p>Thanks!</p>
","['python', 'pandas', 'numpy']",51861101,"<p>Here's one NumPy solution with sliding windows off <code>view_as_windows</code> -</p>

<pre><code>from skimage.util.shape import view_as_windows

# Setup o/p array
out = np.full(len(df),np.nan)

# Get sliding windows of length n_roll along axis=0
w = view_as_windows(df.values,(n_roll,1))[...,0]

# Assign nan-ignored mean values computed along last 2 axes into o/p
out[n_roll-1:] = np.nanmean(w, (1,2))
</code></pre>

<p>Memory efficiency with <code>views</code> -</p>

<pre><code>In [62]: np.shares_memory(df,w)
Out[62]: True
</code></pre>
",Rolling average values pandas DataFrame I pandas DataFrame I want calculate rolling basis average value columns observations rolling window I solution loops feels inefficient Note I NaNs data calculating sum dividing shape window would safe I want nanmean Any better approach Setup import numpy np import pandas pd np random seed df pd DataFrame np random randint size columns A B df df gt np nan EDIT add nans My Attempt n roll df stacked df values roll avg idx range n roll len df stacked roll avg idx np nanmean df stacked idx n roll idx flatten roll avg pd Series roll avg roll avg index df index n roll roll avg roll avg reindex df index Desired Result roll avg Out NaN Thanks,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
51870088,2018-08-16,2018,6,Django AttributeError &#39;datetime.date&#39; object has no attribute &#39;utcoffset&#39;,"<p>I'm a newbie in Django, so sorry if explanation of problem looks weird.</p>

<p>I created a blog app within a Django project. </p>

<p><strong>models.py</strong>:</p>

<pre><code>from django.db import models

class Blog(models.Model):
    title = models.CharField(max_length=255)
    pub_date = models.DateTimeField() # issue appears because of this statement
    body = models.TextField()
    image = models.ImageField(upload_to='images/')
</code></pre>

<p>When I create a new blog object in admin page of site, everything's fine and object gets created and stored to the database. But when I want to edit this blog object (clicking on it in the list of blogs), I get this error:</p>

<pre><code>AttributeError at /admin/blog/blog/2/change/
'datetime.date' object has no attribute 'utcoffset'
</code></pre>

<p>Full traceback:</p>

<pre><code>Environment:


Request Method: GET
Request URL: http://localhost:8000/admin/blog/blog/2/change/

Django Version: 2.0.2
Python Version: 3.6.4
Installed Applications:
['jobs.apps.JobsConfig',
 'blog.apps.BlogConfig',
 'django.contrib.admin',
 'django.contrib.auth',
 'django.contrib.contenttypes',
 'django.contrib.sessions',
 'django.contrib.messages',
 'django.contrib.staticfiles']
Installed Middleware:
['django.middleware.security.SecurityMiddleware',
 'django.contrib.sessions.middleware.SessionMiddleware',
 'django.middleware.common.CommonMiddleware',
 'django.middleware.csrf.CsrfViewMiddleware',
 'django.contrib.auth.middleware.AuthenticationMiddleware',
 'django.contrib.messages.middleware.MessageMiddleware',
 'django.middleware.clickjacking.XFrameOptionsMiddleware']


Template error:
In template C:\Users\LENOVO\Anaconda3\lib\site-packages\django\contrib\admin\templates\admin\includes\fieldset.html, error at line 19
   'datetime.date' object has no attribute 'utcoffset'
   9 :             {% for field in line %}
   10 :                 &lt;div{% if not line.fields|length_is:'1' %} class=""field-box{% if field.field.name %} field-{{ field.field.name }}{% endif %}{% if not field.is_readonly and field.errors %} errors{% endif %}{% if field.field.is_hidden %} hidden{% endif %}""{% elif field.is_checkbox %} class=""checkbox-row""{% endif %}&gt;
   11 :                     {% if not line.fields|length_is:'1' and not field.is_readonly %}{{ field.errors }}{% endif %}
   12 :                     {% if field.is_checkbox %}
   13 :                         {{ field.field }}{{ field.label_tag }}
   14 :                     {% else %}
   15 :                         {{ field.label_tag }}
   16 :                         {% if field.is_readonly %}
   17 :                             &lt;div class=""readonly""&gt;{{ field.contents }}&lt;/div&gt;
   18 :                         {% else %}
   19 :                              {{ field.field }} 
   20 :                         {% endif %}
   21 :                     {% endif %}
   22 :                     {% if field.field.help_text %}
   23 :                         &lt;div class=""help""&gt;{{ field.field.help_text|safe }}&lt;/div&gt;
   24 :                     {% endif %}
   25 :                 &lt;/div&gt;
   26 :             {% endfor %}
   27 :         &lt;/div&gt;
   28 :     {% endfor %}
   29 : &lt;/fieldset&gt;


Traceback:

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\core\handlers\exception.py"" in inner
  35.             response = get_response(request)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\core\handlers\base.py"" in _get_response
  158.                 response = self.process_exception_by_middleware(e, request)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\core\handlers\base.py"" in _get_response
  156.                 response = response.render()

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\response.py"" in render
  106.             self.content = self.rendered_content

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\response.py"" in rendered_content
  83.         content = template.render(context, self._request)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\backends\django.py"" in render
  61.             return self.template.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render
  175.                     return self._render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in _render
  167.         return self.nodelist.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render
  943.                 bit = node.render_annotated(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render_annotated
  910.             return self.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\loader_tags.py"" in render
  155.             return compiled_parent._render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in _render
  167.         return self.nodelist.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render
  943.                 bit = node.render_annotated(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render_annotated
  910.             return self.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\loader_tags.py"" in render
  155.             return compiled_parent._render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in _render
  167.         return self.nodelist.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render
  943.                 bit = node.render_annotated(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render_annotated
  910.             return self.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\loader_tags.py"" in render
  67.                 result = block.nodelist.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render
  943.                 bit = node.render_annotated(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render_annotated
  910.             return self.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\loader_tags.py"" in render
  67.                 result = block.nodelist.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render
  943.                 bit = node.render_annotated(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render_annotated
  910.             return self.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\defaulttags.py"" in render
  211.                     nodelist.append(node.render_annotated(context))

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render_annotated
  910.             return self.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\loader_tags.py"" in render
  194.                 return template.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render
  177.                 return self._render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in _render
  167.         return self.nodelist.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render
  943.                 bit = node.render_annotated(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render_annotated
  910.             return self.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\defaulttags.py"" in render
  211.                     nodelist.append(node.render_annotated(context))

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render_annotated
  910.             return self.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\defaulttags.py"" in render
  211.                     nodelist.append(node.render_annotated(context))

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render_annotated
  910.             return self.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\defaulttags.py"" in render
  314.                 return nodelist.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render
  943.                 bit = node.render_annotated(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render_annotated
  910.             return self.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\defaulttags.py"" in render
  314.                 return nodelist.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render
  943.                 bit = node.render_annotated(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render_annotated
  910.             return self.render(context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render
  999.         return render_value_in_context(output, context)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\template\base.py"" in render_value_in_context
  978.             value = str(value)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\utils\html.py"" in &lt;lambda&gt;
  371.     klass.__str__ = lambda self: mark_safe(klass_str(self))

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\forms\boundfield.py"" in __str__
  36.         return self.as_widget()

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\forms\boundfield.py"" in as_widget
  118.             **kwargs

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\forms\widgets.py"" in render
  234.         context = self.get_context(name, value, attrs)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\contrib\admin\widgets.py"" in get_context
  104.         context = super().get_context(name, value, attrs)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\forms\widgets.py"" in get_context
  806.             value = self.decompress(value)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\forms\widgets.py"" in decompress
  894.             value = to_current_timezone(value)

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\forms\utils.py"" in to_current_timezone
  176.     if settings.USE_TZ and value is not None and timezone.is_aware(value):

File ""C:\Users\LENOVO\Anaconda3\lib\site-packages\django\utils\timezone.py"" in is_aware
  247.     return value.utcoffset() is not None

Exception Type: AttributeError at /admin/blog/blog/2/change/
Exception Value: 'datetime.date' object has no attribute 'utcoffset'
</code></pre>

<p>If I change <code>models.DateTimeField()</code> to <code>models.DateField()</code> or if I add <code>auto_now_add=True</code> to <code>DateTimeField</code> (so that it makes this field uneditable) everything becomes ok and blog object becomes editable without errors.</p>

<p>So the question obviously is what caused this error and how to fix it?</p>

<p>Django version 2.0.2 (also tried on 2.0.5)</p>

<p>Python version 3.6.4</p>
","['python', 'django', 'django-models']",51870283,"<p>Seems like model instance returns you date data. Take a look on database column and see if it its type not date. Probably previously during database creation your model had <code>pub_date</code> as <a href=""https://kite.com/python/docs/django.db.models.fields.DateField"" rel=""noreferrer"">DateField</a>. If so, run <code>makemigrations</code> and <code>migrate</code> to adjust database type according your current model.</p>
",Django AttributeError datetime date object attribute utcoffset I newbie Django sorry explanation problem looks weird I created blog app within Django project models py django db import models class Blog models Model title models CharField max length pub date models DateTimeField issue appears statement body models TextField image models ImageField upload images When I create new blog object admin page site everything fine object gets created stored database But I want edit blog object clicking list blogs I get error AttributeError admin blog blog change datetime date object attribute utcoffset Full traceback Environment Request Method GET Request URL http localhost admin blog blog change Django Version Python Version Installed Applications jobs apps JobsConfig blog apps BlogConfig django contrib admin django contrib auth django contrib contenttypes django contrib sessions django contrib messages django contrib staticfiles Installed Middleware django middleware security SecurityMiddleware django contrib sessions middleware SessionMiddleware django middleware common CommonMiddleware django,"startoftags, python, django, djangomodels, endoftags",python django djangorestframework endoftags,python django djangomodels,python django djangorestframework,0.67
52249421,2018-09-09,2018,2,Getting buttons to align to the left if there&#39;s lots of widgets,"<p>I had been trying to make a program. I am currently having trouble with getting all of the buttons to align to the left when I have got lots of widgets. When I use the following code, which has got lots of widgets and their various griddings on, the buttons I put don't align left, and I can't seem to find a way to make them align left.   </p>

<pre><code>from tkinter import *
import tkinter as tk

def banana():
    print (""Sundae"")
def tomato():
    print (""Ketchup"")
def potato():
    print (""Potato chips"")

root = tk.Tk()
root.geometry(""960x600"")

label_toptitle = tk.Label(root,
                          text=""Program Name"",
                          font=(None, 40),
)
label_toptitle.grid(row=0,
                    columnspan=3
                    )

description = ""To create rhythm, press the red record button. While recording, use the clicked note button to\n create a series of rectangle notes on screen. They can be held to extend the rectangles. \n\n Once you are done, press the red stop button to stop recording""

pixel = tk.PhotoImage(width=1, height=1)
label_desc = tk.Label(root,
                      image=pixel,
                      compound=""center"",
                      width=900,
                      font=(None, 14),
                      padx=20,
                      pady=10,
                      text=description
                      )
                      #bd=1,
                      #relief=""solid"",

#label_desc.pack(side=""top"", fill=""both"")
label_desc.grid(row=1,
                columnspan=3,
                #sticky=E
                )

canvas = Canvas(width=960, height=400, bg='white')

canvas.grid(row=2,
            column=0,
            columnspan=3,
            #expand=YES,
            #fill=BOTH
            )

canvas.create_rectangle(70, 18, 888, 208, width=5, fill='pink')
canvas.create_rectangle(257, 268, 349, 357, width=5, fill='pink')
canvas.create_rectangle(430, 268, 520, 357, width=5, fill='pink')
canvas.create_rectangle(597, 268, 689, 357, width=5, fill='pink')

gridbutt = Label(root, text="""", anchor=W)
gridbutt.grid(row=3, column=0, columnspan=3)

f1 = tk.Frame(root, width=70, height=30)
f2 = tk.Frame(root, width=70, height=30)
f3 = tk.Frame(root, width=70, height=30)

f1.grid(row=3, column=0)
f2.grid(row=3, column=1)
f3.grid(row=3, column=2)

button_qwer = Button(f1, text=""asdfasdf"", command=banana)
button_asdf = Button(f2, text=""asdfasdf"", command=tomato)
button_zxcv = Button(f3, text=""asdfasdf"", command=potato)

button_qwer.place(x=0, y=0, relwidth=1, relheight=1)
button_asdf.place(x=0, y=0, relwidth=1, relheight=1)
button_zxcv.place(x=0, y=0, relwidth=1, relheight=1)

root.mainloop()
</code></pre>

<p>But when I try to put small amount of code like below, the buttons to align left as I would expect.</p>

<pre><code>from tkinter import *
import tkinter as tk

root = tk.Tk()
root.geometry(""960x600"")

f1 = tk.Frame(root, width=70, height=30)
f2 = tk.Frame(root, width=70, height=30)
f3 = tk.Frame(root, width=70, height=30)

f1.grid(row=3, column=0)
f2.grid(row=3, column=1)
f3.grid(row=3, column=2)

button_qwer = Button(f1, text=""asdfasdf"")
button_asdf = Button(f2, text=""asdfasdf"")
button_zxcv = Button(f3, text=""asdfasdf"")

button_qwer.place(x=0, y=0, relwidth=1, relheight=1)
button_asdf.place(x=0, y=0, relwidth=1, relheight=1)
button_zxcv.place(x=0, y=0, relwidth=1, relheight=1)

root.mainloop()
</code></pre>

<p>I also attempted to anchor the buttons to the left with the ""anchor"" function, but it only gives an error. I did the same attempt to frames, but also with errors.</p>

<p>How do I get my buttons of my main program to align to the left and be bunched up towards the left nicely like the second program would do by default?</p>
","['python', 'python-3.x', 'tkinter']",52250452,"<p>You were placing your 3 buttons in 3 different frames, and gridding them in different columns. Here I placed all 3 buttons in a frame, packed them to the left, and gridded the single frame with 3 buttons in column 0, with a sticky option to the West.</p>

<p>It produces the same appearance as your second code sample.</p>

<pre><code>import tkinter as tk
from tkinter import PhotoImage

def banana():
    print (""Sundae"")

def tomato():
    print (""Ketchup"")

def potato():
    print (""Potato chips"")


root = tk.Tk()
root.geometry(""960x600"")

label_toptitle = tk.Label(root, text=""Program Name"", font=(None, 40),)
label_toptitle.grid(row=0, columnspan=3)

description = ""To create rhythm, press the red record button. While recording, use the clicked note button to\n create a series of rectangle notes on screen. They can be held to extend the rectangles. \n\n Once you are done, press the red stop button to stop recording""

pixel = PhotoImage(width=1, height=1)
label_desc = tk.Label(root, image=pixel, compound=""center"", width=900, font=(None, 14),
                      padx=20, pady=10, text=description)

label_desc.grid(row=1, columnspan=3)

canvas = tk.Canvas(width=960, height=400, bg='white')
canvas.grid(row=2, column=0, columnspan=3)

canvas.create_rectangle(70, 18, 888, 208, width=5, fill='pink')
canvas.create_rectangle(257, 268, 349, 357, width=5, fill='pink')
canvas.create_rectangle(430, 268, 520, 357, width=5, fill='pink')
canvas.create_rectangle(597, 268, 689, 357, width=5, fill='pink')

gridbutt = tk.Label(root, text="""", anchor='w')
gridbutt.grid(row=3, column=0, columnspan=3)

f1 = tk.Frame(root, width=70, height=30)
f1.grid(row=3, column=0, sticky='W')

button_qwer = tk.Button(f1, text=""asdfasdf"", command=banana)
button_asdf = tk.Button(f1, text=""asdfasdf"", command=tomato)
button_zxcv = tk.Button(f1, text=""asdfasdf"", command=potato)

button_qwer.pack(side='left')
button_asdf.pack(side='left')
button_zxcv.pack(side='left')

root.mainloop()
</code></pre>
",Getting buttons align left lots widgets I trying make program I currently trouble getting buttons align left I got lots widgets When I use following code got lots widgets various griddings buttons I put align left I seem find way make align left tkinter import import tkinter tk def banana print Sundae def tomato print Ketchup def potato print Potato chips root tk Tk root geometry x label toptitle tk Label root text Program Name font None label toptitle grid row columnspan description To create rhythm press red record button While recording use clicked note button n create series rectangle notes screen They held extend rectangles n n Once done press red stop button stop recording pixel tk PhotoImage width height label desc tk Label root image pixel compound center width font None padx pady text description bd relief solid label desc pack side top fill label desc grid row,"startoftags, python, python3x, tkinter, endoftags",python python3x list endoftags,python python3x tkinter,python python3x list,0.67
52319772,2018-09-13,2018,2,Python Pandas Cumulative Multiplication,"<p>I created a small dataframe and I want to multiply 0.5 to the previous row and so on. </p>

<p>In:</p>

<pre><code>1
2
3
4
</code></pre>

<p>Out:</p>

<pre><code>1
0.5
0.25
0.125
</code></pre>

<p>I tried the following but does not work the right way. It is not cumulative and let's say perpetual. </p>

<pre><code>x = pd.DataFrame([1, 2, 3, 4])
y = np.zeros(x.shape)

y[0] = 1
yd = pd.DataFrame(y)

k =  yd.shift(1) * 0.5

print (k)
</code></pre>

<p>Any idea? Thank you</p>

<hr>

<p>2nd more complexed question based on previous issue.</p>

<pre><code>data['y'] = np.where((data['a']&lt;50) &amp; (data['b']&gt;0), data['initial'], pd.Series(0.99, data['y'].index).cumprod() / 0.99)
</code></pre>

<p>I tried this code but does not work. If the premises are true then call the 'initial' otherwise proceed to the cumulative multiplication.</p>
","['python', 'pandas', 'dataframe']",52319816,"<p>Use <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.power.html"" rel=""nofollow noreferrer""><strong><code>numpy.power</code></strong></a></p>

<pre><code>np.power(.5, x - 1)

       0
0  1.000
1  0.500
2  0.250
3  0.125
</code></pre>

<hr>

<p>Or as <a href=""https://stackoverflow.com/questions/52319772/python-pandas-cumulative-multiplication/52319816#comment91586959_52319816"">@DSM pointed out</a> (more intuitively)</p>

<pre><code>.5 ** (x - 1)

       0
0  1.000
1  0.500
2  0.250
3  0.125
</code></pre>

<hr>

<p>On the other hand, if you just want strictly successive powers of <code>.5</code></p>

<pre><code>.5 ** pd.Series(range(len(x)))

0    1.000
1    0.500
2    0.250
3    0.125
dtype: float64
</code></pre>

<p>Another alternative with <code>cumprod</code></p>

<pre><code>pd.Series(.5, x.index).cumprod() / .5

0    1.000
1    0.500
2    0.250
3    0.125
dtype: float64
</code></pre>

<p>Or</p>

<pre><code>pd.Series({**dict.fromkeys(range(4), .5), **{0: 1}}).cumprod()

0    1.000
1    0.500
2    0.250
3    0.125
dtype: float64
</code></pre>
",Python Pandas Cumulative Multiplication I created small dataframe I want multiply previous row In Out I tried following work right way It cumulative let say perpetual x pd DataFrame np zeros x shape yd pd DataFrame k yd shift print k Any idea Thank nd complexed question based previous issue data np data lt amp data b gt data initial pd Series data index cumprod I tried code work If premises true call initial otherwise proceed cumulative multiplication,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
52529293,2018-09-27,2018,4,How to reclassify pandas dataframe column?,"<p>I have a Pandas dataframe that looks something like this:</p>

<pre><code>&gt; print(df)

           image_name                       tags
0                img1       class1 class2 class3
1                img2                     class2
2                img3              class2 class3
3                img4                     class1
</code></pre>

<p>How can I reclassify the <code>tags</code> column such that any row with a <code>class3</code> value gets assigned the string ""yes"" and everything else the string ""no""?</p>

<p>I am aware that I can check for instances of a search word using the following:</p>

<pre><code>df['tags'].str.contains('class3')
</code></pre>

<p>However, I am not sure how to integrate this into the task at hand.</p>

<p>The following is the intended output:</p>

<pre><code>           image_name                       tags
0                img1                        yes
1                img2                         no
2                img3                        yes
3                img4                         no
</code></pre>
","['python', 'pandas', 'dataframe']",52529315,"<p>Use <a href=""https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.where.html"" rel=""nofollow noreferrer""><code>np.where</code></a> as: </p>

<pre><code>df['tags'] = np.where(df['tags'].str.contains('class3'),'yes','no')
</code></pre>

<p>Or</p>

<pre><code>df['tags'] = 'no'
df.loc[df['tags'].str.contains('class3'),'tags'] = 'yes'
</code></pre>

<p>Or</p>

<pre><code>df['tags'] = ['yes' if 'class3' in s else 'no' for s in df3.tags.values]
</code></pre>

<hr>

<p>The output for above methods:</p>

<pre><code>print(df)
  image_name tags
0       img1  yes
1       img2   no
2       img3  yes
3       img4   no
</code></pre>
",How reclassify pandas dataframe column I Pandas dataframe looks something like gt print df image name tags img class class class img class img class class img class How I reclassify tags column row class value gets assigned string yes everything else string I aware I check instances search word using following df tags str contains class However I sure integrate task hand The following intended output image name tags img yes img img yes img,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
52601707,2018-10-02,2018,2,Pandas DataFrame: How to extract the last two string type numbers from a column which doesn&#39;t always end with the two numbers,"<p>Sorry for the possible confusion in the title, here's what I'm trying to do:</p>

<p>I'm trying to merge my Parcels data frame with my Municipality Code look up table. The Parcels dataframe:</p>

<pre><code>df1.head()

    PARID           OWNER1
0   B10 2 1 0131    WILSON ROBERT JR
1   B10 2 18B 0131  COMUNALE MICHAEL J &amp; MARY ANN
2   B10 2 18D 0131  COMUNALE MICHAEL J &amp; MARY ANN
3   B10 2 19F 0131  MONROE &amp; JEFFERSON HOLDINGS LLC
4   B10 4 11 0131   NOEL JAMES H
</code></pre>

<p>The Municipality Code dataframe:</p>

<pre><code>df_LU.head()
  PARID  Municipality
0   01  Allen Twp.
1   02  Bangor
2   03  Bath
3   04  Bethlehem
4   05  Bethlehem Twp.
</code></pre>

<p>The last two numbers in the first column of df1 ('31' in 'B10 2 1 0131') are the Municipality Code that I need to merge with the Municipality Code DataFrame. But in my 30,000 or so records, there are about 200 records end with letters as shown below:</p>

<pre><code>        PARID           OWNER1  
299    D11 10 10 0131F  HOWARD THEODORE P &amp; CLAUDIA S   
1007    F10 4 3 0134F   KNEEBONE JUDY ANN   
1011    F10 5 2 0134F   KNEEBONE JUDY ANN   
1114    F8 18 10 0626F  KNITTER WILBERT D JR &amp; AMY J    
1115    F8 18 8 0626F   KNITTER DONALD  
</code></pre>

<p>For these rows, the two numbers before the last letter are the Code that I need to extract out (like '31' in 'D11 10 10 0131F')</p>

<p>If I just use 
    pd.DataFrame(df1['PARID'].str[-2:])
This will give me:</p>

<pre><code>PARID
...
299 1F
...
</code></pre>

<p>While what I need is:</p>

<pre><code>PARID
...
299 31
...
</code></pre>

<p>My code of accomplishing this is pretty lengthy, which pretty much invloves:</p>

<ol>
<li>Join all the rows that end with 2 numbers.</li>
<li>Find out the index of the rows that end with a letter in the 'PARID' field</li>
<li>Join the results from step 2 again with the Municipality look up dataframe.</li>
</ol>

<p>The code is there:</p>

<pre><code>#Do the extraction and merge for the rows that end with numbers
df_2015= df1[['PARID','OWNER1']]
df_2015['PARID'] = df_2015['PARID'].str[-2:]
df_15r =pd.merge(df_2015, df_LU, how = 'left', on = 'PARID')
df_15r

#The pivot result for rows generated from above.
Result15_First = df_15r.groupby('Municipality').count()
Result15_First.to_clipboard()

#Check the ID field for rows that end with letters
check15 = df_2015['PARID'].unique()
check15
C = pd.DataFrame({'ID':check15})
NC = C.dropna()
LNC = NC[NC['ID'].str.endswith('F')]
MNC = NC[NC['ID'].str.endswith('A')]
F = [LNC, MNC]
NNC = pd.concat(F, axis = 0)


s = NNC['ID'].tolist()
s

# Identify the records in s

df_p15 = df_2015.loc[df_2015['PARID'].isin(s)]
df_p15

# Separate out a dataframe with just the rows that end with a letter
df15= df1[['PARID','OWNER1']]
df15c = df15[df15.index.isin(df_p15.index)]
df15c

#This step is to create the look up field from the new data frame, the two numbers before the ending letter.
df15c['PARID1'] = df15c['PARID'].str[-3:-1]
df15c

#Then I will join the look up table
df_15t =df15c.merge(df_LU.set_index('PARID'), left_on = 'PARID1', right_index = True)

df_15b = df_15t.groupby('Municipality').count()
df_15b
</code></pre>

<p>It wasn't until I finished that I realized how lengthy my code was for a seemingly simple task. If there is a better way to achieve, which is a sure thing, please let me know. Thanks.</p>
","['python', 'pandas', 'dataframe']",52602325,"<p>You can use pandas string methods to extract the last two numbers</p>

<pre><code>df1['PARID'].str.extract('.*(\d{2})', expand = False)
</code></pre>

<p>You get</p>

<pre><code>0    31
1    31
2    13
3    13
4    31
</code></pre>
",Pandas DataFrame How extract last two string type numbers column always end two numbers Sorry possible confusion title I trying I trying merge Parcels data frame Municipality Code look table The Parcels dataframe df head PARID OWNER B WILSON ROBERT JR B B COMUNALE MICHAEL J amp MARY ANN B D COMUNALE MICHAEL J amp MARY ANN B F MONROE amp JEFFERSON HOLDINGS LLC B NOEL JAMES H The Municipality Code dataframe df LU head PARID Municipality Allen Twp Bangor Bath Bethlehem Bethlehem Twp The last two numbers first column df B Municipality Code I need merge Municipality Code DataFrame But records records end letters shown PARID OWNER D F HOWARD THEODORE P amp CLAUDIA S F F KNEEBONE JUDY ANN F F KNEEBONE JUDY ANN F F KNITTER WILBERT D JR amp AMY J F F KNITTER DONALD For rows two numbers last letter Code I need extract like,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
52934512,2018-10-22,2018,2,Passing a header value to a get request in django,"<p>I want to pass a value through the Headers of a get request.</p>

<p>Im trying the below but it doesn't work, </p>

<pre><code>class ListCategoriesView(generics.ListAPIView):
""""""
Provides a get method handler.
""""""
serializer_class = CategorySerializer

def get(self, request, *args, **kwargs):
    token = request.data.get(""token"", """")
    if not token:
      """""" 
          do some action here
      """"""
    if not UserAccess.objects.filter(accessToken=token).exists():
      """""" 
          do some action here
      """"""
    else:
      """""" 
          do some action here
      """"""
</code></pre>

<p>I want to pass the token in the headers like that : </p>

<p><a href=""https://i.stack.imgur.com/zzQpy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zzQpy.png"" alt=""enter image description here""></a></p>

<p>can anyone help me with this issue,
thanks a lot in advance. </p>
","['python', 'django', 'django-rest-framework']",52934628,"<p>You said it yourself, you're passing it in the headers, so you need to get it from there. DRF does not do anything special to access the headers, so it proxies to the underlying Django HttpRequest object, which makes them available via the <a href=""https://docs.djangoproject.com/en/2.1/ref/request-response/#django.http.HttpRequest.META"" rel=""nofollow noreferrer""><code>META</code> attribute</a>, converted to uppercase and prefixed by <code>HTTP_</code>:</p>

<pre><code>token = request.META.get(""HTTP_TOKEN"", """")
</code></pre>
",Passing header value get request django I want pass value Headers get request Im trying work class ListCategoriesView generics ListAPIView Provides get method handler serializer class CategorySerializer def get self request args kwargs token request data get token token action UserAccess objects filter accessToken token exists action else action I want pass token headers like anyone help issue thanks lot advance,"startoftags, python, django, djangorestframework, endoftags",python django djangorestframework endoftags,python django djangorestframework,python django djangorestframework,1.0
53005444,2018-10-26,2018,3,Datetime milliseconds to seconds in Pandas,"<p>Have a datetime column in pandas dataframe with values like these:</p>

<pre><code>time
2018-04-11 22:18:30.122
2018-04-11 23:00:21.399
</code></pre>

<p>I'm wondering how can I round these values, get rid of milliseconds and represent only date, hour, minutes and 00 as seconds like this:</p>

<pre><code>time
2018-04-11 22:18:00
2018-04-11 23:00:00
</code></pre>
","['python', 'pandas', 'datetime']",53005477,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.floor.html"" rel=""nofollow noreferrer""><code>floor</code></a> with <code>T</code> for minutes for set <code>0 seconds</code>:</p>

<pre><code>#if necessary
#df['time'] = pd.to_datetime(df['time'])
df['time'] = df['time'].dt.floor('T')
#alternative solution
#df['time'] = df['time'].dt.floor('Min')

print (df)
                 time
0 2018-04-11 22:18:00
1 2018-04-11 23:00:00
</code></pre>

<p>I want <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.round.html"" rel=""nofollow noreferrer""><code>round</code></a> values time after <code>30sec</code> is changed to next one:</p>

<pre><code>df['time'] = df['time'].dt.round('T')
print (df)
                 time
0 2018-04-11 22:19:00
1 2018-04-11 23:00:00
</code></pre>
",Datetime milliseconds seconds Pandas Have datetime column pandas dataframe values like time I wondering I round values get rid milliseconds represent date hour minutes seconds like time,"startoftags, python, pandas, datetime, endoftags",python pandas numpy endoftags,python pandas datetime,python pandas numpy,0.67
53075436,2018-10-31,2018,2,Rolling sum of all values associated with the ids from two columns,"<p>Basically, I want it to groupby all ids from the two columns (['id1','id2]), and get the rolling sum (from past 2 rows) of their respective values from columns ['value1','value2']</p>

<p>df:</p>

<pre><code>id1     id2     value1    value2     
-----------------------------------
a       b       10        5            
c       a       5         10           
b       c       0         0                    
c       d       2         1            
d       a       10        20           
a       c       5         10             
b       a       10        5            
a       b       5         2                      
c       a       2         5            
d       b       5         2     
</code></pre>

<p>df (if df.id = 'a') -- just to simplify I'm showing with only id 'a':</p>

<pre><code>id1     id2     value1    value2    a.rolling.sum(2)  
-----------------------------------------------------
a       b       10        5         NaN   
c       a       5         10        20             
d       a       10        20        30   
a       c       5         10        25  
b       a       10        5         10
a       b       5         2         10            
c       a       2         5         10    
</code></pre>

<p>expect df (including all ids in columns ['id1','id2']):</p>

<pre><code>id1     id2     value1    value2       a.rolling.sum(2)     b.rolling.sum(2)      c.rolling.sum(2)
---------------------------------------------------------------------------------------------
a       b       10        5            NaN                 NaN               NaN
c       a       5         10           20                  NaN               NaN
b       c       0         0            NaN                 5                 5
c       d       2         1            NaN                 NaN               2
d       a       10        20           30                  NaN               NaN
a       c       5         10           25                  NaN               12
b       a       10        5            10                  10                NaN
a       b       5         2            10                  12                NaN
c       a       2         5            10                  NaN               12
d       b       5         2            NaN                 4                 NaN
</code></pre>

<p>Preferably I need a groupby function that assigns all ids involved with a x.rolling(2) as original dataset has hundreds of ids to compute.</p>
","['python', 'pandas', 'pandas-groupby']",53084321,"<h3>Reconfigure</h3>

<pre><code>i = df.filter(like='id')
i.columns = [i.columns.str[:2], i.columns.str[2:]]

v = df.filter(like='va')
v.columns = [v.columns.str[:5], v.columns.str[5:]]

d = i.join(v)

d

  id    value    
   1  2     1   2
0  a  b    10   5
1  c  a     5  10
2  b  c     0   0
3  c  d     2   1
4  d  a    10  20
5  a  c     5  10
6  b  a    10   5
7  a  b     5   2
8  c  a     2   5
9  d  b     5   2
</code></pre>

<hr>

<h3>Shuffle Stuff About</h3>

<pre><code>def modified_roll(x):
  return x.dropna().rolling(2).sum()


extra_bit = d.stack().set_index('id', append=True).unstack().value \
             .apply(modified_roll).groupby(level=0).first()

extra_bit

id     a     b     c     d
0    NaN   NaN   NaN   NaN
1   20.0   NaN   NaN   NaN
2    NaN   5.0   5.0   NaN
3    NaN   NaN   2.0   NaN
4   30.0   NaN   NaN  11.0
5   25.0   NaN  12.0   NaN
6   10.0  10.0   NaN   NaN
7   10.0  12.0   NaN   NaN
8   10.0   NaN  12.0   NaN
9    NaN   4.0   NaN  15.0
</code></pre>

<hr>

<h3><code>join</code></h3>

<pre><code>df.join(extra_bit)

  id1 id2  value1  value2     a     b     c     d
0   a   b      10       5   NaN   NaN   NaN   NaN
1   c   a       5      10  20.0   NaN   NaN   NaN
2   b   c       0       0   NaN   5.0   5.0   NaN
3   c   d       2       1   NaN   NaN   2.0   NaN
4   d   a      10      20  30.0   NaN   NaN  11.0
5   a   c       5      10  25.0   NaN  12.0   NaN
6   b   a      10       5  10.0  10.0   NaN   NaN
7   a   b       5       2  10.0  12.0   NaN   NaN
8   c   a       2       5  10.0   NaN  12.0   NaN
9   d   b       5       2   NaN   4.0   NaN  15.0
</code></pre>
",Rolling sum values associated ids two columns Basically I want groupby ids two columns id id get rolling sum past rows respective values columns value value df id id value value b c b c c c b b c b df df id simplify I showing id id id value value rolling sum b NaN c c b b c expect df including ids columns id id id id value value rolling sum b rolling sum c rolling sum b NaN NaN NaN c NaN NaN b c NaN c NaN NaN NaN NaN c NaN b NaN b NaN c NaN b NaN NaN Preferably I need groupby function assigns ids involved x rolling original dataset hundreds ids compute,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
53137822,2018-11-04,2018,2,Adding a field to a structured numpy array (4),"<p>This has been addressed before (<a href=""https://stackoverflow.com/questions/21413947/adding-a-field-to-a-structured-numpy-array-3"">here</a>, <a href=""https://stackoverflow.com/questions/5288736/adding-a-field-to-a-structured-numpy-array-2"">here</a> and <a href=""https://stackoverflow.com/questions/1201817/adding-a-field-to-a-structured-numpy-array"">here</a>). I want to add a new field to a structure array returned by numpy <code>genfromtxt</code> (also asked <a href=""https://stackoverflow.com/questions/40182466/is-it-possible-to-add-a-new-field-in-a-numpy-genfromtxt-output"">here</a>).</p>

<p>My new problem is that the csv file I'm reading has only a header line and a single data row:</p>

<p><code>output-Summary.csv</code>:
</p>

<pre><code>Wedge, DWD, Yield (wedge), Efficiency
1, 16.097825, 44283299.473156, 2750887.118836
</code></pre>

<p>I'm reading it via <code>genfromtxt</code> and calculate a new value <code>'tl'</code>:</p>

<pre class=""lang-python prettyprint-override""><code>test_out = np.genfromtxt('output-Summary.csv', delimiter=',', names=True)
tl = 300 / test_out['DWD']
</code></pre>

<p><code>test_out</code> looks like this:</p>

<pre class=""lang-python prettyprint-override""><code>array((1., 16.097825, 44283299.473156, 2750887.118836),
      dtype=[('Wedge', '&lt;f8'), ('DWD', '&lt;f8'), ('Yield_wedge', '&lt;f8'), ('Efficiency', '&lt;f8')])
</code></pre>

<p>Using <code>recfunctions.append_fields</code> (as suggested in the examples 1-3 above) fails over the use of <code>len()</code> for the size 1 array:</p>

<pre class=""lang-python prettyprint-override""><code>from numpy.lib import recfunctions as rfn
rfn.append_fields(test_out,'tl',tl)

TypeError: len() of unsized object
</code></pre>

<p>Searching for alternatives (one of the answers <a href=""https://stackoverflow.com/questions/21413947/adding-a-field-to-a-structured-numpy-array-3"">here</a>) I find that <code>mlab.rec_append_fields</code> works well (but is deprecated):</p>

<pre class=""lang-python prettyprint-override""><code>mlab.rec_append_fields(test_out,'tl',tl)

C:\ProgramData\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: MatplotlibDeprecationWarning: The rec_append_fields function was deprecated in version 2.2.
  """"""Entry point for launching an IPython kernel.
</code></pre>

<pre class=""lang-python prettyprint-override""><code>rec.array((1., 16.097825, 44283299.473156, 2750887.118836, 18.63605798),
          dtype=[('Wedge', '&lt;f8'), ('DWD', '&lt;f8'), ('Yield_wedge', '&lt;f8'), ('Efficiency', '&lt;f8'), ('tl', '&lt;f8')])
</code></pre>

<p>I can also copy the array over to a new structured array ""by hand"" as suggested <a href=""https://stackoverflow.com/questions/25427197/numpy-add-column-to-existing-structured-array"">here</a>. This works:</p>

<pre class=""lang-python prettyprint-override""><code>test_out_new = np.zeros(test_out.shape, dtype=new_dt)
for name in test_out.dtype.names:
    test_out_new[name]=test_out[name]
test_out_new['tl']=tl
</code></pre>

<p>So in summary - is there a way to get <code>recfunctions.append_fields</code> to work with the <code>genfromtxt</code> output from my single row csv file?
I would really rather use a standard way to handle this rather than a home brew..</p>
","['python', 'arrays', 'numpy']",53137925,"<p>Reshape the array (and new field) to size (1,).  With just one line, the <code>genfromtxt</code> is loading the data as a 0d array, shape ().  The <code>rfn</code> code isn't heavily used, and isn't a robust as it should be.  In other words, the 'standard way' is still bit buggy.</p>

<p>For example:</p>

<pre><code>In [201]: arr=np.array((1,2,3), dtype='i,i,i')
In [202]: arr.reshape(1)
Out[202]: array([(1, 2, 3)], dtype=[('f0', '&lt;i4'), ('f1', '&lt;i4'), ('f2', '&lt;i4')])

In [203]: rfn.append_fields(arr.reshape(1), 't1',[1], usemask=False)
Out[203]: 
array([(1, 2, 3, 1)],
      dtype=[('f0', '&lt;i4'), ('f1', '&lt;i4'), ('f2', '&lt;i4'), ('t1', '&lt;i8')])
</code></pre>

<p>Nothing wrong with the home_brew.  Most of the <code>rfn</code> functions use that mechanism - define a new dtype, create a recipient array with that dtype, and copy the fields over, name by name.  </p>
",Adding field structured numpy array This addressed I want add new field structure array returned numpy genfromtxt also asked My new problem csv file I reading header line single data row output Summary csv Wedge DWD Yield wedge Efficiency I reading via genfromtxt calculate new value tl test np genfromtxt output Summary csv delimiter names True tl test DWD test looks like array dtype Wedge lt f DWD lt f Yield wedge lt f Efficiency lt f Using recfunctions append fields suggested examples fails use len size array numpy lib import recfunctions rfn rfn append fields test tl tl TypeError len unsized object Searching alternatives one answers I find mlab rec append fields works well deprecated mlab rec append fields test tl tl C ProgramData Anaconda lib site packages ipykernel launcher py The rec append fields function deprecated version Entry point launching IPython kernel rec array dtype Wedge lt f,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
53163380,2018-11-05,2018,4,"In pygame &lt;alien invasion&gt;, why my aliens appear just one row?","<p>(instance.py)</p>

<pre><code>    import pygame
    from settings import Settings
    from ship import Ship
    import game_functions as gf

def run_game():
    pygame.init()
    ai_settings = Settings()
    screen = pygame.display.set_mode(
        (ai_settings.screen_width,ai_settings.screen_height))
    pygame.display.set_caption('Alien Invasion')
    ship = Ship(ai_settings,screen)
    bullets = pygame.sprite.Group()
    aliens = pygame.sprite.Group()

    gf.create_fleet(ai_settings,screen,ship,aliens)

    while 1:
        gf.check_events(ai_settings,screen,ship,bullets)
        ship.update()
        gf.update_bullet(bullets)
        gf.update_alien(ai_settings,aliens)
        gf.update_screen(ai_settings,screen,ship,bullets,aliens)

run_game()
</code></pre>

<p>(game_functions.py)</p>

<pre><code>import pygame
import sys
from bullet import Bullet
from alien import Alien

def check_events(ai_settings,screen,ship,bullets):
    for event in pygame.event.get():
        if event.type == pygame.KEYDOWN:
            check_keydown_events(ai_settings,screen,ship,bullets,event)
        if event.type == pygame.KEYUP:
            check_keyup_events(ship,event)

def check_keydown_events(ai_settings,screen,ship,bullets,event):
    if event.key == pygame.K_RIGHT:
        ship.moving['right'] = True
    elif event.key == pygame.K_LEFT:
        ship.moving['left'] = True
    elif event.key == pygame.K_UP:
        ship.moving['up'] = True
    elif event.key == pygame.K_DOWN:
        ship.moving['down'] = True
    elif event.key == pygame.K_SPACE:
        new_bullet = Bullet(ai_settings,screen,ship)
        bullets.add(new_bullet)
    elif event.key == pygame.K_q:
        sys.exit()

def check_keyup_events(ship,event):
    if event.key == pygame.K_RIGHT:
        ship.moving['right'] = False
    elif event.key == pygame.K_LEFT:
        ship.moving['left'] = False
    elif event.key == pygame.K_UP:
        ship.moving['up'] = False
    elif event.key == pygame.K_DOWN:
        ship.moving['down'] = False

def update_screen(ai_settings,screen,ship,bullets,aliens):
    screen.fill(ai_settings.bg_color)
    ship.blitme()
    for bullet in bullets.sprites():
        bullet.draw_bullet()
    aliens.draw(screen)
    pygame.display.flip()

def update_bullet(bullets):
    bullets.update()
    for bullet in bullets.copy():
        if bullet.rect.y &lt; 0:
            bullets.remove(bullet)

def create_fleet(ai_settings,screen,ship,aliens):
    sample_alien = Alien(ai_settings,screen)
    alien_width = sample_alien.rect.width
    alien_height = sample_alien.rect.height

    available_space_x = ai_settings.screen_width - 2 * alien_width
    numbers_alien_x = int(available_space_x / ( 2 * alien_width ))
    available_space_y = ai_settings.screen_height - 3 * alien_height - ship.rect.height
    numbers_alien_y = int(available_space_y / ( 2 * alien_height ))

    for number_alien_y in range(numbers_alien_y):
        for number_alien_x in range(numbers_alien_x):
            alien = Alien(ai_settings,screen)
            alien.rect.x = alien_width + 2 * alien_width * number_alien_x
            alien.rect.y = alien_height + 2 * alien_height * number_alien_y
            aliens.add(alien)

def update_alien(ai_settings,aliens):
    check_fleet_edges(ai_settings,aliens)
    aliens.update()

def check_fleet_edges(ai_settings,aliens):
    for alien in aliens.sprites():
        if alien.check_edges():
            change_fleet_direction(ai_settings,aliens)
            break

def change_fleet_direction(ai_settings,aliens):
    for alien in aliens.sprites():
        alien.rect.y += ai_settings.fleet_drop_speed
    ai_settings.fleet_direction *= -1
</code></pre>

<p>(settings.py)</p>

<pre><code>class Settings(object):
    def __init__(self):
        self.screen_width = 1200
        self.screen_height = 700
        self.bg_color = 230,230,230

        self.ship_speed_factor = 1.5

        self.bullet_width = 3
        self.bullet_height = 15
        self.bullet_color = 15,15,15
        self.bullet_speed_factor = 1

        self.alien_speed_factor = 2

        self.fleet_drop_speed = 10
        self.fleet_direction = 1
</code></pre>

<p>(ship.py)</p>

<pre><code>import pygame

class Ship(object):
    def __init__(self,ai_settings,screen):
        self.ai_settings = ai_settings
        self.screen = screen

        self.image = pygame.image.load('images/ship.bmp')
        self.rect = self.image.get_rect()
        self.screen_rect = self.screen.get_rect()

        self.rect.centerx = self.screen_rect.centerx
        self.rect.bottom = self.screen_rect.bottom

        self.moving = {'right':False,'left':False,'up':False,'down':False}
        self.centerx = float(self.rect.centerx)
        self.centery = float(self.rect.centery)
        self.speed = self.ai_settings.ship_speed_factor

    def update(self):
        if self.moving['right'] and self.rect.right &lt; self.ai_settings.screen_width:
            self.centerx += self.speed
        if self.moving['left'] and self.rect.left &gt; 0:
            self.centerx -= self.speed
        if self.moving['up'] and self.rect.top &gt; 0:
            self.centery -= self.speed
        if self.moving['down'] and self.rect.bottom &lt; self.ai_settings.screen_height:
            self.centery += self.speed
        self.rect.centerx = self.centerx
        self.rect.centery = self.centery

    def blitme(self):
        self.screen.blit(self.image,self.rect)
</code></pre>

<p>(bullet.py)</p>

<pre><code>import pygame
from random import randint

class Bullet(pygame.sprite.Sprite):
    def __init__(self,ai_settings,screen,ship):
        super().__init__()
        self.ai_settings = ai_settings
        self.screen = screen
        self.ship = ship

        self.rect = pygame.Rect(0,0,self.ai_settings.bullet_width,self.ai_settings.bullet_height)
        self.rect.top = self.ship.rect.top
        self.rect.centerx = self.ship.rect.centerx

        self.speed = self.ai_settings.bullet_speed_factor
        self.y = float(self.rect.y)

        self.color = randint(1,255),randint(1,255),randint(1,255)

    def update(self):
        self.y -= self.speed
        self.rect.y = self.y

    def draw_bullet(self):
        pygame.draw.rect(self.screen,self.color,self.rect)
</code></pre>

<p>(alien.py)</p>

<pre><code>import pygame

class Alien(pygame.sprite.Sprite):
    def __init__(self,ai_settings,screen):
        super().__init__()
        self.ai_settings = ai_settings
        self.screen = screen

        self.image = pygame.image.load('images/alien.bmp')
        self.rect = self.image.get_rect()

        self.speed = self.ai_settings.alien_speed_factor
        self.x = float(self.rect.x)
        self.y = float(self.rect.y)

    def check_edges(self):
        screen_rect = self.screen.get_rect()
        if self.rect.right &gt;= screen_rect.right:
            return True
        elif self.rect.left &lt;= 0:
            return True

    def update(self):
        self.x += (self.speed * self.ai_settings.fleet_direction)
        self.rect.x = self.x
</code></pre>

<p>My goal is create alien fleet in the whole screen.
Why is there just one row of aliens?</p>
","['python', 'python-3.x', 'pygame']",53163939,"<p>The problem is that in <code>Alien</code> you <code>update(self)</code> using the value of <code>self.x</code>, but you never set this value when you run <code>create_fleet(...)</code>. So all your aliens are there already, they are just all on top of each other!</p>

<p>A simple solution is to set <code>self.x</code> at the same time as you set <code>self.rect.x</code> for each <code>Alien</code> when you initialize them in <code>create_fleet(...)</code>. So instead of:</p>

<pre><code>alien.rect.x = alien_width + 2 * alien_width * number_alien_x
</code></pre>

<p>you would have:</p>

<pre><code>alien.rect.x = alien.x = alien_width + 2 * alien_width * number_alien_x
</code></pre>
",In pygame lt alien invasion gt aliens appear one row instance py import pygame settings import Settings ship import Ship import game functions gf def run game pygame init ai settings Settings screen pygame display set mode ai settings screen width ai settings screen height pygame display set caption Alien Invasion ship Ship ai settings screen bullets pygame sprite Group aliens pygame sprite Group gf create fleet ai settings screen ship aliens gf check events ai settings screen ship bullets ship update gf update bullet bullets gf update alien ai settings aliens gf update screen ai settings screen ship bullets aliens run game game functions py import pygame import sys bullet import Bullet alien import Alien def check events ai settings screen ship bullets event pygame event get event type pygame KEYDOWN check keydown events ai settings screen ship bullets event event type pygame KEYUP check keyup events ship event,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
53378420,2018-11-19,2018,2,How to perform mathematical operations on all CSV file columns &amp; rows using Pandas,"<p>Here is my code:</p>

<pre><code>all_data = pd.read_csv('data2.csv')
mu = np.array([all_data.mean(0)])
sigma = np.array([np.std(all_data,axis=0)])
print(all_data.shape)
print(mu.shape)
print(sigma.shape)
</code></pre>

<blockquote>
  <p>Output:</p>
  
  <p>(20, 24)</p>
  
  <p>(1, 24)</p>
  
  <p>(1, 24)</p>
</blockquote>

<p>Sigma and Mu are numpy array or matrix.</p>

<p>I want to perform this operaiton:</p>

<blockquote>
  <p>all_data = (all_data - mu)/sigma</p>
</blockquote>

<p>Here, first column (all rows), of <code>all_data</code> first gets substracted by first column of <code>mu</code> and then divided by first column of <code>sigma</code></p>

<p>second column (all rows) of <code>all_data</code> first gets substracted by second column of <code>mu</code> and then divided by second column of <code>sigma</code></p>

<p>Like that</p>
","['python', 'pandas', 'numpy']",53378936,"<p>If you work with the underlying numpy arrays of your dataframe, the broadcasting takes care of the work for you:</p>

<pre><code>(all_data.values - mu)/sigma
</code></pre>

<p>And put it back in a dataframe with the same columns/index as <code>all_data</code>:</p>

<pre><code>pd.DataFrame((all_data.values - mu)/sigma, columns=all_data.columns, index=all_data.index)
</code></pre>

<p><strong>Example</strong>:</p>

<p>On this mini dataframe:</p>

<pre><code>all_data = pd.DataFrame(np.random.randint(0,9,(5,5)))
&gt;&gt;&gt; all_data
   0  1  2  3  4
0  5  7  1  8  6
1  5  8  0  3  0
2  8  2  0  1  6
3  5  8  7  7  0
4  4  6  0  2  5
</code></pre>

<p>With:</p>

<pre><code>mu = np.array([all_data.mean(0)])
sigma = np.array([np.std(all_data,axis=0)])

&gt;&gt;&gt; mu
array([[5.6, 2. , 4. , 4.4, 7.6]])
&gt;&gt;&gt; sigma
array([[1.62480768, 1.26491106, 3.40587727, 2.41660919, 0.48989795]])
</code></pre>

<p>You can get:</p>

<pre><code>&gt;&gt;&gt; pd.DataFrame((all_data.values - mu)/sigma, columns=all_data.columns, index=all_data.index)
          0         1        2         3          4
0 -0.369274  3.952847 -0.88083  1.489691  -3.265986
1 -0.369274  4.743416 -1.17444 -0.579324 -15.513435
2  1.477098  0.000000 -1.17444 -1.406930  -3.265986
3 -0.369274  4.743416  0.88083  1.075888 -15.513435
4 -0.984732  3.162278 -1.17444 -0.993127  -5.307228
</code></pre>

<p>Feel free the check the math, but it satisfies your requirements: the operation is applied on the first column of the dataframe with the first values of <code>sigma</code> and <code>mu</code>, second column with second values, etc...</p>
",How perform mathematical operations CSV file columns amp rows using Pandas Here code data pd read csv data csv mu np array data mean sigma np array np std data axis print data shape print mu shape print sigma shape Output Sigma Mu numpy array matrix I want perform operaiton data data mu sigma Here first column rows data first gets substracted first column mu divided first column sigma second column rows data first gets substracted second column mu divided second column sigma Like,"startoftags, python, pandas, numpy, endoftags",python pandas numpy endoftags,python pandas numpy,python pandas numpy,1.0
53422006,2018-11-21,2018,2,how to do this operation in numpy (chaining of tiling operation)?,"<p>I'm trying to do fast generation of numpy array, possibly without passing through python.</p>

<p>I want to build an 1D index numpy array that would take this as an input:</p>

<p><code>[2,3]</code> and this <code>[2,4]</code> and would return this</p>

<pre><code> [0,1,0,1,0,1,2,0,1,2,0,1,2,0,1,2]
</code></pre>

<p>Explanation:</p>

<p>I iterate from 0 to 2 (so [0,1] array) and repeat it 2 times : <code>[0,1,0,1]</code></p>

<p>Then I iterate from 0 to 3 (so [0,1,2] array) and repeat it 4 times : <code>[0,1,2,0,1,2,0,1,2,0,1,2]</code>
Then I flattened everything.</p>

<p>Is there a way to do this fully in numpy?
For now I'm building each table separately in numpy by using np.tile() and flattening everything afterwards but I feel like there is a more efficient way that would only translate to C functions calls and no python</p>
","['python', 'arrays', 'numpy']",53422328,"<p>Here is a vectorized solution:</p>

<pre><code>def cycles(spec):
    steps = np.repeat(*spec)
    ps = steps.cumsum()
    psj = np.zeros(ps[-1], int)
    psj[ps[:-1]] = steps[:-1]
    return np.arange(ps[-1]) - psj.cumsum()
</code></pre>

<p>Demo:</p>

<pre><code>&gt;&gt;&gt; cycles(((2,3),(2,4)))
array([0, 1, 0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2])
</code></pre>
",operation numpy chaining tiling operation I trying fast generation numpy array possibly without passing python I want build D index numpy array would take input would return Explanation I iterate array repeat times Then I iterate array repeat times Then I flattened everything Is way fully numpy For I building table separately numpy using np tile flattening everything afterwards I feel like efficient way would translate C functions calls python,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
53541366,2018-11-29,2018,2,How to join two dataframes where IDs do not match and create new column to represent what dataframe ID came from?,"<p>I have two dataframes like this</p>

<p>df1:</p>

<pre><code>id    column1    column2 
1      30          90
2      1            2
</code></pre>

<p>df2:</p>

<pre><code>id    column1    column2 
1      30          90
3      1            2
</code></pre>

<p>I want to create logic that merges these two dataframes where IDs do not match (column names are the same) and then I want to create a new column that states what dataframe the ID came from. How would I do this? </p>

<p>Final merged df:</p>

<pre><code>id    column1    column2    df_name
2      30          90         df1
3      1            2         df2
</code></pre>

<p>edit:</p>

<p>could final df pull in all columns from both dataframes? </p>

<pre><code> id    column1.df1    column2.df1   column1.df2    column2.df2     df_name
    2      30          90                 30            90           df1
    3      1            2                  1             2           df2
</code></pre>
","['python', 'python-3.x', 'pandas', 'dataframe']",53541599,"<p>First <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a> DataFrames together:</p>

<pre><code>df = (pd.concat([df1, df2],  keys=('df1','df2'))
        .rename_axis(('df_name','idx'))
        .reset_index(level=1, drop=True)
        .reset_index())

print (df)
  df_name  id  column1  column2
0     df1   1       30       90
1     df1   2        1        2
2     df2   1       30       90
3     df2   3        1        2
</code></pre>

<p>Then get all same <code>id</code>:</p>

<pre><code>a = df1.merge(df2, on='id')['id']
</code></pre>

<p>And last filter by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.isin.html"" rel=""nofollow noreferrer""><code>isin</code></a>:</p>

<pre><code>df = df[~df['id'].isin(a)]
print (df)
  df_name  id  column1  column2
1     df1   2        1        2
3     df2   3        1        2
</code></pre>

<p>EDIT:</p>

<p>Similar solution like @W-B, only added parameter <code>id</code> and <code>suffixes</code>:</p>

<pre><code>df = (df1.merge(df2,indicator=True,how='outer', on='id', suffixes=('_df1','_df2'))
         .query(""_merge != 'both'""))
df['_merge'] = df['_merge'].map({'left_only':'df1','right_only':'df2'})

print (df)
   id  column1_df1  column2_df1  column1_df2  column2_df2 _merge
1   2          1.0          2.0          NaN          NaN    df1
2   3          NaN          NaN          1.0          2.0    df2
</code></pre>

<p>If want all rows, also rows with same <code>id</code>:</p>

<pre><code>df['_merge'] = df['_merge'].map({'left_only':'df1','right_only':'df2', 'both':'df1+df2'})

print (df)
   id  column1_df1  column2_df1  column1_df2  column2_df2   _merge
0   1         30.0         90.0         30.0         90.0  df1+df2
1   2          1.0          2.0          NaN          NaN      df1
2   3          NaN          NaN          1.0          2.0      df2
</code></pre>
",How join two dataframes IDs match create new column represent dataframe ID came I two dataframes like df id column column df id column column I want create logic merges two dataframes IDs match column names I want create new column states dataframe ID came How would I Final merged df id column column df name df df edit could final df pull columns dataframes id column df column df column df column df df name df df,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
53872956,2018-12-20,2018,3,KeyError while using MultiIndex slices,"<p>Although I was able to get around the issue, I wanted to understand why this error occurs..
<strong>DataFrame</strong></p>

<pre><code>import pandas as pd
import itertools

sl_df=pd.DataFrame(
    data=list(range(18)), 
    index=pd.MultiIndex.from_tuples(
        list(itertools.product(
            ['A','B','C'],
            ['I','II','III'],
            ['x','y']))),
    columns=['one'])
</code></pre>

<p>Out:</p>

<pre><code>         one
A I   x    0
      y    1
  II  x    2
      y    3
  III x    4
      y    5
B I   x    6
      y    7
  II  x    8
      y    9
  III x   10
      y   11
C I   x   12
      y   13
  II  x   14
      y   15
  III x   16
      y   17
</code></pre>

<p><strong>Simple slicing that works</strong></p>

<pre><code>sl_df.loc[pd.IndexSlice['A',:,'x']]
</code></pre>

<p>Out:</p>

<pre><code>         one
A I   x    0
  II  x    2
  III x    4
</code></pre>

<p><strong>The part that throws an error:</strong></p>

<pre><code>sl_df.loc[pd.IndexSlice[:,'II']]
</code></pre>

<p>Out:</p>

<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-6-4bfd2d65fd21&gt; in &lt;module&gt;()
----&gt; 1 sl_df.loc[pd.IndexSlice[:,'II']]

...\pandas\core\indexing.pyc in __getitem__(self, key)
   1470             except (KeyError, IndexError):
   1471                 pass
-&gt; 1472             return self._getitem_tuple(key)
   1473         else:
   1474             # we by definition only have the 0th axis

...\pandas\core\indexing.pyc in _getitem_tuple(self, tup)
    868     def _getitem_tuple(self, tup):
    869         try:
--&gt; 870             return self._getitem_lowerdim(tup)
    871         except IndexingError:
    872             pass

...\pandas\core\indexing.pyc in _getitem_lowerdim(self, tup)
    977         # we may have a nested tuples indexer here
    978         if self._is_nested_tuple_indexer(tup):
--&gt; 979             return self._getitem_nested_tuple(tup)
    980
    981         # we maybe be using a tuple to represent multiple dimensions here

...\pandas\core\indexing.pyc in _getitem_nested_tuple(self, tup)
   1056
   1057             current_ndim = obj.ndim
-&gt; 1058             obj = getattr(obj, self.name)._getitem_axis(key, axis=axis)
   1059             axis += 1
   1060

...\pandas\core\indexing.pyc in _getitem_axis(self, key, axis)
   1909
   1910         # fall thru to straight lookup
-&gt; 1911         self._validate_key(key, axis)
   1912         return self._get_label(key, axis=axis)
   1913

...\pandas\core\indexing.pyc in _validate_key(self, key, axis)
   1796                 raise
   1797             except:
-&gt; 1798                 error()
   1799
   1800     def _is_scalar_access(self, key):

...\pandas\core\indexing.pyc in error()
   1783                 raise KeyError(u""the label [{key}] is not in the [{axis}]""
   1784                                .format(key=key,
-&gt; 1785                                        axis=self.obj._get_axis_name(axis)))
   1786
   1787             try:

KeyError: u'the label [II] is not in the [columns]'
</code></pre>

<p><strong>The work around:</strong>( OR the proper way to do it when there is a ':' on the first level of the index.)</p>

<pre><code>sl_df.loc[pd.IndexSlice[:,'II'],:]
</code></pre>

<p>Out:</p>

<pre><code>        one
A II x    2
     y    3
B II x    8
     y    9
C II x   14
     y   15
</code></pre>

<p>Question: Why do we have to specify ':' on axis 1 only when we use ':' on first level of the MultiIndex? Wouldn't you agree that it is a bit quirky that it works on other levels but not on the first level of the MultiIndex (see simple slicing that works above)?</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",65038909,"<p>From the current version of the pandas documentation, it appears that indexing using slicers requires to specify both axes in the <code>.loc</code> method.</p>
<p><a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#using-slicers"" rel=""nofollow noreferrer"">See first warning here</a></p>
<p>The rationale is that without specifying both axes, it can be ambiguous along which axis selection is done.</p>
<p>I don't get exactly how pandas internals work, but on your specific case it looks like when you write <code>sl_df.loc[pd.IndexSlice[:,'II']]</code> the <code>:</code> is dispatched to row axis (i.e. select all rows) an the <code>'II'</code> to the columns, hence the error : <code>KeyError: u'the label [II] is not in the [columns]'</code>.</p>
",KeyError using MultiIndex slices Although I able get around issue I wanted understand error occurs DataFrame import pandas pd import itertools sl df pd DataFrame data list range index pd MultiIndex tuples list itertools product A B C I II III x columns one Out one A I x II x III x B I x II x III x C I x II x III x Simple slicing works sl df loc pd IndexSlice A x Out one A I x II x III x The part throws error sl df loc pd IndexSlice II Out KeyError Traceback recent call last lt ipython input bfd fd gt lt module gt gt sl df loc pd IndexSlice II pandas core indexing pyc getitem self key except KeyError IndexError pass gt return self getitem tuple key else definition th axis pandas core indexing pyc getitem tuple self tup def getitem tuple,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
53885404,2018-12-21,2018,3,Python Setting Values Without Loop,"<p>I have a time series dataframe where there is 1 or 0 in it (true/false). I wrote a function that loops through all rows with values 1 in them. Given user defined integer parameter called <code>n_hold</code>, I will set values 1 to n rows forward from the initial row.</p>

<p>For example, in the dataframe below I will be loop to row <code>2016-08-05</code>. If <code>n_hold = 2</code>, then I will set both <code>2016-08-08</code> and <code>2016-08-09</code> to 1 too.:</p>

<pre><code>2016-08-03    0
2016-08-04    0
2016-08-05    1
2016-08-08    0
2016-08-09    0
2016-08-10    0
</code></pre>

<p>The resulting <code>df</code> will then is</p>

<pre><code>2016-08-03    0
2016-08-04    0
2016-08-05    1
2016-08-08    1
2016-08-09    1
2016-08-10    0
</code></pre>

<p>The problem I have is this is being run 10s of thousands of times and my current solution where I am looping over rows where there are ones and subsetting is way too slow. I was wondering if there are any solutions to the above problem that is really fast.</p>

<p>Here is my (slow) solution, <code>x</code> is the initial signal dataframe:</p>

<pre><code>n_hold = 2
entry_sig_diff = x.diff()
entry_sig_dt = entry_sig_diff[entry_sig_diff == 1].index
final_signal = x * 0
for i in range(0, len(entry_sig_dt)):
    row_idx = entry_sig_diff.index.get_loc(entry_sig_dt[i])

    if (row_idx + n_hold) &gt;= len(x):
        break

    final_signal[row_idx:(row_idx + n_hold + 1)] = 1
</code></pre>
","['python', 'pandas', 'dataframe']",53885651,"<p>Completely changed answer, because working differently with consecutive <code>1</code> values:</p>

<p><strong>Explanation</strong>:</p>

<p>Solution remove each consecutive <code>1</code> first by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.where.html"" rel=""nofollow noreferrer""><code>where</code></a> with chained boolean mask by comparing with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.ne.html"" rel=""nofollow noreferrer""><code>ne</code></a> (not equal <code>!=</code>) with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.shift.html"" rel=""nofollow noreferrer""><code>shift</code></a> to <code>NaN</code>s, forward filling them by <code>ffill</code> with <code>limit</code> parameter and last replace <code>0</code> back:</p>

<pre><code>n_hold = 2
s = x.where(x.ne(x.shift()) &amp; (x == 1)).ffill(limit=n_hold).fillna(0, downcast='int')
</code></pre>

<p>Timings and comparing outputs:</p>

<pre><code>np.random.seed(123)
x = pd.Series(np.random.choice([0,1], p=(.8,.2), size=1000))
x1 = x.copy()
#print (x)


def orig(x):
    n_hold = 2
    entry_sig_diff = x.diff()
    entry_sig_dt = entry_sig_diff[entry_sig_diff == 1].index
    final_signal = x * 0
    for i in range(0, len(entry_sig_dt)):
        row_idx = entry_sig_diff.index.get_loc(entry_sig_dt[i])

        if (row_idx + n_hold) &gt;= len(x):
            break

        final_signal[row_idx:(row_idx + n_hold + 1)] = 1
    return final_signal

#print (orig(x))
</code></pre>

<hr>

<pre><code>n_hold = 2
s = x.where(x.ne(x.shift()) &amp; (x == 1)).ffill(limit=n_hold).fillna(0, downcast='int')
#print (s)

df = pd.concat([x,orig(x1), s], axis=1, keys=('input', 'orig', 'new'))
print (df.head(20))
    input  orig  new
0       0     0    0
1       0     0    0
2       0     0    0
3       0     0    0
4       0     0    0
5       0     0    0
6       1     1    1
7       0     1    1
8       0     1    1
9       0     0    0
10      0     0    0
11      0     0    0
12      0     0    0
13      0     0    0
14      0     0    0
15      0     0    0
16      0     0    0
17      0     0    0
18      0     0    0
19      0     0    0

#check outputs
#print (s.values == orig(x).values)
</code></pre>

<p><strong>Timings</strong>:</p>

<pre><code>%timeit (orig(x))
24.8 ms Â± 653 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

%timeit x.where(x.ne(x.shift()) &amp; (x == 1)).ffill(limit=n_hold).fillna(0, downcast='int')
1.36 ms Â± 12.7 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
</code></pre>
",Python Setting Values Without Loop I time series dataframe true false I wrote function loops rows values Given user defined integer parameter called n hold I set values n rows forward initial row For example dataframe I loop row If n hold I set The resulting df The problem I run thousands times current solution I looping rows ones subsetting way slow I wondering solutions problem really fast Here slow solution x initial signal dataframe n hold entry sig diff x diff entry sig dt entry sig diff entry sig diff index final signal x range len entry sig dt row idx entry sig diff index get loc entry sig dt row idx n hold gt len x break final signal row idx row idx n hold,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
54056992,2019-01-05,2019,2,Making new txt file with size info,"<p>I would like to create new csv or txt file with information of input and output file size(for all file in files) for example: SIZE OF INPUT FILE IS 42, SIZE OF OUTPUT FILE IS 320, all this according previous part of my code. Thank you :D</p>

<pre><code>files = glob.glob('*.csv')

for file in files:

    df = pd.read_csv(file, header= None)

    df1 = df.iloc[:, :4].agg(['sum','max','std'])

    df1.columns = range(1, len(df1.columns) + 1)

    s = df1.stack()

    L = ['{} of the {}. column is {}'.format(a, b, c) for (a, b), c in s.items()]

    output_file_name = ""output_"" + file

    pd.Series(L).to_csv(output_file_name, index=False)
</code></pre>
","['python', 'pandas', 'dataframe']",54059128,"<p>IIUC, for getting file size , you can use the <code>os</code> module:</p>

<pre><code>Using os.path.getsize:

&gt;&gt;&gt; import os
&gt;&gt;&gt; s = os.path.getsize(r'C:\path\test.txt')
&gt;&gt;&gt; s
6551
</code></pre>

<p>or :</p>

<pre><code>os.stat(r'C:\path\test.txt').st_size
6551
</code></pre>

<p>The result returned is in bytes.</p>
",Making new txt file size info I would like create new csv txt file information input output file size file files example SIZE OF INPUT FILE IS SIZE OF OUTPUT FILE IS according previous part code Thank D files glob glob csv file files df pd read csv file header None df df iloc agg sum max std df columns range len df columns df stack L column format b c b c items output file name output file pd Series L csv output file name index False,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
54065329,2019-01-06,2019,2,Writing with different fonts in pygame,"<p>I'm trying to use this font I grabbed from the internet in an opening screen in pygame. Here's the function I used, but no matter what I used as the font name it doesn't change what appears. After using <code>pygame.font.SysFont</code> it stopped giving not found error. All I want is to use this other font </p>

<pre><code>def text_objects(text, font):
    textSurface = font.render(text, True, WHITE)
    return textSurface, textSurface.get_rect()

def message_display(text):
    largeText = pygame.font.SysFont('Minecraftia.ttf',115)
    TextSurf, TextRect = text_objects(text, largeText)
    TextRect.center = ((HEIGHT/2),(WIDTH/2))
    screen.blit(TextSurf, TextRect)
</code></pre>
","['python', 'python-3.x', 'pygame']",54069682,"<p>You should use the <a href=""https://www.pygame.org/docs/ref/freetype.html#pygame.freetype.Font"" rel=""nofollow noreferrer""><code>freetype</code></a> module instead of the <a href=""https://www.pygame.org/docs/ref/font.html"" rel=""nofollow noreferrer""><code>font</code></a> module. It's a replacement for <code>font</code> and better in every way.</p>

<p>Both modules have two classes for representing fonts: </p>

<p><code>SysFont</code>: create a Font object from the system fonts  </p>

<p><code>Font</code>: create a new Font instance from a supported font file.</p>

<p>In your case, since you want to load a font from a font file instead of using a system font, use the <code>Font</code> class instead of <code>SysFont</code>.</p>
",Writing different fonts pygame I trying use font I grabbed internet opening screen pygame Here function I used matter I used font name change appears After using pygame font SysFont stopped giving found error All I want use font def text objects text font textSurface font render text True WHITE return textSurface textSurface get rect def message display text largeText pygame font SysFont Minecraftia ttf TextSurf TextRect text objects text largeText TextRect center HEIGHT WIDTH screen blit TextSurf TextRect,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
54113268,2019-01-09,2019,2,Why isn&#39;t &quot;sort_values&quot; working properly?,"<p>I'm trying to print values in <code>target_playlist</code>. The problem is that I want to order values in <code>target_playlist</code> by  <code>percentuali</code> column and I used <code>target_playlist.sort_values('percentuali', inplace=True, ascending=False)</code>
Before the  <code>sort_values</code> function, the result of:</p>

<pre><code>print(""{}"".format(target_playlist['percentuali'][i]))
</code></pre>

<p>are:</p>

<pre><code>0.7010264012452779
0.19662758090847976
0.6508863154849628
0.557740362863367
0.47418798688188313
0.6634307395184526
0.17661982395954637
0.6334661569944786
0.5226247859195567
0.37647399781797003
0.6107562358792401
0.10866013071895426
0.6259167928556538
0.5107723732317271
0.5107723732317271
0.440188723891383
0.473270990299173
0.5807994015581672
0.45540535868625753
0.4156854080449265
0.5659237264842225
0.5942257114281826
0.5763053500588216
0.43676171660260443
0.6947640279542424
0.37155299947773396
0.6055124707313475
0.6642522917728619
0.6339323841512609
0.6836084778718268
0.4585485761594801
0.7687767193517359
0.7739306342996543
0.6792746883779797
0.5688985142793829
0.5763507447689178
0.6265388222033668
0.5262211637961803
0.631776719351736
0.7016345319242638
0.6549247063300238
0.6218895455057429
0.3926510809451985
0.5081035167373568
0.6149459682682933
0.44069739392952245
0.46799465192894985
0.69161263493496
0.5534053586862575
0.6968509819258842
0.4988988577428972
0.5059165111353879
0.7355655050414504
0.6792746883779797
0.4401208506283063
0.49320548887003335
0.5112768045242271
0.7361528565218765
0.2329438202247191
0.6123902228073447
0.49864712823852325
0.6909989415739581
0.6754433860184025
0.566520509644565
0.37663089180304893
0.6529677236233883
0.6089596366830047
0.7687767193517359
0.6101347817993262
0.7559795411177228
</code></pre>

<p>While, when I print values after I have called <code>sort_values</code>, they are:</p>

<pre><code>Titolo: Possibili Scenari,  Artista:  Cesare Cremonini,  Probabilita: 0.7559795411177228 
Titolo: Shallow,  Artista:  Lady Gaga,  Probabilita: 0.7559795411177228 
Titolo: To the Trees,  Artista:  An Early Bird,  Probabilita: 0.7559795411177228 
Titolo: If You Wanna Love Somebody - Acoustic,  Artista:  Tom Odell,  Probabilita: 0.7559795411177228 
Titolo: Happier - Acoustic,  Artista:  Ed Sheeran,  Probabilita: 0.7559795411177228 
Titolo: Lie With Me,  Artista:  Josiah and the Bonnevilles,  Probabilita: 0.7559795411177228 
Titolo: Jubilee Road,  Artista:  Tom Odell,  Probabilita: 0.7559795411177228 
Titolo: I'll Never Love Again - Film Version,  Artista:  Lady Gaga,  Probabilita: 0.7559795411177228 
Titolo: Rise - Acoustic,  Artista:  Jonas Blue,  Probabilita: 0.7559795411177228 
Titolo: Hold My Girl,  Artista:  George Ezra,  Probabilita: 0.7559795411177228 
Titolo: Love Someone,  Artista:  Lukas Graham,  Probabilita: 0.7559795411177228 
Titolo: Angels,  Artista:  Tom Walker,  Probabilita: 0.7559795411177228 
Titolo: These Days (feat. Jess Glynne, Macklemore &amp; Dan Caplen) - Acoustic,  Artista:  Rudimental,  Probabilita: 0.7559795411177228 
Titolo: Just For Tonight - Acoustic,  Artista:  James Bay,  Probabilita: 0.7559795411177228 
Titolo: Perfect,  Artista:  Ed Sheeran,  Probabilita: 0.7559795411177228 
Titolo: No Roots,  Artista:  Joshua Hyslop,  Probabilita: 0.7559795411177228 
Titolo: Slide,  Artista:  James Bay,  Probabilita: 0.7559795411177228 
Titolo: Be Your Man,  Artista:  Rhys Lewis,  Probabilita: 0.7559795411177228 
Titolo: No Matter What,  Artista:  Calum Scott,  Probabilita: 0.7559795411177228 
Titolo: Woes,  Artista:  Tom Rosenthal,  Probabilita: 0.7559795411177228 
Titolo: Barbed Wire (Acoustic),  Artista:  Tom Grennan,  Probabilita: 0.7559795411177228 
Titolo: Stay Awake with Me,  Artista:  Dan Owen,  Probabilita: 0.7559795411177228 
Titolo: Spent So Long,  Artista:  Jamie Harrison,  Probabilita: 0.7559795411177228 
Titolo: Tummy,  Artista:  Tamino,  Probabilita: 0.7559795411177228 
Titolo: LOVISA,  Artista:  FELIX SANDMAN,  Probabilita: 0.7559795411177228 
Titolo: Girl - Acoustic,  Artista:  SYML,  Probabilita: 0.7559795411177228 
Titolo: Party Of One (feat. Sam Smith),  Artista:  Brandi Carlile,  Probabilita: 0.7559795411177228 
Titolo: Electricity - Acoustic,  Artista:  Silk City,  Probabilita: 0.7559795411177228 
Titolo: Leftovers,  Artista:  Dennis Lloyd,  Probabilita: 0.7559795411177228 
Titolo: Hand That You Hold,  Artista:  Dan Owen,  Probabilita: 0.7559795411177228 
Titolo: Company (feat. Molly Hammar),  Artista:  Paul Rey,  Probabilita: 0.7559795411177228 
Titolo: Too Good At Goodbyes - Edit,  Artista:  Sam Smith,  Probabilita: 0.7559795411177228 
Titolo: Need You Now - Acoustic,  Artista:  Dean Lewis,  Probabilita: 0.7559795411177228 
Titolo: Such A Simple Thing,  Artista:  Ray LaMontagne,  Probabilita: 0.7559795411177228 
Titolo: Acoustic,  Artista:  Billy Raffoul,  Probabilita: 0.7559795411177228 
Titolo: Donât Matter To Me,  Artista:  Drake,  Probabilita: 0.7559795411177228 
Titolo: when the party's over,  Artista:  Billie Eilish,  Probabilita: 0.7559795411177228 
Titolo: Someone You Loved,  Artista:  Lewis Capaldi,  Probabilita: 0.7559795411177228 
Titolo: Collide,  Artista:  Tom Speight,  Probabilita: 0.7559795411177228 
Titolo: Fading Into Grey - Acoustic,  Artista:  Billy Lockett,  Probabilita: 0.7559795411177228 
Titolo: Never Let You Go (feat. John Newman) - Acoustic Version,  Artista:  Kygo,  Probabilita: 0.7559795411177228 
Titolo: T-Shirts,  Artista:  James Smith,  Probabilita: 0.7559795411177228 
Titolo: In My Head,  Artista:  Peter Manos,  Probabilita: 0.7559795411177228 
Titolo: Where Were You In The Morning?,  Artista:  Shawn Mendes,  Probabilita: 0.7559795411177228 
Titolo: come out and play,  Artista:  Billie Eilish,  Probabilita: 0.7559795411177228 
Titolo: Tear Me Down,  Artista:  Paul Rey,  Probabilita: 0.7559795411177228 
Titolo: Come As You Are,  Artista:  Imaginary Future,  Probabilita: 0.7559795411177228 
Titolo: Consequences - orchestra,  Artista:  Camila Cabello,  Probabilita: 0.7559795411177228 
Titolo: All I Am - Acoustic,  Artista:  Jess Glynne,  Probabilita: 0.7559795411177228 
</code></pre>

<p>This is the part of program I'm working on   </p>

<pre><code>import tkinter as tk                
from tkinter import font  as tkfont 
import pandas as pd 
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
import spotipy
import spotipy.util as util
from numpy import integer
from tkinter import Radiobutton
sp = spotipy.Spotify() 
from spotipy.oauth2 import SpotifyClientCredentials 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.cluster import KMeans
import itertools
import threading
import time
import sys
from operator import itemgetter, attrgetter, methodcaller

 target_playlist = pd.DataFrame(newPlaylist_features)

    if(algoritmo_scelto==1):
        pred = c.predict(target_playlist[features])
        p = c.predict_proba(target_playlist[features])
    if(algoritmo_scelto==2):
        pred = knn.predict(target_playlist[features])
        p = knn.predict_proba(target_playlist[features])
    if(algoritmo_scelto==3):
        pred = forest.predict(target_playlist[features])
        p = forest.predict_proba(target_playlist[features])
    if(algoritmo_scelto==4):
        pred = k_means.predict(target_playlist[features])
        p = k_means.predict_proba(target_playlist[features])

    likedSongs = 0
    i = 0

    for prediction in pred:
        target_playlist['percentuali'] = p[i][1]
        print(""{}"".format(target_playlist['percentuali'][i]))
        i = i +1


    target_playlist.sort_values('percentuali', inplace=True, ascending=False)

    i=0
    for prediction in pred:

        if(prediction == 1):
            print (""Titolo: "" + target_playlist[""song_title""][i] + "",  Artista:  ""+ target_playlist[""artist""][i] + "",  Probabilita: {} "".format(target_playlist[""percentuali""][i]))
            likedSongs= likedSongs + 1
        i = i +1
</code></pre>

<p>Where am I wrong?</p>
","['python', 'python-3.x', 'pandas']",54113506,"<p>In this loop, you are setting the <code>""target_playlist['percentuali']""</code> Series to a single value:</p>

<pre><code>i = 0

for prediction in pred:
    target_playlist['percentuali'] = p[i][1]
    print(""{}"".format(target_playlist['percentuali'][i]))
    i = i +1
</code></pre>

<p>Since <code>""target_playlist['percentuali'] = p[i][1]""</code> applies <code>""p[i][1]""</code> as the value to every row.</p>

<p>As show in this example:</p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; for i in [0, 1, 2]:
...     print(i)
...     df['this'] = i
...
0
1
2
&gt;&gt;&gt; df
   id   col_1  col_2  col_3  this
0   1    blue     15   True    2
1   2     red     25  False    2
2   3  orange     35  False    2
3   4  yellow     24   True    2
4   5   green     12   True    2
</code></pre>

<h3>Fix:</h3>

<p>I don't know the object <code>p</code> but you should turn the results into a <code>pd.Series</code>.
You can revise <em>that</em> whole loop to something like this:</p>

<pre><code>target_playlist['percentuali'] = pd.Series(item[1] for item in p)
print(target_playlist['percentuali'])
</code></pre>

<p>After you have called <code>sort_values</code> on your DataFrame your values won't print in descending order since you are referencing the rows by the index <code>e.g. (0, 1, 2)</code>.</p>

<p>You can do a quick fix by resetting the index, see my example below:</p>

<pre><code>&gt;&gt;&gt; df.sort_values('col_2', inplace=True, ascending=False)
&gt;&gt;&gt; df
   id   col_1  col_2  col_3
2   3  orange     35  False
1   2     red     25  False
3   4  yellow     24   True
0   1    blue     15   True
4   5   green     12   True
&gt;&gt;&gt; df['col_2'][0]
15
&gt;&gt;&gt; df.reset_index(inplace=True)
&gt;&gt;&gt; df['col_2'][0]
35
</code></pre>

<h3>Looping over dataframe rows</h3>

<p>Instead of referencing by the index, you can loop through the rows like so:</p>

<pre><code>for _, row in df.iterrows():
    print(""Title: {}, Artist: {}, Probability: {}"".format(
        row['song_title'], row['artist'], row['percentuali']
    ))
</code></pre>
",Why quot sort values quot working properly I trying print values target playlist The problem I want order values target playlist percentuali column I used target playlist sort values percentuali inplace True ascending False Before sort values function result print format target playlist percentuali While I print values I called sort values Titolo Possibili Scenari Artista Cesare Cremonini Probabilita Titolo Shallow Artista Lady Gaga Probabilita Titolo To Trees Artista An Early Bird Probabilita Titolo If You Wanna Love Somebody Acoustic Artista Tom Odell Probabilita Titolo Happier Acoustic Artista Ed Sheeran Probabilita Titolo Lie With Me Artista Josiah Bonnevilles Probabilita Titolo Jubilee Road Artista Tom Odell Probabilita Titolo I Never Love Again Film Version Artista Lady Gaga Probabilita Titolo Rise Acoustic Artista Jonas Blue Probabilita Titolo Hold My Girl Artista George Ezra Probabilita Titolo Love Someone Artista Lukas Graham Probabilita Titolo Angels Artista Tom Walker Probabilita Titolo These Days feat Jess,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
54184055,2019-01-14,2019,2,Create 3D NumPy array with sequential number,"<p>I wanted to create a 3D NumPy array with sequential numbers like so:</p>

<pre><code>[[[11 27 43]
  [12 28 44]
  [13 29 45]
  [14 30 46]]

 [[15 31 47]
  [16 32 48]
  [17 33 49]
  [18 34 50]]

 [[19 35 51]
  [20 36 52]
  [21 37 53]
  [22 38 54]]

 [[23 39 55]
  [24 40 56]
  [25 41 57]
  [26 42 58]]]
</code></pre>

<p>I did this: <code>A = np.arange(11, 59).reshape((4, 4, 3))</code> but I got this instead:</p>

<pre><code>[[[11 12 13]
  [14 15 16]
  [17 18 19]
  [20 21 22]]

 [[23 24 25]
  [26 27 28]
  [29 30 31]
  [32 33 34]]

 [[35 36 37]
  [38 39 40]
  [41 42 43]
  [44 45 46]]

 [[47 48 49]
  [50 51 52]
  [53 54 55]
  [56 57 58]]]
</code></pre>

<p>So it's not the sequence that I wanted. I had done some additional steps to get the correct 3D array. First, I shaped the numbers into a 2D array: <code>A = np.arange(11, 59).reshape((-1, 4))</code> to get this:</p>

<pre><code>[[11 12 13 14]
 [15 16 17 18]
 [19 20 21 22]
 [23 24 25 26]
 [27 28 29 30]
 [31 32 33 34]
 [35 36 37 38]
 [39 40 41 42]
 [43 44 45 46]
 [47 48 49 50]
 [51 52 53 54]
 [55 56 57 58]]
</code></pre>

<p>Then, I splitted and stacked the 2D array and got the 3D array that I wanted: <code>A = np.dstack(np.vsplit(A, 3))</code></p>

<pre><code>[[[11 27 43]
  [12 28 44]
  [13 29 45]
  [14 30 46]]

 [[15 31 47]
  [16 32 48]
  [17 33 49]
  [18 34 50]]

 [[19 35 51]
  [20 36 52]
  [21 37 53]
  [22 38 54]]

 [[23 39 55]
  [24 40 56]
  [25 41 57]
  [26 42 58]]]
</code></pre>

<p>Now I'm wondering if there is a more elegant and straightforward way to achieve the same result. Thanks you.</p>
","['python', 'arrays', 'numpy']",54184146,"<p>Get the ranged array, reshape and then permute axes -</p>

<pre><code>np.arange(11, 59).reshape(3,4,4).transpose(1,2,0)
</code></pre>

<p>Another way to permute axes would be to use <code>np.moveaxis</code> -</p>

<pre><code>np.moveaxis(np.arange(11, 59).reshape(3,4,4),0,2)
</code></pre>

<p><a href=""https://stackoverflow.com/a/47978032/3293881""><code>Discussion : A general intuition to solving such problems.</code></a></p>
",Create D NumPy array sequential number I wanted create D NumPy array sequential numbers like I A np arange reshape I got instead So sequence I wanted I done additional steps get correct D array First I shaped numbers D array A np arange reshape get Then I splitted stacked D array got D array I wanted A np dstack np vsplit A Now I wondering elegant straightforward way achieve result Thanks,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
54249706,2019-01-18,2019,2,Split list into sublists of elements between opening and closing tokens,"<p>I have a list of strings (see below). I want to get the elements in the list by looking for two specific tokens (begin and end) and then save all the strings present between those tokens.</p>

<p>For example, I have below list and I want to get all strings between any occurrence of the strings <code>'RATED'</code> and <code>'Like'</code>. There can be multiple occurrences of these subsequences, too. </p>

<pre><code>['RATED',
 '  Awesome food at a good price .',
 'Delivery was very quick even on New Year\xe2\x80\x99s Eve .',
 'Please try crispy corn and veg noodles From this place .',
 'Taste maintained .',
 'Like',
 '1',
 'Comment',
 '0',
 'Share',
 'Divyansh Agarwal',
 '1 Review',
 'Follow',
 '3 days ago',
 'RATED',
 '  I have tried schezwan noodles and the momos with kitkat shake',
 ""And I would say just one word it's best for the best reasonable rates.... Gotta recommend it to everyone"",
 'Like']
</code></pre>

<p>I have tried different methods, like regex, but none solved the problem.</p>
","['python', 'python-3.x', 'list']",54250280,"<p>using regex you can do this is way .</p>

<pre><code>a= ['RATED','  Awesome food at a good price .', 
 'Delivery was very quick even on New Yearâs Eve .', 
 'Please try crispy corn and veg noodles From this place .', 
 'Taste maintained .', 'Like', '1', 'Comment', '0', 
 'Share', 'Divyansh Agarwal', '1 Review', 'Follow', 
 '3 days ago', 'RATED', 
 '  I have tried schezwan noodles and the momos with kitkat shake', ""And I would say just one word it's best for the best reasonable rates.... Gotta recommend it to everyone"", 
 'Like']


import re
string = ' '.join(a)
b = re.compile(r'(?&lt;=RATED).*?(?=Like)').findall(string)
print(b)
</code></pre>

<p>output </p>

<pre><code>['   Awesome food at a good price . Delivery was very quick even on New Yearâs Eve . Please try crispy corn and veg noodles From this place . Taste maintained . ',
 ""   I have tried schezwan noodles and the momos with kitkat shake And I would say just one word it's best for the best reasonable rates.... Gotta recommend it to everyone ""]
</code></pre>
",Split list sublists elements opening closing tokens I list strings see I want get elements list looking two specific tokens begin end save strings present tokens For example I list I want get strings occurrence strings RATED Like There multiple occurrences subsequences RATED Awesome food good price Delivery quick even New Year xe x x Eve Please try crispy corn veg noodles From place Taste maintained Like Comment Share Divyansh Agarwal Review Follow days ago RATED I tried schezwan noodles momos kitkat shake And I would say one word best best reasonable rates Gotta recommend everyone Like I tried different methods like regex none solved problem,"startoftags, python, python3x, list, endoftags",python python3x list endoftags,python python3x list,python python3x list,1.0
54251653,2019-01-18,2019,3,How to set the value of a certain field in a pandas dataframe?,"<p>Lets say I have a pandas dataframe like so:</p>

<pre><code>d = {'col1': [1, 2, 3, 4], 'col2': ['','','','']}
df = pd.DataFrame(data=d)
</code></pre>

<p>and for some reason, I must loop over all its rows and give <code>col2</code> a certain value. My loop is now in <code>i=1</code> and <code>col2</code> should be <code>'Check!'</code>.</p>

<p>Easy, it seems, just</p>

<pre><code>df.iloc[i]['col2']='Check'
</code></pre>

<p>however, this throws a warning, and <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy"" rel=""nofollow noreferrer"">a link</a> and it seems like this is warranted, since <code>df.iloc[i]['col2']</code> just returns my empy <code>''</code> instead of the <code>'Check!'</code> it should.</p>

<p>Almost the same question is asked (and solved) <a href=""https://stackoverflow.com/questions/40138090/work-with-a-row-in-a-pandas-dataframe-without-incurring-chain-indexing-not-copi"">here</a>, however, the key <code>.is_copy</code> command is now deprecated, it appears (I don't get the error everytime, weirdâ¦), thus I'm hesitant to use it.</p>

<p>So</p>

<pre><code>ri = df.iloc[i]
ri.is_copy = False
newval = 'Check!'
ri['col2']=newval
df.iloc[i]=ri
</code></pre>

<p>works and is nice and simple, but apparently, it will soon cease to work, so this is not optimal.</p>

<p><strong>EDIT</strong></p>

<p>Maybe it makes sense to have the warning in the question, in order to make it more clear and increase its googleability:</p>

<pre><code>In [1]: df.iloc[i]['col2']='Check'
Out[1]: /some/path/ipython:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
</code></pre>
","['python', 'pandas', 'dataframe']",54251681,"<p>Here is necessary set by positions by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iat.html"" rel=""nofollow noreferrer""><code>DataFrame.iat</code></a>, but necessary get position by column by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.get_loc.html"" rel=""nofollow noreferrer""><code>get_loc</code></a>:</p>

<pre><code>i = 1
df.iat[i, df.columns.get_loc('col2')]='Check'
#slowier
#df.iloc[i, df.columns.get_loc('col2')]='Check'
</code></pre>

<p>Or set by labels with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.at.html"" rel=""nofollow noreferrer""><code>DataFrame.at</code></a>:</p>

<pre><code>i = 1
df.at[df.index[i], 'col2']='Check'
#slowier
#df.loc[df.index[i], 'col2']='Check'
</code></pre>

<p>For default RangeIndex:</p>

<pre><code>df.at[i, 'col2']='Check'
</code></pre>

<hr>

<pre><code>print (df)
   col1   col2
0     1       
1     2  Check
2     3       
3     4       
</code></pre>
",How set value certain field pandas dataframe Lets say I pandas dataframe like col col df pd DataFrame data reason I must loop rows give col certain value My loop col Check Easy seems df iloc col Check however throws warning link seems like warranted since df iloc col returns empy instead Check Almost question asked solved however key copy command deprecated appears I get error everytime weird thus I hesitant use So ri df iloc ri copy False newval Check ri col newval df iloc ri works nice simple apparently soon cease work optimal EDIT Maybe makes sense warning question order make clear increase googleability In df iloc col Check Out path ipython A value trying set copy slice DataFrame See caveats documentation http pandas pydata org pandas docs stable indexing html indexing view versus copy,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
54459554,2019-01-31,2019,4,numpy: find index in sorted array (in an efficient way),"<p>I would like to sort a numpy array and find out where each element went.</p>

<p><a href=""https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.argsort.html#numpy.argsort"" rel=""nofollow noreferrer""><code>numpy.argsort</code></a> will tell me for each index in the sorted array, which index in the unsorted array goes there. I'm looking for something like the inverse: For each index in the unsorted array, where does it go in the sorted array.</p>

<pre><code>a = np.array([1, 4, 2, 3])

# a sorted is [1,2,3,4]
# the 1 goes to index 0
# the 4 goes to index 3
# the 2 goes to index 1
# the 3 goes to index 2

# desired output
[0, 3, 1, 2]

# for comparison, argsort output
[0, 2, 3, 1]
</code></pre>

<p>A simple solution uses <a href=""https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.searchsorted.html"" rel=""nofollow noreferrer""><code>numpy.searchsorted</code></a></p>

<pre><code>np.searchsorted(np.sort(a), a)
# produces [0, 3, 1, 2]
</code></pre>

<p>I'm unhappy with this solution, because it seems very inefficient. It sorts and searches in two separate steps.</p>

<p>This fancy indexing fails for arrays with duplicates, look at:</p>

<pre><code>a = np.array([1, 4, 2, 3, 5])
print(np.argsort(a)[np.argsort(a)])
print(np.searchsorted(np.sort(a),a))


a = np.array([1, 4, 2, 3, 5, 2])
print(np.argsort(a)[np.argsort(a)])
print(np.searchsorted(np.sort(a),a))
</code></pre>
","['python', 'arrays', 'numpy']",54459996,"<p>You just need to <a href=""https://stackoverflow.com/q/11649577"">invert the permutation</a> that sorts the array. As shown in the linked question, you can do that like this:</p>

<pre><code>import numpy as np

def sorted_position(array):
    a = np.argsort(array)
    a[a.copy()] = np.arange(len(a))
    return a

print(sorted_position([0.1, 0.2, 0.0, 0.5, 0.8, 0.4, 0.7, 0.3, 0.9, 0.6]))
# [1 2 0 5 8 4 7 3 9 6]
</code></pre>
",numpy find index sorted array efficient way I would like sort numpy array find element went numpy argsort tell index sorted array index unsorted array goes I looking something like inverse For index unsorted array go sorted array np array sorted goes index goes index goes index goes index desired output comparison argsort output A simple solution uses numpy searchsorted np searchsorted np sort produces I unhappy solution seems inefficient It sorts searches two separate steps This fancy indexing fails arrays duplicates look np array print np argsort np argsort print np searchsorted np sort np array print np argsort np argsort print np searchsorted np sort,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
54489274,2019-02-02,2019,3,Tuple of tuples not allowed in pandas.DataFrame constructor,"<p>Working with pandas dataframe in python3, I tried to call dataframe constructor on tuple of tuples. It resulted in an improper constructor call error. A quick reference to documentation of pandas.DataFrame revealed that data parameter can be initialized with numpy ndarray (structured or homogeneous), dict, or DataFrame, Dict can contain Series, arrays, constants, or list-like objects.
I'm unable to reckon the reason for tuple of tuples being invalid and list of tuples being valid.</p>

<p>I converted the tuple of tuples into list of tuples, and it saved my ass.</p>

<pre><code>batch_computer_science = ('r1', 'r2', 'r3', 'r4') #roll number of students
batch_mechanical_engg = ('a1', 'a2', 'a3', 'a4') #roll number of students
session_2018 = (batch_computer_science, batch_mechanical_engg)

#In the actual code there are 8 types of batches with 30 students each, sorted in order of registration in the class.`

session_df = pd.DataFrame(session_2018) # This throws an error, improper constructor called.
</code></pre>

<p>I expected tuple of tuples to work, but list of tuples work, tuple of tuples don't.</p>
","['python', 'pandas', 'dataframe']",54489323,"<p>You're right. Lists of tuples work, tuples of tuples don't work.</p>

<p>As of v0.23.4, <a href=""https://github.com/pandas-dev/pandas/blob/v0.23.4/pandas/core/frame.py#L380"" rel=""nofollow noreferrer"">the source</a> for the <a href=""https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html"" rel=""nofollow noreferrer""><code>pd.DataFrame</code></a> class indicates <code>list</code> is considered as a special case:</p>

<pre><code>elif isinstance(data, (list, types.GeneratorType)):
    if isinstance(data, types.GeneratorType):
        data = list(data)
    if len(data) &gt; 0:
        if is_list_like(data[0]) and getattr(data[0], 'ndim', 1) == 1:
           # ....
</code></pre>

<p>So just use a list of tuples:</p>

<pre><code>session_df = pd.DataFrame(list(session_2018))
</code></pre>

<p>Remember, Pandas is an API, not a programming language. The details here are liable to change without notice.</p>
",Tuple tuples allowed pandas DataFrame constructor Working pandas dataframe python I tried call dataframe constructor tuple tuples It resulted improper constructor call error A quick reference documentation pandas DataFrame revealed data parameter initialized numpy ndarray structured homogeneous dict DataFrame Dict contain Series arrays constants list like objects I unable reckon reason tuple tuples invalid list tuples valid I converted tuple tuples list tuples saved ass batch computer science r r r r roll number students batch mechanical engg roll number students session batch computer science batch mechanical engg In actual code types batches students sorted order registration class session df pd DataFrame session This throws error improper constructor called I expected tuple tuples work list tuples work tuple tuples,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
54542470,2019-02-05,2019,7,How to keep columns based on a given row values,"<p>Here how the datalooks like in <code>df</code> dataframe:</p>

<pre><code>        A   B   C   D
0.js    2   1   1  -1
1.js    3  -5   1  -4
total   5  -4   2  -5
</code></pre>

<p>And I would get new dataframe <code>df1</code>:</p>

<pre><code>        A     C
0.js    2     1
1.js    3     1
total   5     2
</code></pre>

<p>So basically it should look like this:
<code>df1 = df[df[""total""] &gt; 0]</code> 
but it should filter on row instead of column and I can't figure it out..</p>
","['python', 'pandas', 'dataframe']",54542813,"<p>You can use, loc with boolean indexing or reindex:</p>

<pre><code>df.loc[:, df.columns[(df.loc['total'] &gt; 0)]]
</code></pre>

<p>OR</p>

<pre><code>df.reindex(df.columns[(df.loc['total'] &gt; 0)], axis=1)
</code></pre>

<p>Output:</p>

<pre><code>       A  C
0.js   2  1
1.js   3  1
total  5  2
</code></pre>
",How keep columns based given row values Here datalooks like df dataframe A B C D js js total And I would get new dataframe df A C js js total So basically look like df df df total gt filter row instead column I figure,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
54602838,2019-02-09,2019,4,regex: \w EXCEPT underscore (add to class and then exclude from class),"<p>This question applies to Python 3 regular expressions. I think it might apply to other languages as well.</p>

<p>The question could easily be misunderstood so I'll be careful in describing it.</p>

<p>As background, <code>\w</code> means ""a word character."" In certain circumstances Python 3 will treat this as just <code>[a-zA-Z0-9_]</code> but if the regular expression is a string, it will be Unicode-aware so that <code>\w</code> means ""any Unicode word character."" This is generally a good thing as people use different languages, and it would be hard to construct a range like <code>[a-zA-Z0-9_]</code> for all languages at once. I think <code>\w</code> is therefore most useful in a multilingual setting.</p>

<p>But there is a problem: What if you don't want to match underscores because you don't think they're really a word character (for your particular application)?</p>

<p>If you're only focused on English applications, the best solution is probably to skip <code>\w</code> entirely and just use <code>[a-zA-Z0-9]</code>. But if you're focused on global applications and you don't want underscores, it seems like you might be in a really unfortunate situation. I haven't done it, but I assume it would be really tough to write a range that represents 100 languages at once just so you can avoid that underscore.</p>

<p>So my question is: Is there any way to use <code>\w</code> to match any Unicode word character, but somehow also exclude underscores (or some other undesirable character) from the class? I don't think I've seen anything like this described, but it would be highly useful. Something like <code>[\w^_]</code>. Of course that won't actually work, but what I mean is ""use a character class that starts with everything represented by <code>\w</code>, but then go ahead and remove underscores from that class.""</p>

<p>Thoughts?</p>
","['python', 'regex', 'python-3.x']",54602908,"<p>I have two options.</p>

<ol>
<li><p><code>[^\W_]</code></p>

<p>This is very effective and does exactly what you want. It's also straightforward.</p></li>
<li><p>With <a href=""https://pypi.org/project/regex/"" rel=""noreferrer"">regex</a>: <code>[[\w]--[_]]</code>, note you need ""V1"" flag set, so you need</p>

<pre><code>r = regex.compile(r""(?V1)[\w--_]"")
</code></pre>

<p>or</p>

<pre><code>r = regex.compile(r""[\w--_]"", flags=regex.V1)
</code></pre>

<p>This looks better (readability) IMO if you're familiar with Matthew Barnett's <code>regex</code> module, which is more powerful than Python's stock <code>re</code>.</p></li>
</ol>
",regex w EXCEPT underscore add class exclude class This question applies Python regular expressions I think might apply languages well The question could easily misunderstood I careful describing As background w means word character In certain circumstances Python treat zA Z regular expression string Unicode aware w means Unicode word character This generally good thing people use different languages would hard construct range like zA Z languages I think w therefore useful multilingual setting But problem What want match underscores think really word character particular application If focused English applications best solution probably skip w entirely use zA Z But focused global applications want underscores seems like might really unfortunate situation I done I assume would really tough write range represents languages avoid underscore So question Is way use w match Unicode word character somehow also exclude underscores undesirable character class I think I seen anything like described would highly,"startoftags, python, regex, python3x, endoftags",python arrays numpy endoftags,python regex python3x,python arrays numpy,0.33
54609960,2019-02-09,2019,3,python 3 get the column name depending of a condition,"<p>So i have a pandas df (python 3.6) like this</p>

<pre><code>index   A   B   C  ... 
  A     1   5   0
  B     0   0   1 
  C     1   2   4
 ...
</code></pre>

<p>As you can see, the index values are the same as the columns names.</p>

<p>What i'm trying to do is to get a new column in the dataframe that has the name of the columns where the value is > than 0</p>

<pre><code>index   A   B   C  ... NewColumn
  A     1   5   0       [A,B]
  B     0   0   1       [C]
  C     1   2   4       [A,B,C]
 ...
</code></pre>

<p>i've been trying with iterrows with no success </p>

<p>also i know i can melt and pivot but i think there should be a way with apply lamnda maybe?</p>

<p>Thanks in advance </p>
","['python', 'python-3.x', 'pandas']",54610003,"<p>If new column should be string compare by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.gt.html"" rel=""nofollow noreferrer""><code>DataFrame.gt</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dot.html"" rel=""nofollow noreferrer""><code>dot</code></a> product with columns, last remove trailing separator:</p>

<pre><code>df['NewColumn'] = df.gt(0).dot(df.columns + ', ').str.rstrip(', ')
print (df)
   A  B  C NewColumn
A  1  5  0      A, B
B  0  0  1         C
C  1  2  4   A, B, C
</code></pre>

<p>And for lists use <code>apply</code> with lambda function:</p>

<pre><code>df['NewColumn'] = df.gt(0).apply(lambda x: x.index[x].tolist(), axis=1)
print (df)
   A  B  C  NewColumn
A  1  5  0     [A, B]
B  0  0  1        [C]
C  1  2  4  [A, B, C]
</code></pre>
",python get column name depending condition So pandas df python like index A B C A B C As see index values columns names What trying get new column dataframe name columns value index A B C NewColumn A A B B C C A B C trying iterrows success also know melt pivot think way apply lamnda maybe Thanks advance,"startoftags, python, python3x, pandas, endoftags",python python3x pandas endoftags,python python3x pandas,python python3x pandas,1.0
54879395,2019-02-26,2019,2,Check if a value exists in pandas dataframe,"<p>I have a pandas dataframe which consists of 3000 latitude longitude values. I want to check if a lat-long exists in the dataframe or not.</p>

<p>The data frame looks like the following:</p>

<pre><code>lat      long
31.76    77.84
31.77    77.84
31.78    77.84
32.76    77.85
</code></pre>

<p>Now, I want to check if (31.76, 77.84) exists or not in the above dataframe. If yes, then the index also.</p>
","['python', 'pandas', 'dataframe']",54879454,"<p>Working with <code>float</code>s, so need <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.isclose.html"" rel=""nofollow noreferrer""><code>numpy.isclose</code></a> for check both columns, chain with <code>&amp;</code> for bitwise <code>AND</code> and test with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.any.html"" rel=""nofollow noreferrer""><code>any</code></a> for at least one <code>True</code> of boolean mask:</p>

<pre><code>tup = (31.76, 77.84)
lat, long = tup

a = (np.isclose(df['lat'], lat) &amp; np.isclose(df['long'], long)).any()
print (a)
True
</code></pre>
",Check value exists pandas dataframe I pandas dataframe consists latitude longitude values I want check lat long exists dataframe The data frame looks like following lat long Now I want check exists dataframe If yes index also,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
54951632,2019-03-01,2019,2,How can I convert a pandas DataFrame to an arrayed representation?,"<p>My <code>Y</code> is</p>

<pre><code>       AdoptionSpeed
0                  2
1                  0
2                  3
3                  2
4                  2
5                  2
6                  1
7                  3
8                  1
9                  4
...              ...
14987              0
14988              2
14989              4
14990              3
14991              4
14992              3

[14993 rows x 1 columns]
</code></pre>

<p>I want to convert this to be something like...</p>

<pre><code>        0   1   2   3
0       1   1   0   0
1       0   0   0   0
2       1   1   1   0
....
14991   1   1   1   1
</code></pre>

<p>So basically all entries with 2 become <code>[1  1  0  0]</code> and so on.</p>
","['python', 'pandas', 'numpy']",54951721,"<p>Same like my previous solution </p>

<pre><code>pd.DataFrame([np.ones(x) for x in df['AdoptionSpeed']]).fillna(0).astype(int)
Out[70]: 
   0  1  2  3
0  1  1  0  0
1  0  0  0  0
2  1  1  1  0
3  1  1  0  0
4  1  1  0  0
5  1  1  0  0
6  1  0  0  0
7  1  1  1  0
8  1  0  0  0
9  1  1  1  1
</code></pre>

<hr>

<p>Or more like <code>numpy</code> broadcast : should be faster than for loop</p>

<pre><code>x=df.AdoptionSpeed.max()
pd.DataFrame((df.AdoptionSpeed.values[:,None]&gt;np.arange(x)).astype(int))
Out[81]: 
   0  1  2  3
0  1  1  0  0
1  0  0  0  0
2  1  1  1  0
3  1  1  0  0
4  1  1  0  0
5  1  1  0  0
6  1  0  0  0
7  1  1  1  0
8  1  0  0  0
9  1  1  1  1
</code></pre>
",How I convert pandas DataFrame arrayed representation My Y AdoptionSpeed rows x columns I want convert something like So basically entries become,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
55074571,2019-03-09,2019,2,Python 3.2 W/ Pygame Crashes,"<p>Well, I was happy starting on Pygame with a bit of knowledge on python, But while i was following some starter tutorials i noticed that, At the moment of running my code the Pygame window didn't see to respond, so i put some ""print"" commands to see how far did it get, I noticed that it stopped on the Loop, Any ideas on how i can fix it? I will leave the code around here</p>

<pre><code>import pygame
pygame.init()

print(""First Fase"")
win = pygame.display.set_mode((500,500)) 

pygame.display.set_caption(""Test"")

print(""Second Fase"")
x = 50
y =50
width = 40
height = 60
vel = 7

print(""Third Fase"")

done = False
while not done:
 pygame.time.delay(100)

for event in pygame.event.get():
 if event.type == pygame.QUIT:
  done = True

if event.type == KEYDOWN:
 if event.key == K_ESC:
  done = True

pygame.draw.rect(win, (255, 0, 0), (x, y, width, height))
pygame.display.update()
print(""NoErrors"")
</code></pre>
","['python', 'python-3.x', 'pygame']",55074894,"<p>It has to be <code>pygame.KEYDOWN</code> and <code>pygame.K_ESC</code> rather than <code>KEYDOWN</code> and <code>K_ESC</code>.</p>

<p>But fist of all you have to respect the <strong><a href=""https://docs.python.org/3/reference/lexical_analysis.html?highlight=indent#indentation"" rel=""nofollow noreferrer"">Indentation</a></strong>. In the following code the loops are not nested:</p>

<blockquote>
<pre><code>done = False
while not done:
 pygame.time.delay(100)

for event in pygame.event.get():
 if event.type == pygame.QUIT:
  done = True
</code></pre>
</blockquote>

<p>The <code>for</code> loop is not in the <code>while</code>, it is a separate loop after the <code>while</code> loop.</p>

<p>You have to format your code like this:   </p>

<pre class=""lang-py prettyprint-override""><code>import pygame
pygame.init()

print(""First Fase"")
win = pygame.display.set_mode((500,500)) 

pygame.display.set_caption(""Test"")

print(""Second Fase"")
x = 50
y =50
width = 40
height = 60
vel = 7

print(""Third Fase"")

done = False
while not done:
    pygame.time.delay(100)

    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            done = True

        if event.type == pygame.KEYDOWN:
            if event.key == pygame.K_ESC:
                done = True

    pygame.draw.rect(win, (255, 0, 0), (x, y, width, height))
    pygame.display.update()
    print(""NoErrors"")
</code></pre>
",Python W Pygame Crashes Well I happy starting Pygame bit knowledge python But following starter tutorials noticed At moment running code Pygame window see respond put print commands see far get I noticed stopped Loop Any ideas fix I leave code around import pygame pygame init print First Fase win pygame display set mode pygame display set caption Test print Second Fase x width height vel print Third Fase done False done pygame time delay event pygame event get event type pygame QUIT done True event type KEYDOWN event key K ESC done True pygame draw rect win x width height pygame display update print NoErrors,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
55092543,2019-03-10,2019,3,bs4 python not finding text,"<p>I have an html document that I grabbed via beautiful soup.  An extract of the html is at the bottom of this question.  I'm using beautiful soup and selenium.</p>

<p>I have been told that I'm only allowed to extract so much data per hour, and when I get this page to wait a while (a good hour).</p>

<p>This is how I'm trying to extract the data:</p>

<pre><code>def get_page_data(self):
    opts = Options()
    opts.headless = True
    assert opts.headless  # Operating in headless mode
    browser_detail = Firefox(options=opts)
    url = self.base_url.format(str(self.tracking_id))
    print(url)
    browser_detail.get(url)
    self.page_data = bs4(browser_detail.page_source, 'html.parser')
    Error_Check = 1 if len(self.page_data.findAll(text='Error Report Number')) &gt; 0 else 0
    Error_Check = 2 if len(self.page_data.findAll(text='exceeded the maximum number of sessions per hour allowed')) &gt; 0 else Error_Check
    print(self.page_data.findAll(text='waiting an hour and trying your query again')). ##&lt;&lt;--- The Problem is this line.
    print(self.page_data)
    return Error_Check
</code></pre>

<p>THe problem is this line:</p>

<pre><code>print(self.page_data.findAll(text='waiting an hour and trying your query again')). ##&lt;&lt;--- The Problem is this line.
</code></pre>

<p>The code can't find the line in the page.  What am I missing?  Thanks</p>

<pre><code>&lt;html&gt;&lt;head&gt;
&lt;meta content=""text/html; charset=utf-8"" http-equiv=""Content-Type""/&gt;
&lt;link href=""/CMPL/styles/ogm_style.css;jsessionid=rw9pc8-bncrIy_4KSZmJ8BxN2Z2hnKVwcr79Vho4-99gxTPrxNbo!-68716939"" rel=""stylesheet"" type=""text/css""/&gt;
&lt;body&gt;
&lt;!-- Content Area --&gt;
&lt;table style=""width:100%; margin:auto;""&gt;
&lt;tbody&gt;&lt;tr valign=""top""&gt;
&lt;td class=""ContentArea"" style=""width:100%;""&gt;
&lt;span id=""messageArea""&gt;
&lt;!-- /tiles/messages.jsp BEGIN --&gt;
&lt;ul&gt;
&lt;/ul&gt;&lt;b&gt;
&lt;/b&gt;&lt;table style=""width:100%; margin:auto; white-space: pre-wrap; text-align: left;""&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;td align=""left""&gt;&lt;b&gt;&lt;li&gt;&lt;font color=""red""&gt;&lt;/font&gt;&lt;/li&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td align=""left""&gt;&lt;font color=""red""&gt;You have exceeded the maximum number of sessions per hour allowed for the public queries. You may still access the public&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;font color=""red""&gt;&lt;li style=""list-style: none;""&gt;&lt;/li&gt;&lt;/font&gt;&lt;/td&gt;
&lt;td align=""left""&gt;&lt;font color=""red""&gt;queries by waiting an hour and trying your query again. The RRC public queries are provided to facilitate online research and are not intended to be accessed by automated tools or scripts. For questions or concerns please contact the RRC HelpDesk at helpdesk@rrc.state.tx.us or 512-463-7229&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;....more html...&lt;/p&gt;
&lt;/body&gt;&lt;/html&gt;
</code></pre>
","['python', 'web-scraping', 'beautifulsoup']",55093576,"<p>I'm not sure this is what you're looking for, but if your are:</p>

<pre><code>html = [your code above]
from bs4 import BeautifulSoup as bs4
soup = bs4(html, 'lxml')
data = soup.find_all('font', color=""red"")
data[3].text
</code></pre>

<p>Output:</p>

<pre><code>'queries by waiting an hour and trying your query again. The RRC public queries are provided to facilitate online research and are not intended to be accessed by automated tools or scripts. For questions or concerns please contact the RRC HelpDesk at helpdesk@rrc.state.tx.us or 512-463-7229'
</code></pre>
",bs python finding text I html document I grabbed via beautiful soup An extract html bottom question I using beautiful soup selenium I told I allowed extract much data per hour I get page wait good hour This I trying extract data def get page data self opts Options opts headless True assert opts headless Operating headless mode browser detail Firefox options opts url self base url format str self tracking id print url browser detail get url self page data bs browser detail page source html parser Error Check len self page data findAll text Error Report Number gt else Error Check len self page data findAll text exceeded maximum number sessions per hour allowed gt else Error Check print self page data findAll text waiting hour trying query lt lt The Problem line print self page data return Error Check THe problem line print self page data findAll,"startoftags, python, webscraping, beautifulsoup, endoftags",python arrays numpy endoftags,python webscraping beautifulsoup,python arrays numpy,0.33
55258882,2019-03-20,2019,2,"threshold must be numeric and non-NAN, When printing numpy array, Why numpy.nan is undefined in python3","<p>Im trying to print all the values of a matrix in python2 by </p>

<pre><code>import numpy as np
np.set_printoptions(threshold=np.nan)
print myMatrix
</code></pre>

<p>but its giving me the error
<code>threshold must be numeric and non-NAN</code></p>

<p>In python3 I can print the array by</p>

<pre><code>import sys
numpy.set_printoptions(threshold=sys.maxsize)
print(myMatrix)
</code></pre>

<p>But why numpy.nan isn't working in python3</p>
","['python', 'python-3.x', 'numpy']",55259030,"<p><code>nunmpy.nan</code> was never <a href=""https://github.com/numpy/numpy/issues/12987"" rel=""nofollow noreferrer"">supposed to be supported</a>. This is why it doesn't work in python 3</p>
",threshold must numeric non NAN When printing numpy array Why numpy nan undefined python Im trying print values matrix python import numpy np np set printoptions threshold np nan print myMatrix giving error threshold must numeric non NAN In python I print array import sys numpy set printoptions threshold sys maxsize print myMatrix But numpy nan working python,"startoftags, python, python3x, numpy, endoftags",python python3x list endoftags,python python3x numpy,python python3x list,0.67
55274450,2019-03-21,2019,4,Python Beautifulsoup (bs4) findAll not finding all elements,"<p>From the url that is in the code, I am ultimately trying to gather all of the players names from the page. However, when I am using .findAll in order to get all of the list elements, I am yet to be successful. Please advise.</p>

<pre><code>from urllib.request import urlopen as uReq
from bs4 import BeautifulSoup as soup

players_url = 'https://stats.nba.com/players/list/?Historic=Y'

# Opening up the Connection and grabbing the page
uClient = uReq(players_url)
page_html = uClient.read()

players_soup = soup(page_html, ""html.parser"")

# Taking all of the elements from the unordered lists that contains all of the players.

list_elements = players_soup.findAll('li', {'class': 'players-list__name'})
</code></pre>
","['python', 'web-scraping', 'beautifulsoup']",55279954,"<p>You can do this with requests alone by pulling direct from the js script which provides the names.</p>

<pre><code>import requests
import json

r = requests.get('https://stats.nba.com/js/data/ptsd/stats_ptsd.js')
s = r.text.replace('var stats_ptsd = ','').replace('};','}')
data = json.loads(s)['data']['players']
players = [item[1] for item in data]
print(players)
</code></pre>
",Python Beautifulsoup bs findAll finding elements From url code I ultimately trying gather players names page However I using findAll order get list elements I yet successful Please advise urllib request import urlopen uReq bs import BeautifulSoup soup players url https stats nba com players list Historic Y Opening Connection grabbing page uClient uReq players url page html uClient read players soup soup page html html parser Taking elements unordered lists contains players list elements players soup findAll li class players list name,"startoftags, python, webscraping, beautifulsoup, endoftags",python webscraping beautifulsoup endoftags,python webscraping beautifulsoup,python webscraping beautifulsoup,1.0
55373964,2019-03-27,2019,4,how to interpolate a numpy array with linear interpolation,"<p>I have a numpy array which has the following shape: (1, 128, 160, 1).</p>

<p>Now, I have an image which has the shape: (200, 200).</p>

<p>So, I do the following:</p>

<pre><code>orig = np.random.rand(1, 128, 160, 1)
orig = np.squeeze(orig)
</code></pre>

<p>Now, what I want to do is take my original array and interpolate it to be of the same size as the input image i.e. <code>(200, 200)</code> using linear interpolation. I think I have to specify the grid on which the numpy array should be evaluated but I am unable to figure out how to do it. </p>
","['python', 'arrays', 'numpy']",55376281,"<p>You can do it with <code>scipy.interpolate.interp2d</code> like this:</p>

<pre><code>from scipy import interpolate

# Make a fake image - you can use yours.
image = np.ones((200,200))

# Make your orig array (skipping the extra dimensions).
orig = np.random.rand(128, 160)

# Make its coordinates; x is horizontal.
x = np.linspace(0, image.shape[1], orig.shape[1])
y = np.linspace(0, image.shape[0], orig.shape[0])

# Make the interpolator function.
f = interpolate.interp2d(x, y, orig, kind='linear')

# Construct the new coordinate arrays.
x_new = np.arange(0, image.shape[1])
y_new = np.arange(0, image.shape[0])

# Do the interpolation.
new_orig = f(x_new, y_new)
</code></pre>

<p>Note the -1 adjustment to the coordinate range when forming <code>x</code> and <code>y</code>. This ensures that the image coordinates go from 0 to 199 inclusive.</p>
",interpolate numpy array linear interpolation I numpy array following shape Now I image shape So I following orig np random rand orig np squeeze orig Now I want take original array interpolate size input image e using linear interpolation I think I specify grid numpy array evaluated I unable figure,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
55401781,2019-03-28,2019,7,Cycle over list indefinitely,"<pre class=""lang-py prettyprint-override""><code>a = 1
b = 2
c = 3
x = a

def change_x():
    x = next??
    print(""x:"", x)

for i in range(10):
    change_x()
</code></pre>

<p>How can I cycle through a, b, c by calling change_x() indefinitely?</p>

<p>Output should be:</p>

<pre><code>x: 2
x: 3
x: 1
x: 2
...
</code></pre>
","['python', 'python-3.x', 'list']",55401878,"<p>You could use <a href=""https://docs.python.org/3/library/itertools.html#itertools.cycle"" rel=""nofollow noreferrer""><code>cycle()</code></a> and call <code>next()</code> as many times as you want to get cycled values.</p>

<pre><code>from itertools import cycle

values = [1, 2, 3]
c = cycle(values)

for _ in range(10):
    print(next(c))
</code></pre>

<p>Output:</p>

<pre><code>1
2
3
1
2
3
1
2
3
1
</code></pre>

<p>or as @chepner suggested without using <code>next()</code>:</p>

<pre><code>from itertools import islice

for i in islice(c, 10):
    print(i)
</code></pre>

<p>To get the same result.</p>
",Cycle list indefinitely b c x def change x x next print x x range change x How I cycle b c calling change x indefinitely Output x x x x,"startoftags, python, python3x, list, endoftags",python python3x list endoftags,python python3x list,python python3x list,1.0
55716692,2019-04-16,2019,2,Select where a subset of columns in a pandas DataFrame match a tuple,"<p>This is a simple problem that I can't seem to find an elegant solution to. I am trying to select the rows of a data frame where two of the columns form a pair from a separate list.</p>
<p>For example:</p>
<pre><code>import pandas as pd

df = pd.DataFrame({'a': range(8), 'b': range(8), 'c': list('zyxwvuts')})
pairs = [(4, 4), (5, 6), (6, 6), (7, 9)]

# The data has an arbitrary number of columns, but I just want
# to match 'a' and 'b'
df
    a   b   c
0   0   0   z
1   1   1   y
2   2   2   x
3   3   3   w
4   4   4   v
5   5   5   u
6   6   6   t
7   7   7   s
</code></pre>
<p>In this example, my list <code>pairs</code> contains the combination of <code>df.a</code> and <code>df.b</code> at rows 4 and 6. What I would like is a clean way to get the data frame given by <code> df.iloc[[4, 6], :]</code>.</p>
<p>Is there a <code>pandas</code> or <code>numpy</code> way to do this without explicitly looping over <code>pairs</code>?</p>
<hr />
<h3>Answer comparison</h3>
<p>The solution using broadcasting is both clean and fast, as well as scaling very well.</p>
<pre><code>def with_set_index(df, pairs):
    return df.set_index(['a','b']).loc[pairs].dropna()

def with_tuple_isin(df, pairs):
    return df[df[['a','b']].apply(tuple,1).isin(pairs)]

def with_array_views(df, pairs):
    def view1D(a, b): # a, b are arrays
        a = np.ascontiguousarray(a)
        b = np.ascontiguousarray(b)
        void_dt = np.dtype((np.void, a.dtype.itemsize * a.shape[1]))
        return a.view(void_dt).ravel(), b.view(void_dt).ravel()

    A, B = view1D(df[['a','b']].values, np.asarray(pairs))
    return df[np.isin(A, B)]

def with_broadcasting(df, pairs):
    return df[(df[['a','b']].values[:,None] == pairs).all(2).any(1)]

%timeit with_set_index(df, pairs)
# 7.35 ms Â± 119 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)

%timeit with_tuple_isin(df, pairs)
# 1.89 ms Â± 24.4 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)

%timeit with_array_views(df, pairs)
# 917 Âµs Â± 17.9 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)

%timeit with_broadcasting(df, pairs)
# 879 Âµs Â± 8.85 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
</code></pre>
","['python', 'pandas', 'numpy']",55716861,"<p>A vectorized one based on array-views -</p>

<pre><code># https://stackoverflow.com/a/45313353/ @Divakar
def view1D(a, b): # a, b are arrays
    a = np.ascontiguousarray(a)
    b = np.ascontiguousarray(b)
    void_dt = np.dtype((np.void, a.dtype.itemsize * a.shape[1]))
    return a.view(void_dt).ravel(),  b.view(void_dt).ravel()

A,B = view1D(df[['a','b']].values,np.asarray(pairs))
out = df[np.isin(A,B)]
</code></pre>

<p>Output for given sample -</p>

<pre><code>In [263]: out
Out[263]: 
   a  b  c
4  4  4  v
6  6  6  t
</code></pre>

<p>If you are looking for a compact/clean version, we can also leverage <code>broadcasting</code> -</p>

<pre><code>In [269]: df[(df[['a','b']].values[:,None] == pairs).all(2).any(1)]
Out[269]: 
   a  b  c
4  4  4  v
6  6  6  t
</code></pre>
",Select subset columns pandas DataFrame match tuple This simple problem I seem find elegant solution I trying select rows data frame two columns form pair separate list For example import pandas pd df pd DataFrame range b range c list zyxwvuts pairs The data arbitrary number columns I want match b df b c z x w v u In example list pairs contains combination df df b rows What I would like clean way get data frame given df iloc Is pandas numpy way without explicitly looping pairs Answer comparison The solution using broadcasting clean fast well scaling well def set index df pairs return df set index b loc pairs dropna def tuple isin df pairs return df df b apply tuple isin pairs def array views df pairs def view D b b arrays np ascontiguousarray b np ascontiguousarray b void dt np dtype np void dtype,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
55873037,2019-04-26,2019,2,How to find all numbers containing a specific digit in a range of numbers from 1 to 100?,"<p>I have a list of items from 1 to 100 inside 1 list.</p>

<p>Query 1:</p>

<p>Suppose if I want to find 6 from 1 to 100, the new list 
should have - <code>6,16,26,36,46,56,60,61,62,63,64,65,66,67,68,69,76,86,96</code>.<br>
How do we achieve this?</p>

<p>Query 2:</p>

<p>The specific number can be anything of our choice like if I have 4
then the new list should be - <code>4,14,24,34,40,41,42,43,44,45,46,47,48,49,54,64,74,84,94</code>. </p>

<p>So can we have a generic logic for both or we need to have different logic for any specific numbers?</p>

<p><strong>EDIT</strong></p>

<p>This is what I have tried so far,</p>

<pre><code>z = [1,2,3,4,5,6,7,8,9,10] 
for i in z: 
    if i == 6: 
        print(i)
</code></pre>

<p>I was only able to get it from for 1 to 10 but could not get it for 1 to 100.</p>
","['python', 'python-3.x', 'python-2.7']",55873241,"<p>Here is the program,</p>
<pre><code>x = list(range(1, 101)) 
n = str(input(&quot;Enter a number: &quot;))
output = [i for i in x if n in str(i)]
print (output)
</code></pre>
<p>Output:</p>
<blockquote>
<p>Enter a number: <code>4</code></p>
<p>Output: <code>[4, 14, 24, 34, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 54, 64, 74, 84, 94]</code></p>
</blockquote>
<p>where the number of your choice <code>n</code> is <code>4</code>.</p>
<blockquote>
<p>Enter a number: <code>6</code></p>
<p>Output: <code>[6, 16, 26, 36, 46, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 76, 86, 96]</code></p>
</blockquote>
<p>where the number of your choice <code>n</code> is <code>6</code>.</p>
<blockquote>
<p><code>x = list(range(1, 101))</code></p>
</blockquote>
<ul>
<li>The <code>range()</code> function can be used to create long lists. The <code>list()</code> function then uses numbers from this sequence to create a list.</li>
</ul>
<p>Hope this helps!</p>
",How find numbers containing specific digit range numbers I list items inside list Query Suppose I want find new list How achieve Query The specific number anything choice like I new list So generic logic need different logic specific numbers EDIT This I tried far z z print I able get could get,"startoftags, python, python3x, python27, endoftags",python python3x list endoftags,python python3x python27,python python3x list,0.67
56512931,2019-06-09,2019,2,Does making DataFrame smaller makes it faster?,"<p>I read an article (<a href=""https://www.ritchieng.com/pandas-making-dataframe-smaller-faster/"" rel=""nofollow noreferrer"">https://www.ritchieng.com/pandas-making-dataframe-smaller-faster/</a>) which mentions that it makes the DataFrame faster by making it smaller (by converting data type).</p>

<p>Is there any association between smaller (memory usage) and faster (cpu time)?</p>

<p>Let's say I have a DataFrame column of <strong>int64</strong>. If we convert it to <strong>int8</strong>, will the operation on the column be faster?  e.g. assuming that the operation is <code>d[col] = d[col] + 1</code></p>
","['python', 'pandas', 'dataframe']",56514095,"<p>why not to test it?</p>
<h1>int64 dtype</h1>
<pre><code>In [29]: df = pd.DataFrame(np.random.randint(100, size=(10**7, 10), dtype=&quot;int64&quot;))

In [30]: df.dtypes
Out[30]:
0    int64
1    int64
2    int64
3    int64
4    int64
5    int64
6    int64
7    int64
8    int64
9    int64
dtype: object
</code></pre>
<p>memory usage (in MiB):</p>
<pre><code>In [31]: df.memory_usage().sum() / 1024**2
Out[31]: 762.9395294189453
</code></pre>
<p>timings:</p>
<pre><code>In [32]: %timeit df.agg([&quot;min&quot;,&quot;max&quot;,&quot;mean&quot;])
4.68 s Â± 84.1 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

In [33]: %timeit df+1
818 ms Â± 26 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
</code></pre>
<hr />
<h1>int8 dtype</h1>
<pre><code>In [34]: df2 = df.astype(&quot;int8&quot;)

In [35]: df2.dtypes
Out[35]:
0    int8
1    int8
2    int8
3    int8
4    int8
5    int8
6    int8
7    int8
8    int8
9    int8
dtype: object
</code></pre>
<p>memory usage (in MiB):</p>
<pre><code>In [38]: df2.memory_usage().sum() / 1024**2
Out[38]: 95.36750793457031
</code></pre>
<p>timings:</p>
<pre><code>In [36]: %timeit df2.agg([&quot;min&quot;,&quot;max&quot;,&quot;mean&quot;])
2.4 s Â± 222 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

In [37]: %timeit df2+1
170 ms Â± 10.6 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
</code></pre>
",Does making DataFrame smaller makes faster I read article https www ritchieng com pandas making dataframe smaller faster mentions makes DataFrame faster making smaller converting data type Is association smaller memory usage faster cpu time Let say I DataFrame column int If convert int operation column faster e g assuming operation col col,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
56710022,2019-06-21,2019,2,how to read and save time a user went offline in a discord server,"<p>what I want to do is that everytime a user goes offline the time he gets offline gets saved in a json file. I know how to save it in a json and all that but what I dont know is, how do I save the time and how do I check when people go offline. thx for your help</p>
","['python', 'discord', 'discord.py']",56710290,"<p>You can do it like this:</p>

<pre><code>import time

@client.event
async def on_member_update(before, after):
    if str(before.status) == ""online"":
        if str(after.status) == ""offline"":
            timestr = time.strftime(""%Y%m%d-%H%M%S"")
            print(""{} has gone {} at date-time {}."".format(after.name,after.status,timestr))
</code></pre>

<p>This will print something like:</p>

<p><code>JackMoody has gone offline at 20190621-155145</code> </p>

<p>Every time a user's status goes from ""online"" to ""offline"".</p>
",read save time user went offline discord server I want everytime user goes offline time gets offline gets saved json file I know save json I dont know I save time I check people go offline thx help,"startoftags, python, discord, discordpy, endoftags",python django djangorestframework endoftags,python discord discordpy,python django djangorestframework,0.33
56896663,2019-07-05,2019,2,How to separate one column into multiple columns in python?,"<p>I have one 'csv' file it looks like this:</p>

<p>sample data :</p>

<pre><code>    Name : Jai
    Age : 25
    Address: N P IV 
    Country: 
    Name : Jack
    Age : 18
    Address: T U W IX 
    Country: USA

</code></pre>

<p>I want to split this single column into multiple, just like this,
Expected result:</p>

<pre><code>    Name        Age        Address        Country
    Jai         25          N P IV         NA
    Jack        18          T U W IX       USA
</code></pre>

<p>Thank you</p>
","['python', 'python-3.x', 'pandas']",56896711,"<p>First create 2 columns <code>DataFrame</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html"" rel=""nofollow noreferrer""><code>read_csv</code></a> with separator <code>:\s+</code> for <code>:</code> with one or more spaces, then convert second column to numpy array and reshape for 4 <code>'columns'</code>, create DataFrame by constructor with first 4 values of first column to new columns names and last if necessary convert <code>Age</code> column to integers:</p>

<pre><code>import pandas as pd
import numpy as np

temp=u""""""Name : Jai
Age : 25
Address: N P IV 
Country: 
Name : Jack
Age : 18
Address: T U W IX 
Country: USA""""""
#after testing replace 'pd.compat.StringIO(temp)' to 'filename.csv'
df = pd.read_csv(pd.compat.StringIO(temp), sep="":\s+"", names=['col1','col2'])

print (df)
       col1      col2
0     Name        Jai
1      Age         25
2   Address    N P IV
3  Country:      None
4     Name       Jack
5      Age         18
6   Address  T U W IX
7   Country       USA
</code></pre>

<hr>

<pre><code>c = df['col1'].iloc[:4].str.strip(' :')
#pandas 0.24+
df = pd.DataFrame(df['col2'].to_numpy().reshape(-1, 4), columns=c).rename_axis(None, axis=1)
#pandas below 0.24
#df = pd.DataFrame(df['col2'].values.reshape(-1, 4), columns=c).rename_axis(None, axis=1)

df['Age'] = df['Age'].astype(int)
print (df)
   Name  Age   Address Country
0   Jai   25    N P IV    None
1  Jack   18  T U W IX     USA
</code></pre>
",How separate one column multiple columns python I one csv file looks like sample data Name Jai Age Address N P IV Country Name Jack Age Address T U W IX Country USA I want split single column multiple like Expected result Name Age Address Country Jai N P IV NA Jack T U W IX USA Thank,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
56910950,2019-07-06,2019,7,Keras predict loop memory leak using tf.data.Dataset but not with a numpy array,"<p>I encounter a memory leak and decreasing performance when looping over a Keras model <code>predict</code> function when using a <code>tf.data.Dataset</code> to feed the model, but not when feeding it with a numpy array.</p>

<p>Does anyone understand what is causing this and/or how to resolve the issue?</p>

<p><strong>Minimal reproducible code snippet (copy/paste runnable):</strong></p>

<pre><code>import tensorflow as tf
import numpy as np
import time

SIZE = 5000

inp = tf.keras.layers.Input(shape=(SIZE,), dtype='float32')
x = tf.keras.layers.Dense(units=SIZE)(inp)

model = tf.keras.Model(inputs=inp, outputs=x)

np_data = np.random.rand(1, SIZE)
ds = tf.data.Dataset.from_tensor_slices(np_data).batch(1).repeat()

debug_time = time.time()
while True:
    model.predict(x=ds, steps=1)
    print('Processing {:.2f}'.format(time.time() - debug_time))
    debug_time = time.time()
</code></pre>

<p><strong>Result:</strong> Predict loop timing starts around 0.04s per iteration, within a minute or two it's up to about 0.5s and process memory continues to increase from a few hundred MB to close to a GB.</p>

<hr>

<p>Swap out the <code>tf.data.Dataset</code> for an equivalent numpy array and runtime is ~0.01s consistently.</p>

<p><strong>Working case code snippet (copy/paste runnable):</strong></p>

<pre><code>import tensorflow as tf
import numpy as np
import time

SIZE = 5000

inp = tf.keras.layers.Input(shape=(SIZE,), dtype='float32')
x = tf.keras.layers.Dense(units=SIZE)(inp)

model = tf.keras.Model(inputs=inp, outputs=x)

np_data = np.random.rand(1, SIZE)

debug_time = time.time()
while True:
    model.predict(x=np_data)  # using numpy array directly
    print('Processing {:.2f}'.format(time.time() - debug_time))
    debug_time = time.time()
</code></pre>

<hr>

<p>Related discussions:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/52493849/memory-leak-tf-data-keras"">Memory leak tf.data + Keras</a> - Doesn't seem to address the core issue, but the question appears similar.</li>
<li><a href=""https://github.com/tensorflow/tensorflow/issues/22098"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/issues/22098</a> - Possibly an open issue in Keras/Github, but I can't confirm it, changing <code>inter_op_paralellism</code> as suggested in that thread has no impact on the results posted here.</li>
</ul>

<hr>

<p>Additional info:</p>

<ul>
<li>I can reduce the rate of performance degradation by around 10x by passing in an iterator instead of a dataset object. I noticed in <code>training_utils.py:1314</code> the Keras code is creating an iterator each call to predict.</li>
</ul>

<p>TF 1.14.0</p>
","['python', 'tensorflow', 'keras']",56916421,"<p>The root of the problem appears to be that Keras is creating dataset operations each <code>predict</code> loop. Notice at <code>training_utils.py:1314</code> a dataset iterator is created in each predict loop. </p>

<p>The problem can be reduced in severity by passing in an iterator, and is solved entirely by passing in the iterators <code>get_next()</code> tensor. </p>

<p>I have posted the issue on the Tensorflow Github page: <a href=""https://github.com/tensorflow/tensorflow/issues/30448"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/30448</a></p>

<p>Here is the solution, this example runs in constant time using the TF dataset, you just can't pass in the dataset object:</p>

<pre><code>import tensorflow as tf
import numpy as np
import time

SIZE = 5000

inp = tf.keras.layers.Input(shape=(SIZE,), dtype='float32')
x = tf.keras.layers.Dense(units=SIZE)(inp)

model = tf.keras.Model(inputs=inp, outputs=x)

np_data = np.random.rand(1, SIZE)
ds = tf.data.Dataset.from_tensor_slices(np_data).batch(1).repeat()
it = tf.data.make_one_shot_iterator(ds)
tensor = it.get_next()

debug_time = time.time()
while True:
    model.predict(x=tensor, steps=1)
    print('Processing {:.2f}'.format(time.time() - debug_time))
    debug_time = time.time()
</code></pre>
",Keras predict loop memory leak using tf data Dataset numpy array I encounter memory leak decreasing performance looping Keras model predict function using tf data Dataset feed model feeding numpy array Does anyone understand causing resolve issue Minimal reproducible code snippet copy paste runnable import tensorflow tf import numpy np import time SIZE inp tf keras layers Input shape SIZE dtype float x tf keras layers Dense units SIZE inp model tf keras Model inputs inp outputs x np data np random rand SIZE ds tf data Dataset tensor slices np data batch repeat debug time time time True model predict x ds steps print Processing f format time time debug time debug time time time Result Predict loop timing starts around per iteration within minute two process memory continues increase hundred MB close GB Swap tf data Dataset equivalent numpy array runtime consistently Working case code snippet copy paste,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
57034608,2019-07-15,2019,2,"How to combine dict objects which contain the same key value, and output a list?","<p>I want to add dict objects which contain the same key value, and get a list. I have a function and I do:</p>

<pre><code>def arizakestirimi_func(self):
    arizatahmin = []
    ...
    record = {} //create a new dict
    record[""isyeri""]=key
    record[""mekanikarizatahminsuresi""]= arizatahminsÃ¼resi
    arizatahmin.append(record);
    ...
    record = {} //create a new dict
    record[""isyeri""]=key //key can be same 
    record[""elektrikarizatahminsuresi""]= arizatahminsÃ¼resi //this is different key value
    arizatahmin.append(record);
    ...
    record = {} //create a new dict
    record[""isyeri""]=key //key can be same 
    record[""kesintilerarizatahminsuresi""]= arizatahminsÃ¼resi //this is different key value

    arizatahmin.append(record);

    #Return value as a HttpResponse.
    return HttpResponse(json.dumps({'arizaTahminSuresi': arizatahmin},indent=2), content_type=""application/json"") 
</code></pre>

<p>My output is json like this:</p>

<pre><code>{
  ""arizaTahminSuresi"": [
    {
      ""isyeri"": ""15400005"",//The same key
      ""mekanikarizatahminsuresi"": 38.95637287225691
    },

    {
      ""isyeri"": ""15400005"",//The same key
      ""elektrikarizatahminsuresi"": 427.9069449086019
    },

      ""isyeri"": ""15400005"",//The same key
      ""kesintilerarizatahminsuresi"": 4.882768280126584
    }

  ]
}
</code></pre>

<p>I want the output to be as follows. How can I do it?</p>

<pre><code>{
  ""arizaTahminSuresi"": [
    {
      ""isyeri"": ""15400005"",
      ""mekanikarizatahminsuresi"": 38.95637287225691
      ""elektrikarizatahminsuresi"": 427.9069449086019
      ""kesintilerarizatahminsuresi"": 4.882768280126584
    }   
  ]
}
</code></pre>
","['python', 'list', 'dictionary']",57034707,"<p>you can use <code>groupby</code> to get all the matching dicts, then unfy them using <code>update</code>, like this:</p>

<pre class=""lang-py prettyprint-override""><code>from itertools import groupby
from operator import itemgetter
from pprint import pprint

original = {
    ""arizaTahminSuresi"": [
        {
            ""isyeri"": ""15400005"",
            ""mekanikarizatahminsuresi"": 38.95637287225691
        },
        {
            ""isyeri"": ""15400003"",
            ""mekanikarizatahminsuresi"": 312.05942081303965
        },
        {
            ""isyeri"": ""15400002"",
            ""mekanikarizatahminsuresi"": 773.600387779049
        },
        {
            ""isyeri"": ""15400004"",
            ""mekanikarizatahminsuresi"": 275.25620818317475
        },
        {
            ""isyeri"": ""15400001"",
            ""mekanikarizatahminsuresi"": 9.032135504089638
        },
        {
            ""isyeri"": ""15400004"",
            ""elektrikarizatahminsuresi"": 105.3605215202133
        },
        {
            ""isyeri"": ""15400003"",
            ""elektrikarizatahminsuresi"": 463.8990083193239
        },
        {
            ""isyeri"": ""15400001"",
            ""elektrikarizatahminsuresi"": 49.71052726257949
        },
        {
            ""isyeri"": ""15400005"",
            ""elektrikarizatahminsuresi"": 427.9069449086019
        },
        {
            ""isyeri"": ""15400002"",
            ""elektrikarizatahminsuresi"": 25.78649617892279
        },
        {
            ""isyeri"": ""15400004"",
            ""kesintilerarizatahminsuresi"": 67.41996630760787
        },
        {
            ""isyeri"": ""15400005"",
            ""kesintilerarizatahminsuresi"": 4.882768280126584
        },
        {
            ""isyeri"": ""15400002"",
            ""kesintilerarizatahminsuresi"": 58.96635620693996
        },
        {
            ""isyeri"": ""15400001"",
            ""kesintilerarizatahminsuresi"": 357.28675445627414
        },
        {
            ""isyeri"": ""15400003"",
            ""kesintilerarizatahminsuresi"": 434.16131082294305
        }

    ]
}

grouped_subdicts = groupby(sorted(original[""arizaTahminSuresi""], key=itemgetter(""isyeri"")), itemgetter(""isyeri""))

result = []
for k,g in grouped_subdicts:
    d = {}
    for sub_d in g:
        d.update(sub_d)
    result.append(d)

original[""arizaTahminSuresi""] = result

pprint(original)
</code></pre>
",How combine dict objects contain key value output list I want add dict objects contain key value get list I function I def arizakestirimi func self arizatahmin record create new dict record isyeri key record arizatahmins resi arizatahmin append record record create new dict record isyeri key key record arizatahmins resi different key value arizatahmin append record record create new dict record isyeri key key record arizatahmins resi different key value arizatahmin append record Return value HttpResponse return HttpResponse json dumps arizaTahminSuresi arizatahmin indent content type application json My output json like arizaTahminSuresi isyeri The key isyeri The key isyeri The key I want output follows How I arizaTahminSuresi isyeri,"startoftags, python, list, dictionary, endoftags",python python3x list endoftags,python list dictionary,python python3x list,0.67
57077442,2019-07-17,2019,2,"Error when checking input: expected lstm_1_input to have 3 dimensions, but got array with shape (0, 1) only on epoch&gt;1 and at a specific dataset split","<p>This question is different than <a href=""https://stackoverflow.com/questions/48978609/valueerror-error-when-checking-input-expected-lstm-1-input-to-have-3-dimension"">ValueError: Error when checking input: expected lstm_1_input to have 3 dimensions, but got array with shape (10, 1)</a> as the answers there does not satisfy my case.</p>

<hr>

<p>Following the tutorial here: <a href=""https://www.altumintelligence.com/articles/a/Time-Series-Prediction-Using-LSTM-Deep-Neural-Networks"" rel=""nofollow noreferrer"">https://www.altumintelligence.com/articles/a/Time-Series-Prediction-Using-LSTM-Deep-Neural-Networks</a>, 
Here is the model summary:</p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 49, 100)           41200     
_________________________________________________________________
dropout_1 (Dropout)          (None, 49, 100)           0         
_________________________________________________________________
lstm_2 (LSTM)                (None, 49, 100)           80400     
_________________________________________________________________
lstm_3 (LSTM)                (None, 100)               80400     
_________________________________________________________________
dropout_2 (Dropout)          (None, 100)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 101       
=================================================================
Total params: 202,101
Trainable params: 202,101
Non-trainable params: 0
_________________________________________________________________
None
</code></pre>

<p>The size of my data set is 79005, sequence length is 50 and timesteps/batch size is 32. The problem is when I configure epoch to be 1, all goes perfect. But when I change it to 2, I get the below error <strong>right at the start of the second epoch</strong>:</p>

<blockquote>
  <p>ValueError: Error when checking input: expected lstm_1_input to have 3 dimensions, but got array with shape (0, 1)</p>
</blockquote>

<p>I just want to understand why this is not the problem with 1 epoch and why only with 2 (or more)?  <strong>EDIT: setting the train-test split as 0.80 instead of 0.85 actually removed the error! I still would like to know the reason for this as I am not getting it.</strong></p>

<p>Below is my data loading code:</p>

<pre><code>import math
import numpy as np
import pandas as pd

class DataLoader():
    """"""A class for loading and transforming data for the lstm model""""""

    def __init__(self, filename, split, cols):
        dataframe = pd.read_csv(filename)
        print(""data shape:"",dataframe.shape)
        i_split = int(len(dataframe) * split)
        self.data_train = dataframe.get(cols).values[:i_split]
        self.data_test  = dataframe.get(cols).values[i_split:]
        self.len_train  = len(self.data_train)
        self.len_test   = len(self.data_test)
        self.len_train_windows = None

    def get_test_data(self, seq_len, normalise):
        '''
        Create x, y test data windows
        Warning: batch method, not generative, make sure you have enough memory to
        load data, otherwise reduce size of the training split.
        '''
        data_windows = []
        #data_x=[]
        #data_y=[]
        for i in range(self.len_test - seq_len):
            data_windows.append(self.data_test[i:i+seq_len])

        data_windows = np.array(data_windows).astype(float)
        data_windows = self.normalise_windows(data_windows, single_window=False) if normalise else data_windows

        x = data_windows[:, :-1]
        y = data_windows[:, -1, [0]]
            #x,y=self._next_window(i,seq_len,normalise,train=False)
            #data_x.append(x)
            #data_y.append(y)
        #return np.array(data_x),np.array(data_y)
        return x,y

    def get_train_data(self, seq_len, normalise):
        '''
        Create x, y train data windows
        Warning: batch method, not generative, make sure you have enough memory to
        load data, otherwise use generate_training_window() method.
        '''
        data_x = []
        data_y = []
        for i in range(self.len_train - seq_len):
            x, y = self._next_window(i, seq_len, normalise)
            data_x.append(x)
            data_y.append(y)
        return np.array(data_x), np.array(data_y)

    def generate_train_batch(self, seq_len, batch_size, normalise, epochs):
        '''Yield a generator of training data from filename on given list of cols split for train/test'''
        i = 0
        print(""train length:"",self.len_train)
        #while epoch &lt; epochs:
        while i &lt; ((self.len_train - seq_len)*(epochs+1)):
            print(""i:"",i)
            x_batch = []
            y_batch = []
            for b in range(batch_size):
                if i &gt;= (self.len_train - seq_len):
                    # stop-condition for a smaller final batch if data doesn't divide evenly
                    yield np.array(x_batch), np.array(y_batch)
                    i = 0
                    print(""i set to 0"")
                x, y = self._next_window(i, seq_len, normalise)
                x_batch.append(x)
                y_batch.append(y)
                i += 1

            print (""x:"",np.array(x_batch).shape)
            print (""y:"",np.array(y_batch).shape)
            yield np.array(x_batch), np.array(y_batch)
        #epoch += 1

    def _next_window(self, i, seq_len, normalise):
        '''Generates the next data window from the given index location i'''
        window = self.data_train[i:i+seq_len]
        #if train:
        #    window = self.data_train[i:i+seq_len]
        #else:
        #    window = self.data_test[i:i+seq_len]
        window = self.normalise_windows(window, single_window=True)[0] if normalise else window
        x = window[:-1]
        y = window[-1, [0]]
        return x, y

    def normalise_windows(self, window_data, single_window=False):
        '''Normalise window with a base value of zero'''
        eps=0.00001
        normalised_data = []
        window_data = [window_data] if single_window else window_data
        for window in window_data:
            normalised_window = []
            for col_i in range(window.shape[1]):
                normalised_col = [((float(p) / (float(window[0, col_i])+eps)) - 1) for p in window[:, col_i]]
                normalised_window.append(normalised_col)
            normalised_window = np.array(normalised_window).T # reshape and transpose array back into original multidimensional format
            normalised_data.append(normalised_window)
        return np.array(normalised_data)
</code></pre>

<p>Below is the Model building code:</p>

<pre><code>    class Model():
    """"""A class for an building and inferencing an lstm model""""""

    def __init__(self):
        self.model = Sequential()

    def load_model(self, filepath):
        print('[Model] Loading model from file %s' % filepath)
        self.model = load_model(filepath)

    def build_model(self, configs):
        timer = Timer()
        timer.start()

        for layer in configs['model']['layers']:
            neurons = layer['neurons'] if 'neurons' in layer else None
            dropout_rate = layer['rate'] if 'rate' in layer else None
            activation = layer['activation'] if 'activation' in layer else None
            return_seq = layer['return_seq'] if 'return_seq' in layer else None
            input_timesteps = layer['input_timesteps'] if 'input_timesteps' in layer else None
            input_dim = layer['input_dim'] if 'input_dim' in layer else None

            if layer['type'] == 'dense':
                self.model.add(Dense(neurons, activation=activation))
            if layer['type'] == 'lstm':
                self.model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=return_seq))
            if layer['type'] == 'dropout':
                self.model.add(Dropout(dropout_rate))

        self.model.compile(loss=configs['model']['loss'], optimizer=configs['model']['optimizer'],metrics=['mean_squared_error'])
        print(self.model.summary())
        plot_model(self.model, to_file='model.png')

        print('[Model] Model Compiled')
        timer.stop()
        return self.model

    def train(self, x, y, epochs, batch_size, save_dir=""""):
        timer = Timer()
        timer.start()
        print('X shape:', (x.shape))
        print('[Model] Training Started')
        print('[Model] %s epochs, %s batch size' % (epochs, batch_size))

        save_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=2),
            ModelCheckpoint(filepath=save_fname, monitor='val_loss', save_best_only=True)
        ]
        modelhistory=self.model.fit(
            x,
            y,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks
        )
        self.model.save(save_fname)
        print('[Model] Training Completed. Model saved as %s' % save_fname)
        timer.stop()
        return modelhistory

    def train_generator(self, data_gen, epochs, batch_size, steps_per_epoch, save_dir=""""):
        timer = Timer()
        timer.start()
        #print('X shape:', (x.shape))
        print('[Model] Training Started')
        print('[Model] %s epochs, %s batch size, %s batches per epoch' % (epochs, batch_size, steps_per_epoch))

        save_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))
        callbacks = [
            ModelCheckpoint(filepath=save_fname, monitor='loss', save_best_only=True)
        ]
        modelhistory=self.model.fit_generator(
            data_gen,
            steps_per_epoch=steps_per_epoch,
            epochs=epochs,
            callbacks=callbacks,
            workers=1
        )

        print('[Model] Training Completed. Model saved as %s' % save_fname)
        timer.stop()
        return modelhistory
</code></pre>

<p>Below is the model configuration where all the parameters are defined:</p>

<pre><code>configJson={
    ""data"": {
        ""filename"": ""C:/projects!/Experiments/2015-02-02-To-2019-5-19-5-Min.csv"",
        ""columns"": [
            ""Close"",""Volume""
        ],
        ""sequence_length"": 50,
        ""train_test_split"": 0.85,
        ""normalise"": True
    },
    ""training"": {
        ""epochs"": 2,
        ""batch_size"": 32
    },
    ""model"": {
        ""loss"": ""mse"",
        ""optimizer"": ""adam"",
        ""layers"": [
            {
                ""type"": ""lstm"",
                ""neurons"": 100,
                ""input_timesteps"": 49,
                ""input_dim"": 2,
                ""return_seq"": True
            },
            {
                ""type"": ""dropout"",
                ""rate"": 0.2
            },
            {
                ""type"": ""lstm"",
                ""neurons"": 100,
                ""return_seq"": True
            },
            {
                ""type"": ""lstm"",
                ""neurons"": 100,
                ""return_seq"": False
            },
            {
                ""type"": ""dropout"",
                ""rate"": 0.2
            },
            {
                ""type"": ""dense"",
                ""neurons"": 1,
                ""activation"": ""linear""
            }
        ]
    }
}
</code></pre>

<p>Below is how I am building my model:</p>

<pre><code>data = DataLoader(
    os.path.join(configs['data']['filename']),
    configs['data']['train_test_split'],
    configs['data']['columns']
)

model = Model()
model.build_model(configs)

# out-of memory generative training
steps_per_epoch = math.ceil((data.len_train - configs['data']['sequence_length']) / configs['training']['batch_size'])
modelhistory=model.train_generator(
    data_gen = data.generate_train_batch(
        seq_len = configs['data']['sequence_length'],
        batch_size = configs['training']['batch_size'],
        normalise = configs['data']['normalise'],
      epochs = configs['training']['epochs']
    ),
    epochs = configs['training']['epochs'],
    batch_size = configs['training']['batch_size'],
    steps_per_epoch = steps_per_epoch
)
</code></pre>

<p>Please help.</p>
","['python', 'tensorflow', 'keras']",57096490,"<p>Ok, I think I know what your issue is (or was).</p>

<p>So, in <code>generate_train_batch</code>, this line: </p>

<pre><code>if i &gt;= (self.len_train - seq_len):
</code></pre>

<p>checks to see when to reset the counter to 0 but does not add anything else to the batches for computation. When you are running it with 2 epochs and 0.85 split, it just so happens that when that line gets executed (right at the beginning of the second epoch), it's still the beginning of the generation for that batch (ie. <code>i</code>=67104 and so the <code>if i &gt;= (self.len_train - seq_len):</code> condition is met as soon as the for loop is started). So <code>i</code> never increases and so your batch is empty. </p>

<p>For all of the other configurations this error just does not happen because this specific case mentioned above doesn't occur. On your side, to make sure this does not happen, I would recommend just removing the <code>yield np.array(x_batch), np.array(y_batch)</code> line after the if-statement mentioned above. It's okay if your last batch reuses some of the first elements. I think that would be the easiest way to solve this issue here.</p>
",Error checking input expected lstm input dimensions got array shape epoch gt specific dataset split This question different ValueError Error checking input expected lstm input dimensions got array shape answers satisfy case Following tutorial https www altumintelligence com articles Time Series Prediction Using LSTM Deep Neural Networks Here model summary Layer type Output Shape Param lstm LSTM None dropout Dropout None lstm LSTM None lstm LSTM None dropout Dropout None dense Dense None Total params Trainable params Non trainable params None The size data set sequence length timesteps batch size The problem I configure epoch goes perfect But I change I get error right start second epoch ValueError Error checking input expected lstm input dimensions got array shape I want understand problem epoch EDIT setting train test split instead actually removed error I still would like know reason I getting Below data loading code import math import numpy np import,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
57126350,2019-07-20,2019,2,Boolean indexing on Nan values,"<p>I have a DataFrame as follows- </p>

<pre><code>                 ID        Date Element  Data_Value    day  max   min
131541  USC00203712  2015-02-16    TMIN       -29.4  02-16  NaN -29.4
131566  USC00203712  2015-02-16    TMAX       -12.2  02-16  NaN   NaN
131638  USC00208972  2015-02-16    TMAX       -15.6  02-16  NaN   NaN
131641  USC00208972  2015-02-16    TMIN       -29.4  02-16  NaN -29.4
131727  USC00201250  2015-02-16    TMAX       -13.9  02-16  NaN   NaN
131728  USC00201250  2015-02-16    TMIN       -28.9  02-16  NaN -28.9
</code></pre>

<p>I want to drop rows where both 'max' and 'min' are Nan, such as the row with index 131566, 131638 etc. here.</p>

<p>I was able to create the correct boolean mask with - </p>

<pre><code>bool = ~((~df['min'].notnull()) &amp; (~df['max'].notnull()))
</code></pre>

<p>On applying this mask to the Frame I get</p>

<pre><code>                 ID        Date Element  Data_Value    day  max   min
131541  USC00203712  2015-02-16    TMIN       -29.4  02-16  NaN -29.4
131566          NaN         NaN     NaN         NaN    NaN  NaN   NaN
131638          NaN         NaN     NaN         NaN    NaN  NaN   NaN
131641  USC00208972  2015-02-16    TMIN       -29.4  02-16  NaN -29.4
131727          NaN         NaN     NaN         NaN    NaN  NaN   NaN
131728  USC00201250  2015-02-16    TMIN       -28.9  02-16  NaN -28.9
</code></pre>

<p>Now I can't simply use the dropna() method because that would drop all of the rows and columns. Any way to circumvent this?</p>
","['python', 'python-3.x', 'pandas']",57126405,"<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isna.html"" rel=""nofollow noreferrer""><code>df.isna()</code></a> to check for null values and <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.all.html"" rel=""nofollow noreferrer""><code>df.all()</code></a> along <code>axis=1</code> to check if all values in the list of columns are <code>NaN</code>:</p>

<pre><code>l=['max','min'] #list of cols to check
df[~df[l].isna().all(1)]
</code></pre>

<hr>

<pre><code>                 ID        Date Element  Data_Value    day  max   min
131541  USC00203712  2015-02-16    TMIN       -29.4  02-16  NaN -29.4
131641  USC00208972  2015-02-16    TMIN       -29.4  02-16  NaN -29.4
131728  USC00201250  2015-02-16    TMIN       -28.9  02-16  NaN -28.9
</code></pre>
",Boolean indexing Nan values I DataFrame follows ID Date Element Data Value day max min USC TMIN NaN USC TMAX NaN NaN USC TMAX NaN NaN USC TMIN NaN USC TMAX NaN NaN USC TMIN NaN I want drop rows max min Nan row index etc I able create correct boolean mask bool df min notnull amp df max notnull On applying mask Frame I get ID Date Element Data Value day max min USC TMIN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN USC TMIN NaN NaN NaN NaN NaN NaN NaN NaN USC TMIN NaN Now I simply use dropna method would drop rows columns Any way circumvent,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
57222220,2019-07-26,2019,2,"Simliar strings and want to create 3 separate dataframes using RegEx, Pandas in python","<p>I am currently trying to create groups for numbers in two very similar strings. I can't seem to separate the expressions, I started learning RegEx recently. I want to have 3 dataframes. A dataframe for ""V1"", ""V2"", and ""V3"". I only want the first value, within each bracket. So for example in V1, 1-22, I just want 75.43. Hopefully that makes sense, i'm a bit stuck. </p>

<pre><code>TEXT,TEXT,20190726,TEXT,TEXT00000,,NORMAL;
*
TEXT,TEXT-LT.V1,,,4.0,TEXT,NORMAL;
1-22,,{(75.43,0.0),(75.43,110.0),(75.45,119.0),(96.54,139.0),(109.25,159.0)},
23,,{(20.82,0.0),(20.82,110.0),(20.84,119.0),(41.93,139.0),(54.64,159.0)},
24,,{(81.26,0.0),(81.26,110.0),(81.28,119.0),(102.37,139.0),(115.08,159.0)},
*
*
TEXT,TEXT,20190726,TEXT,TEXT00000,,NORMAL;
*
TEXT,TEXT-TEXT.V2,,,4.0,TEXT,NORMAL;
1-22,,{(74.93,0.0),(74.93,110.0),(74.95,119.0),(74.95,139.0),(74.95,163.0)},
23,,{(24.98,0.0),(24.98,110.0),(25.00,119.0),(25.00,139.0),(25.00,163.0)},
24,,{(80.76,0.0),(80.76,110.0),(80.78,119.0),(80.78,139.0),(80.78,163.0)},
*
*
TEXT,TEXT,20190726,TEXT,TEXT00000,,NORMAL;
*
TEXT,TEXT-TEXT.V3,,,2.0,TEXT,NORMAL;
1-22,,{(74.94,0.0),(74.94,70.0),(75.46,147.0),(96.54,167.0),(109.25,186.0),(109.27,210.0)},
23-24,,{(80.77,0.0),(80.77,70.0),(81.29,147.0),(102.37,167.0),(115.08,186.0),(115.10,210.0)},
*
</code></pre>

<pre><code>What I tried
f = open(""TextFile.txt"",""r"")
TextFile_str = f.read()
Value_Only = re.compile(r'(\d+-?\d+),+\{\((\d+\.\d+),\d+\.\d+\),\((\d+\.\d+),\d+\.\d+\),\((\d+\.\d+),\d+\.\d+\),\((\d+\.\d+),\d+\.\d+\),\((\d+\.\d+),\d+\.\d+\),*\(*(\d*\.*\d*),*\d*\.*\d*\)*\}*')
match_Value = Value_Only.findall(TextFile_str)
match_Value_df = pd.DataFrame(match_Value)
match_Value_df.columns = ['Hour', 'Value 1', 'Value 2', 'Value 3', 'Value 4', 'Value 5', 'Value 6']

#How it looks 
    Hour Value 1 Value 2 Value 3 Value 4 Value 5 Value 6
0   1-22   75.43   75.43   75.45   96.54  109.25        
1     23   20.82   20.82   20.84   41.93   54.64        
2     24   81.26   81.26   81.28  102.37  115.08        
3   1-22   74.93   74.93   74.95   74.95   74.95        
4     23   24.98   24.98   25.00   25.00   25.00        
5     24   80.76   80.76   80.78   80.78   80.78        
6   1-22   74.94   74.94   75.46   96.54  109.25  109.27
7  23-24   80.77   80.77   81.29  102.37  115.08  115.10
</code></pre>

<p>Ideally I want to have 3 separate dataframes for V1, V2, and V3. </p>

<pre><code>Expected Result
Dataframe 1 
    Hour Value 1 Value 2 Value 3 Value 4 Value 5 Value 6
0   1-22   75.43   75.43   75.45   96.54  109.25        
1     23   20.82   20.82   20.84   41.93   54.64        
2     24   81.26   81.26   81.28  102.37  115.08

Dataframe 2
    Hour Value 1 Value 2 Value 3 Value 4 Value 5 Value 6
0   1-22   74.93   74.93   74.95   74.95   74.95        
1     23   24.98   24.98   25.00   25.00   25.00        
2     24   80.76   80.76   80.78   80.78   80.78 

Dataframe 3
    Hour Value 1 Value 2 Value 3 Value 4 Value 5 Value 6
0   1-22   74.94   74.94   75.46   96.54  109.25  109.27
1  23-24   80.77   80.77   81.29  102.37  115.08  115.10
</code></pre>
","['python', 'regex', 'pandas']",57226620,"<p>If I understand you correctly, you want to split the dataframe whenever <code>Hour1 = 1-22</code>. Try this:</p>

<pre><code>s = (match_Value_df['Hour'] == '1-22').cumsum()
dfs = []
for i in range(s.min(), s.max() + 1):
    subDF = match_Value_df.loc[s == i]
    dfs.append(subDF)
</code></pre>

<p>Result:</p>

<pre><code>dfs[0]:
   Hour Value 1 Value 2 Value 3 Value 4 Value 5 Value 6
0  1-22   75.43   75.43   75.45   96.54  109.25        
1    23   20.82   20.82   20.84   41.93   54.64        
2    24   81.26   81.26   81.28  102.37  115.08        

dfs[1]:
   Hour Value 1 Value 2 Value 3 Value 4 Value 5 Value 6
3  1-22   74.93   74.93   74.95   74.95   74.95        
4    23   24.98   24.98   25.00   25.00   25.00        
5    24   80.76   80.76   80.78   80.78   80.78        

dfs[2]:
    Hour Value 1 Value 2 Value 3 Value 4 Value 5 Value 6
6   1-22   74.94   74.94   75.46   96.54  109.25  109.27
7  23-24   80.77   80.77   81.29  102.37  115.08  115.10
</code></pre>

<p>If you want to get them into 3 different variables:</p>

<pre><code>v1, v2, v3 = dfs[slice(0, 3)]
</code></pre>
",Simliar strings want create separate dataframes using RegEx Pandas python I currently trying create groups numbers two similar strings I seem separate expressions I started learning RegEx recently I want dataframes A dataframe V V V I want first value within bracket So example V I want Hopefully makes sense bit stuck TEXT TEXT TEXT TEXT NORMAL TEXT TEXT LT V TEXT NORMAL TEXT TEXT TEXT TEXT NORMAL TEXT TEXT TEXT V TEXT NORMAL TEXT TEXT TEXT TEXT NORMAL TEXT TEXT TEXT V TEXT NORMAL What I tried f open TextFile txt r TextFile str f read Value Only compile r match Value Value Only findall TextFile str match Value df pd DataFrame match Value match Value df columns Hour Value Value Value Value Value Value How looks Hour Value Value Value Value Value Value Ideally I want separate dataframes V V V Expected Result Dataframe Hour Value Value Value,"startoftags, python, regex, pandas, endoftags",python arrays numpy endoftags,python regex pandas,python arrays numpy,0.33
57823209,2019-09-06,2019,4,Calculate recurring customer,"<p>I'm analyzing sales data from a shop and want to calculate the percentage of ""first order customer"" who turn into recurring customers in following month.</p>

<p>I have a DataFrame with all the orders. This includes a customer id, a date and a flag if this is his/her first order. This is my data:</p>

<pre><code>import pandas as pd 

data = {'Name': ['Tom', 'nick', 'krish', 'Tom'], 
        'First_order': [1, 1, 1, 0], 
        'Date' :['01-01-2018', '01-01-2018', '01-01-2018', '02-02-2018']} 

df = pd.DataFrame(data) 
</code></pre>

<p>I would now create a list of all new customers in January and a list of all recurring customers in February and inner-join them. Then I have two numbers with which I could calculate the percentage.</p>

<p>But I have no clue, how I could calculate this rolling for a whole year without looping over the data frame. Is there a nice pandas/python way to do so?</p>

<p>The goal would be to have a new dataframe with the month and the percentage of recurring customers from the previous month.</p>
","['python', 'pandas', 'dataframe']",57823863,"<p>One thought would be to take all orders Jan-November and have a column ""reccurr"" which gives you a true/false based on if this customer ordered in the next month. Then you can take a per-month groupby with count / sum of true/falses and add a column giving the ratio.</p>

<p>EDIT: before this you may need to convert dates:</p>

<pre><code>df.Date = pd.to_datetime(df.Date)
</code></pre>

<p>Then:</p>

<pre><code>df['month'] = df['Date'].apply(lambda x: x.month) #this is for simplicity's sake, not hard to extend to MMYYYY
df1 = df[df.month != 12].copy() #now we select everything but Nov
df1 = df1[df1.First_order == 1].copy() #and filter out non-first orders
df1['recurr'] = df1.apply(lambda x: True if len(df[(df.month == x.month + 1)&amp;(df.Name == x.Name)])&gt;0 else False, axis=1) #Now we fill a column with True if it finds an order from the same person next month
df2 = df1[['month','Name','recurr']].groupby('month').agg({'Name':'count','recurr':'sum'})
</code></pre>

<p>At this point, for each month, the ""Name"" column has number of first orders and ""recurr"" column has number of those that ordered again the following month. A simple extra column gives you percentage:</p>

<pre><code>df2['percentage_of_recurring_customer'] = (df2.recurr/df2.Name)*100
</code></pre>

<p>EDIT: For any number of dates, here's a clumsy solution. Choose a start date and use that year's January as month 1, and number all months sequentially after that.</p>

<pre><code>df.Date = pd.to_datetime(df.Date)
start_year = df.Date.min().year
def get_month_num(date):
    return (date.year-start_year)*12+date.month
</code></pre>

<p>Now that we have a function to convert dates, the slightly changed code:</p>

<pre><code>df['month'] = df['Date'].apply(lambda x: get_month_num(x))
df1 = df[df.First_order == 1].copy()
df1['recurr'] = df1.apply(lambda x: True if len(df[(df.month == x.month + 1)&amp;(df.Name == x.Name)])&gt;0 else False, axis=1)
df2 = df1[['month','Name','recurr']].groupby('month').agg({'Name':'count','recurr':'sum'})
</code></pre>

<p>Finally, you can make a function to revert your month numbers into dates:</p>

<pre><code>def restore_month(month_num):
    year = int(month_num/12)+start_year #int rounds down so we can do this.
    month = month_num%12 #modulo gives us month
    return pd.Timestamp(str(year)+'-'+str(month)+'-1') #This returns the first of that month
df3 = df2.reset_index().copy() #removing month from index so we can change it.
df3['month_date'] = df3['month'].apply(lambda x: restore_month(x))
</code></pre>
",Calculate recurring customer I analyzing sales data shop want calculate percentage first order customer turn recurring customers following month I DataFrame orders This includes customer id date flag first order This data import pandas pd data Name Tom nick krish Tom First order Date df pd DataFrame data I would create list new customers January list recurring customers February inner join Then I two numbers I could calculate percentage But I clue I could calculate rolling whole year without looping data frame Is nice pandas python way The goal would new dataframe month percentage recurring customers previous month,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
57845233,2019-09-08,2019,3,Comparing abbreviated words in pandas,"<p>I'm experimenting/learning Python with a data set containing information on companies.</p>

<p>The DataFrame structure is the following (these are made up records):</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'key': [111, 222, 333, 444, 555, 666, 777, 888, 999], 
                   'left_name' : ['ET CETERA SYSTEMS', 'ODDS AND ENDS', 'MAXIMA COMPANY', 'MUSIC MANY', 
                                  'GRAPHIC MASTER', 'ARC SECURITY', 'MINDNSOLES', 'REX ENERGY', 'THESIS COMPANY'],
                  'right_name' : ['ET CETERA SYS', 'ODDSNENDS', 'MAX COMP', 'MUSICMANY', 'GRAPHIC MSTR', 
                                  'ARC SECU', 'MIND AND SOLES', 'REXX', 'THESIS COMP']})

print(df)

   key          left_name      right_name
0  111  ET CETERA SYSTEMS   ET CETERA SYS
1  222      ODDS AND ENDS       ODDSNENDS
2  333     MAXIMA COMPANY        MAX COMP
3  444         MUSIC MANY       MUSICMANY
4  555     GRAPHIC MASTER    GRAPHIC MSTR
5  666       ARC SECURITY        ARC SECU
6  777         MINDNSOLES  MIND AND SOLES
7  888         REX ENERGY            REXX
8  999     THESIS COMPANY     THESIS COMP
</code></pre>

<p>My <strong>goal</strong> is to compare the acronyms of each <code>(left_name, right_name)</code> pair. Specifically, if the abbreviated string formed by the concatenation of the initial letters of <code>left_name</code> is equal to the abbreviated string formed by the concatenation of the initial letters of <code>right_name</code>, then return a flag of <code>1</code>. Else, return <code>0</code>. </p>

<p>For instance, if we compare the first two abbreviated pairs, then: </p>

<ul>
<li><code>ECS == ECS</code> &rarr; <code>1</code></li>
<li><code>OAE != O</code> &rarr; <code>0</code></li>
</ul>

<p>Visually, the resulting DataFrame I'm looking for should look like this:</p>

<pre><code>   key          left_name      right_name  name_flag
0  111  ET CETERA SYSTEMS   ET CETERA SYS          1
1  222      ODDS AND ENDS       ODDSNENDS          0
2  333     MAXIMA COMPANY        MAX COMP          1
3  444         MUSIC MANY       MUSICMANY          0
4  555     GRAPHIC MASTER    GRAPHIC MSTR          1
5  666       ARC SECURITY        ARC SECU          1
6  777         MINDNSOLES  MIND AND SOLES          0
7  888         REX ENERGY            REXX          0
8  999     THESIS COMPANY     THESIS COMP          1
</code></pre>

<p>I think my question is closely related to this one: <a href=""https://stackoverflow.com/questions/5775719/upper-case-first-letter-of-each-word-in-a-phrase"">Upper case first letter of each word in a phrase</a></p>

<p>Unfortunately, I wasn't able to adapt the code appropriately for my problem. Any additional help would be greatly appreciated.</p>
","['python', 'pandas', 'dataframe']",57845305,"<pre><code>def abbr(x):
    return ''.join([letter[0] for letter in x.split(' ')])

df['name_flag'] = (df['left_name'].apply(abbr) == df['right_name'].apply(abbr)).astype(int)
</code></pre>

<p>output:</p>

<pre><code>0    1
1    0
2    1
3    0
4    1
5    1
6    0
7    0
8    1


''.join(re.findall(r'^[A-Z]|\s[A-Z]',s)).replace(' ','')
</code></pre>

<p>or </p>

<pre><code>''.join(re.findall(r'\b\w',s))
</code></pre>

<p>also works in the function</p>
",Comparing abbreviated words pandas I experimenting learning Python data set containing information companies The DataFrame structure following made records import pandas pd df pd DataFrame key left name ET CETERA SYSTEMS ODDS AND ENDS MAXIMA COMPANY MUSIC MANY GRAPHIC MASTER ARC SECURITY MINDNSOLES REX ENERGY THESIS COMPANY right name ET CETERA SYS ODDSNENDS MAX COMP MUSICMANY GRAPHIC MSTR ARC SECU MIND AND SOLES REXX THESIS COMP print df key left name right name ET CETERA SYSTEMS ET CETERA SYS ODDS AND ENDS ODDSNENDS MAXIMA COMPANY MAX COMP MUSIC MANY MUSICMANY GRAPHIC MASTER GRAPHIC MSTR ARC SECURITY ARC SECU MINDNSOLES MIND AND SOLES REX ENERGY REXX THESIS COMPANY THESIS COMP My goal compare acronyms left name right name pair Specifically abbreviated string formed concatenation initial letters left name equal abbreviated string formed concatenation initial letters right name return flag Else return For instance compare first two abbreviated pairs ECS ECS rarr,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
57971393,2019-09-17,2019,2,Join based only on condition python,"<p>I want to join but only in cases where a certain other condition is <code>True</code></p>

<p>I have a DataFrame <code>df1</code> that looks like this</p>

<pre><code>ID    Bool   Val
1111  True   AAA
2222  False  BBB
3333  True   CCC
4444  False  DDD
</code></pre>

<p>I then have another DataFrame <code>df2</code> like this</p>

<pre><code>ID    Val
1111  EEE
3333  FFF
5555  GGG
</code></pre>

<p>And I want to overwrite the <code>Val</code> column in <code>df1</code> where the <code>ID</code> match and the <code>Bool</code> is <code>True</code>. This would look like this</p>

<pre><code>ID    Bool   Val
1111  True   EEE 
2222  False  BBB
3333  True   FFF
4444  False  DDD
</code></pre>

<p>As you can see the <code>AAA</code> and <code>CCC</code> have been overwritten. </p>

<p>I am thinking of doing this with an <code>if</code> statement</p>
","['python', 'pandas', 'dataframe']",57971707,"<p>You can use <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer"">boolean indexing</a> with <code>loc</code> to select the rows of df1 where <code>'bool'</code> column is True ,also use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer"">pandas.Series.map</a> to assign correctly:</p>

<pre><code>b=df1['Bool']
df1.loc[b,'Val']=df1.loc[b,'ID'].map(df2.set_index('ID')['Val'])
print(df1)

     ID   Bool  Val
0  1111   True  EEE
1  2222  False  BBB
2  3333   True  FFF
3  4444  False  DDD
</code></pre>

<p>Also you can use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.replace.html"" rel=""nofollow noreferrer"">pandas.Series.replace</a>:</p>

<pre><code>df1.loc[b,'Val']=df1.loc[b,'ID'].replace(df2.set_index('ID')['Val'])
</code></pre>
",Join based condition python I want join cases certain condition True I DataFrame df looks like ID Bool Val True AAA False BBB True CCC False DDD I another DataFrame df like ID Val EEE FFF GGG And I want overwrite Val column df ID match Bool True This would look like ID Bool Val True EEE False BBB True FFF False DDD As see AAA CCC overwritten I thinking statement,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
58057897,2019-09-23,2019,4,partial intersection - multiple groups,"<p>I am not sure how to approach my problem, thus I haven't been able to see if it already exists (apologies in advance) </p>

<pre><code>Group    Item
 A         1
 A         2
 A         3
 B         1
 B         3
 C         1
 D         2
 D         3
</code></pre>

<p>I want to know all combinations of groups that share more than X items (2 in this example). And I want to know which items they share. </p>

<p><strong>RESULT</strong>:</p>

<pre><code>A-B: 2 (item 1 and item 3)
A-D: 2 (item 2 and item 3)
</code></pre>

<p>The list of groups and items is really long and the maximum number of item matches across groups is probably not more than 3-5.</p>

<p><strong>NB</strong> More than 2 groups can have shared items - e.g. A-B-E: 3
So it's not sufficient to only compare two groups at a time. I  need to compare all combination of groups. </p>

<p><strong>My thoughts</strong></p>

<ol>
<li><em>First round</em>: one pile of all groups - are at least two values shared amongst all?</li>
<li><em>Second round</em>: All-1 group (all combinations)</li>
<li><em>Third round</em>: All-2 groups (all combinations) </li>
</ol>

<p>Untill I reach the comparison between only two groups (all combinations). 
However this seems super heavy performance-wise!! And I have no idea of how to do this.</p>

<p>What are your thoughts?</p>

<p>Thanks!</p>
","['python', 'python-3.x', 'pandas']",58059075,"<p>Unless you have additional information to restrict the search, I would just process all subsets (having size >= 2) of the set of unique groups.</p>

<p>For each subset, I would search the items belonging to all members of the set:</p>

<pre><code>a = df['Group'].unique()
for cols in chain(*(combinations(a, i) for i in range(2, len(a) + 1))):
    vals = df['Item'].unique()
    for col in cols:
        vals = df.loc[(df.Group==col)&amp;(df.Item.isin(vals)), 'Item'].unique()
    if len(vals) &gt; 0: print(cols, vals)
</code></pre>

<p>it gives:</p>

<pre><code>('A', 'B') [1 3]
('A', 'C') [1]
('A', 'D') [2 3]
('B', 'C') [1]
('B', 'D') [3]
('A', 'B', 'C') [1]
('A', 'B', 'D') [3]
</code></pre>
",partial intersection multiple groups I sure approach problem thus I able see already exists apologies advance Group Item A A A B B C D D I want know combinations groups share X items example And I want know items share RESULT A B item item A D item item The list groups items really long maximum number item matches across groups probably NB More groups shared items e g A B E So sufficient compare two groups time I need compare combination groups My thoughts First round one pile groups least two values shared amongst Second round All group combinations Third round All groups combinations Untill I reach comparison two groups combinations However seems super heavy performance wise And I idea What thoughts Thanks,"startoftags, python, python3x, pandas, endoftags",python python3x list endoftags,python python3x pandas,python python3x list,0.67
58110695,2019-09-26,2019,2,Drop columns from a DataFrame based on their data types,"<p>How can I drop columns from a <code>DataFrame</code> <strong>elegantly</strong> where the criteria is to drop all columns of a specific data type.</p>

<p>I have used two approaches</p>

<pre><code>for i in df.columns:
    if(df[i].dtype)!='int64':
        df.drop(columns=i, inplace=True)

</code></pre>

<p><strong>Problem</strong> - It is not the greatest of approaches plus it does not display or show data until or unless done with <code>inplace=True</code> , which is not a good thinking at all</p>

<pre><code>df.select_dtypes(include='int64').head()

</code></pre>

<p>Second approach is to use this but it does not drop columns.</p>

<p>Any suggestions on a third approach?</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",58110721,"<p>Use <code>exclude</code> parameter in <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html"" rel=""nofollow noreferrer""><code>DataFrame.select_dtypes</code></a>:</p>

<pre><code>df = pd.DataFrame({
        'A':list('abcdef'),
         'B':[4,5,4,5,5,4],
         'C':[7,8,9,4,2,3],
         'D':[1.3,3,5,7,1,0],

})

print (df.dtypes)
A     object
B      int64
C      int64
D    float64
dtype: object

print (df.select_dtypes(exclude='int64'))
   A    D
0  a  1.3
1  b  3.0
2  c  5.0
3  d  7.0
4  e  1.0
5  f  0.0
</code></pre>
",Drop columns DataFrame based data types How I drop columns DataFrame elegantly criteria drop columns specific data type I used two approaches df columns df dtype int df drop columns inplace True Problem It greatest approaches plus display show data unless done inplace True good thinking df select dtypes include int head Second approach use drop columns Any suggestions third approach,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
58181262,2019-10-01,2019,6,GroupBy with ffill deletes group and does not put group in index,"<p>I'm running into a very strange issue ever since I have ported my code from one computer to another. I'm using pandas version 0.25.1 on this system, but am unsure on the pandas version I was using previously. </p>

<p>The issue is as follows: </p>

<p>I create a simple, unsorted (mock) dataframe on which I want to sort values and forward-fill all the NaN values.</p>

<pre><code>In [1]: import pandas as pd
   ...: import numpy as np

In [2]: test = pd.DataFrame({""group"" : [""A"", ""A"", ""A"", ""B"", ""B"", ""B"", ""C"", ""C""],
   ...:                      ""count"" : [2, 3, 1, 2, 1, 3, 1, 2],
   ...:                      ""value"" : [10, np.nan, 30, np.nan, 19, np.nan, 25, np.nan]})

In [3]: test
Out[3]:
  group  count  value
0     A      2   10.0
1     A      3    NaN
2     A      1   30.0
3     B      2    NaN
4     B      1   19.0
5     B      3    NaN
6     C      1   25.0
7     C      2    NaN
</code></pre>

<p>However, when I do that I lose the entire ""group"" column, and it does not reappear in my index either. </p>

<pre><code>In [4]: test.sort_values([""group"", ""count""]).groupby(""group"").ffill()
Out[4]:
   count  value
2      1   30.0
0      2   10.0
1      3   10.0
4      1   19.0
3      2   19.0
5      3   19.0
6      1   25.0
7      2   25.0
</code></pre>

<p>I've also tried to use the following using fillna, but that gives me the same result: </p>

<pre><code>In [5]: test.sort_values([""group"", ""count""]).groupby(""group"").fillna(method = ""ffill"")
Out[5]:
   count  value
2      1   30.0
0      2   10.0
1      3   10.0
4      1   19.0
3      2   19.0
5      3   19.0
6      1   25.0
7      2   25.0
</code></pre>

<p>Does anyone know what I am doing wrong? The issue seems to be with the ffill method, since I CAN use .mean() on the groupby and retain my groupings. </p>
","['python', 'pandas', 'pandas-groupby']",58181366,"<p>IICU, you have to use 'update` to get the results back to the dataframe</p>

<pre><code>test.update(test.sort_values([""group"", ""count""]).groupby(""group"").ffill())
print(test)
</code></pre>

<p><strong>Output</strong></p>

<pre><code>group   count   value
0   A   2   10.0
1   A   3   10.0
2   A   1   30.0
3   B   2   19.0
4   B   1   19.0
5   B   3   19.0
6   C   1   25.0
7   C   2   25.0
</code></pre>
",GroupBy ffill deletes group put group index I running strange issue ever since I ported code one computer another I using pandas version system unsure pandas version I using previously The issue follows I create simple unsorted mock dataframe I want sort values forward fill NaN values In import pandas pd import numpy np In test pd DataFrame group A A A B B B C C count value np nan np nan np nan np nan In test Out group count value A A NaN A B NaN B B NaN C C NaN However I I lose entire group column reappear index either In test sort values group count groupby group ffill Out count value I also tried use following using fillna gives result In test sort values group count groupby group fillna method ffill Out count value Does anyone know I wrong The issue seems ffill method,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas numpy endoftags,python pandas pandasgroupby,python pandas numpy,0.67
58503746,2019-10-22,2019,2,Remove tuples from list that have duplicate key element,"<p>I'm working on a reference implementing of a thinning mechanism comparable to that <a href=""https://forum.duplicati.com/t/new-retention-policy-deletes-old-backups-in-a-smart-way/2195"" rel=""nofollow noreferrer"">duplicati</a> that removes backups depending on age. Going back in time, it establishes a minimum time difference between backups in growing time frames. This algorithm is controlled by a <em>retention policy</em> written in a mini language <code>&lt;frame1&gt;:&lt;limit1&gt;,&lt;frame2&gt;:&lt;limit2&gt;,...,&lt;frameN&gt;:&lt;limitN&gt;</code></p>

<p>For example, <code>""1D:3h,1W:1D,3M:1W""</code> means</p>

<ul>
<li>keep one backup every 3 hours for one day</li>
<li>... every day for one week</li>
<li>... every week for a 3 months</li>
<li>remove all that are older</li>
</ul>

<p>Due to the incremental process,<sup>[1]</sup> ""programs"" in this mini language can be ""optimized"":</p>

<ul>
<li><code>""4W:2D,4W:1D""</code> into <code>""4W:2D""</code> applying rule #1 <code>x:a,x:b =&gt; x:a</code></li>
<li><code>""2W:2D,4W:2D""</code> into <code>""4W:2D""</code> applying rule #2 <code>a:x,b:x =&gt; max(a,b):x</code> </li>
<li><code>""2W:2D,4W:1D""</code> into <code>""4W:2D""</code> applying rule #3 <code>a:x,b:y =&gt; max(a,b):x (if x &gt;= y)</code></li>
</ul>

<p><em>Rule #3 is about consecutive time frames (<code>a &lt; b</code>) with a more restrictive limit (<code>x</code>) applied first. The limits are absolute distances, not frequencies, so (<code>x &gt; y</code>) means that <code>x</code> is more restrictive than <code>y</code>.</em></p>

<h3>Normalizing:</h3>

<p>As illustrated in rule #1, only the first of two same-frame thinning levels are applied. I chose to take the one with the least restrictive limit. So a pre-processing to the thinning strategy can be done that reduces the sorted list of pairs <em>(frame, limit)</em> to those of each <em>frame</em> with the lowest <em>limit</em> (I replaced time deltas by numbers):</p>

<pre><code>user_input = [(30, 3), (20, 4), (20, 5), (10, 1), (10, 2)]
expected_output = [(10, 1), (20, 4), (30, 3)]
</code></pre>

<p>This is what I have so far:</p>

<pre><code>def normalized(thinning):
    thinning = sorted(thinning)
    deduped = []
    key = None
    for t in thinning:
        if t[0] != key:
            deduped.append(t)
            key = t[0]
    thinning = deduped
    return thinning
</code></pre>

<p>Is there an obvious way that would lead to a more compact (maybe less procedural) version?</p>

<hr>

<p><sup>[1]</sup> At the beginning of the process all backups are collected in a to-do list. This list is sorted anti-chronological. Now the thinning strategy is applied time frame for time frame (smallest first): each time frame selects (and removes) timestamps from the to-do list.</p>
","['python', 'python-3.x', 'list']",58503802,"<p>You can use <a href=""https://docs.python.org/3/library/itertools.html#itertools.groupby"" rel=""nofollow noreferrer""><code>itertools.groupby</code></a> here:</p>

<pre><code>from itertools import groupby
from operator import itemgetter

[next(v) for _,v in groupby(sorted(user_input), key=itemgetter(0))]
# [(10, 1), (20, 4), (30, 3)]
</code></pre>
",Remove tuples list duplicate key element I working reference implementing thinning mechanism comparable duplicati removes backups depending age Going back time establishes minimum time difference backups growing time frames This algorithm controlled retention policy written mini language lt frame gt lt limit gt lt frame gt lt limit gt lt frameN gt lt limitN gt For example D h W D M W means keep one backup every hours one day every day one week every week months remove older Due incremental process programs mini language optimized W D W D W D applying rule x x b gt x W D W D W D applying rule x b x gt max b x W D W D W D applying rule x b gt max b x x gt Rule consecutive time frames lt b restrictive limit x applied first The limits absolute distances frequencies x gt means,"startoftags, python, python3x, list, endoftags",python python3x pandas endoftags,python python3x list,python python3x pandas,0.67
58626479,2019-10-30,2019,4,Is there a function for getting the number of unique values in the dataframe in each group?,"<p>I have a dataframe that has two columns: label and value. I would like to identify the number of unique values in the dataframe that occurs in each label group.</p>

<p>For example, given the following dataframe:</p>

<pre><code>test_df = pd.DataFrame({
    'label': [1, 1, 1, 1, 2, 2, 3, 3, 3], 
    'value': [0, 0, 1, 2, 1, 2, 2, 3, 4]})
test_df
</code></pre>

<pre><code>  label     value
0   1         0
1   1         0
2   1         1
3   1         2
4   2         1
5   2         2
6   3         2
7   3         3
8   3         4
</code></pre>

<p>The expected output is:</p>

<pre><code>  label     uni_val
0   1         1 -&gt; {0} is unique value for this label compared to other labels
1   2         0 -&gt; no unique values for this label compared to other labels
2   3         2 -&gt; {3, 4} are unique values for this label compared to other labels
</code></pre>

<p>One way of doing this is to get the unique values for each label and then count the non-duplicates of them across all elements.</p>

<pre><code>test_df.groupby('label')['value'].unique()

label
1    [0, 1, 2]
2       [1, 2]
3    [2, 3, 4]
Name: value, dtype: object
</code></pre>

<p>Is there a more efficient and simpler way?</p>
","['python', 'pandas', 'pandas-groupby']",58626616,"<p>You could drop duplicates on <code>['label', 'value']</code>, then drop duplicates on <code>value</code>:</p>

<pre><code>(test_df.drop_duplicates(['label','value'])         # remove duplicates on pair (label, value)
    .drop_duplicates('value', keep=False)           # only keep unique `value`
    .groupby('label')['value'].count()              # count as usual
    .reindex(test_df.label.unique(), fill_value=0)  # fill missing labels with 0
)
</code></pre>

<p>Output:</p>

<pre><code>label
1    1
2    0
3    2
Name: value, dtype: int64
</code></pre>
",Is function getting number unique values dataframe group I dataframe two columns label value I would like identify number unique values dataframe occurs label group For example given following dataframe test df pd DataFrame label value test df label value The expected output label uni val gt unique value label compared labels gt unique values label compared labels gt unique values label compared labels One way get unique values label count non duplicates across elements test df groupby label value unique label Name value dtype object Is efficient simpler way,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
58750432,2019-11-07,2019,2,Sort date in string format in a pandas dataframe?,"<p>I have a dataframe like this, how to sort this.</p>

<pre><code>  df = pd.DataFrame({'Date':['Oct20','Nov19','Jan19','Sep20','Dec20']})


        Date
    0   Oct20
    1   Nov19
    2   Jan19
    3   Sep20
    4   Dec20
</code></pre>

<p>I familiar in sorting list of dates(string)</p>

<pre><code> a.sort(key=lambda date: datetime.strptime(date, ""%d-%b-%y""))
</code></pre>

<p>Any thoughts? Should i split it ?</p>
","['python', 'pandas', 'dataframe']",58750585,"<p>First convert column to datetimes and get positions of sorted values by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.argsort.html"" rel=""noreferrer""><code>Series.argsort</code></a> what is used for change ordering with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html"" rel=""noreferrer""><code>DataFrame.iloc</code></a>:</p>

<pre><code>df = df.iloc[pd.to_datetime(df['Date'], format='%b%y').argsort()]
print (df)
    Date
2  Jan19
1  Nov19
3  Sep20
0  Oct20
4  Dec20
</code></pre>

<p><strong>Details</strong>:</p>

<pre><code>print (pd.to_datetime(df['Date'], format='%b%y'))
0   2020-10-01
1   2019-11-01
2   2019-01-01
3   2020-09-01
4   2020-12-01
Name: Date, dtype: datetime64[ns]
</code></pre>
",Sort date string format pandas dataframe I dataframe like sort df pd DataFrame Date Oct Nov Jan Sep Dec Date Oct Nov Jan Sep Dec I familiar sorting list dates string sort key lambda date datetime strptime date b Any thoughts Should split,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
59305514,2019-12-12,2019,6,Tensorflow: How to use tf.keras.metrics in multiclass classification?,"<p>I want to use some of these metrics when training my neural network:</p>

<pre><code>METRICS = [
  keras.metrics.TruePositives(name='tp'),
  keras.metrics.FalsePositives(name='fp'),
  keras.metrics.TrueNegatives(name='tn'),
  keras.metrics.FalseNegatives(name='fn'), 
  keras.metrics.Precision(name='precision'),
  keras.metrics.Recall(name='recall'),
  keras.metrics.CategoricalAccuracy(name='acc'),
  keras.metrics.AUC(name='auc'),
]

BATCH_SIZE = 1024
SHUFFLE_BUFFER_SIZE = 4000
train_dataset = tf.data.Dataset.from_tensor_slices((sent_vectors, labels))
train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embed_dim)))
for units in [256, 256]:
    model.add(tf.keras.layers.Dense(units, activation='relu'))
model.add(tf.keras.layers.Dense(4, activation='softmax'))
model.compile(optimizer='adam',
          loss='sparse_categorical_crossentropy',
          metrics=METRICS)
model.fit(
    train_dataset, 
    epochs=100)
</code></pre>

<p>But I get <code>Shapes (None, 4) and (None, 1) are incompatible</code>. I believe this is because I am doing multiclass classification on 4 classes but the metrics are calculated based on binary classification. How do I adjust my code for multiclass classification?</p>

<p><strong>Update:</strong> I am interested in gathering the metrics <strong>during the learning process</strong> like in <a href=""https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights"" rel=""nofollow noreferrer"">Tensorflow Imbalanced Classification</a>, not just at the end of the fitting process.</p>

<p><strong>Additional infos:</strong>
My input data are numpy arrays with the shape <code>sent_vectors.shape = (number_examples, 65, 300)</code> and <code>labels=(number_examples, 1)</code>. I have 4 labels: 0-3.</p>

<p>Stacktrace:</p>

<pre><code>ValueErrorTraceback (most recent call last)
&lt;ipython-input-46-2b73afaf7726&gt; in &lt;module&gt;
      1 model.fit(
      2     train_dataset,
----&gt; 3     epochs=10)

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--&gt; 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    322                 mode=ModeKeys.TRAIN,
    323                 training_context=training_context,
--&gt; 324                 total_epochs=epochs)
    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    326 

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    121         step=step, mode=mode, size=current_batch_size) as batch_logs:
    122       try:
--&gt; 123         batch_outs = execution_function(iterator)
    124       except (StopIteration, errors.OutOfRangeError):
    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     84     # `numpy` translates Tensors to values in Eager mode.
     85     return nest.map_structure(_non_none_constant_value,
---&gt; 86                               distributed_function(input_fn))
     87 
     88   return execution_function

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--&gt; 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    501       # This is the first call of __call__, so we have to initialize.
    502       initializer_map = object_identity.ObjectIdentityDictionary()
--&gt; 503       self._initialize(args, kwds, add_initializers_to=initializer_map)
    504     finally:
    505       # At this point we know that the initialization is complete (or less

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    406     self._concrete_stateful_fn = (
    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 408             *args, **kwds))
    409 
    410     def invalid_creator_scope(*unused_args, **unused_kwds):

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1846     if self.input_signature:
   1847       args, kwargs = None, None
-&gt; 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1849     return graph_function
   1850 

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2148         graph_function = self._function_cache.primary.get(cache_key, None)
   2149         if graph_function is None:
-&gt; 2150           graph_function = self._create_graph_function(args, kwargs)
   2151           self._function_cache.primary[cache_key] = graph_function
   2152         return graph_function, args, kwargs

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2039             arg_names=arg_names,
   2040             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 2041             capture_by_value=self._capture_by_value),
   2042         self._function_attributes,
   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    913                                           converted_func)
    914 
--&gt; 915       func_outputs = python_func(*func_args, **func_kwargs)
    916 
    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    357         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    359     weak_wrapped_fn = weakref.ref(wrapped_fn)
    360 

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in distributed_function(input_iterator)
     71     strategy = distribution_strategy_context.get_strategy()
     72     outputs = strategy.experimental_run_v2(
---&gt; 73         per_replica_function, args=(model, x, y, sample_weights))
     74     # Out of PerReplica outputs reduce or pick values to return.
     75     all_outputs = dist_utils.unwrap_output_dict(

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)
    758       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),
    759                                 convert_by_default=False)
--&gt; 760       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    761 
    762   def reduce(self, reduce_op, value, axis):

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)
   1785       kwargs = {}
   1786     with self._container_strategy().scope():
-&gt; 1787       return self._call_for_each_replica(fn, args, kwargs)
   1788 
   1789   def _call_for_each_replica(self, fn, args, kwargs):

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)
   2130         self._container_strategy(),
   2131         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):
-&gt; 2132       return fn(*args, **kwargs)
   2133 
   2134   def _reduce_to(self, reduce_op, value, destinations):

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    290   def wrapper(*args, **kwargs):
    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
--&gt; 292       return func(*args, **kwargs)
    293 
    294   if inspect.isfunction(func) or inspect.ismethod(func):

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)
    262       y,
    263       sample_weights=sample_weights,
--&gt; 264       output_loss_metrics=model._output_loss_metrics)
    265 
    266   if reset_metrics:

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py in train_on_batch(model, inputs, targets, sample_weights, output_loss_metrics)
    313     outs = [outs]
    314   metrics_results = _eager_metrics_fn(
--&gt; 315       model, outs, targets, sample_weights=sample_weights, masks=masks)
    316   total_loss = nest.flatten(total_loss)
    317   return {'total_loss': total_loss,

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py in _eager_metrics_fn(model, outputs, targets, sample_weights, masks)
     72         masks=masks,
     73         return_weighted_and_unweighted_metrics=True,
---&gt; 74         skip_target_masks=model._prepare_skip_target_masks())
     75 
     76   # Add metric results from the `add_metric` metrics.

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _handle_metrics(self, outputs, targets, skip_target_masks, sample_weights, masks, return_weighted_metrics, return_weighted_and_unweighted_metrics)
   2061           metric_results.extend(
   2062               self._handle_per_output_metrics(self._per_output_metrics[i],
-&gt; 2063                                               target, output, output_mask))
   2064         if return_weighted_and_unweighted_metrics or return_weighted_metrics:
   2065           metric_results.extend(

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _handle_per_output_metrics(self, metrics_dict, y_true, y_pred, mask, weights)
   2012       with K.name_scope(metric_name):
   2013         metric_result = training_utils.call_metric_function(
-&gt; 2014             metric_fn, y_true, y_pred, weights=weights, mask=mask)
   2015         metric_results.append(metric_result)
   2016     return metric_results

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py in call_metric_function(metric_fn, y_true, y_pred, weights, mask)
   1065 
   1066   if y_pred is not None:
-&gt; 1067     return metric_fn(y_true, y_pred, sample_weight=weights)
   1068   # `Mean` metric only takes a single value.
   1069   return metric_fn(y_true, sample_weight=weights)

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py in __call__(self, *args, **kwargs)
    191     from tensorflow.python.keras.distribute import distributed_training_utils  # pylint:disable=g-import-not-at-top
    192     return distributed_training_utils.call_replica_local_fn(
--&gt; 193         replica_local_fn, *args, **kwargs)
    194 
    195   @property

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py in call_replica_local_fn(fn, *args, **kwargs)
   1133     with strategy.scope():
   1134       return strategy.extended.call_for_each_replica(fn, args, kwargs)
-&gt; 1135   return fn(*args, **kwargs)
   1136 
   1137 

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py in replica_local_fn(*args, **kwargs)
    174     def replica_local_fn(*args, **kwargs):
    175       """"""Updates the state of the metric in a replica-local context.""""""
--&gt; 176       update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable
    177       with ops.control_dependencies([update_op]):
    178         result_t = self.result()  # pylint: disable=not-callable

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/metrics_utils.py in decorated(metric_obj, *args, **kwargs)
     73 
     74     with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):
---&gt; 75       update_op = update_state_fn(*args, **kwargs)
     76     if update_op is not None:  # update_op will be None in eager execution.
     77       metric_obj.add_update(update_op)

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py in update_state(self, y_true, y_pred, sample_weight)
    881         y_pred,
    882         thresholds=self.thresholds,
--&gt; 883         sample_weight=sample_weight)
    884 
    885   def result(self):

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/metrics_utils.py in update_confusion_matrix_variables(variables_to_update, y_true, y_pred, thresholds, top_k, class_id, sample_weight)
    276    y_true], _ = ragged_assert_compatible_and_get_flat_values([y_pred, y_true],
    277                                                              sample_weight)
--&gt; 278   y_pred.shape.assert_is_compatible_with(y_true.shape)
    279 
    280   if not any(

/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py in assert_is_compatible_with(self, other)
   1113     """"""
   1114     if not self.is_compatible_with(other):
-&gt; 1115       raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))
   1116 
   1117   def most_specific_compatible_shape(self, other):

ValueError: Shapes (None, 4) and (None, 1) are incompatible
</code></pre>
","['python', 'tensorflow', 'keras']",59309717,"<h2>Update:</h2>

<p>As OP edited his question, I decided to edit my solution either with the intention of providing a more compact answer:</p>

<p><strong>Import and define all we need later:</strong></p>

<pre><code>import numpy as np
from numpy import random as random
import tensorflow as tf
import keras
import keras.backend as K

tf.config.experimental_run_functions_eagerly(False)

VERBOSE = 1

keras.backend.clear_session()
sess = tf.compat.v1.Session()
sess.as_default()


### Just for dummy data
sent_vectors = random.rand(100, 65, 300).astype(np.float32)
labels = random.randint(0, 4, (100, 1))
labels = np.squeeze(labels, 1)

NUM_CLASSES = np.max(labels) + 1
BATCH_SIZE = 10
SHUFFLE_BUFFER_SIZE = 200
embed_dim = 8
### Just for dummy data
</code></pre>

<p><strong>Create custom metric:</strong> </p>

<pre><code>class CategoricalTruePositives(tf.keras.metrics.Metric):

    def __init__(self, num_classes, batch_size,
                 name=""categorical_true_positives"", **kwargs):
        super(CategoricalTruePositives, self).__init__(name=name, **kwargs)

        self.batch_size = batch_size
        self.num_classes = num_classes    

        self.cat_true_positives = self.add_weight(name=""ctp"", initializer=""zeros"")

    def update_state(self, y_true, y_pred, sample_weight=None):     

        y_true = K.argmax(y_true, axis=-1)
        y_pred = K.argmax(y_pred, axis=-1)
        y_true = K.flatten(y_true)

        true_poss = K.sum(K.cast((K.equal(y_true, y_pred)), dtype=tf.float32))

        self.cat_true_positives.assign_add(true_poss)

    def result(self):

        return self.cat_true_positives
</code></pre>

<p><strong>First compile and fit your model using only the metrics for multilabel evaluation including our custom function:</strong></p>

<blockquote>
  <p><strong>Important note:</strong> <br>OP provided a <code>label</code> shape <code>(number_examples, 1)</code>. Originally he used <code>loss='sparse_categorical_crossentropy'</code>, but the built_in metric <code>keras.metrics.CategoricalAccuracy</code>, he wanted to use, is not compatible with <code>sparse_categorical_crossentropy</code>, instead I used <code>categorical_crossentropy</code> i.e. the <strong>one-hot</strong> version of the original loss, which is appropriate for <code>keras.metrics.CategoricalAccuracy</code>. Thus I one-hot encoded <code>labels</code> for the loss function.</p>
</blockquote>

<pre><code>METRICS = [
  tf.keras.metrics.CategoricalAccuracy(name='acc'),
  CategoricalTruePositives(NUM_CLASSES, BATCH_SIZE),
]

# Transform labels to onehot encoding for metric CategoricalAccuracy
labels = tf.compat.v1.one_hot(labels, depth=NUM_CLASSES)
train_dataset = tf.data.Dataset.from_tensor_slices((sent_vectors, labels))
train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embed_dim)))

for units in [256, 256]:
    model.add(tf.keras.layers.Dense(units, activation='relu'))
model.add(tf.keras.layers.Dense(4, activation='softmax'))


model.compile(optimizer='adam',
          loss='categorical_crossentropy',
          metrics=[METRICS])
model.fit(
    train_dataset, 
    epochs=10,
    verbose=VERBOSE,
    shuffle=True)
</code></pre>

<p><strong>Predict and postprocess the results:</strong></p>

<pre><code>result = model.predict(train_dataset)

pred_size = sent_vectors.shape[0]

preds = K.argmax(result, axis=-1)
preds = K.one_hot(preds, NUM_CLASSES)

print(""\nTrue positives per classes:"")
for i in range(4):
    m = tf.keras.metrics.TruePositives(name='tp')    
    m.update_state(labels[:, i], preds[:, i])
    print(""Class {} true positives: {}"".format(i, m.result()))
</code></pre>

<p><strong>Out:</strong></p>

<pre><code>Epoch 1/10
10/10 [==============================] - 3s 328ms/step - loss: 1.4226 - acc: 0.2300 - categorical_true_positives: 23.0000
Epoch 2/10
10/10 [==============================] - 0s 21ms/step - loss: 1.3876 - acc: 0.2900 - categorical_true_positives: 29.0000
Epoch 3/10
10/10 [==============================] - 0s 20ms/step - loss: 1.3721 - acc: 0.2800 - categorical_true_positives: 28.0000
Epoch 4/10
10/10 [==============================] - 0s 20ms/step - loss: 1.3628 - acc: 0.2900 - categorical_true_positives: 29.0000
Epoch 5/10
10/10 [==============================] - 0s 22ms/step - loss: 1.3447 - acc: 0.3800 - categorical_true_positives: 38.0000
Epoch 6/10
10/10 [==============================] - 0s 22ms/step - loss: 1.3187 - acc: 0.3800 - categorical_true_positives: 38.0000
Epoch 7/10
10/10 [==============================] - 0s 22ms/step - loss: 1.2653 - acc: 0.4300 - categorical_true_positives: 43.0000
Epoch 8/10
10/10 [==============================] - 0s 21ms/step - loss: 1.1760 - acc: 0.6000 - categorical_true_positives: 60.0000
Epoch 9/10
10/10 [==============================] - 0s 22ms/step - loss: 1.1809 - acc: 0.4600 - categorical_true_positives: 46.0000
Epoch 10/10
10/10 [==============================] - 0s 22ms/step - loss: 1.2739 - acc: 0.3800 - categorical_true_positives: 38.0000

True positives per classes:
Class 0 true positives: 16.0
Class 1 true positives: 0.0
Class 2 true positives: 5.0
Class 3 true positives: 7.0
</code></pre>

<p><strong>Note:</strong></p>

<p>We can recognize, that the true positives' sum is not equal with our training result, that is because we trained and predicted our model against a different data get from <code>train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)</code>.</p>
",Tensorflow How use tf keras metrics multiclass classification I want use metrics training neural network METRICS keras metrics TruePositives name tp keras metrics FalsePositives name fp keras metrics TrueNegatives name tn keras metrics FalseNegatives name fn keras metrics Precision name precision keras metrics Recall name recall keras metrics CategoricalAccuracy name acc keras metrics AUC name auc BATCH SIZE SHUFFLE BUFFER SIZE train dataset tf data Dataset tensor slices sent vectors labels train dataset train dataset shuffle SHUFFLE BUFFER SIZE batch BATCH SIZE model tf keras Sequential model add tf keras layers Bidirectional tf keras layers LSTM embed dim units model add tf keras layers Dense units activation relu model add tf keras layers Dense activation softmax model compile optimizer adam loss sparse categorical crossentropy metrics METRICS model fit train dataset epochs But I get Shapes None None incompatible I believe I multiclass classification classes metrics calculated based binary classification How,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
59319399,2019-12-13,2019,3,Can I check pandas dataframe index is end?,"<p>I use while loop for show data in dataframe</p>

<pre><code>while True:
    last_id = get_last_id()
    res = df.iloc[last_id + 1]
</code></pre>

<p>when last_id end of data and still use last_id + 1 it show error </p>

<pre><code>IndexError: single positional indexer is out-of-bounds
</code></pre>

<p>Can I check if no last_id + 1 in dataframe index then not show anything ?</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",59319417,"<p>Use <code>if-else</code> with test by length of <code>DataFrame</code>, because selecting by positions:</p>

<pre><code>while True:
    last_id = get_last_id()
    if len(df) != last_id + 1:
        res = df.iloc[last_id + 1]
     else:
        print ('cannot select next value after last index')
</code></pre>

<p>Another idea is use <code>try-except</code> statement:</p>

<pre><code>while True:
    try:
        last_id = get_last_id()
        res = df.iloc[last_id + 1]
    except IndexError:
        print ('cannot select next value after last index')
</code></pre>
",Can I check pandas dataframe index end I use loop show data dataframe True last id get last id res df iloc last id last id end data still use last id show error IndexError single positional indexer bounds Can I check last id dataframe index show anything,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
59436710,2019-12-21,2019,2,How do I run the tensorflow sample code? (throws an error),"<p>I am very new to TF2 and tried to customize the example code on the tensorflow guide documentation:</p>

<p><a href=""https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example</a></p>

<p>The code given in the guide does not run if the latent dimension is set 1, it runs fine for every latent dimension >1!</p>

<p>For training I tried to use the code given in the example but set the latent dim to 1:</p>

<pre><code>vae = VariationalAutoEncoder(784, 64, 1)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())
vae.fit(x_train, x_train, epochs=3, batch_size=64)
</code></pre>

<p>The error when trying to train is:</p>

<p><strong>ValueError: The last dimension of the inputs to Dense should be defined. Found None</strong>
and is thrown upon return from the Sample function where I think </p>

<pre><code>epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
</code></pre>

<p>can not handle shape=(?,1).</p>

<p>Can someone help I am trying to use the code as a template but I need latent dimension to be 1!</p>

<p>Thanks</p>
","['python', 'tensorflow', 'keras']",59668249,"<p>OK.  The answer came from MarkDoust in Github <a href=""https://github.com/tensorflow/tensorflow/issues/35464#issuecomment-572177889"" rel=""nofollow noreferrer"">here</a></p>

<p>The problem here is the interaction of broadcasting with the <code>z_mean + tf.exp(0.5 * z_log_var) * epsilon</code> line.</p>

<p>Normally the last dimension of z_mean, and z_log_var are known but the last dimension of epsilon is not.</p>

<p>Since you mul and add epsilon it assumes the two have the same shape.</p>

<p>When they have a last dimension of 1, It thinks you might be boradcasting z_mean and z_log_var with epsilon, it can't tell.</p>

<p>So the fix is to tell it that you know the shape of epsilon, and are not broadcastng. Add the following before the <code>z_mean + tf.exp(0.5 * z_log_var) * epsilon</code> line: <code>epsilon.set_shape(z_mean.shape)</code>.</p>
",How I run tensorflow sample code throws error I new TF tried customize example code tensorflow guide documentation https www tensorflow org guide keras custom layers models putting together end end example The code given guide run latent dimension set runs fine every latent dimension For training I tried use code given example set latent dim vae optimizer tf keras optimizers Adam learning rate e vae compile optimizer loss tf keras losses MeanSquaredError vae fit x train x train epochs batch size The error trying train ValueError The last dimension inputs Dense defined Found None thrown upon return Sample function I think epsilon tf keras backend random normal shape batch dim handle shape Can someone help I trying use code template I need latent dimension Thanks,"startoftags, python, tensorflow, keras, endoftags",python django djangorestframework endoftags,python tensorflow keras,python django djangorestframework,0.33
59446800,2019-12-22,2019,2,What should be my input shape for the code below,"<p>Here i have a very simple regression code written with Tensorflow:</p>

<pre><code>import tensorflow as tf
import numpy as np

x_train = np.array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]],
               dtype=""float32"")
y_train = np.array([[-1,  1,  3,  5,  7,  9, 11, 13, 15, 17, 19]],
               dtype=""float32"")

loss = 'mean_squared_error'
optimizer = tf.keras.optimizers.Adam(0.1)

model = tf.keras.Sequential([
tf.keras.layers.Dense(units=1, input_shape=[1])])

model.compile(loss=loss, optimizer=optimizer)

history = model.fit(x_train, y_train, epochs=500)
</code></pre>

<p>When i run this code i get this error:</p>

<p><strong>ValueError: Error when checking input: expected dense_24_input to have shape (1,) but got array with shape (11,)</strong></p>

<p>Now i know if i remove a pair of brackets from my x_train and y_train, the code will run fine, like this:</p>

<pre><code>x_train = np.array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10],
               dtype=""float32"")
</code></pre>

<p>But my question is, <strong>how can i run this code with the same data shape?</strong> What should be the <strong>input_shape</strong> so i don't get the error above?</p>
","['python', 'tensorflow', 'keras']",59453324,"<p>if I understand you want to use your data format</p>

<p>So you've to  do this litle change on your layer <code>[tf.keras.layers.Dense(units=11, input_dim=11)]</code>. As you use <code>mean_squared_error</code>. Your output should be the same as your <code>inut_dim</code>, I set <code>units=11</code>.</p>

<p>Wrap up.</p>

<pre><code>import tensorflow as tf
import numpy as np

x_train = np.array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]], dtype=""float32"")
y_train = np.array([[-1,  1,  3,  5,  7,  9, 11, 13, 15, 17, 19]], dtype=""float32"")

loss = 'mean_squared_error'
optimizer = tf.keras.optimizers.Adam(0.1)

model = tf.keras.Sequential([tf.keras.layers.Dense(units=11, input_dim=11)])

model.compile(loss=loss, optimizer=optimizer)

history = model.fit(x_train, y_train, epochs=500)
</code></pre>
",What input shape code Here simple regression code written Tensorflow import tensorflow tf import numpy np x train np array dtype float train np array dtype float loss mean squared error optimizer tf keras optimizers Adam model tf keras Sequential tf keras layers Dense units input shape model compile loss loss optimizer optimizer history model fit x train train epochs When run code get error ValueError Error checking input expected dense input shape got array shape Now know remove pair brackets x train train code run fine like x train np array dtype float But question run code data shape What input shape get error,"startoftags, python, tensorflow, keras, endoftags",python django djangorestframework endoftags,python tensorflow keras,python django djangorestframework,0.33
59615483,2020-01-06,2020,2,Pandas: How to map the values of a Dataframe to another Dataframe?,"<p>I am totally new to Python and just learning with some use cases I have.</p>

<p>I have 2 Data Frames, one is where I need the values in the Country Column, and another is having the values in the column named 'Countries' which needs to be mapped in the main Data Frame referring to the column named 'Data'.
(Please accept my apology if this question has already been answered)</p>

<p>Below is the Main DataFrame:</p>

<pre><code>Name Data                     | Country
----------------------------- | ---------
Arjun Kumar Reddy las Vegas   |
Divya london Khosla           |
new delhi Pragati Kumari      |
Will London Turner            |
Joseph Mascurenus Bombay      |
Jason New York Bourne         |
New york Vice Roy             |
Joseph Mascurenus new York    |
Peter Parker California       |
Bruce (istanbul) Wayne        |
</code></pre>

<p>Below is the Referenced DataFrame:</p>

<pre><code>Data           | Countries
-------------- | ---------
las Vegas      | US
london         | UK
New Delhi      | IN
London         | UK
bombay         | IN
New York       | US
New york       | US
new York       | US
California     | US
istanbul       | TR
Moscow         | RS
Cape Town      | SA
</code></pre>

<p>And what I want in the result will look like below:</p>

<pre><code>Name Data                     | Country
----------------------------- | ---------
Arjun Kumar Reddy las Vegas   | US
Divya london Khosla           | UK
new delhi Pragati Kumari      | IN
Will London Turner            | UK
Joseph Mascurenus Bombay      | IN
Jason New York Bourne         | US
New york Vice Roy             | US
Joseph Mascurenus new York    | US
Peter Parker California       | US
Bruce (istanbul) Wayne        | TR
</code></pre>

<p>Please note, Both the dataframes are not same in size.
I though of using map or Fuzzywuzzy method but couldn't really achieved the result.</p>
","['python', 'pandas', 'dataframe']",59616884,"<p>Find the country key that matches in the reference dataframe and extract it.</p>

<pre><code>regex = '(' + ')|('.join(ref_df['Data']) + ')'
df['key'] = df['Name Data'].str.extract(regex, flags=re.I).bfill(axis=1)[0]

&gt;&gt;&gt; df
                     Name Data        key
0  Arjun Kumar Reddy las Vegas  las Vegas
1       Bruce (istanbul) Wayne   istanbul
2   Joseph Mascurenus new York   new York


&gt;&gt;&gt; ref_df
        Data Country
0  las Vegas      US
1   new York      US
2   istanbul      TR
</code></pre>

<p>Merge both the dataframes on key extracted.</p>

<pre><code>pd.merge(df, ref_df, left_on='key', right_on='Data')
                     Name Data        key       Data Country
0  Arjun Kumar Reddy las Vegas  las Vegas  las Vegas      US
1       Bruce (istanbul) Wayne   istanbul   istanbul      TR
2   Joseph Mascurenus new York   new York   new York      US
</code></pre>
",Pandas How map values Dataframe another Dataframe I totally new Python learning use cases I I Data Frames one I need values Country Column another values column named Countries needs mapped main Data Frame referring column named Data Please accept apology question already answered Below Main DataFrame Name Data Country Arjun Kumar Reddy las Vegas Divya london Khosla new delhi Pragati Kumari Will London Turner Joseph Mascurenus Bombay Jason New York Bourne New york Vice Roy Joseph Mascurenus new York Peter Parker California Bruce istanbul Wayne Below Referenced DataFrame Data Countries las Vegas US london UK New Delhi IN London UK bombay IN New York US New york US new York US California US istanbul TR Moscow RS Cape Town SA And I want result look like Name Data Country Arjun Kumar Reddy las Vegas US Divya london Khosla UK new delhi Pragati Kumari IN Will London Turner UK Joseph,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
59663777,2020-01-09,2020,2,Find maximum in Dataframe based on variable values,"<p>I have a dataframe of the form:</p>

<pre><code>A| B| C | D
a| x| r | 1
a| x| s | 2
a| y| r | 1
b| w| t | 4
b| z| v | 2
</code></pre>

<p>I'd like to be able to return something like (showing unique values and frequency)</p>

<pre><code>A| freq of most common value in Column B |maximum of column D based on the most common value in Column B | most common value in Column B
a       2                                                  2                                                           x
b       1                                                  4                                                           w
</code></pre>

<p>at the moment i can calculate the everything but the 3 column of the result dataframe quiet fast via </p>

<pre><code>df = (df.groupby('A', sort=False)['B']
    .apply(lambda x: x.value_counts().head(1))
    .reset_index()
</code></pre>

<p>but to calculate the 2 Column (""maximum of column D based on the most common value in Column B"") i have writen a for-loop witch is slow for a lot of data. 
Is there a fast way?</p>

<p>The question is linked to: <a href=""https://stackoverflow.com/questions/59646659/count-values-in-dataframe-based-on-entry"">Count values in dataframe based on entry</a></p>
","['python', 'pandas', 'pandas-groupby']",59664038,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge.html"" rel=""noreferrer""><code>merge</code></a> with get rows by maximum <code>D</code> per groups by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.idxmax.html"" rel=""noreferrer""><code>DataFrameGroupBy.idxmax</code></a>:</p>

<pre><code>df1 = (df.groupby('A', sort=False)['B']
        .apply(lambda x: x.value_counts().head(1))
        .reset_index()
        .rename(columns={'level_1':'E'}))
#print (df1)

df = df1.merge(df, left_on=['A','E'], right_on=['A','B'], suffixes=('','_'))
df = df.loc[df.groupby('A')['D'].idxmax(), ['A','B','D','E']]
print (df)
   A  B  D  E
1  a  2  2  x
2  b  1  4  w
</code></pre>
",Find maximum Dataframe based variable values I dataframe form A B C D x r x r b w b z v I like able return something like showing unique values frequency A freq common value Column B maximum column D based common value Column B common value Column B x b w moment calculate everything column result dataframe quiet fast via df df groupby A sort False B apply lambda x x value counts head reset index calculate Column maximum column D based common value Column B writen loop witch slow lot data Is fast way The question linked Count values dataframe based entry,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
59836303,2020-01-21,2020,3,"Select rows where 3 consecutive values match condition - Python, Pandas","<p>I have a dataframe like:</p>

<pre><code>   values
0   45
1   47
2   58
3   40
4   45
5   40
6   50
7   55
8   60
9   60
10  20
...
</code></pre>

<p>I would like to obtain a dataframe containing only rows where 3 consecutive values are greater than a specific number, let's say greater than 44.
The resultin df would be:</p>

<pre><code>  values
0   45
1   47
2   58
6   50
7   55
8   60
9   60
...
</code></pre>

<p>Please note that value=45 in index=3 has been excluded because there are no 3 consecutive values greater than 44.
Thank you!</p>
","['python', 'pandas', 'dataframe']",59836422,"<p>Use:</p>

<pre><code>A = 44
B = 3

m = df['values'].gt(A)
s = (~m).cumsum()[m]
df1 = df[s.map(s.value_counts()).ge(B).reindex(df.index, fill_value=False)]
print (df1)
   values
0      45
1      47
2      58
6      50
7      55
8      60
9      60
</code></pre>

<p>Explanation/details:</p>

<p>First compare by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.gt.html"" rel=""nofollow noreferrer""><code>Series.gt</code></a> for greater:</p>

<pre><code>print (df['values'].gt(A))
0      True
1      True
2      True
3     False
4      True
5     False
6      True
7      True
8      True
9      True
10    False
Name: values, dtype: bool
</code></pre>

<p>Then create groups by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cumsum.html"" rel=""nofollow noreferrer""><code>Series.cumsum</code></a> with inverted mask by <code>~</code>:</p>

<pre><code>print ((~m).cumsum())
0     0
1     0
2     0
3     1
4     1
5     2
6     2
7     2
8     2
9     2
10    3
Name: values, dtype: int32
</code></pre>

<p>Filter mask only by greater values with <code>m</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer""><code>boolean indexing</code></a>:</p>

<pre><code>print ((~m).cumsum()[m])
0    0
1    0
2    0
4    1
6    2
7    2
8    2
9    2
Name: values, dtype: int32
</code></pre>

<p>Compare by second value by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ge.html"" rel=""nofollow noreferrer""><code>Series.ge</code></a> for greater od equal:</p>

<pre><code>print (s.map(s.value_counts()).ge(B))
0     True
1     True
2     True
4    False
6     True
7     True
8     True
9     True
Name: values, dtype: bool
</code></pre>

<p>Last add filter out rows by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.reindex.html"" rel=""nofollow noreferrer""><code>Series.reindex</code></a>, so possible filter by <a href=""http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer""><code>boolean indexing</code></a>:</p>

<pre><code>print (s.map(s.value_counts()).ge(B).reindex(df.index, fill_value=False))
0      True
1      True
2      True
3     False
4     False
5     False
6      True
7      True
8      True
9      True
10    False
Name: values, dtype: bool
</code></pre>
",Select rows consecutive values match condition Python Pandas I dataframe like values I would like obtain dataframe containing rows consecutive values greater specific number let say greater The resultin df would values Please note value index excluded consecutive values greater Thank,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
59861580,2020-01-22,2020,4,Pandas df reorder rows and columns according to integer index list,"<p>I have the following structure for my data frame: </p>

<pre><code>          col1    col2   col3
 myindex
 apple    A       B      C
 pear     Ab      Bb     Cb
 turtle   A1      B1     C1
</code></pre>

<p>Now I get two lists, one with reordered column indices, one with reordered row indices, but as integers, for example <code>rowindices = [3,1,2]</code> and <code>colindices = [1,3,2]</code>. The data frame should now be reordered according to these indices and then look like this:</p>

<pre><code>          col1   col3   col2
 myindex
 turtle   A1     C1     B1
 apple    A      C      B
 pear     Ab     Cb     Bb
</code></pre>

<p>How can this be done?</p>
","['python', 'pandas', 'dataframe']",59861604,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html"" rel=""noreferrer""><code>DataFrame.iloc</code></a> and because python counts from <code>0</code> convert list to arrays and subtract <code>1</code>:</p>

<pre><code>rowindices = [3,1,2]
colindices = [1,3,2]

df = df.iloc[np.array(rowindices)-1, np.array(colindices)-1]
print (df)
        col1 col3 col2
myindex               
turtle    A1   C1   B1
apple      A    C    B
pear      Ab   Cb   Bb
</code></pre>

<p>If is possible change values in lists solution is simplier:</p>

<pre><code>rowindices1 = [2,0,1]
colindices1 = [0,2,1]

df = df.iloc[rowindices1, colindices1]
print (df)
        col1 col3 col2
myindex               
turtle    A1   C1   B1
apple      A    C    B
pear      Ab   Cb   Bb
</code></pre>
",Pandas df reorder rows columns according integer index list I following structure data frame col col col myindex apple A B C pear Ab Bb Cb turtle A B C Now I get two lists one reordered column indices one reordered row indices integers example rowindices colindices The data frame reordered according indices look like col col col myindex turtle A C B apple A C B pear Ab Cb Bb How done,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
59874744,2020-01-23,2020,3,Elegant way to reduce/filter redundant sample data with bitcode,"<p>I want to ask for ideas of an elegant(time saving) way to reduce redundant digitally sampled data. 
I got a .csv files with a structure like that:</p>

<pre><code>time, data1, data2, data3
0.1, 1, 1, 0
0.2, 1, 1, 0
0.3, 1, 1, 0
0.4, 1, 0, 0
0.5, 1, 0, 0
0.6, 1, 0, 0
0.7, 0, 1, 0
0.8, 0, 1, 0
</code></pre>

<p>I want to reduce the incoming data in such manner that all unnecessary time steps are removed. E.g.</p>

<pre><code>time, data1, data2, data3
0.1, 1, 1, 0 
0.4, 1, 0, 0
0.7, 0, 1, 0
</code></pre>

<p>The remaining data should just consist of the timesteps whenever the bitcode has changed.
I already implemented a method importing the csv file with <strong>pandas</strong> and using the corresponding <em>mask</em> and <em>compress</em> functions. But the way I implemented it is very time consuming.</p>
","['python', 'pandas', 'csv']",59874785,"<p>If first column is index use:</p>

<pre><code>df = df[df.ne(df.shift()).any(axis=1)]
print (df)
      data1  data2  data3
time                     
0.1       1      1      0
0.4       1      0      0
0.7       0      1      0
</code></pre>

<p>If first column is not index:</p>

<pre><code>df1 = df.iloc[:, 1:]
df = df[df1.ne(df1.shift()).any(axis=1)]
print (df)
   time  data1  data2  data3
0   0.1      1      1      0
3   0.4      1      0      0
6   0.7      0      1      0
</code></pre>
",Elegant way reduce filter redundant sample data bitcode I want ask ideas elegant time saving way reduce redundant digitally sampled data I got csv files structure like time data data data I want reduce incoming data manner unnecessary time steps removed E g time data data data The remaining data consist timesteps whenever bitcode changed I already implemented method importing csv file pandas using corresponding mask compress functions But way I implemented time consuming,"startoftags, python, pandas, csv, endoftags",python pandas datetime endoftags,python pandas csv,python pandas datetime,0.67
59954640,2020-01-28,2020,2,Python How to access by keys when iterating a dictionary returns tuple,"<p>Iterating over a dictionary returns a tuple. Then how can the elements be accessed with <code>key</code> if the iteration retunes a tuple? 
I am expecting the iteration to give the nested dictionary so I can traverse further and access items with keys. With tuple returned I can't.</p>

<pre><code>&gt;&gt;&gt; d = { 'f': { 'f1': { 'f11':''}, }, 's': {  }}
&gt;&gt;&gt; d
{'f': {'f1': {'f11': ''}}, 's': {}}
&gt;&gt;&gt; for p in d.items():
...                 print(type(p))
...                 print(p['f1'])
...
&lt;class 'tuple'&gt;
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 3, in &lt;module&gt;
TypeError: tuple indices must be integers or slices, not str
</code></pre>
","['python', 'list', 'dictionary']",59954714,"<p>It is probably easiest if you unpack the tuple in the for loop:</p>

<pre class=""lang-py prettyprint-override""><code>for key, val in d.items():
    print(key, val)
</code></pre>
",Python How access keys iterating dictionary returns tuple Iterating dictionary returns tuple Then elements accessed key iteration retunes tuple I expecting iteration give nested dictionary I traverse access items keys With tuple returned I gt gt gt f f f gt gt gt f f f gt gt gt p items print type p print p f lt class tuple gt Traceback recent call last File lt stdin gt line lt module gt TypeError tuple indices must integers slices str,"startoftags, python, list, dictionary, endoftags",python python3x list endoftags,python list dictionary,python python3x list,0.67
59992582,2020-01-30,2020,3,Is there a way to shift multiple rows from pandas dataframes with different lengths?,"<p>I have a pandas dataframe that looks like the image attached
<a href=""https://i.stack.imgur.com/hHdPN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hHdPN.png"" alt=""Dataframe I am working with""></a>
Each column with name Wells 1,2,..,n have different beginnings of production. Some of them at month 5 or month 9, some after 24 months.
I want to normalize the start date for all of them. That is, shifting all non-zero values up. 
I know that this sample code works well only for Well 7 but I want to optimize it and do all of them at once.</p>

<pre><code>df['Well 7'] = df['Well 7'].shift(-1)
</code></pre>

<p>I am new to pandas, i tried in a loop but the dataframe name does not work in a loop.</p>

<pre><code>  df['Well {0}'].format(well)
</code></pre>

<p>Any help is appreciated!</p>
","['python', 'pandas', 'dataframe']",59992838,"<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.startswith.html"" rel=""nofollow noreferrer""><code>Series.str.startswith</code></a> to detect Well columns ,<strong>cols_Well</strong> in example (this step could be omitted and select columns yourself).</p>

<p>Then we can calculate the number of 0 initials with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cumsum.html"" rel=""nofollow noreferrer""><code>Series.cumsum</code></a>, <strong>shift_cols_Well</strong> in example.
Therefore this series tells us the parameter to pass to <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.shift.html"" rel=""nofollow noreferrer""><code>Series.shift</code></a>:</p>

<pre><code>cols_Well = df.columns[df.columns.str.startswith('Well')]
shift_cols_Well = df[cols_Well].ne(0).cumsum().eq(0).sum()
#shift_cols_Well = df[cols_Well].eq(0).cumprod().sum()
for col in cols_Well:
    df[col] = df[col].shift(-shift_cols_Well.loc[col])
</code></pre>

<hr>

<p><strong>Example</strong></p>

<pre><code>df = pd.DataFrame({'Time':range(1,10),
                   'Well 1':[0,2,3,4,5,6,7,8,9],'Well 2':[0,0,3,4,5,6,7,8,9]})

   Time  Well 1  Well 2
0     1       0       0
1     2       2       0
2     3       3       3
3     4       4       4
4     5       5       5
5     6       6       6
6     7       7       7
7     8       8       8
8     9       9       9
</code></pre>

<hr>

<p><strong>Solution Example</strong></p>

<pre><code>cols_Well = df.columns[df.columns.str.startswith('Well')]
shift_cols_Well = df[cols_Well].ne(0).cumsum().eq(0).sum()
#shift_cols_Well = df[cols_Well].eq(0).cumprod().sum()
for col in cols_Well:
    df[col] = df[col].shift(-shift_cols_Well.loc[col])
print(df)

   Time  Well 1  Well 2
0     1     2.0     3.0
1     2     3.0     4.0
2     3     4.0     5.0
3     4     5.0     6.0
4     5     6.0     7.0
5     6     7.0     8.0
6     7     8.0     9.0
7     8     9.0     NaN
8     9     NaN     NaN
</code></pre>

<hr>

<p><strong>Detail</strong></p>

<pre><code>print(shift_cols_Well)
Well 1    1
Well 2    2
dtype: int64
</code></pre>
",Is way shift multiple rows pandas dataframes different lengths I pandas dataframe looks like image attached Each column name Wells n different beginnings production Some month month months I want normalize start date That shifting non zero values I know sample code works well Well I want optimize df Well df Well shift I new pandas tried loop dataframe name work loop df Well format well Any help appreciated,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
60008563,2020-01-31,2020,2,Combine dataframes based on a shared key,"<p>I have a directory that each contains dataframes stored as csv files. Each dataframe has some columns, one of which is a key value which has a partner dataframe somewhere else in the directory (which uses the same key). I am trying to find a way to concatenate the two dataframes based on this key. </p>

<p>Each data-frame broadly looks like this:</p>

<pre><code>----------------------------------------
    my_key | variable 1 | variable 2  
----------------------------------------
     A           1             2      
     A           6             5      
     A           7             8     
</code></pre>

<p>So, the key for this data-frame is A. I am essentially trying to find a way to find the other data-frame where A is the key. What I want to end up with is something like this</p>

<pre><code>---------------------------------------------------------------
    df_key | variable 1 | variable 2 | variable 3 | variable 4
---------------------------------------------------------------
     A           1             2           3             4      
     A           5             6           7             8      
     A           9             10          11            12     
</code></pre>

<p>I had initially thought to store data-frames into two separate data-frame lists, and then do some sort of vlookup type function on the key value between lists. But I'm not sure how to achieve this. Some pseudo-code that will hopefully explain what I'm trying to achieve would be something like:</p>

<pre><code>for df1 in dflist_1:
    for df2 in dflist_2
        if df1[key] == df2[key]
            df1.concatenate(df2)
</code></pre>

<p>Columns between the two partner dataframes are unique, so there shouldn't be a problem concatenating them.</p>
","['python', 'pandas', 'dataframe']",60008624,"<p>I believe what you're looking for is a <a href=""https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=join#pyspark.sql.DataFrame.join"" rel=""nofollow noreferrer"">JOIN</a>. This allows a user to combine two datasets based on a mutual key.</p>

<pre><code>df_1.join(df_2, df_1.my_key == df_2.my_key, 'inner').show()
</code></pre>
",Combine dataframes based shared key I directory contains dataframes stored csv files Each dataframe columns one key value partner dataframe somewhere else directory uses key I trying find way concatenate two dataframes based key Each data frame broadly looks like key variable variable A A A So key data frame A I essentially trying find way find data frame A key What I want end something like df key variable variable variable variable A A A I initially thought store data frames two separate data frame lists sort vlookup type function key value lists But I sure achieve Some pseudo code hopefully explain I trying achieve would something like df dflist df dflist df key df key df concatenate df Columns two partner dataframes unique problem concatenating,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
60100534,2020-02-06,2020,2,tf.function input parameters,"<p>I write a function in tensorflow 2, and use tf.keras to write a model. The function is defined below:</p>

<pre class=""lang-py prettyprint-override""><code>@tf.function
def mask_output(input_tensor,mask):
    if tf.math.count_nonzero(mask) &gt; 0:
        output_tensor = tf.multiply(input_tensor, mask)
    else:
        output_tensor = input_tensor
    return output_tensor
</code></pre>

<p>The two parameters I give it is the tensor in the model. However, when I define the model, and call that function in the model definition, it says:</p>

<blockquote>
  <p>{_SymbolicException}Inputs to eager execution function cannot be Keras symbolic tensors, but found </p>
</blockquote>

<pre><code>[&lt;tf.Tensor 'a_dense2/Identity:0' shape=(None, 12, 5) dtype=float32&gt;, &lt;tf.Tensor 'a_mask_input:0' shape=(None, 12, 5) dtype=float32&gt;]
</code></pre>

<p>How to solve that? Why can't I call that funciton with two keras tensor input?</p>
","['python', 'tensorflow', 'keras']",62189969,"<p>If running under eager mode, tensorflow operations will check if the inputs are of type <code>tensorflow.python.framework.ops.EagerTensor</code> and keras ops are implemented as DAGs. So if the inputs to the eager mode is of <code>tensorflow.python.framework.ops.Tensor</code>, then this throws the error.</p>

<p>You can change the input type to EagerTensor by explicity telling tensorflow to run in eager mode for keras by using <code>tf.config.experimental_run_functions_eagerly(True)</code>. Adding this statement should solve your issue.</p>

<p>For example, this program throws the error you are facing -</p>

<p><strong>Code to reproduce the error-</strong></p>

<pre><code>import numpy as np
import tensorflow as tf
print(tf.__version__)
from tensorflow.keras import layers, losses, models

def get_loss_fcn(w):
    def loss_fcn(y_true, y_pred):
        loss = w * losses.mse(y_true, y_pred)
        return loss
    return loss_fcn

data_x = np.random.rand(5, 4, 1)
data_w = np.random.rand(5, 4)
data_y = np.random.rand(5, 4, 1)

x = layers.Input([4, 1])
w = layers.Input([4])
y = layers.Activation('tanh')(x)
model = models.Model(inputs=[x, w], outputs=y)
loss = get_loss_fcn(model.input[1])

model.compile(loss=loss)
model.fit((data_x, data_w), data_y)
</code></pre>

<p><strong>Output -</strong></p>

<pre><code>2.2.0
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---&gt; 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:

TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: input_8:0

During handling of the above exception, another exception occurred:

_SymbolicException                        Traceback (most recent call last)
8 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     72       raise core._SymbolicException(
     73           ""Inputs to eager execution function cannot be Keras symbolic ""
---&gt; 74           ""tensors, but found {}"".format(keras_symbolic_tensors))
     75     raise e
     76   # pylint: enable=protected-access

_SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [&lt;tf.Tensor 'input_8:0' shape=(None, 4) dtype=float32&gt;]
</code></pre>

<p><strong>Solution -</strong> Adding this <code>tf.config.experimental_run_functions_eagerly(True)</code> at the top of the program runs the program successfully. Also adding <code>tf.compat.v1.disable_eager_execution()</code> at the top of the progrm to disable eager execution also runs the program successfully.</p>

<p><strong>Fixed code -</strong></p>

<pre><code>import numpy as np
import tensorflow as tf
print(tf.__version__)
from tensorflow.keras import layers, losses, models

tf.config.experimental_run_functions_eagerly(True)

def get_loss_fcn(w):
    def loss_fcn(y_true, y_pred):
        loss = w * losses.mse(y_true, y_pred)
        return loss
    return loss_fcn

data_x = np.random.rand(5, 4, 1)
data_w = np.random.rand(5, 4)
data_y = np.random.rand(5, 4, 1)

x = layers.Input([4, 1])
w = layers.Input([4])
y = layers.Activation('tanh')(x)
model = models.Model(inputs=[x, w], outputs=y)
loss = get_loss_fcn(model.input[1])

model.compile(loss=loss)
model.fit((data_x, data_w), data_y)

print('Done.')
</code></pre>

<p><strong>Output -</strong></p>

<pre><code>2.2.0
1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00
Done.
</code></pre>

<p>Hope this answers your question. Happy Learning.</p>
",tf function input parameters I write function tensorflow use tf keras write model The function defined tf function def mask output input tensor mask tf math count nonzero mask gt output tensor tf multiply input tensor mask else output tensor input tensor return output tensor The two parameters I give tensor model However I define model call function model definition says SymbolicException Inputs eager execution function cannot Keras symbolic tensors found lt tf Tensor dense Identity shape None dtype float gt lt tf Tensor mask input shape None dtype float gt How solve Why I call funciton two keras tensor input,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
60349071,2020-02-22,2020,4,How to utilise the date_parser parameter of pandas.read_csv(),"<p>I am getting an issue with the <code>timestamp</code> column in my csv file.</p>

<blockquote>
  <p>ValueError: could not convert string to float: '2020-02-21 22:00:00'</p>
</blockquote>

<p>for this line:</p>

<pre><code>    import numpy as np
import pandas as pd
import matplotlib.pylab as plt 
from datetime import datetime
from statsmodels.tools.eval_measures import rmse
from sklearn.preprocessing import MinMaxScaler
from keras.preprocessing.sequence import TimeseriesGenerator
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
import warnings
warnings.filterwarnings(""ignore"")

""Import dataset""
df = pd.read_csv('fx_intraday_1min_GBP_USD.csv')


train, test = df[:-3], df[-3:]
scaler = MinMaxScaler()
scaler.fit(train) &lt;----------- This line
train = scaler.transform(train)
test = scaler.transform(test)

n_input = 3
n_features = 4

generator = TimeseriesGenerator(train, train, length=n_input, batch_size=6)

model = Sequential()
model.add(LSTM(200, activation='relu', input_shape=(n_input, n_features)))
model.add(Dropout(0.15))
model.add(Dense(1))
model.compile(optimizers='adam', loss='mse')
model.fit_generator(generator, epochs=180)
</code></pre>

<p>How can I convert the <code>timestamp</code> column (preferably when reading the csv) to a float?</p>

<p><a href=""https://i.stack.imgur.com/RjzsE.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RjzsE.jpg"" alt=""enter image description here""></a></p>

<p><strong>Link to the dataset</strong>: <a href=""http://Stock%20DataSet"" rel=""nofollow noreferrer"">https://www.alphavantage.co/query?function=FX_INTRADAY&amp;from_symbol=GBP&amp;to_symbol=USD&amp;interval=1min&amp;apikey=OF7SE183CNQLT9DW&amp;datatype=csv</a></p>
","['python', 'pandas', 'dataframe']",60349587,"<h2>Performing Conversion On CSV Input Columns While Reading In The Data</h2>

<p>Reading in CSV data applying conversion to the timestamp column to get float values:</p>

<pre><code>&gt;&gt;&gt; df = pd.read_csv('~/Downloads/fx_intraday_1min_GBP_USD.csv', 
...                  converters={'timestamp': 
...                                 lambda t: pd.Timestamp(t).timestamp()})
&gt;&gt;&gt; df
       timestamp    open    high     low   close
0   1.582322e+09  1.2953  1.2964  1.2953  1.2964
1   1.582322e+09  1.2955  1.2957  1.2952  1.2957
2   1.582322e+09  1.2956  1.2958  1.2954  1.2957
3   1.582322e+09  1.2957  1.2958  1.2954  1.2957
4   1.582322e+09  1.2957  1.2958  1.2955  1.2956
..           ...     ...     ...     ...     ...
95  1.582317e+09  1.2966  1.2967  1.2964  1.2965
96  1.582317e+09  1.2967  1.2968  1.2965  1.2966
97  1.582317e+09  1.2965  1.2967  1.2964  1.2966
98  1.582317e+09  1.2964  1.2967  1.2962  1.2966
99  1.582316e+09  1.2963  1.2965  1.2961  1.2964

[100 rows x 5 columns]
</code></pre>

<p>This can be applied to other columns too. The <code>converters</code> parameter takes a dictionary with the key being the column name and the value a function.</p>

<p><code>date_parser</code> could be useful if the timestamp data spans more than one column or is in some strange format. The callback can receive the text from one or more columns for processing. The <code>parse_dates</code> parameter may need to be supplied with <code>date_parser</code> to indicate which columns to apply the callback to. <code>date_parser</code> is just a list of the column names or indices. An example of usage:</p>

<pre><code>df = pd.read_csv('~/Downloads/fx_intraday_1min_GBP_USD.csv', 
                 date_parser=lambda t: pd.Timestamp(t), 
                 parse_dates=['timestamp'])
</code></pre>

<p><code>pd.read_csv()</code> with no date/time parameters produces a timestamp column of type <code>object</code>. Simply specifying which column is the timestamp using <code>parse_dates</code> and no other additional parameters fixes that:</p>

<pre><code>&gt;&gt;&gt; df = pd.read_csv('~/Downloads/fx_intraday_1min_GBP_USD.csv', 
                     parse_dates=['timestamp'])
&gt;&gt;&gt; df.dtypes
timestamp    datetime64[ns]
open                float64
high                float64
low                 float64
close               float64
</code></pre>

<h2>Conversion of DataFrame Columns After Reading in CSV</h2>

<p>As another user suggested, there's another way to convert the contents of a column using <code>pd.to_datetime()</code>.</p>

<pre><code>&gt;&gt;&gt; df = pd.read_csv('~/Downloads/fx_intraday_1min_GBP_USD.csv')
&gt;&gt;&gt; df.dtypes
timestamp     object
open         float64
high         float64
low          float64
close        float64
dtype: object
&gt;&gt;&gt; df['timestamp'] = pd.to_datetime(df['timestamp'])
&gt;&gt;&gt; df.dtypes
timestamp    datetime64[ns]
open                float64
high                float64
low                 float64
close               float64
dtype: object
&gt;&gt;&gt; 
&gt;&gt;&gt; df['timestamp'] = df['timestamp'].apply(lambda t: t.timestamp())
&gt;&gt;&gt; df
       timestamp    open    high     low   close
0   1.582322e+09  1.2953  1.2964  1.2953  1.2964
1   1.582322e+09  1.2955  1.2957  1.2952  1.2957
2   1.582322e+09  1.2956  1.2958  1.2954  1.2957
3   1.582322e+09  1.2957  1.2958  1.2954  1.2957
4   1.582322e+09  1.2957  1.2958  1.2955  1.2956
..           ...     ...     ...     ...     ...
95  1.582317e+09  1.2966  1.2967  1.2964  1.2965
96  1.582317e+09  1.2967  1.2968  1.2965  1.2966
97  1.582317e+09  1.2965  1.2967  1.2964  1.2966
98  1.582317e+09  1.2964  1.2967  1.2962  1.2966
99  1.582316e+09  1.2963  1.2965  1.2961  1.2964

[100 rows x 5 columns]
</code></pre>

<p>Or to do it all in one shot without <code>pd.to_datetime()</code>:</p>

<pre><code>&gt;&gt;&gt; df = pd.read_csv('~/Downloads/fx_intraday_1min_GBP_USD.csv')
&gt;&gt;&gt;
&gt;&gt;&gt; df['timestamp'] = df['timestamp'] \
...                      .apply(lambda t: pd.Timestamp(t).timestamp())
&gt;&gt;&gt;
</code></pre>
",How utilise date parser parameter pandas read csv I getting issue timestamp column csv file ValueError could convert string float line import numpy np import pandas pd import matplotlib pylab plt datetime import datetime statsmodels tools eval measures import rmse sklearn preprocessing import MinMaxScaler keras preprocessing sequence import TimeseriesGenerator keras models import Sequential keras layers import Dense keras layers import LSTM keras layers import Dropout import warnings warnings filterwarnings ignore Import dataset df pd read csv fx intraday min GBP USD csv train test df df scaler MinMaxScaler scaler fit train lt This line train scaler transform train test scaler transform test n input n features generator TimeseriesGenerator train train length n input batch size model Sequential model add LSTM activation relu input shape n input n features model add Dropout model add Dense model compile optimizers adam loss mse model fit generator generator epochs How I convert timestamp column,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
60575828,2020-03-07,2020,2,More Pythonic way to build random clusters in Python,"<p>I would like to create a function which will create a uniform distribution random cluster centered around a set of co-ordinates and with a specified radius, I have done this with the below:</p>

<pre><code>import numpy as np

# create cluster builder
def cluster(center, radius=10, n=50):
    xx = np.random.uniform(center[0]-radius, center[0]+radius, size=n)
    yy = np.random.uniform(center[1]-radius, center[1]+radius, size=n)
    zz = np.random.uniform(center[2]-radius, center[2]+radius, size=n)
    return xx, yy, zz

# create random cluster
xx1, yy1, zz1 = cluster((25, 15, 5))
</code></pre>

<p>This works as expected, but I just feel that they're must be a more Pythonic way to build the cluster function. Does anyone have any suggestions?</p>
","['python', 'arrays', 'numpy']",60575943,"<p><a href=""https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.uniform.html"" rel=""nofollow noreferrer""><code>np.random.uniform</code></a> also accepts <code>low</code> and <code>high</code> as arrays/lists. Hence, we can simply do -</p>

<pre><code>c = np.asarray(center)
xx,yy,zz = np.random.uniform(c-radius, c+radius, size=(n,3)).T
</code></pre>

<p>If any older version only supported scalar <code>low</code> and <code>high</code>, we can use some scaling -</p>

<pre><code>xx,yy,zz = np.random.uniform(size=(3,n))*radius*2 + c[:,None] - radius
</code></pre>
",More Pythonic way build random clusters Python I would like create function create uniform distribution random cluster centered around set co ordinates specified radius I done import numpy np create cluster builder def cluster center radius n xx np random uniform center radius center radius size n yy np random uniform center radius center radius size n zz np random uniform center radius center radius size n return xx yy zz create random cluster xx yy zz cluster This works expected I feel must Pythonic way build cluster function Does anyone suggestions,"startoftags, python, arrays, numpy, endoftags",python pandas numpy endoftags,python arrays numpy,python pandas numpy,0.67
60618799,2020-03-10,2020,3,How to subtract values in a column using groupby,"<p>I have the following dataframe:</p>

<pre><code>ID  Days TreatmentGiven TreatmentNumber
--- ---- -------------- ---------------
1    0      False             NaN
1    30     False             NaN
1    40     True               1
1    56     False             NaN 
2    0      False             NaN
2    14     True               1
2    28     True               2 
</code></pre>

<p>I'd like to create a new column with a new baseline for Days based on when the first treatment was given (TreatmentNumber==1), grouped by ID so that the result is the following:</p>

<pre><code>ID  Days TreatmentGiven TreatmentNumber New_Baseline
--- ---- -------------- --------------- ------------
1    0      False             NaN          -40
1    30     False             NaN          -10
1    40     True               1            0
1    56     False             NaN           16
2    0      False             NaN          -14
2    14     True               1            0
2    28     True               2            14
</code></pre>

<p>What is the best way to do this?</p>

<p>Thank you.</p>
","['python', 'pandas', 'pandas-groupby']",60619102,"<p>Here is one approach with <code>series.where</code> + <code>groupby+transform</code>:</p>

<pre><code>s = df['Days'].where(df['TreatmentGiven']).groupby(df['ID']).transform('first')
df['New_Baseline'] = df['Days'].sub(s)
</code></pre>

<hr>

<p>Output</p>

<pre><code>   ID  Days  TreatmentGiven  TreatmentNumber  New_Baseline
0   1     0           False              NaN         -40.0
1   1    30           False              NaN         -10.0
2   1    40            True              1.0           0.0
3   1    56           False              NaN          16.0
4   2     0           False              NaN         -14.0
5   2    14            True              1.0           0.0
6   2    28            True              2.0          14.0
</code></pre>
",How subtract values column using groupby I following dataframe ID Days TreatmentGiven TreatmentNumber False NaN False NaN True False NaN False NaN True True I like create new column new baseline Days based first treatment given TreatmentNumber grouped ID result following ID Days TreatmentGiven TreatmentNumber New Baseline False NaN False NaN True False NaN False NaN True True What best way Thank,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
60721248,2020-03-17,2020,3,Pandas group or resample dataframe excluding columns,"<pre><code>import pandas as pd
import numpy as np
data = {'dateTimeGmt': {0: pd.Timestamp('2020-01-01 06:44:00'),
      1: pd.Timestamp('2020-01-01 06:45:00'),      2: pd.Timestamp('2020-01-01 07:11:00'),      3: pd.Timestamp('2020-01-01 07:12:00'),      4: pd.Timestamp('2020-01-01 07:12:00'),      5: pd.Timestamp('2020-01-01 07:14:00'),      6: pd.Timestamp('2020-01-01 10:04:00'),      7: pd.Timestamp('2020-01-01 10:04:00'),      8: pd.Timestamp('2020-01-01 11:45:00'),      9: pd.Timestamp('2020-01-01 06:45:00')},
     'id': {0: 4, 1: 4, 2: 4, 3: 5, 4: 5, 5: 5, 6: 5, 7: 6, 8: 6, 9: 6},
     'name': {0: 'four',      1: 'four',      2: 'four',      3: 'five',      4: 'five',      5: 'five',      6: 'five',      7: 'six',      8: 'six',      9: 'six'},     'a': {0: 1.0,      1: np.nan,      2: np.nan,      3: np.nan,      4: np.nan,      5: np.nan,      6: np.nan,      7: 5.0,      8: np.nan,      9: np.nan},     'b': {0: np.nan,      1: 3.0,      2: np.nan,      3: np.nan,      4: np.nan,      5: np.nan,      6: np.nan,      7: np.nan,      8: np.nan,      9: 3.0},     'c': {0: np.nan,      1: np.nan,      2: np.nan,      3: np.nan,      4: 2.0,      5: np.nan,      6: np.nan,      7: np.nan,      8: 0.0,      9: np.nan}}
df = pd.DataFrame(data)
</code></pre>

<p>I would like to flatten my dataframe such that all columns after <code>name</code> are grouped by the hour in <code>dateTimeGmt</code> and then by <code>id</code>/<code>name</code>.</p>

<p>I tried <code>df2 = df.groupby([df.dateTimeGmt.dt.date, df.dateTimeGmt.dt.hour, df.id, df.name]).sum()</code> This seems to work but combines all my grouping columns into the index.</p>

<p><code>df3 = df.groupby([df.dateTimeGmt.dt.date, df.dateTimeGmt.dt.hour, df.id, df.name], as_index = False).sum()</code> keeps <code>id</code> and <code>name</code> but the <code>dateTimeGmt</code> data is lost.</p>

<p>How do I group my data without losing the columns that have been grouped by?</p>
","['python', 'pandas', 'dataframe']",60721321,"<p>In your solution is necessary add <code>rename</code> for <code>date</code> and <code>hour</code>s columns names for avoid duplicated columns names, and last <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>DataFrame.reset_index</code></a>:</p>

<pre><code>df2 = (df.groupby([df.dateTimeGmt.dt.date.rename('date'),
                   df.dateTimeGmt.dt.hour.rename('h'), 'id', 'name'])
         .sum()
         .reset_index())
print (df2)
         date   h  id  name    a    b    c
0  2020-01-01   6   4  four  1.0  3.0  0.0
1  2020-01-01   6   6   six  0.0  3.0  0.0
2  2020-01-01   7   4  four  0.0  0.0  0.0
3  2020-01-01   7   5  five  0.0  0.0  2.0
4  2020-01-01  10   5  five  0.0  0.0  0.0
5  2020-01-01  10   6   six  5.0  0.0  0.0
6  2020-01-01  11   6   six  0.0  0.0  0.0
</code></pre>

<p>Or is possible use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Grouper.html"" rel=""nofollow noreferrer""><code>Grouper</code></a> by hour frequency:</p>

<pre><code>df2 = df.groupby([pd.Grouper(freq='H', key='dateTimeGmt'), 'id', 'name']).sum().reset_index()
print (df2)
          dateTimeGmt  id  name    a    b    c
0 2020-01-01 06:00:00   4  four  1.0  3.0  0.0
1 2020-01-01 06:00:00   6   six  0.0  3.0  0.0
2 2020-01-01 07:00:00   4  four  0.0  0.0  0.0
3 2020-01-01 07:00:00   5  five  0.0  0.0  2.0
4 2020-01-01 10:00:00   5  five  0.0  0.0  0.0
5 2020-01-01 10:00:00   6   six  5.0  0.0  0.0
6 2020-01-01 11:00:00   6   six  0.0  0.0  0.0
</code></pre>
",Pandas group resample dataframe excluding columns import pandas pd import numpy np data dateTimeGmt pd Timestamp pd Timestamp pd Timestamp pd Timestamp pd Timestamp pd Timestamp pd Timestamp pd Timestamp pd Timestamp pd Timestamp id name four four four five five five five six six six np nan np nan np nan np nan np nan np nan np nan np nan b np nan np nan np nan np nan np nan np nan np nan np nan c np nan np nan np nan np nan np nan np nan np nan np nan df pd DataFrame data I would like flatten dataframe columns name grouped hour dateTimeGmt id name I tried df df groupby df dateTimeGmt dt date df dateTimeGmt dt hour df id df name sum This seems work combines grouping columns index df df groupby df dateTimeGmt dt date df dateTimeGmt dt hour df id,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
60799274,2020-03-22,2020,2,Explode multiple coumns with different size list columns in pandas,"<p>I have following situation where i may get 300 columns in csv file and some of them are list parameter (50 columns)and they can uneven size including no values.</p>

<pre><code>    Time               COL1                        COL2
2020-03-13 10:43:00.500  0.0 10.0 20.0 30.0 40.0   50.0 60.0 70.0 80.0 90.0
2020-03-13 10:43:00.900  10.0 20.0 30.0 70.0       10.0 20.0
2020-03-13 10:44:00.100  10.0 20.0 30.0 70.0   
</code></pre>

<p>I want do<br>
1. explode the column based on highest frequency to rows.ex: 1st row max freq(space separated values) 
    is 5 and in 2nd row it is 4    </p>

<pre><code>Time                  COL1      COL2
2020-03-13 10:43:00.500   0.0    50.0
2020-03-13 10:43:00.580   10.0   60.0
2020-03-13 10:43:00.660   20.0   70.0
2020-03-13 10:43:00.740   30.0   80.0
2020-03-13 10:43:00.820   40.0   90.0    
2020-03-13 10:43:00.900   10.0   10.0 
2020-03-13 10:43:00.1150  20.0   20.0 
2020-03-13 10:43:00.1400  30.0 
2020-03-13 10:43:00.1650  80.0 
2020-03-13 10:44:00.100   10.0 
2020-03-13 10:44:00.350   20.0 
2020-03-13 10:44:00.600   30.0
2020-03-13 10:44:00.850   70.0
</code></pre>

<ol start=""2"">
<li><p>and Time filed should be between two subsequent row time. 
something like this.    </p>

<p>curr_row_time = data_frame['Time'][ind1]<br>
next_row_time = data_frame['Time'][ind1+1]
timestamp1 = datetime.datetime.strptime(str(curr_row_time ),
                                            '%Y-%m-%d %H:%M:%S.%f')
timestamp2 = datetime.datetime.strptime(str(next_row_time),
                                            '%Y-%m-%d %H:%M:%S.%f')
time_delta = (timestamp2 - timestamp1).total_seconds() * 1000
time_step_increment = time_delta / max_frequency (should be max for a row)
for last row max frequency can be taken either avg of it or last used max frequency
Appreciate if any suggestion for point 1 however 2nd point is on second priority but its good to have that also.</p></li>
</ol>

<p>For point 1 i have used below </p>

<p>I tried solution mentioned in the below links by converting space separated to  list
10.0 20.0 30.0 70.0   ----> [10.0, 20.0, 30.0, 70.0] 
<a href=""https://stackoverflow.com/questions/59377817/pandas-explode-on-multiple-columns"">Pandas Explode on Multiple columns</a>   </p>

<pre><code> File ""&lt;stdin&gt;"", line 3, in &lt;listcomp&gt;
  File ""&lt;__array_function__ internals&gt;"", line 6, in concatenate
ValueError: zero-dimensional arrays cannot be concatenated
</code></pre>

<p>Below links works for equal size list column
<a href=""https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows"">Split (explode) pandas dataframe string entry to separate rows</a></p>
","['python', 'pandas', 'dataframe']",60803049,"<p>First you can create the exploded columns like you want with <code>concat</code>, <code>str.split</code> and <code>stack</code>. Use <code>reset_index</code> and <code>join</code> to be able to get the column 'Time' associated. Then you need to change the values in the column 'Time' to create the interpolation. I'm not sure if one can interpolate directly a datetime column, but you can change the type to <code>int64</code>, <code>mask</code> the values if same than previous row with <code>shift</code> and <code>interpolate</code>. So like this:</p>

<pre><code>l_col = ['COL1', 'COL2']

df_f = pd.concat([df[col].str.split(' ', expand=True) for col in l_col ], 
                 axis=1, keys=l_col)\
         .stack()\
         .reset_index(level=1, drop=True)\
         .join(df[['Time']])\
         .reset_index(drop=True)

df_f['Time'] = pd.to_datetime(df_f['Time'].astype('int64')
                                          .mask(df_f.Time.eq(df_f.Time.shift()))
                                          .interpolate(method='linear'))

print (df_f)
    COL1  COL2                    Time
0    0.0  50.0 2020-03-13 10:43:00.500
1   10.0  60.0 2020-03-13 10:43:00.580
2   20.0  70.0 2020-03-13 10:43:00.660
3   30.0  80.0 2020-03-13 10:43:00.740
4   40.0  90.0 2020-03-13 10:43:00.820
5   10.0  10.0 2020-03-13 10:43:00.900
6   20.0  20.0 2020-03-13 10:43:15.700
7   30.0  None 2020-03-13 10:43:30.500
8   70.0  None 2020-03-13 10:43:45.300
9   10.0       2020-03-13 10:44:00.100
10  20.0  None 2020-03-13 10:44:00.100
11  30.0  None 2020-03-13 10:44:00.100
12  70.0  None 2020-03-13 10:44:00.100
</code></pre>

<p>I'm not sure what you want for the missing values in COL2 e.g. so you may need some <code>fillna</code> to work this out.</p>
",Explode multiple coumns different size list columns pandas I following situation may get columns csv file list parameter columns uneven size including values Time COL COL I want explode column based highest frequency rows ex st row max freq space separated values nd row Time COL COL Time filed two subsequent row time something like curr row time data frame Time ind next row time data frame Time ind timestamp datetime datetime strptime str curr row time Y H M S f timestamp datetime datetime strptime str next row time Y H M S f time delta timestamp timestamp total seconds time step increment time delta max frequency max row last row max frequency taken either avg last used max frequency Appreciate suggestion point however nd point second priority good also For point used I tried solution mentioned links converting space separated list Pandas Explode Multiple columns File lt stdin,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
60808081,2020-03-23,2020,2,Pandas Use a column consists of column names to populate the values dynamically into another column,"<p>I would like to obtain the 'Value' column below, from the original df:</p>

<pre><code>    A   B   C   Column_To_Use
0   2   3   4   A            
1   5   6   7   C            
2   8   0   9   B            


    A   B   C   Column_To_Use   Value       
0   2   3   4   A               2
1   5   6   7   C               7
2   8   0   9   B               0
</code></pre>
","['python', 'pandas', 'dataframe']",60808088,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.lookup.html"" rel=""nofollow noreferrer""><code>DataFrame.lookup</code></a>:</p>

<pre><code>df['Value'] = df.lookup(df.index, df['Column_To_Use'])
print (df)
   A  B  C Column_To_Use  Value
0  2  3  4             A      2
1  5  6  7             C      7
2  8  0  9             B      0
</code></pre>
",Pandas Use column consists column names populate values dynamically another column I would like obtain Value column original df A B C Column To Use A C B A B C Column To Use Value A C B,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
60851208,2020-03-25,2020,2,How to display a dataframe while changing one of its column value?,"<p>Let's take this dataframe :</p>

<pre><code>df = pd.DataFrame(dict(Col1=['a','b','c'], Col2=[1,-3,2]))
  Col1  Col2
0    a     1
1    b    -3
2    c     2
</code></pre>

<p>I would like to display this dataframe while changing Col2, replacing negative numbers by ""neg"" and positive ones by ""pos"".<br>
I could modify the column / add a new column then display or create a new dataframe specially to display that but I wonder if there is a more optimal way to do as <strong>I don't want to keep this modification</strong>. </p>

<p>I tried the following but I get the error ""lambda cannot contain assignment"" :</p>

<pre><code>df.apply(lambda x : x['Col2'] = ""pos"" if x['Col2'] &gt;= 0 else ""neg"")
</code></pre>

<p>Is there please a way to do ?</p>
","['python', 'pandas', 'dataframe']",60851487,"<p>In agreement with @anky_91 comment, the solution is using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html"" rel=""nofollow noreferrer"">df.assign</a> :</p>

<pre><code>df.assign(Col2=np.where(df['Col2'] &gt;= 0 ,'pos','neg'))
</code></pre>

<p>Output :</p>

<pre><code>  Col1 Col2
0    a  pos
1    b  neg
2    c  pos
</code></pre>
",How display dataframe changing one column value Let take dataframe df pd DataFrame dict Col b c Col Col Col b c I would like display dataframe changing Col replacing negative numbers neg positive ones pos I could modify column add new column display create new dataframe specially display I wonder optimal way I want keep modification I tried following I get error lambda cannot contain assignment df apply lambda x x Col pos x Col gt else neg Is please way,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
60851475,2020-03-25,2020,2,Scraping specific dd item using Python beautifulSoup,"<p>I am trying to extract specific 'dd' element from the website using Python</p>

<pre><code>headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) '\
           'AppleWebKit/537.36 (KHTML, like Gecko) '\
           'Chrome/75.0.3770.80 Safari/537.36'}

url = ""https://www.ranger5g.com/forum/threads/pre-collision-assist.3239""
page = requests.get(url, headers=headers)
soup = BeautifulSoup(page.text, 'html.parser')


vehicle=[]

for i in soup.findAll(""div"", class_=""message-userExtras""):
    for item in soup.find_all(""dd"")[::-1]:
        vehicle.append(item.get_text())
print(vehicle)

</code></pre>

<p>I am trying to extract only vehicle list from the url and my output should be as follows </p>

<pre><code>2019 Ford Ranger XLT FX4
2019 Ford Ranger Lariat FX4, 1973 Mercury Capri
Tahoe/Tundra/Fusion
2019 Ford Ranger Lariat - Saber; 2014 GMC Terrain

</code></pre>

<p>But my result is not what I expect it to be</p>
","['python', 'web-scraping', 'beautifulsoup']",60851695,"<p>Use regular expression re and search the <code>dt</code> tag with text <code>Vehicle</code> and then find the next <code>dd</code> tag.</p>

<pre><code>import re
from bs4 import BeautifulSoup
import requests
headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) '\
           'AppleWebKit/537.36 (KHTML, like Gecko) '\
           'Chrome/75.0.3770.80 Safari/537.36'}

url = ""https://www.ranger5g.com/forum/threads/pre-collision-assist.3239""
page = requests.get(url, headers=headers)
soup = BeautifulSoup(page.text, 'html.parser')

for item in soup.find_all(""div"",class_='message-userExtras'):
    print(item.find('dt',text=re.compile(""Vehicle"")).find_next('dd').text.strip())
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>2019 Ford Ranger XLT FX4
2019 Ford Ranger Lariat FX4, 1973 Mercury Capri
Tahoe/Tundra/Fusion
2019 Ford Ranger Lariat - Saber; 2014 GMC Terrain
2019 Ford Ranger Lariat FX4, 1973 Mercury Capri
2019 Ranger Lariat - 2019 Honda CRV Touring
2019 Ford Ranger XLT FX4
2019 Ford Ranger Lariat FX4, 1973 Mercury Capri
2019 Ranger Lariat SuperCab
2019 Ranger Lariat
Ranger Lariat
2019 Ford Ranger Lariat
Ranger Lariat
Ranger Lariat
2019 Ranger XLT 301A SuperCrew 4X4 2015 Ecoboost Mustang 50 Year Appereance Package convertible
</code></pre>
",Scraping specific dd item using Python beautifulSoup I trying extract specific dd element website using Python headers User Agent Mozilla X Linux x AppleWebKit KHTML like Gecko Chrome Safari url https www ranger g com forum threads pre collision assist page requests get url headers headers soup BeautifulSoup page text html parser vehicle soup findAll div class message userExtras item soup find dd vehicle append item get text print vehicle I trying extract vehicle list url output follows Ford Ranger XLT FX Ford Ranger Lariat FX Mercury Capri Tahoe Tundra Fusion Ford Ranger Lariat Saber GMC Terrain But result I expect,"startoftags, python, webscraping, beautifulsoup, endoftags",python webscraping beautifulsoup endoftags,python webscraping beautifulsoup,python webscraping beautifulsoup,1.0
61024128,2020-04-04,2020,2,Lookup and join dataframes with multiple column names in Python,"<p><strong>df1:</strong></p>

<pre><code>col1        col2        col3        col4        col5        col6        col7        col8 
7865                                                                                abc
                                    7269                                            def
            8726                                                                    ghi
                                                                        986         jkl
                                                7689                                mno
                        8762                                                        pqr
                                                                                    stu
                        9698                                                        vwx
            3568                                                                    yz
</code></pre>

<p><strong>df2:</strong></p>

<pre><code>Scientific value         mapping_value
1                        8726
2                        9698
3                        3568
4                        986
5                        7269
</code></pre>

<p>I want to match ""col1, col2, col3, col4, col5, col6, col7"" column values in df1 with ""mapping_value"" column in df2 and create a new column called ""Scientific value"" in df1 which would have entries from ""Scientific value"" column in df2.  </p>

<p><strong>Output:</strong></p>

<pre><code>col1        col2        col3        col4        col5        col6        col7        col8   Scientific value
7865                                                                                abc    
                                    7269                                            def    5
            8726                                                                    ghi    1
                                                                        986         jkl    4
                                                7689                                mno
                        8762                                                        pqr
                                                                                    stu
                        9698                                                        vwx    2
            3568                                                                    yz     3
</code></pre>

<p>Would merge work in this case..or would be happy to learn any other efficient method as well!!</p>

<p>Thanks!</p>
","['python', 'pandas', 'dataframe']",61024160,"<p>If there are only one number per rows for any of column without last solution should be simplify by <code>sum</code>, <code>max</code>, <code>min</code> function for one column, so is possible use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>Series.map</code></a>:</p>

<pre><code>s = df2.set_index('mapping_value')['Scientific value']
df1['Scientific value'] = df1.iloc[:, :-1].max(axis=1).map(s)
#if empty string are not missing values
#df1['Scientific value'] = df1.iloc[:, :-1].replace('',np.nan).max(axis=1).map(s)
</code></pre>

<p>If possible multiple values first select by position by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html"" rel=""nofollow noreferrer""><code>DataFrame.iloc</code></a>, then reshape by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>DataFrame.stack</code></a>, mapping by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>Series.map</code></a>, remove possible missing values for non match values and last aggregate lists:</p>

<pre><code>s = df2.set_index('mapping_value')['Scientific value']
df1['Scientific value'] = (df1.iloc[:, :-1]
                              .stack()
                              .map(s)
                              .dropna()
                              .groupby(level=0)
                              .agg(list))
</code></pre>
",Lookup join dataframes multiple column names Python df col col col col col col col col abc def ghi jkl mno pqr stu vwx yz df Scientific value mapping value I want match col col col col col col col column values df mapping value column df create new column called Scientific value df would entries Scientific value column df Output col col col col col col col col Scientific value abc def ghi jkl mno pqr stu vwx yz Would merge work case would happy learn efficient method well Thanks,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
61104317,2020-04-08,2020,4,ModuleNotFoundError: No module named &#39;tf&#39;,"<p>I'm having problem with tensorflow. I want to use ImageDataGenerator, but I'm receiving error ModuleNotFoundError: No module named 'tf'. Not sure what is the problem. I added this tf.<strong>version</strong> to test will it work, and it shows the version of tensorflow.</p>

<pre><code>    import tensorflow as tf
    from tensorflow import keras
    print(tf.__version__)
    from tf.keras.preprocessing.image import ImageDataGenerator
</code></pre>

<p>When I run this code, I get this:</p>

<pre><code>2.1.0
Traceback (most recent call last):
  File ""q:/TF/Kamen papir maaze/rks.py"", line 14, in &lt;module&gt;
    from tf.keras.preprocessing.image import ImageDataGenerator
ModuleNotFoundError: No module named 'tf'
</code></pre>
","['python', 'tensorflow', 'keras']",61104545,"<p>The line </p>

<pre><code>import tensorflow as tf 
</code></pre>

<p>means you are importing tensorflow with an alias as tf to call it modules/functions. </p>

<p>You cannot use the alias to import other modules. </p>

<p>For your case, if you call directly </p>

<pre><code>tf.keras.preprocessing.image.ImageDataGenerator(...) 
</code></pre>

<p>then it will work. </p>

<p>or </p>

<p>you need to import the module with the right module name. i.e. </p>

<pre><code>from tensorflow.keras.preprocessing.image import ImageDataGenerator
</code></pre>
",ModuleNotFoundError No module named tf I problem tensorflow I want use ImageDataGenerator I receiving error ModuleNotFoundError No module named tf Not sure problem I added tf version test work shows version tensorflow import tensorflow tf tensorflow import keras print tf version tf keras preprocessing image import ImageDataGenerator When I run code I get Traceback recent call last File q TF Kamen papir maaze rks py line lt module gt tf keras preprocessing image import ImageDataGenerator ModuleNotFoundError No module named tf,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
61129366,2020-04-09,2020,3,Python Regex to split comments into dataframe,"<p>I have a bunch of strings users have entered of various comments concatenated together. Sometimes they entered a date if there were comments on multiple days. I'm trying to find a way to split each date and the corresponding comment. The text comments might look like this:</p>

<pre><code>raw_text = ['3/30: The dog is red. 4/01: The dog is blue', 'there is a green door', '3-25:Foobar baz'] 
</code></pre>

<p>I would like to transform that text to:</p>

<pre><code>df = pd.DataFrame([[0,'3/30','The dog is red.'],[0,'4/01','The dog is blue'],[1,np.nan,'there is a green door'],[2,'3-25','Foobar baz']],columns = 'row_id','date','text')

print(df)

   row_id  date                   text
0       0  3/30        The dog is red.
1       0  4/01        The dog is blue
2       1   NaN  there is a green door
3       2  3-25             Foobar baz
</code></pre>

<p>I think what I need to do is find the semicolons, then work back to the first number before that semicolon to identify the dates (sometimes they use / to separate and sometimes -). </p>

<p>Any ideas on how to approach this with regex would be appreciated - it's beyond my simple split/findall knowledge.</p>

<p>Thanks!</p>
","['python', 'regex', 'pandas']",61131083,"<p>I do not know regex very well (so there probably is be a better solution) but this seems to work...</p>

<pre><code># sample list
raw_text = ['10-30: The dog is red. 4/01: The dog is blue', 'there is a green door',
            '3-25:Foobar baz', '11-25:Foobar baz. 12/20: something else']

# create regex (e.g., the variable 'n' in the comment below represents a number)
# if 'nn/nn' OR 'nn-nn' OR ' n-nn' OR ' n/nn' OR ' nn-nn' OR ' nn/nn' OR string starts with a number
regex = r'(?=\d\d/\d\d:)|(?=\d\d-\d\d:)|(?= \d-\d\d:)|(?= \d/\d\d:)|(?= \d\d-\d\d:)|(?= \d\d/\d\d:)|(?=^\d)'
# if string starts with alpha characters or there is a ':'
regex2 = r'(?=^\D)|:'

# create a Series by splitting on regex and explode
s = pd.DataFrame(raw_text)[0].str.split(regex).explode()
# boolean indexing to remove blanks
s2 = s[(s != '') &amp; (s != ' ')]

# strip leading or trailing white space then split on regex2
df = s2.str.strip().str.split(regex2, expand=True).reset_index()
# rename columns
df.columns = ['row_id', 'date', 'text']


   row_id   date                         text
0       0  10-30   The dog is red until 5/15.
1       0   4/01              The dog is blue
2       1               there is a green door
3       2   3-25                   Foobar baz
4       3  11-25                  Foobar baz.
5       3  12/20               something else
</code></pre>
",Python Regex split comments dataframe I bunch strings users entered various comments concatenated together Sometimes entered date comments multiple days I trying find way split date corresponding comment The text comments might look like raw text The dog red The dog blue green door Foobar baz I would like transform text df pd DataFrame The dog red The dog blue np nan green door Foobar baz columns row id date text print df row id date text The dog red The dog blue NaN green door Foobar baz I think I need find semicolons work back first number semicolon identify dates sometimes use separate sometimes Any ideas approach regex would appreciated beyond simple split findall knowledge Thanks,"startoftags, python, regex, pandas, endoftags",python arrays numpy endoftags,python regex pandas,python arrays numpy,0.33
61170036,2020-04-12,2020,3,"Error: None of [Index([&#39;...&#39;], dtype=&#39;object&#39;)] are in the [index]","<p>I am trying to delete a grouped set of rows in pandas according to the following condition:</p>

<p>If a group (grouped by col1) has more than 2 values 'c' in col2, then remove the whole group.</p>

<p>What I have looks like this</p>

<pre><code>  col1  col2                       
0  A     10:10 
1  A     20:05
2  A     c
3  A     00:10
4  B     04:15
2  B     c
3  B     c
4  B     13:40
</code></pre>

<p>And I am trying to get here:</p>

<pre><code>  col1  col2                       
0  A     10:10 
1  A     20:05
2  A     c
3  A     00:10
</code></pre>

<p>Typically I do this for other very similar dataframes (and it works):</p>

<pre><code>df = df.groupby('col1').filter(lambda x: x[""col2""].value_counts()[['c']].sum() &lt; 2)
</code></pre>

<p>But for this one is not working and I receive this error:</p>

<pre><code>KeyError: ""None of [Index(['c'], dtype='object')] are in the [index]""
</code></pre>

<p>Does someone have an idea on how I could do this?</p>

<p>Thanks!</p>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",61170540,"<p>I suggest use for improve performance <a href=""http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer""><code>boolean indexing</code></a>:  </p>

<pre><code>df = df[df['col2'].eq('c').groupby(df['col1']).transform('sum').lt(2)]
print (df)
  col1   col2
0    A  10:10
1    A  20:05
2    A      c
3    A  00:10
</code></pre>

<p><strong>Details</strong>:</p>

<p>First compare values  by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.eq.html"" rel=""nofollow noreferrer""><code>Series.eq</code></a> for <code>==</code>:</p>

<pre><code>print (df['col2'].eq('c'))
0    False
1    False
2     True
3    False
4    False
2     True
3     True
4    False
Name: col2, dtype: bool
</code></pre>

<p>Then count <code>True</code> value per groups by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.transform.html"" rel=""nofollow noreferrer""><code>GroupBy.transform</code></a> with <code>sum</code>, <code>True</code>s are processing like <code>1</code>:</p>

<pre><code>print (df['col2'].eq('c').groupby(df['col1']).transform('sum'))
0    1.0
1    1.0
2    1.0
3    1.0
4    2.0
2    2.0
3    2.0
4    2.0
Name: col2, dtype: float64
</code></pre>

<p>And last filter by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.lt.html"" rel=""nofollow noreferrer""><code>Series.lt</code></a> for less:</p>

<pre><code>print (df['col2'].eq('c').groupby(df['col1']).transform('sum').lt(2))
0     True
1     True
2     True
3     True
4    False
2    False
3    False
4    False
Name: col2, dtype: bool
</code></pre>
",Error None Index dtype object index I trying delete grouped set rows pandas according following condition If group grouped col values c col remove whole group What I looks like col col A A A c A B B c B c B And I trying get col col A A A c A Typically I similar dataframes works df df groupby col filter lambda x x col value counts c sum lt But one working I receive error KeyError None Index c dtype object index Does someone idea I could Thanks,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python python3x pandas endoftags,python pandas dataframe pandasgroupby,python python3x pandas,0.58
61208351,2020-04-14,2020,3,RegEx to span multiple lines,"<p>I have a text file with loads of unstructured data from which I'm trying to pull names and birthdates using 1 RegEx. The wall I've hit as of now is that the dates can end in one line and continue in another and re.DOTALL doesn't seem to be working. The format of the data I want is always:</p>

<p>last name, middle name(sometimes), first name, f. DD-MM-YYYY</p>

<p>This is my RegEx:</p>

<pre><code>re.findall(r'\w+,*\sf\.\s\d\d-\d\d-\d\d\d\d', re.DOTALL):
</code></pre>

<p>This doesn't get the below line breaks:</p>

<p>Smith, John,</p>

<p>f. 25-12-1990</p>

<p>or only first part of below:</p>

<p>Smith, John, f. 25-12-</p>

<p>1990</p>

<p>Smith, John, f. 25-</p>

<p>12-1990</p>
","['python', 'regex', 'python-3.x']",61208665,"<p>If you want all the options to match the dates on possible newlines, you could repeat the whitespace char 0+ times between all the characters.</p>

<p>Note that in your pattern you are repeating the comma 0+ times <code>,*</code> instead of the <code>\s</code></p>

<p>Using <code>re.DOTALL</code> makes the <code>.</code> match a newline, but in your pattern you are not using a dot, only a literal dot <code>\.</code></p>

<p>The <code>\s</code> will match a whitespace char including a newline. In your data there are multiple newlines between the date part. You could also use <code>[\r\n]*</code> to match the newlines in between.</p>

<pre><code>\w+,\s*f\s*\.\s*\d\s*\d\s*-\s*\d\s*\d\s*-\s*\d\s*\d\s*\d\s*\d
</code></pre>

<p><a href=""https://regex101.com/r/m0PYn8/1"" rel=""nofollow noreferrer"">Regex demo</a> | <a href=""https://ideone.com/tEIz6L"" rel=""nofollow noreferrer"">Python demo</a></p>

<p>If the break is only after the hyphen:</p>

<pre><code>\w+,\s*f\s*\.\s*\d\d-\s*\d\d-\s*\d\d\d\d
</code></pre>

<p><a href=""https://regex101.com/r/pXuDWx/1"" rel=""nofollow noreferrer"">Regex demo</a> | <a href=""https://ideone.com/ZoGsx9"" rel=""nofollow noreferrer"">Python demo</a></p>
",RegEx span multiple lines I text file loads unstructured data I trying pull names birthdates using RegEx The wall I hit dates end one line continue another DOTALL seem working The format data I want always last name middle name sometimes first name f DD MM YYYY This RegEx findall r w sf DOTALL This get line breaks Smith John f first part Smith John f Smith John f,"startoftags, python, regex, python3x, endoftags",python arrays numpy endoftags,python regex python3x,python arrays numpy,0.33
61445822,2020-04-26,2020,2,Sort python list basis another list,"<p>I have the following master list:</p>

<pre><code>[
   {'entity': 'Country', 'cnt': 4},
   {'entity': 'State', 'cnt': 3},
   {'entity': 'City', 'cnt': 2}
]
</code></pre>

<p>Note the order of the entities.</p>

<p>I wish to order another list based on the order of entities in the first list.</p>

<p>Second list:</p>

<pre><code>[
   {'entity': 'State', 'values': 'AK'},
   {'entity': 'Country', 'values': 'USA'}
]
</code></pre>

<p>Desired output:</p>

<pre><code>[
   {'entity': 'Country', 'values': 'USA'},
   {'entity': 'State', 'values': 'AK'}
]
</code></pre>

<p>I tried the following:</p>

<pre><code>secondList.sort(key=lambda x: firstList.index(x[""entity""]))
</code></pre>

<p>However, it gives me an error since City is not found in the second list.</p>

<p>Can someone tell me what I'm doing wrong here?</p>
","['python', 'python-3.x', 'list']",61445993,"<p>You could maybe make a bit preprocessing:</p>

<pre><code>master = [
   {'entity': 'Country', 'cnt': 4},
   {'entity': 'State', 'cnt': 3},
   {'entity': 'City', 'cnt': 2}
]

to_sort = [
   {'entity': 'State', 'values': 'AK'},
   {'entity': 'Country', 'values': 'USA'}
]

prep = { record['entity'] : i for  i, record in enumerate(master)  }


sorted(to_sort, key = lambda x: prep[x['entity']])
</code></pre>
",Sort python list basis another list I following master list entity Country cnt entity State cnt entity City cnt Note order entities I wish order another list based order entities first list Second list entity State values AK entity Country values USA Desired output entity Country values USA entity State values AK I tried following secondList sort key lambda x firstList index x entity However gives error since City found second list Can someone tell I wrong,"startoftags, python, python3x, list, endoftags",python python3x pandas endoftags,python python3x list,python python3x pandas,0.67
61512957,2020-04-29,2020,3,Pandas: Compare all values within a dataframe by row,"<p>I am trying to match rows and aggregate them in a single row. </p>

<p>For example for the table below, I want to aggregate the first three rows because they are similar. 4th isnt similar. In my check, I do nothing for any row that has col 1 as B. And then again aggregation for final two rows:</p>

<pre><code>|---------------------|------------------|------------------|
|      Col 1          |     Col 2        |       Col 3      |
|---------------------|------------------|------------------|
|        A            |       12st       |        13        |
|---------------------|------------------|------------------|
|        A            |       12st       |        13        |
|---------------------|------------------|------------------|
|        A            |       12st       |        13        |
|---------------------|------------------|------------------|
|        A            |       12st       |        17        |
|---------------------|------------------|------------------|
|        B            |       11aa       |        10        |
|---------------------|------------------|------------------|
|        C            |       10ee       |        10        |
|---------------------|------------------|------------------|
|        C            |       10ee       |        10        |
|---------------------|------------------|------------------|
</code></pre>

<pre><code>df = pd.DataFrame({'Col 1': ['A', 'A', 'A','A', 'B', 'C', 'C'],'Col 2': ['12st', '12st', '12st', '12st', '11aa' ,'10ee','10ee'],'Col 3': [13, 13, 13, 17, 10, 10, 10 ]})


</code></pre>

<p>I want to get the following output:</p>

<pre><code>|---------------------|------------------|------------------|---------------|
|      Col 1          |     Col 2        |       Col 3      |    Col 4      |
|---------------------|------------------|------------------|---------------|
|        A            |       12st       |        13        |      3        |
|---------------------|------------------|------------------|---------------|
|        A            |       12st       |        17        |      1        |
|---------------------|------------------|------------------|---------------|
|        B            |       11a        |        10        |      1        |
|---------------------|------------------|------------------|---------------|
|        C            |       10ee       |        10        |      2        |
|---------------------|------------------|------------------|---------------|

</code></pre>

<p>I have tried simpler things like df.shift() but that seems to only work for a specific col and not row. Plus I want to do this iteratively for the rows (i) it keeps on matching (i==i+1==i+2). </p>

<p>Thanks</p>
","['python', 'pandas', 'dataframe']",61513111,"<p>I think <code>groupby.size</code> can do it like:</p>

<pre><code>print (df.groupby(['Col 1','Col 2', 'Col 3']).size().reset_index(name='Col 4'))
  Col 1 Col 2  Col 3  Col 4
0     A  12st     13      3
1     A  12st     17      1
2     B  11aa     10      1
3     C  10ee     10      2
</code></pre>
",Pandas Compare values within dataframe row I trying match rows aggregate single row For example table I want aggregate first three rows similar th isnt similar In check I nothing row col B And aggregation final two rows Col Col Col A st A st A st A st B aa C ee C ee df pd DataFrame Col A A A A B C C Col st st st st aa ee ee Col I want get following output Col Col Col Col A st A st B C ee I tried simpler things like df shift seems work specific col row Plus I want iteratively rows keeps matching Thanks,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
61764643,2020-05-13,2020,2,Pyspark create column based on maximum of multiple columns that match a certain condition in corresponding columns,"<p>Let's say I have a Pyspark dataframe with id and 3 columns representing code buckets.</p>

<pre><code>col_buckets [""code_1"", ""code_2"", ""code_3""]
</code></pre>

<p>and 3 columns representing amounts for corresponding code buckets.</p>

<pre><code>amt_buckets = [""code_1_amt"", ""code_2_amt"", ""code_3_amt"" ] 
</code></pre>

<p>Here is a pseudocode for what I am trying to do.</p>

<pre><code>for el in ['01', '06', '07']
    df= df.withColumn(""max_amt_{el}"", max(df.select(max(**amt_buckets**) for corresponding col_indices of amt_buckets if ***any of col_buckets*** ==el)))
</code></pre>

<p>how would I accomplish this?</p>

<p>here is a dataframe example for this:</p>

<p><a href=""https://i.stack.imgur.com/5lcyW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5lcyW.png"" alt=""enter image description here""></a></p>

<pre><code>Primary_id  Code_1  Code_2  Code_3  Amt_1   Amt_2   Amt_3   Max_01  Max_07  Max_06
Xxxxx998    Null    01      04      2000    1000    100     1000    0       0
Xxxxx997    01      01      07      200     300     400     300     400     0
Xxxxx996    07      Null    Null    100     Null    Null    0       100     0
Xxxx910     Null    Null    Null    300     100     200     0       0       0
</code></pre>

<p>I am trying to get the max_01, max_07 and max_06 columns</p>
","['python', 'apache-spark', 'pyspark']",61766411,"<p>For <strong><code>spark2.4+</code></strong>, you can try this.</p>

<pre><code>df.show() #sample dataframe
#+----------+------+------+------+-----+-----+-----+
#|Primary_id|Code_1|Code_2|Code_3|Amt_1|Amt_2|Amt_3|
#+----------+------+------+------+-----+-----+-----+
#|  Xxxxx998|  null|    01|    04| 2000| 1000|  100|
#|  Xxxxx997|    01|    01|    07|  200|  300|  400|
#|  Xxxxx996|    07|  null|  null|  100| null| null|
#|   Xxxx910|  null|  null|  null|  300|  100|  200|
#+----------+------+------+------+-----+-----+-----+

from pyspark.sql import functions as F

dictionary = dict(zip(['Code_1','Code_2','Code_3'], ['Amt_1','Amt_2','Amt_3']))

df.withColumn(""trial"", F.array(*[F.array(F.col(x),F.col(y).cast(""string""))\
                                          for x,y in dictionary.items()]))\
  .withColumn(""Max_01"",F.when(F.size(F.expr(""""""filter(trial,x-&gt; exists(x,y-&gt;y='01'))""""""))!=0,\
       F.expr(""""""array_max(transform(filter(trial, x-&gt; exists(x,y-&gt; y='01')),z-&gt; float(z[1])))""""""))\
             .otherwise(F.lit(0)))\
  .withColumn(""Max_06"",F.when(F.size(F.expr(""""""filter(trial,x-&gt; exists(x,y-&gt;y='06'))""""""))!=0,\
       F.expr(""""""array_max(transform(filter(trial, x-&gt; exists(x,y-&gt; y='06')),z-&gt; float(z[1])))""""""))\
             .otherwise(F.lit(0)))\
  .withColumn(""Max_07"",F.when(F.size(F.expr(""""""filter(trial,x-&gt; exists(x,y-&gt;y='07'))""""""))!=0,\
       F.expr(""""""array_max(transform(filter(trial, x-&gt; exists(x,y-&gt; y='07')),z-&gt; float(z[1])))""""""))\
             .otherwise(F.lit(0)))\
  .drop(""trial"").show(truncate=False)

#+----------+------+------+------+-----+-----+-----+------+------+------+
#|Primary_id|Code_1|Code_2|Code_3|Amt_1|Amt_2|Amt_3|Max_01|Max_07|Max_06|
#+----------+------+------+------+-----+-----+-----+------+------+------+
#|Xxxxx998  |null  |01    |04    |2000 |1000 |100  |1000  |0     |0     |
#|Xxxxx997  |01    |01    |07    |200  |300  |400  |300   |400   |0     |
#|Xxxxx996  |07    |null  |null  |100  |null |null |0     |100   |0     |
#|Xxxx910   |null  |null  |null  |300  |100  |200  |0     |0     |0     |
#+----------+------+------+------+-----+-----+-----+------+------+------+
</code></pre>
",Pyspark create column based maximum multiple columns match certain condition corresponding columns Let say I Pyspark dataframe id columns representing code buckets col buckets code code code columns representing amounts corresponding code buckets amt buckets code amt code amt code amt Here pseudocode I trying el df df withColumn max amt el max df select max amt buckets corresponding col indices amt buckets col buckets el would I accomplish dataframe example Primary id Code Code Code Amt Amt Amt Max Max Max Xxxxx Null Xxxxx Xxxxx Null Null Null Null Xxxx Null Null Null I trying get max max max columns,"startoftags, python, apachespark, pyspark, endoftags",python pandas numpy endoftags,python apachespark pyspark,python pandas numpy,0.33
61983158,2020-05-24,2020,17,How to concat multiple Pandas DataFrame columns with different token separator?,"<p>I am trying to concat multiple Pandas DataFrame columns with different tokens.</p>

<p>For example, my dataset looks like this :</p>

<pre><code>dataframe = pd.DataFrame({'col_1' : ['aaa','bbb','ccc','ddd'], 
                          'col_2' : ['name_aaa','name_bbb','name_ccc','name_ddd'], 
                          'col_3' : ['job_aaa','job_bbb','job_ccc','job_ddd']})
</code></pre>

<p>I want to output something like this:</p>

<pre><code>    features
0   aaa &lt;0&gt; name_aaa &lt;1&gt; job_aaa
1   bbb &lt;0&gt; name_bbb &lt;1&gt; job_bbb
2   ccc &lt;0&gt; name_ccc &lt;1&gt; job_ccc
3   ddd &lt;0&gt; name_ddd &lt;1&gt; job_ddd
</code></pre>

<p>Explanation :</p>

<p>concat each column with ""&lt;{}>"" where {} will be increasing numbers.</p>

<p>What I've tried so far:</p>

<p>I don't want to modify original DataFrame so I created two new dataframe:</p>

<pre><code>features_df = pd.DataFrame()
final_df    = pd.DataFrame()
for iters in range(len(dataframe.columns)):
    features_df[dataframe.columns[iters]] = dataframe[dataframe.columns[iters]] + ' ' + ""&lt;{}&gt;"".format(iters)
final_df['features'] = features_df[features_df.columns].agg(' '.join, axis=1)
</code></pre>

<p>There is an issue I am facing, It's adding &lt;2> at last but I want output like above, also this is not panda's way to do this task, How I can make it more efficient?</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",61983279,"<pre><code>from itertools import chain

dataframe['features'] = dataframe.apply(lambda x: ''.join([*chain.from_iterable((v, f' &lt;{i}&gt; ') for i, v in enumerate(x))][:-1]), axis=1)

print(dataframe)
</code></pre>

<p>Prints:</p>

<pre><code>  col_1     col_2    col_3                      features
0   aaa  name_aaa  job_aaa  aaa &lt;0&gt; name_aaa &lt;1&gt; job_aaa
1   bbb  name_bbb  job_bbb  bbb &lt;0&gt; name_bbb &lt;1&gt; job_bbb
2   ccc  name_ccc  job_ccc  ccc &lt;0&gt; name_ccc &lt;1&gt; job_ccc
3   ddd  name_ddd  job_ddd  ddd &lt;0&gt; name_ddd &lt;1&gt; job_ddd
</code></pre>
",How concat multiple Pandas DataFrame columns different token separator I trying concat multiple Pandas DataFrame columns different tokens For example dataset looks like dataframe pd DataFrame col aaa bbb ccc ddd col name aaa name bbb name ccc name ddd col job aaa job bbb job ccc job ddd I want output something like features aaa lt gt name aaa lt gt job aaa bbb lt gt name bbb lt gt job bbb ccc lt gt name ccc lt gt job ccc ddd lt gt name ddd lt gt job ddd Explanation concat column lt increasing numbers What I tried far I want modify original DataFrame I created two new dataframe features df pd DataFrame final df pd DataFrame iters range len dataframe columns features df dataframe columns iters dataframe dataframe columns iters lt gt format iters final df features features df features df columns agg join axis There issue,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
62076257,2020-05-28,2020,2,Discord.py @bot.event,"<p>So I have a script that uses both <code>@bot.event</code> and <code>@bot.command()</code>. The problem is that when I have a <code>@bot.event</code> waiting the <code>@bot.command()</code> will not run.</p>
<p>Here is my code:</p>
<pre><code>@bot.event
async def on_ready():
    print(&quot;Bot Is Ready And Online!&quot;)
    
async def react(message): 
    if message.content == &quot;Meeting&quot;:
        await message.add_reaction(&quot;ð&quot;)

@bot.command()
async def info(ctx):
    await ctx.send(&quot;Hello, thanks for testing out our bot. ~ techNOlogics&quot;)

@bot.command(pass_context=True)
async def meet(ctx,time):
    if ctx.message.author.name == &quot;techNOlogics&quot;:
        await ctx.channel.purge(limit=1)
        await ctx.send(&quot;**Meeting at &quot; + time + &quot; today!** React if you read.&quot;)

@bot.event ##THIS ONE HOLDS UP THE WHOLE SCRIPT
async def on_message(message):
    await react(message)
</code></pre>
","['python', 'discord', 'discord.py']",62076331,"<p>When using a mixture of the <code>on_message</code> event with commands, you'll want to add <code>await bot.process_commands(message)</code>, like so:</p>
<pre class=""lang-py prettyprint-override""><code>@bot.event
async def on_message(message):
    await bot.process_commands(message)
    # rest of code
</code></pre>
<p>As said in the docs:</p>
<blockquote>
<p>This function processes the commands that have been registered to the bot and other groups. Without this coroutine, none of the commands will be triggered.</p>
<p>If you choose to override the on_message() event, you then you should invoke this coroutine as well.</p>
</blockquote>
<hr />
<p><strong>References:</strong></p>
<ul>
<li><a href=""https://discordpy.readthedocs.io/en/latest/ext/commands/api.html#discord.ext.commands.Bot.process_commands"" rel=""nofollow noreferrer""><code>Bot.process_commands()</code></a></li>
<li><a href=""https://discordpy.readthedocs.io/en/latest/api.html#discord.on_message"" rel=""nofollow noreferrer""><code>on_message()</code></a></li>
</ul>
",Discord py bot event So I script uses bot event bot command The problem I bot event waiting bot command run Here code bot event async def ready print quot Bot Is Ready And Online quot async def react message message content quot Meeting quot await message add reaction quot quot bot command async def info ctx await ctx send quot Hello thanks testing bot techNOlogics quot bot command pass context True async def meet ctx time ctx message author name quot techNOlogics quot await ctx channel purge limit await ctx send quot Meeting quot time quot today React read quot bot event THIS ONE HOLDS UP THE WHOLE SCRIPT async def message message await react message,"startoftags, python, discord, discordpy, endoftags",python discord discordpy endoftags,python discord discordpy,python discord discordpy,1.0
62297661,2020-06-10,2020,4,Get the column name for the first non-zero value in that row with pandas,"<p>I have a huge dataframe but sharing only sample below. Its a CSV with sample header column names as shown below.</p>

<pre><code>sample.csv
cnum,sup1,sup2,sup3,sup4
285414459,1,0,1,1
445633709,1,0,0,0
556714736,0,0,1,0
1089852074,0,1,0,1
</code></pre>

<p>A cnum can have 0 or 1 set in all sup* columns. I want to select and print the column name where first 1 is encountered for that cnum. All other 1 after that should be ignored and no column name should be printed in output.</p>

<pre><code>expected output:
cnum,supcol
285414459,sup1
445633709,sup1
556714736,sup3
1089852074,sup2
</code></pre>

<p>Currently I tried this code:</p>

<pre><code>import pandas as pd
df=pd.read_csv('sample.csv')
df_union=pd.DataFrame(columns=['cnum','supcol'])
for col in df.columns: 
    df1=df.filter(['cnum']).loc[df[col] == 1]
    df1['supcol']=col
    df_union=df_union.append(df1)
print(df_union)
</code></pre>

<p>However it is printing all column names where 1 is set for the column name. I want only the first one.
Kindly help</p>
","['python', 'pandas', 'dataframe']",62297697,"<p>It seems like you can use <code>idxmax</code> here:</p>

<pre><code>df.set_index('cnum').idxmax(axis=1).reset_index(drop=True)

0    sup1
1    sup1
2    sup3
3    sup2
dtype: object

df['output'] = df.set_index('cnum').idxmax(axis=1).reset_index(drop=True) 
# Slightly faster,
# df['output'] = df.set_index('cnum').idxmax(axis=1).to_numpy() 

df
         cnum  sup1  sup2  sup3  sup4 output
0   285414459     1     0     1     1   sup1
1   445633709     1     0     0     0   sup1
2   556714736     0     0     1     0   sup3
3  1089852074     0     1     0     1   sup2
</code></pre>

<hr>

<p>Another option with <code>dot</code> (will give you all non-zero columns):</p>

<pre><code>d = df.set_index('cnum') 
d.dot(d.columns + ',').str.rstrip(',').reset_index(drop=True)

0    sup1,sup3,sup4
1              sup1
2              sup3
3         sup2,sup4
dtype: object
</code></pre>

<p>Or,</p>

<pre><code>(d.dot(d.columns + ',')
  .str.rstrip(',')
  .str.split(',', 1).str[0] 
  .reset_index(drop=True))

0    sup1
1    sup1
2    sup3
3    sup2
dtype: object
</code></pre>
",Get column name first non zero value row pandas I huge dataframe sharing sample Its CSV sample header column names shown sample csv cnum sup sup sup sup A cnum set sup columns I want select print column name first encountered cnum All ignored column name printed output expected output cnum supcol sup sup sup sup Currently I tried code import pandas pd df pd read csv sample csv df union pd DataFrame columns cnum supcol col df columns df df filter cnum loc df col df supcol col df union df union append df print df union However printing column names set column name I want first one Kindly help,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
62760347,2020-07-06,2020,2,How to write LabelEncoder in Tensorflow?,"<p>I'm trying to parse directories on Google Storage into strings, but I keep getting errors. I want to find the directory of each file and return a numerical encoding of the directory name as a Dataset. This would be trivial in sklearn using LabelEncoder, but I'm having trouble doing this in Tensorflow.</p>
<pre><code>CLASS_NAMES = [b'class_1', b'class_2', b'class_3']
labeler = tfds.features.ClassLabel(names=CLASS_NAMES)

def parse_filenames(filename):
    label = tf.strings.split(tf.expand_dims(filename, axis=-1), sep='/')
    label = label.values[-2]

    # Problem is in the two lines below
    position_feature = tf.feature_column.categorical_column_with_vocabulary_list('label_names', CLASS_NAMES)
    label = tf.io.parse_example(label, features=position_feature)

    return label

folder = b'gs://&lt;bucket&gt;/train/*/*.jpg'
filenames_dataset = tf.data.Dataset.list_files(folder)
label_dataset = filenames_dataset.map(parse_filenames)

next(iter(label_dataset))
</code></pre>
<p>I get an error <code>ValueError: dictionary update sequence element #0 has length 16; 2 is required</code></p>
<p>If I take out the two lines under the &quot;# Problem is here&quot; comment, it works fine, except it returns a string instead of an integer. I've tried other non-tensorflow options, such as &lt;list_name&gt;.index(label), but those of course fail because everything is a tensor instead of a string. Is there another way to do this?</p>
","['python', 'tensorflow', 'keras']",62760616,"<p>Maybe you can try this line instead of these two lines:</p>
<pre><code>label = tf.argmax(tf.cast(parts[-2] == CLASS_NAMES, tf.int32))
</code></pre>
<p>You will get something like <code>[0, 1, 0]</code> (the index of the label in <code>CLASS_NAMES</code>).</p>
<p>Functioning and reproducible example:</p>
<pre><code>import tensorflow as tf
import numpy as np
from string import ascii_lowercase as letters

CLASS_NAMES = [b'class_1', b'class_2', b'class_3']

files = ['\\'.join([np.random.choice(CLASS_NAMES).decode(),
                    ''.join(np.random.choice(list(letters), 5)) + '.jpg']) 
         for i in range(10)]

ds = tf.data.Dataset.from_tensor_slices(files)
</code></pre>
<p>Here are the fake files I generated:</p>
<pre><code>['class_3\\jrxog.jpg',
 'class_1\\slfiq.jpg',
 'class_2\\svldd.jpg',
 'class_2\\avrgt.jpg',
 'class_3\\wqwuv.jpg']
</code></pre>
<p>Now implement this:</p>
<pre><code>def get_label(file_path):
    parts = tf.strings.split(file_path, '\\')
    return file_path, tf.argmax(tf.cast(parts[-2] == CLASS_NAMES, tf.int32))

ds = ds.map(get_label)

next(iter(ds))
</code></pre>
<pre><code>(&lt;tf.Tensor: shape=(), dtype=string, numpy=b'class_1\\bbqrx.jpg'&gt;,
 &lt;tf.Tensor: shape=(), dtype=int64, numpy=0&gt;)
</code></pre>
",How write LabelEncoder Tensorflow I trying parse directories Google Storage strings I keep getting errors I want find directory file return numerical encoding directory name Dataset This would trivial sklearn using LabelEncoder I trouble Tensorflow CLASS NAMES b class b class b class labeler tfds features ClassLabel names CLASS NAMES def parse filenames filename label tf strings split tf expand dims filename axis sep label label values Problem two lines position feature tf feature column categorical column vocabulary list label names CLASS NAMES label tf io parse example label features position feature return label folder b gs lt bucket gt train jpg filenames dataset tf data Dataset list files folder label dataset filenames dataset map parse filenames next iter label dataset I get error ValueError dictionary update sequence element length required If I take two lines quot Problem quot comment works fine except returns string instead integer I tried non,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
62839959,2020-07-10,2020,2,How can I store store index pairs using True values from a boolean-like square symmetric numpy array?,"<p>I have a Numpy Array that with integer values 1 or 0 (can be cast as booleans if necessary). The array is square and symmetric (see note below) and I want a list of the indices where a 1 appears:</p>
<p>Note that <code>array[i][j] == array[j][i]</code> and <code>array[i][i] == 0</code> by design. Also I cannot have any duplicates.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
array = np.array([
    [0, 0, 1, 0, 1, 0, 1],
    [0, 0, 1, 1, 0, 1, 0],
    [1, 1, 0, 0, 0, 0, 1],
    [0, 1, 0, 0, 1, 1, 0],
    [1, 0, 0, 1, 0, 0, 1],
    [0, 1, 0, 1, 0, 0, 0],
    [1, 0, 1, 0, 1, 0, 0]
])
</code></pre>
<p>I would like a result that is like this (order of each sub-list is not important, nor is the order of each element within the sub-list):</p>
<pre class=""lang-py prettyprint-override""><code>[
    [0, 2], 
    [0, 4], 
    [0, 6], 
    [1, 2], 
    [1, 3],
    [1, 5],
    [2, 6],
    [3, 4],
    [3, 5],
    [4, 6]
]
</code></pre>
<p>Another point to make is that I would prefer not to loop over all indices twice using the condition <code>j&lt;i</code> because the size of my array can be large but I am aware that this is a possibility - I have written an example of this using two for loops:</p>
<pre><code>result = []
for i in range(array.shape[0]):
    for j in range(i):
        if array[i][j]:
            result.append([i, j])
print(pd.DataFrame(result).sort_values(1).values)


# using dataframes and arrays for formatting but looking for
# 'result' which is a list

# Returns (same as above but columns are the opposite way round):
[[2 0]
 [4 0]
 [6 0]
 [2 1]
 [3 1]
 [5 1]
 [6 2]
 [4 3]
 [5 3]
 [6 4]]
</code></pre>
","['python', 'arrays', 'numpy']",62840935,"<pre><code>idx = np.argwhere(array)
idx = idx[idx[:,0]&lt;idx[:,1]]
</code></pre>
<p>Another way:</p>
<pre><code>idx = np.argwhere(np.triu(array))
</code></pre>
<p>output:</p>
<pre><code>[[0 2]
 [0 4]
 [0 6]
 [1 2]
 [1 3]
 [1 5]
 [2 6]
 [3 4]
 [3 5]
 [4 6]]
</code></pre>
<hr />
<p><em><strong>Comparison</strong></em>:</p>
<pre><code>#@bousof solution
def method1(array):
  return np.vstack(np.where(np.logical_and(array, np.diff(np.ogrid[:array.shape[0],:array.shape[0]])[0]&gt;=0))).transpose()[:,::-1]

#Also mentioned by @hpaulj
def method2(array):
  return np.argwhere(np.triu(array))

def method3(array):
  idx = np.argwhere(array)
  return idx[idx[:,0]&lt;idx[:,1]]

#The original method in question by OP(d-man)
def method4(array):
  result = []
  for i in range(array.shape[0]):
      for j in range(i):
          if array[i][j]:
              result.append([i, j])
  return result

#suggestd by @bousof in comments
def method5(array):
  return np.vstack(np.where(np.triu(array))).transpose()

inputs = [np.random.randint(0,2,(n,n)) for n in [10,100,1000,10000]]
</code></pre>
<p>Seems like <strong>method1</strong>, <strong>method2</strong> and <strong>method5</strong> are slightly faster for large arrays while <strong>method3</strong> is faster for smaller cases:</p>
<p><a href=""https://i.stack.imgur.com/oF33l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oF33l.png"" alt=""enter image description here"" /></a></p>
",How I store store index pairs using True values boolean like square symmetric numpy array I Numpy Array integer values cast booleans necessary The array square symmetric see note I want list indices appears Note array j array j array design Also I cannot duplicates import numpy np array np array I would like result like order sub list important order element within sub list Another point make I would prefer loop indices twice using condition j lt size array large I aware possibility I written example using two loops result range array shape j range array j result append j print pd DataFrame result sort values values using dataframes arrays formatting looking result list Returns columns opposite way round,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
62932410,2020-07-16,2020,2,How to drop rows in a Dataframe if any of the cells in that row under a column is not a float?,"<p>I have a data frame in which there are 4 columns. Now, the values stored under these columns are all strings. I want to convert them all to float. Occasionally, I might come across a cell where there is a string under a particular column, in which case I would just want to drop the whole row. For instance, consider the following data frame:</p>
<pre><code>   A.    B.    C.    D 
   1.    1.    3.    7
   1.    1.    3s    7
   1.    1.    3.    7
   1.    4s    3.    7
   1.    1.    3.    7
   1.    1.    3.    7
</code></pre>
<p>As you can see row 2 under colC contains a value which cannot be converted to float, in which case I want to drop all of second row. And same with column B row 4, I want to drop the. whole row.</p>
<p>So the output I want is:</p>
<pre><code>   A.    B.    C.    D 
   1.    1.    3.    7
   1.    1.    3.    7
   1.    1.    3.    7
   1.    1.    3.    7
</code></pre>
<p>where i have dropped the second and fourth row, because they contained values which could not be converted to floats</p>
","['python', 'pandas', 'dataframe']",62932499,"<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html"" rel=""nofollow noreferrer""><code>pd.to_numeric</code></a> with the optional parameter <code>errors='coerce'</code> to convert all the columns to <code>numeric</code> type, then use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html"" rel=""nofollow noreferrer""><code>DataFrame.dropna</code></a> to drop the rows which contain <code>NaN</code> values (as invalid values will be set as <code>NaN</code>):</p>
<pre><code>df1 = df.apply(pd.to_numeric, errors='coerce').dropna()
</code></pre>
<p>Result:</p>
<pre><code># print(df1)
    A.   B.   C.  D
0  1.0  1.0  3.0  7
2  1.0  1.0  3.0  7
4  1.0  1.0  3.0  7
5  1.0  1.0  3.0  7
</code></pre>
",How drop rows Dataframe cells row column float I data frame columns Now values stored columns strings I want convert float Occasionally I might come across cell string particular column case I would want drop whole row For instance consider following data frame A B C D As see row colC contains value cannot converted float case I want drop second row And column B row I want drop whole row So output I want A B C D dropped second fourth row contained values could converted floats,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
62958630,2020-07-17,2020,2,Pandas- Function is overwriting original DF even though I am maniuplating copy?,"<p>I am creating a function to categorize data in bins in a df. I have made the function, and am first extracting numbers from a string, and replacing the column of text with a column of numbers.</p>
<p><strong>The function is somehow overwriting the original dataframe, despite me only manipulating a copy of it.</strong></p>
<pre><code>def categorizeColumns(df):

    newdf = df
 
    if 'Runtime' in newdf.columns:
        for row in range(len(newdf['Runtime'])):
            strRuntime = newdf['Runtime'][row]
            numsRuntime = [int(i) for i in strRuntime.split() if i.isdigit()]
            newdf.loc[row,'Runtime'] = numsRuntime[0]
    
return newdf

df = pd.read_csv('moviesSeenRated.csv')
newdf = categorizeColumns(df)
</code></pre>
<p>The original df has a column of runtimes like this [34 mins, 32 mins, 44 mins] etc, and the newdf should have [33,32,44], which it does. <strong>However, the original df also changes outside the function.</strong></p>
<p><strong>Whats going on here? Any fixes?</strong> Thanks in advance.</p>
<p>EDIT: Seems like I wasn't making a copy, I needed to do</p>
<pre><code>df.copy()
</code></pre>
<p>Thank you all!</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",62958715,"<p>The problem is that you aren't actually making a copy of the dataframe in the line <code>newdf = df</code>. To make a copy, you could do <code>newdf = df.copy()</code>.</p>
",Pandas Function overwriting original DF even though I maniuplating copy I creating function categorize data bins df I made function first extracting numbers string replacing column text column numbers The function somehow overwriting original dataframe despite manipulating copy def categorizeColumns df newdf df Runtime newdf columns row range len newdf Runtime strRuntime newdf Runtime row numsRuntime int strRuntime split isdigit newdf loc row Runtime numsRuntime return newdf df pd read csv moviesSeenRated csv newdf categorizeColumns df The original df column runtimes like mins mins mins etc newdf However original df also changes outside function Whats going Any fixes Thanks advance EDIT Seems like I making copy I needed df copy Thank,"startoftags, python, python3x, pandas, dataframe, endoftags",python python3x pandas endoftags,python python3x pandas dataframe,python python3x pandas,0.87
62959701,2020-07-17,2020,2,Why doesn&#39;t my python program start pygame?,"<p>I am making a game in pygame and practicing OOP. I've read a pygame tutorial and because my game is a bit different, I only took the parts I need such as movement, pygame.init(), etc. When I run the the script, there isn't any error just the pygame welcome message. I've checked if I actually added the lines of code to start the program and I did. I also checked if I made the screen and I also did that.</p>
<pre><code>  1 import pygame
  2 from pygame.locals import *
  3 
  4 class character:
  5     def __init__(self, Name, Picture, Attack1, WaitAttack1, Attack2, WaitAttack2, Heal1, WaitHeal1, SuperAttack1, WaitSuperAttack1):
  6         self.Name = Name
  7         self.Player = pygame.image.load(Picture)
  8         self.Coins = 0
  9         self.Backpack = &quot;Empty&quot;
 10         self.WorldMap = &quot;Comming soon&quot;
 11         self.Health = 250
 12         self.Attack1 = Attack1
 13         self.WaitAttack1 = WaitAttack1
 14         self.Attack2 = Attack2
 15         self.WaitAttack2 = WaitAttack2
 16         self.Heal1 = Heal1
 17         self.WaitHeal1 = WaitHeal1
 18         self.SuperAttack1 = SuperAttack1
 19         self.WaitSuperAttack1 = WaitSuperAttack1
 20         self.Keys = [False, False, False, False]
 21         self.playerpos = [100,100]
 22 
 23     def directions(self):
 24 
 25         if self.Keys[0]:
 26             self.playerpos[1] -= 5
 27         elif self.Keys[2]:
 28             self.playerpos[1] += 5
 29         if self.Keys[1]:
 30             self.playerpos[0] -= 5
 31         elif self.Keys[3]:
 32             self.playerpos[0] += 5
 33 
 34 
 35 
 36 class Slade(character):
 37     def __init__(self): 38         character.__init__(self, &quot;Slade&quot;, &quot;Slade.png&quot;, 40, 20, 50, 25, 10, 5, 30, 25)
 39     
 40     def movement(self):
 41         while 1 == 1:
 42             pygame.init()
 43             self.Width, self.Height = 640, 480
 44             self.screen = pygame.display.set_mode((self.Width, self.Height))
 45             self.screen.fill(0)
 46             self.screen.blit(self.Player, self.playerpos)
 47             pygame.display.flip()
 48             for event in pygame.event.get():
 49                 if event.type == pygame.QUIT:
 50                     pygame.quit()
 51                     exit(0)
 52 
 53                 if event.type == pygame.KEYDOWN:
 54                     if event.key == K_w:
 55                         self.Keys[0] = True
 56                     elif event.key == K_a:
 57                         self.Keys[1] = True
 58                     elif event.key == K_s:
 59                         self.Keys[2] = True
 60                     elif event.key == K_d:
 61                         self.Keys[3] = True
 62 
 63                 if event.type == pygame.KEYUP:
 64                     if event.key == pygame.K_w:
 65                         self.Keys[0] = False
 66                     elif event.key == pygame.K_a:
 67                         self.Keys[1] = False
 68                     elif event.key == pygame.K_s:
 69                         self.Keys[2] = False
 70                     elif event.key == pygame.K_d:
 71                         self.Keys[3] = False
 72 
 73 
 74 if __name__ == '__main__':
 75     slade = Slade()
 76     slade.movement()
</code></pre>
<p>Thanks!</p>
<p>P.S.: I'm a newbie at OOP. So please try to ignore the fact that my code is terrible.</p>
","['python', 'python-3.x', 'pygame']",62959726,"<p>At first, you have to assign the class to a variable. Then call a function, because after assigning the class the only thing that will run will be the <code>__init__()</code> function. For example</p>
<pre><code>if __name__ == '__main__':
    obj = Slade()
    obj.movement()
</code></pre>
<p>There are some other issues within your code such as <code>def __init__(self): 37     def __init__(self):</code> but assigning a class to a variable and then calling a function will run the desired function. Also note, that you can use the <code>__init__()</code> class only once.</p>
",Why python program start pygame I making game pygame practicing OOP I read pygame tutorial game bit different I took parts I need movement pygame init etc When I run script error pygame welcome message I checked I actually added lines code start program I I also checked I made screen I also import pygame pygame locals import class character def init self Name Picture Attack WaitAttack Attack WaitAttack Heal WaitHeal SuperAttack WaitSuperAttack self Name Name self Player pygame image load Picture self Coins self Backpack quot Empty quot self WorldMap quot Comming soon quot self Health self Attack Attack self WaitAttack WaitAttack self Attack Attack self WaitAttack WaitAttack self Heal Heal self WaitHeal WaitHeal self SuperAttack SuperAttack self WaitSuperAttack WaitSuperAttack self Keys False False False False self playerpos def directions self self Keys self playerpos elif self Keys self playerpos self Keys self playerpos elif self Keys self playerpos,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
63211311,2020-08-02,2020,2,Splitting cells within dataframe,"<p>I have a CSV that I read into a dataframe to drop certain columns and do some manipulations.</p>
<p>Some example rows are:</p>
<pre><code>20        2/5/1954 13:55          0.5           18
21        2/5/1954 14:35          0.5         18.2
22        2/5/1954 16:35          0.5         18.5
</code></pre>
<p>I want to drop out the time in the datetime, so that, for example, I get <code>2/5/1954</code> instead of <code>2/5/1954 13:55</code>.</p>
<p>I wrote this script:</p>
<pre><code>import pandas as pd
from datetime import datetime as dt

df = pd.read_csv('habsos_20200310.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')

pd.set_option('display.max_rows', None)

# Get only the columns we care about
dfSub = df[['sample_date','sample_depth','water_temp']]

# Remove the NaN values
dfClean = dfSub.dropna()

# Select 0.5 depth measurements only
dfClean2 = dfClean.loc[df['sample_depth'] == '0.5']

print(dfClean2)
</code></pre>
<p>Which gives me:</p>
<pre><code>             sample_date sample_depth   water_temp
20        2/5/1954 13:55          0.5           18
21        2/5/1954 14:35          0.5         18.2
22        2/5/1954 16:35          0.5         18.5
23        2/5/1954 16:52          0.5         18.5
24        2/5/1954 17:10          0.5         18.6
25        2/5/1954 17:25          0.5         18.8
26        2/5/1954 17:43          0.5           19
</code></pre>
<p>I tried to add these lines to my script to transform the <code>sample_date</code> column:</p>
<pre><code>new_df = dfClean2['sample_date'].str.split()[0]

print(new_df)
</code></pre>
<p>But I get this error:</p>
<pre><code>$ python3 habsos.py 
Traceback (most recent call last):
  File &quot;habsos.py&quot;, line 22, in &lt;module&gt;
    new_df = dfClean2['sample_date'].str.split()[0]
  File &quot;/home/reallymemorable/.pyenv/versions/3.5.9/lib/python3.5/site-packages/pandas/core/series.py&quot;, line 1071, in __getitem__
    result = self.index.get_value(self, key)
  File &quot;/home/reallymemorable/.pyenv/versions/3.5.9/lib/python3.5/site-packages/pandas/core/indexes/base.py&quot;, line 4730, in get_value
    return self._engine.get_value(s, k, tz=getattr(series.dtype, &quot;tz&quot;, None))
  File &quot;pandas/_libs/index.pyx&quot;, line 80, in pandas._libs.index.IndexEngine.get_value
  File &quot;pandas/_libs/index.pyx&quot;, line 88, in pandas._libs.index.IndexEngine.get_value
  File &quot;pandas/_libs/index.pyx&quot;, line 131, in pandas._libs.index.IndexEngine.get_loc
  File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 992, in pandas._libs.hashtable.Int64HashTable.get_item
  File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 998, in pandas._libs.hashtable.Int64HashTable.get_item
KeyError: 0
</code></pre>
<p>How can I modify my script so I get this as an ouput?</p>
<pre><code>             sample_date sample_depth   water_temp
20        2/5/1954               0.5           18
21        2/5/1954               0.5         18.2
22        2/5/1954               0.5         18.5
...
</code></pre>
","['python', 'python-3.x', 'pandas', 'dataframe']",63211596,"<p>The comments already suggested you use <code>expand=True</code>. Another option is</p>
<pre><code>dfClean2.sample_date = dfClean2.sample_date.str.split(' ').str.get(0)
</code></pre>
<p>However, pandas has many methods implemented for dtype <code>datetime</code>. I recommend you pass parameter <code>parse_dates=True</code> on <code>.read_csv()</code> (<a href=""https://pandas.pydata.org/docs/user_guide/io.html#datetime-handling"" rel=""nofollow noreferrer"">handle datetime with read_csv</a>) and use <code>.dt</code> series accesor on that column.</p>
<pre><code>dfClean2.sample_date = dfClean2.sample_date.dt.date
</code></pre>
<p>Read more about .dt accessors in the official pandas website:</p>
<p><a href=""https://pandas.pydata.org/docs/reference/series.html#datetimelike-properties"" rel=""nofollow noreferrer"">Reference</a></p>
<p><a href=""https://pandas.pydata.org/docs/user_guide/basics.html#basics-dt-accessors"" rel=""nofollow noreferrer"">User guide</a></p>
",Splitting cells within dataframe I CSV I read dataframe drop certain columns manipulations Some example rows I want drop time datetime example I get instead I wrote script import pandas pd datetime import datetime dt df pd read csv habsos csv sep error bad lines False index col False dtype unicode pd set option display max rows None Get columns care dfSub df sample date sample depth water temp Remove NaN values dfClean dfSub dropna Select depth measurements dfClean dfClean loc df sample depth print dfClean Which gives sample date sample depth water temp I tried add lines script transform sample date column new df dfClean sample date str split print new df But I get error python habsos py Traceback recent call last File quot habsos py quot line lt module gt new df dfClean sample date str split File quot home reallymemorable pyenv versions lib python site packages,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
63300329,2020-08-07,2020,2,Pandas DataFrame removing NaN rows based on condition?,"<p>Pandas DataFrame removing NaN rows based on condition.</p>
<p>I'm trying to remove the rows whose <code>gender==male</code> and <code>status == NaN</code>.</p>
<p>Sample df:</p>
<pre><code>        name     status        gender   leaves
0       tom        NaN          male      5 
1       tom        True         male      6
2       tom        True         male      7
3       mary       True         female    1
4       mary       NaN          female    10
5       mary       True         female    15
6       john       NaN          male       2
7       mark      True          male       3
</code></pre>
<p>Expected Ouput:</p>
<pre><code>        name     status        gender   leaves
0       tom        True         male      6
1       tom        True         male      7
2       mary       True         female    1
3       mary       NaN          female    10
4       mary       True         female    15
5       mark      True          male       3
</code></pre>
","['python', 'pandas', 'numpy', 'dataframe']",63300402,"<p>You can use <a href=""https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.Series.isna.html"" rel=""noreferrer""><code>isna</code></a> (or <code>isnull</code>) function to get the rows with a value of <code>NaN</code>.
With this knowledge, you can filter your dataframe using something like:</p>
<pre class=""lang-py prettyprint-override""><code>conditions = (df.gender == 'male')&amp;(df.status.isna())
filtered_df = df[~conditions]
</code></pre>
",Pandas DataFrame removing NaN rows based condition Pandas DataFrame removing NaN rows based condition I trying remove rows whose gender male status NaN Sample df name status gender leaves tom NaN male tom True male tom True male mary True female mary NaN female mary True female john NaN male mark True male Expected Ouput name status gender leaves tom True male tom True male mary True female mary NaN female mary True female mark True male,"startoftags, python, pandas, numpy, dataframe, endoftags",python pandas dataframe endoftags,python pandas numpy dataframe,python pandas dataframe,0.87
63326377,2020-08-09,2020,2,Set gamma with pygame 2.0.x,"<p>I create a game and I would like to set the gamma of the screen. For example:</p>
<pre><code>import pygame
from pygame.locals import *

pygame.init()
screen = pygame.display.set_mode((600, 450))
font = pygame.font.SysFont('consolas', 25)

red   = 1
green = 1
blue  = 1

while True:
    for event in pygame.event.get():
        if event.type == QUIT:
            pygame.quit()
            exit()

    screen.fill((20, 20, 20))
    
    # draw colored zones
    for x in range(600):
        pygame.draw.line(screen, (x * 255 / 600, 0, 0), (x, 0), (x, 100))
    for x in range(600):
        pygame.draw.line(screen, (0, x * 255 / 600, 0), (x, 100), (x, 200))
    for x in range(600):
        pygame.draw.line(screen, (0, 0, x * 255 / 600), (x, 200), (x, 300))

    if pygame.mouse.get_pressed()[0]:
        x, y = pygame.mouse.get_pos()
        if y &lt; 100:
            red   = x / 300
            pygame.display.set_gamma(red, green, blue)
        elif y &lt; 200:
            green = x / 300
            pygame.display.set_gamma(red, green, blue)
        elif y &lt; 300:
            blue  = x / 300
            pygame.display.set_gamma(red, green, blue)

    text = 'pygame.display.set_gamma(%.2f, %.2f, %.2f)' % (red, green, blue)
    text = font.render(text, 0, (255, 255, 255), True)
    screen.blit(text, (10, 440 - text.get_height())) # render the text

    pygame.display.flip()
</code></pre>
<p>But when I updated my pygame version to 2.0.0.dev6, the gamma correction didn't work, and I saw ugly colors, blue and yellow flashing irregularly.</p>
<p>I'm on Windows 10.</p>
<h3>EDIT 1</h3>
<p>After investigation, the green color seems not to be handled by the <code>set_gamma</code> function. When I change x value in the color <code>(0, x * 255 / 600, 0)</code>, nothing happens.</p>
<h3>EDIT 2</h3>
<p>I now have the latest <code>pygame</code> version - now litteraly <em>nothing</em> happens.<br />
According to the documentation,</p>
<blockquote>
<p>Not all systems and hardware support gamma ramps</p>
</blockquote>
<p>Ok! Maybe my computer doesn't handle this function! But:</p>
<blockquote>
<p>if the function succeeds it will return True.</p>
</blockquote>
<p>And I get <code>True</code>, even if nothing works as expected.<br />
Does someone see clearly in this mess of bugs?</p>
","['python', 'python-3.x', 'pygame']",66621152,"<p>I help develop pygame.</p>
<p>Thank you for reporting this, I believe this is a bug in pygame.</p>
<p>I could replicate everything you said except for <code>display.set_gamma()</code> returning <code>True</code> even when it succeeded. For me it always returned <code>False</code>.</p>
<p>That difference aside, I found the source of the bug in the pygame source code, and submitted a patch to fix it. <a href=""https://github.com/pygame/pygame/pull/2524"" rel=""nofollow noreferrer"">https://github.com/pygame/pygame/pull/2524</a></p>
<p>Assuming that this gets merged, the bug will be fixed in pygame 2.0.2.</p>
<p>But you want your code to work ASAP presumably, so you have a couple options.</p>
<ul>
<li>Downgrade to pygame 1.9.6 for now, then upgrade to 2.0.2 once it comes out.</li>
<li>Build pygame from source on windows (sorta difficult)</li>
<li>If you happen to be on Python 3.8 64bit, I can send you a <code>wheel</code> file of pygame 2.0.2.dev1 to pip install from.</li>
</ul>
<p>Edit: I just realized you posted this in August. Sorry this took so long. If you have an issue like this in the future, pygame's issue tracker on github is the place to go to report this stuff: <a href=""https://github.com/pygame/pygame/issues"" rel=""nofollow noreferrer"">https://github.com/pygame/pygame/issues</a></p>
",Set gamma pygame x I create game I would like set gamma screen For example import pygame pygame locals import pygame init screen pygame display set mode font pygame font SysFont consolas red green blue True event pygame event get event type QUIT pygame quit exit screen fill draw colored zones x range pygame draw line screen x x x x range pygame draw line screen x x x x range pygame draw line screen x x x pygame mouse get pressed x pygame mouse get pos lt red x pygame display set gamma red green blue elif lt green x pygame display set gamma red green blue elif lt blue x pygame display set gamma red green blue text pygame display set gamma f f f red green blue text font render text True screen blit text text get height render text pygame display flip But I updated pygame,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
63418072,2020-08-14,2020,2,Speed up search for nearest upper and lower value in large pandas dataframe,"<p>My dataframe looks similar to this example below (just with way more entries). I want to obtain the nearest upper and lower number for a given value, for each group.</p>
<pre><code>a    b  
600  10
600  12
600  15
600  17
700   8
700  11
700  19
</code></pre>
<p>For example for a value of 13. I would like to obtain a new dataframe similar to:</p>
<pre><code>a    b  
600  12
600  15
700  11
700  19
</code></pre>
<p>I already tried the solution from Ivo Merchiers in <a href=""https://stackoverflow.com/questions/30112202/how-do-i-find-the-closest-values-in-a-pandas-series-to-an-input-number"">How do I find the closest values in a Pandas series to an input number?</a> using groupby and apply to run it for the different groups.</p>
<pre><code>def find_neighbours(value):
  exactmatch=df[df.num==value]
  if !exactmatch.empty:
      return exactmatch.index
  else:
      lowerneighbour_ind = df[df.num&lt;value].num.idxmax()
      upperneighbour_ind = df[df.num&gt;value].num.idxmin()
      return [lowerneighbour_ind, upperneighbour_ind]

df=df.groupby('a').apply(find_neighbours, 13)
</code></pre>
<p>But since my dataset has around 16 million lines this procedure takes extremely long. Is there possibly a faster way to obtain a solution?</p>
<p><strong>Edit</strong>
Thanks for your answers. I forgot to add some info.
If a close number appears multiple times I would like to have all lines transfered to the new dataframe.
And when there is only one upper (lower) and no lower (upper) neighbour, this lines should be ignored.</p>
<pre><code>a    b  
600  10
600  12
600  15
600  17
700   8
700  11
700  19
800  14
800  15
900  12
900  14
900  14
</code></pre>
<p>Leads for 13 to this:</p>
<pre><code>a    b  
600  12
600  15
700  11
700  19
900  12
900  14
900  14
</code></pre>
<p>Thanks for your help!</p>
","['python', 'pandas', 'dataframe']",63418219,"<p>Yes we can speed it up</p>
<pre><code>v=13

s=(df.b-v)
t=s.abs().groupby([df.a,np.sign(s)]).transform('min')
df1=df.loc[s.abs()==t]
df1=df1[df1.b.sub(v).groupby(df.a).transform('nunique')&gt;1]
df1
Out[102]: 
      a   b
1   600  12
2   600  15
5   700  11
6   700  19
9   900  12
10  900  14
11  900  14
</code></pre>
",Speed search nearest upper lower value large pandas dataframe My dataframe looks similar example way entries I want obtain nearest upper lower number given value group b For example value I would like obtain new dataframe similar b I already tried solution Ivo Merchiers How I find closest values Pandas series input number using groupby apply run different groups def find neighbours value exactmatch df df num value exactmatch empty return exactmatch index else lowerneighbour ind df df num lt value num idxmax upperneighbour ind df df num gt value num idxmin return lowerneighbour ind upperneighbour ind df df groupby apply find neighbours But since dataset around million lines procedure takes extremely long Is possibly faster way obtain solution Edit Thanks answers I forgot add info If close number appears multiple times I would like lines transfered new dataframe And one upper lower lower upper neighbour lines ignored b Leads,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
63669539,2020-08-31,2020,2,How to filter a pandas dataframe using the result of pandas query of another dataframe,"<p>I have a <code>pandas</code> df:</p>
<pre><code>import pandas as pd
df = pd.DataFrame({'col_a' : ['a','a', 'b'], 'col_b': [1,2,3]})
df.index = [4,5,6]
</code></pre>
<p>On this <code>df</code> i apply a query:</p>
<pre><code>df_subset = df.query('col_a == &quot;b&quot;')
</code></pre>
<p>Now I have a second dataframe which looks like this:</p>
<pre><code>import numpy as np
df_numpy = pd.DataFrame(np.array([0.1,0.2,0.3]))
</code></pre>
<p>which is like the original <code>df</code> but without the &quot;identification&quot; column (<code>col_a</code>) and the values are transformed in a way (in this toy example, divided by 10)</p>
<p>I would like to select from the <code>df_numpy</code> the same rows that are selected from the <code>df</code> after applying the query. In this toy example the 3rd row.</p>
<p><strong>EDIT</strong>
The tricky part is that the index values between <code>df_numpy</code> and <code>df</code> are not the same.</p>
<p>Is there a way to do that ?</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",63669598,"<p>If there are same index values use:</p>
<pre><code>print (df_numpy[df_numpy.index.isin(df_subset.index)])    
     0
2  0.3
</code></pre>
<p>EDIT: One idea is create same index values in both, because same length:</p>
<pre><code>df = pd.DataFrame({'col_a' : ['a','a', 'b'], 'col_b': [1,2,3]})
df.index = [4,5,6]

df_subset = df.reset_index(drop=True).query('col_a == &quot;b&quot;')

df_numpy = pd.DataFrame(np.array([0.1,0.2,0.3]))


print (df_numpy[df_numpy.reset_index(drop=True).index.isin(df_subset.index)])

     0
2  0.3
</code></pre>
",How filter pandas dataframe using result pandas query another dataframe I pandas df import pandas pd df pd DataFrame col b col b df index On df apply query df subset df query col quot b quot Now I second dataframe looks like import numpy np df numpy pd DataFrame np array like original df without quot identification quot column col values transformed way toy example divided I would like select df numpy rows selected df applying query In toy example rd row EDIT The tricky part index values df numpy df Is way,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
63754663,2020-09-05,2020,2,What is the better(neater) way to find the largest list range in dictionary of lists,"<p>I have the dict which consists of lists as values. List is <code>len(2)</code> representing the range of an array:</p>
<p><code>new_dict = {0: [0, 7], 1:[15, 21], 2:[-5, 3]}</code></p>
<p>I need to find the key with list that has the largest range i.e. largest <code>list[1] - list[0]</code></p>
<p>I've done it this way and it works fine, but I am assuming it can be done in simpler, or more pythonic way.</p>
<pre><code>largest = float(&quot;-inf&quot;)
largest_list = []
for key in new_dict.keys():
        temp = new_dict[key][1] - new_dict[key][0]
        if temp &gt; largest:
            largest = temp
            largest_list = new_dict[key]
</code></pre>
","['python', 'list', 'dictionary']",63754684,"<p>You could use <a href=""https://docs.python.org/3/library/functions.html#max"" rel=""nofollow noreferrer""><code>max()</code></a> with a custom <code>key</code> function:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; new_dict = {0: [0, 7], 1:[15, 21], 2:[-5, 3]}
&gt;&gt;&gt; max(new_dict.items(), key=lambda x: x[1][1] - x[1][0])[0]
2
</code></pre>
",What better neater way find largest list range dictionary lists I dict consists lists values List len representing range array new dict I need find key list largest range e largest list list I done way works fine I assuming done simpler pythonic way largest float quot inf quot largest list key new dict keys temp new dict key new dict key temp gt largest largest temp largest list new dict key,"startoftags, python, list, dictionary, endoftags",python python3x list endoftags,python list dictionary,python python3x list,0.67
63917603,2020-09-16,2020,2,Setting value for dataframe column from another dataframe based on condition,"<p>I have one dataframe</p>
<pre><code>#Around 100000 rows
df = pd.DataFrame({'text':    [ 'Apple is healthy',  'Potato is round', 'Apple might be green'],
                   'category': [&quot;&quot;,&quot;&quot;, &quot;&quot;],
                   })
</code></pre>
<p>A second dataframe</p>
<pre><code>#Around 3000 rows
df_2 = pd.DataFrame({'keyword':    [ 'Apple ',  'Potato'],
                   'category': [&quot;fruit&quot;,&quot;vegetable&quot;],
                   })
</code></pre>
<p>The required result</p>
<pre><code>#Around 100000 rows
df = pd.DataFrame({'text':    [ 'Apple is healthy',  'Potato is round', 'Apple might be green'],
                   'category': [&quot;fruit&quot;,&quot;vegetable&quot;, &quot;fruit&quot;],
                   })
</code></pre>
<p>I tried this currently</p>
<pre><code>df.set_index('text')
df_2.set_index('keyword')
df.update(df_2)
</code></pre>
<p>The result is</p>
<pre><code>text    category
Apple is healthy    fruit
Potato is round vegetable
Apple might be green
</code></pre>
<p>AS you can see it does not add category for last row. How can I achieve that?</p>
","['python', 'pandas', 'dataframe']",63917756,"<p>You need assign back output from <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>DataFrame.set_index</code></a>, because not inplace operation like <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.update.html"" rel=""nofollow noreferrer""><code>DataFrame.update</code></a>, for matching is used <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extract.html"" rel=""nofollow noreferrer""><code>Series.str.extract</code></a> by column <code>df_2[&quot;keyword&quot;]</code>:</p>
<pre><code>df = df.set_index(df['text'].str.extract(f'({&quot;|&quot;.join(df_2[&quot;keyword&quot;])})', expand=False))
df_2 = df_2.set_index('keyword')
print (df)
                        text category
text                                 
Apple       Apple is healthy         
Potato       Potato is round         
Apple   Apple might be green  



df.update(df_2)
print (df)
                        text   category
text                                   
Apple       Apple is healthy      fruit
Potato       Potato is round  vegetable
Apple   Apple might be green      fruit
</code></pre>
<p>If need add only one column use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extract.html"" rel=""nofollow noreferrer""><code>Series.str.extract</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>Series.map</code></a>:</p>
<pre><code>s = df['text'].str.extract(f'({&quot;|&quot;.join(df_2[&quot;keyword&quot;])})', expand=False)
df['category'] = s.map(df_2.set_index(['keyword'])['category'])
print (df)
                   text   category
0      Apple is healthy      fruit
1       Potato is round  vegetable
2  Apple might be green      fruit
</code></pre>
",Setting value dataframe column another dataframe based condition I one dataframe Around rows df pd DataFrame text Apple healthy Potato round Apple might green category quot quot quot quot quot quot A second dataframe Around rows df pd DataFrame keyword Apple Potato category quot fruit quot quot vegetable quot The required result Around rows df pd DataFrame text Apple healthy Potato round Apple might green category quot fruit quot quot vegetable quot quot fruit quot I tried currently df set index text df set index keyword df update df The result text category Apple healthy fruit Potato round vegetable Apple might green AS see add category last row How I achieve,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
63976189,2020-09-20,2020,4,Pad around 2D array with 1D array in clockwise manner - NumPy / Python,"<p>Lets say there are two arrays:</p>
<pre><code>inner_array = np.array([[3, 3, 3],
                        [6, 6, 6],
                        [9, 9, 9]])

outer_array = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])
</code></pre>
<p>What is the cleanest way to create array that looks like this:</p>
<pre><code>[[ 1,  2,  3,  4, 5],
 [16,  3,  3,  3, 6],
 [15,  6,  6,  6, 7],
 [14,  9,  9,  9, 8],
 [13, 12, 11, 10, 9]]
</code></pre>
<p>It would be even better if it is possible to control starting position of outer array</p>
","['python', 'python-3.x', 'numpy']",63976440,"<p>Here's one way with choosing the starting position as additional arg -</p>
<pre><code>def fill_around(inner_array, outer_array, origin=0):
    outer_array = np.roll(outer_array, origin)
    m,n = inner_array.shape
    out = np.pad(inner_array,(1,1))
    
    s = np.split(outer_array, np.cumsum([n+2,m,n+2]))
    out[[0,-1]] = [s[0],s[2][::-1]]
    out[1:-1,::n+1] = np.dstack([s[3][::-1],s[1]])
    return out
</code></pre>
<p>Sample run for given data -</p>
<pre><code>In [181]: fill_around(inner_array, outer_array)
Out[181]: 
array([[ 1,  2,  3,  4,  5],
       [16,  3,  3,  3,  6],
       [15,  6,  6,  6,  7],
       [14,  9,  9,  9,  8],
       [13, 12, 11, 10,  9]])
</code></pre>
<p>Set starting position -</p>
<pre><code>In [8]: fill_around(inner_array, outer_array, origin=2)
Out[8]: 
array([[15, 16,  1,  2,  3],
       [14,  3,  3,  3,  4],
       [13,  6,  6,  6,  5],
       [12,  9,  9,  9,  6],
       [11, 10,  9,  8,  7]])
</code></pre>
",Pad around D array D array clockwise manner NumPy Python Lets say two arrays inner array np array outer array np array What cleanest way create array looks like It would even better possible control starting position outer array,"startoftags, python, python3x, numpy, endoftags",python arrays numpy endoftags,python python3x numpy,python arrays numpy,0.67
64063238,2020-09-25,2020,2,How to transform weekly data to daily for specific columns using Python,"<p>I am a newbie at python and programming in general. I hope the following question is well explained.</p>
<p>I have a big dataset, with 80+ columns and some of these columns have only data on a weekly basis. I would like transform these columns to have values on a daily basis by simply dividing the weekly value by 7 and attributing the result to the value itself and the 6 other days of that week.</p>
<p>This is what my input dataset looks like:</p>
<pre><code>   date                  col1           col2           col3
02-09-2019               14               NaN            1
09-09-2019               NaN              NaN            2
16-09-2019               NaN              7              3
23-09-2019               NaN              NaN            4
30-09-2019               NaN              NaN            5
07-10-2019               NaN              NaN            6
14-10-2019               NaN              NaN            7
21-10-2019               21               NaN            8
28-10-2019               NaN              NaN            9
04-11-2019               NaN              14             10
11-11-2019               NaN              NaN            11
..
</code></pre>
<p>This is what the output should look like:</p>
<pre><code>   date                  col1           col2           col3
02-09-2019                2               NaN            1
09-09-2019                2               NaN            2
16-09-2019                2               1              3
23-09-2019                2               1              4
30-09-2019                2               1              5
07-10-2019                2               1              6
14-10-2019                2               1              7
21-10-2019                3               1              8
28-10-2019                3               1              9
04-11-2019                3               2              10 
11-11-2019                3               2              11
..
</code></pre>
<p>I canÂ´t come up with a solution, but here is what I thought might work:</p>
<pre><code>def convert_to_daily(df):
    for column in df.columns.tolist():
        if column.isna(): # if true 
            for line in range(len(df[column])):
                # check if value is not empty and 
                succeeded by an 6 empty values or some 
                better logic  
                # I donÂ´t know how to do that.
</code></pre>
","['python', 'pandas', 'dataframe']",64063306,"<p>I believe you need select columns contains at least one missing value, forward filling missing values and divide by <code>7</code>:</p>
<pre><code>m = df.isna().any()
df.loc[:, m] = df.loc[:, m].ffill(limit=7).div(7)
print (df)
          date  col1  col2  col3
0   02-09-2019   2.0   NaN     1
1   09-09-2019   2.0   NaN     2
2   16-09-2019   2.0   1.0     3
3   23-09-2019   2.0   1.0     4
4   30-09-2019   2.0   1.0     5
5   07-10-2019   2.0   1.0     6
6   14-10-2019   2.0   1.0     7
7   21-10-2019   3.0   1.0     8
8   28-10-2019   3.0   1.0     9
9   04-11-2019   3.0   2.0    10
10  11-11-2019   3.0   2.0    11
</code></pre>
",How transform weekly data daily specific columns using Python I newbie python programming general I hope following question well explained I big dataset columns columns data weekly basis I would like transform columns values daily basis simply dividing weekly value attributing result value days week This input dataset looks like date col col col NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN This output look like date col col col NaN NaN I come solution I thought might work def convert daily df column df columns tolist column isna true line range len df column check value empty succeeded empty values better logic I know,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
64139634,2020-09-30,2020,2,"Groupby + Count specific items (not all) , put result in New column","<p>I have this set:</p>
<pre><code>df=pd.DataFrame({'user':[1,1,2,2,2,3,3,3,3,3,4,4],
                  'date':['1995-09-01','1995-09-02','1995-10-03','1995-10-04','1995-10-05','1995-11-07','1995-11-08','1995-11-09','1995-11-10','1995-11-15','1995-12-18','1995-12-20'],
                  'type':['a','a','b','a','c','a','b','a','b','b','a','b']})
</code></pre>
<p>Which gives me:</p>
<pre><code>user    date    type
 1  1995-09-01   a
 1  1995-09-02   a
 2  1995-10-03   b
 2  1995-10-04   a
 2  1995-10-05   c
 3  1995-11-07   a
 3  1995-11-08   b
 3  1995-11-09   a
 3  1995-11-10   b
 3  1995-11-15   b
 4  1995-12-18   a
 4  1995-12-20   b
</code></pre>
<p>I want to create a new column where the count of <strong>a</strong> values on &quot;type&quot; column is shown, grouped by column &quot;user&quot;&quot;</p>
<p>Here is the expected outcome:</p>
<pre><code>user    date    type    cta_a
1   1995-09-01    a       2
1   1995-09-02    a       2
2   1995-10-03    b       1
2   1995-10-04    a       1
2   1995-10-05    c       1
3   1995-11-07    a       2
3   1995-11-08    b       2
3   1995-11-09    a       2
3   1995-11-10    b       2
3   1995-11-15    b       2
4   1995-12-18    a       1
4   1995-12-20    b       1
</code></pre>
<p>I tried the following but it did not work.</p>
<pre><code>df['ct_a'] = df.groupby('user')[df['type']== 'a'].transform('count')
</code></pre>
","['python', 'pandas', 'pandas-groupby']",64139806,"<p><code>mask</code> the non <code>a</code> values in column <code>type</code>, then <code>groupby</code> and <code>transform</code> using <code>count</code>:</p>
<pre><code>df['ct_a'] = df['type'].mask(lambda x: x.ne('a'))\
                       .groupby(df['user']).transform('count')
</code></pre>
<hr />
<pre><code>    user        date type  ct_a
0      1  1995-09-01    a     2
1      1  1995-09-02    a     2
2      2  1995-10-03    b     1
3      2  1995-10-04    a     1
4      2  1995-10-05    c     1
5      3  1995-11-07    a     2
6      3  1995-11-08    b     2
7      3  1995-11-09    a     2
8      3  1995-11-10    b     2
9      3  1995-11-15    b     2
10     4  1995-12-18    a     1
11     4  1995-12-20    b     1
</code></pre>
",Groupby Count specific items put result New column I set df pd DataFrame user date type b c b b b b Which gives user date type b c b b b b I want create new column count values quot type quot column shown grouped column quot user quot quot Here expected outcome user date type cta b c b b b b I tried following work df ct df groupby user df type transform count,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas numpy endoftags,python pandas pandasgroupby,python pandas numpy,0.67
64179970,2020-10-03,2020,2,How to control the color of a specific column in a bar plot depending on it&#39;s xtick label?,"<p>I have a number of plots that show transcribed text from a speech to text engine in which I want to show the bars where the S2T engine transcribed correctly. I have labeled the subplots according to their expected values and now want to color the bars where the engine transcribed correctly in a different number than the other bars.</p>
<p>That means I need to access the color of the bars depending on their x-tick label. How do I do that?</p>
<p>Basically:</p>
<pre><code>for xlabel in fig.xlabels:
   if(xlabel.text == fig.title):
      position = xlabel.position
      fig.colorbar(position, 'red')
</code></pre>
<p>Code that is used to generate the plots:</p>
<pre class=""lang-py prettyprint-override""><code>def count_id(id_val, ax=None):
    title = df.loc[df['ID'] == id_val, 'EXPECTED_TEXT'].iloc[0]
    fig = df[df['ID']==id_val]['TRANSCRIPTION_STRING'].value_counts().plot(kind='bar', ax=ax, figsize=(20,6), title=title)
    fig.set_xticklabels(fig.get_xticklabels(), rotation=40, ha ='right')    
    fig.yaxis.set_major_locator(MaxNLocator(integer=True))

fig, axs = plt.subplots(2, 4)
fig.suptitle('Classic subplot')
fig.subplots_adjust(hspace=1.4)

count_id('byte', axs[0,0])
count_id('clefting', axs[0,1])
count_id('left_hander', axs[0,2])
count_id('leftmost', axs[0,3])
count_id('right_hander', axs[1,0])
count_id('rightmost', axs[1,1])
count_id('wright', axs[1,2])
count_id('write', axs[1,3])
</code></pre>
<p>If anyone has an idea how to iterate over <code>axs</code> so I don't have to call <code>count_id()</code> 8 times, that'd be super helpful too. And yea I tried:</p>
<pre class=""lang-py prettyprint-override""><code>misses = ['byte', 'cleftig', 'left_hander', 'leftmost', 'right_hander', 'rightmost', 'wright', 'write']

for ax, miss in zip(axs.flat, misses):
   count_id(ax, miss) # &lt;- computer says no
</code></pre>
<p><a href=""https://i.stack.imgur.com/CPHfo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CPHfo.png"" alt=""enter image description here"" /></a></p>
","['python', 'pandas', 'matplotlib']",64181013,"<p>You can set the color of each bar according to the label both before and after plotting the bars.</p>
<p>I will use the sample data below for demonstration.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
data = pd.DataFrame({'word': list('abcdefg'), 'number': np.arange(1, 8)})
</code></pre>
<p><strong>1. Before plotting</strong>:
This is the most common way of plotting a colored barplot. Your can pass <em>a list of colors</em> to <code>plt.plot()</code>.</p>
<pre class=""lang-py prettyprint-override""><code>def plot_color_label_before(data, target):
    colors = ['red' if word == target else 'blue' for word in data.word]
    bars = plt.bar(x=data.word, height=data.number, color=colors, alpha=0.5)
</code></pre>
<p>The <code>data</code> passed to the function contains two columns, where the first lists all the words on your xtick, and the second lists the corrsponding numbers. The <code>target</code> is your expected word.
The code determines the colors for each word according to whether it is consistent with your target. For example,</p>
<pre class=""lang-py prettyprint-override""><code>plot_color_label_before(data, 'c')
</code></pre>
<p><a href=""https://i.stack.imgur.com/0Cc93.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0Cc93.png"" alt=""enter image description here"" /></a></p>
<p><strong>2. After plotting</strong>:
If you want to access the color after calling <code>plt.plot</code>, use <code>set_color</code> to change the color of a specific bar.</p>
<pre class=""lang-py prettyprint-override""><code>def plot_color_label_after(data, target):
    bars = plt.bar(x=data.word, height=data.number, color='blue', alpha=0.5)
    for idx, word in enumerate(data.word):
        if word == target:
            bars[idx].set_color(c='yellow')
</code></pre>
<p><code>plt.bar</code> returns a <code>BarContainer</code>, the i-th element of which is a patch (rectangle). Iterate over all the labels and change the color if the word hits the target.
For example,</p>
<pre class=""lang-py prettyprint-override""><code>plot_color_label_after(data, 'c')
</code></pre>
<p><a href=""https://i.stack.imgur.com/wc3Vk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wc3Vk.png"" alt=""enter image description here"" /></a></p>
<p>Finally, as to iterating over <code>axs</code>, just ravel it can solve the problem.</p>
<pre class=""lang-py prettyprint-override""><code>fig, axs = plt.subplots(2, 4)
for ax in axs.ravel():
    ax.plot(...)
</code></pre>
",How control color specific column bar plot depending xtick label I number plots show transcribed text speech text engine I want show bars S T engine transcribed correctly I labeled subplots according expected values want color bars engine transcribed correctly different number bars That means I need access color bars depending x tick label How I Basically xlabel fig xlabels xlabel text fig title position xlabel position fig colorbar position red Code used generate plots def count id id val ax None title df loc df ID id val EXPECTED TEXT iloc fig df df ID id val TRANSCRIPTION STRING value counts plot kind bar ax ax figsize title title fig set xticklabels fig get xticklabels rotation ha right fig yaxis set major locator MaxNLocator integer True fig axs plt subplots fig suptitle Classic subplot fig subplots adjust hspace count id byte axs count id clefting axs count id left,"startoftags, python, pandas, matplotlib, endoftags",python tensorflow keras endoftags,python pandas matplotlib,python tensorflow keras,0.33
64181260,2020-10-03,2020,2,using sklearn macro f1-score as a metric in tensorflow.keras,"<p>I have defined custom metric for tensorflow.keras to compute macro-f1-score after every epoch as follows:</p>
<pre><code>from tensorflow import argmax as tf_argmax
from sklearn.metric import f1_score

def macro_f1(y_true, y_pred):
    # labels are one-hot encoded. so, need to convert
    # [1,0,0] to 0 and
    # [0,1,0] to 1 and
    # [0,0,1] to 2. Then pass these arrays to sklearn f1_score.
    y_true = tf_argmax(y_true, axis=1)
    y_pred = tf_argmax(y_pred, axis=1)
    return f1_score(y_true, y_pred, average='macro')
</code></pre>
<p>and using it during model compilation</p>
<pre><code>model_4.compile(loss = 'categorical_crossentropy',
                optimizer = Adam(lr=init_lr, decay=init_lr / num_epochs),
                metrics = [Recall(name='recall') #, weighted_f1
                           macro_f1])
</code></pre>
<p>and when i try to fit like this:</p>
<pre><code>history_model_4 = model_4.fit(train_image_generator.flow(x=train_imgs, y=train_targets, batch_size=batch_size),
                            validation_data = (val_imgs, val_targets),
                            epochs=num_epochs,
                            class_weight=mask_weights_train,
                            callbacks=[model_save_cb, early_stop_cb, epoch_times_cb],
                            verbose=2)
</code></pre>
<p>this is the error:</p>
<pre><code>OperatorNotAllowedInGraphError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *
        return step_function(self, iterator)
    &lt;ipython-input-57-a890ea61878e&gt;:6 macro_f1  *
        return f1_score(y_true, y_pred, average='macro')
    /usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1095 f1_score  *
        return fbeta_score(y_true, y_pred, 1, labels=labels,
    /usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1217 fbeta_score  *
        _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,
    /usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1478 precision_recall_fscore_support  *
        labels = _check_set_wise_labels(y_true, y_pred, average, labels,
    /usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1301 _check_set_wise_labels  *
        y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    /usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:80 _check_targets  *
        check_consistent_length(y_true, y_pred)
    /usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:209 check_consistent_length  *
        uniques = np.unique(lengths)
    &lt;__array_function__ internals&gt;:6 unique  **
        
    /usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:263 unique
        ret = _unique1d(ar, return_index, return_inverse, return_counts)
    /usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:311 _unique1d
        ar.sort()
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:877 __bool__
        self._disallow_bool_casting()
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:487 _disallow_bool_casting
        &quot;using a `tf.Tensor` as a Python `bool`&quot;)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:474 _disallow_when_autograph_enabled
        &quot; indicate you are trying to use an unsupported feature.&quot;.format(task))

    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.
</code></pre>
<p>What caused such errors and how do I fix it and use it as one of my evaluation metrics at the end of ever y epoch?</p>
<p>EDIT 1:<br />
note: all of this has been done in a jupyter notebook, i have added &quot;&gt;&gt;&gt;&quot;s to seperate lines</p>
<pre><code># getting a batch to pass to model
&gt;&gt;&gt; a_batch = train_image_generator.flow(x=train_imgs, y=train_targets, batch_size=batch_size).next()
# checking its' type to ensure that it's what i though it is
&gt;&gt;&gt; type(a_batch)
# passing the batch to the model
&gt;&gt;&gt; logits = model_4(a_batch)
# checking the type of output
&gt;&gt;&gt; type(logits)
tensorflow.python.framework.ops.EagerTensor
# extracting only the passed targets to calculate f1-score
&gt;&gt;&gt; _, dummy_targets = a_batch
# checking it's type
&gt;&gt;&gt; type(dummy_targets)
numpy.ndarray
&gt;&gt;&gt; macro_f1(y_true=dummy_targets, y_pred=logits)
0.0811965811965812
</code></pre>
","['python', 'tensorflow', 'keras']",64181631,"<p><code>sklearn</code> is not TensorFlow code - it is always recommended to avoid using arbitrary Python code in TF that gets executed inside TF's execution graph.</p>
<p><a href=""https://www.tensorflow.org/addons"" rel=""nofollow noreferrer"">TensorFlow addons</a> already has an implementation of the F1 score (<a href=""https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/F1Score"" rel=""nofollow noreferrer"">tfa.metrics.F1Score</a>), so change your code to use that instead of your custom metric</p>
<p>Make sure you <code>pip install tensorflow-addons</code> first and then</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow_addons as tfa

model_4.compile(loss = 'categorical_crossentropy',
                optimizer = Adam(lr=init_lr, decay=init_lr / num_epochs),
                metrics = [Recall(name='recall') #, weighted_f1
                           tfa.metrics.F1Score(average='macro')])
</code></pre>
",using sklearn macro f score metric tensorflow keras I defined custom metric tensorflow keras compute macro f score every epoch follows tensorflow import argmax tf argmax sklearn metric import f score def macro f true pred labels one hot encoded need convert Then pass arrays sklearn f score true tf argmax true axis pred tf argmax pred axis return f score true pred average macro using model compilation model compile loss categorical crossentropy optimizer Adam lr init lr decay init lr num epochs metrics Recall name recall weighted f macro f try fit like history model model fit train image generator flow x train imgs train targets batch size batch size validation data val imgs val targets epochs num epochs class weight mask weights train callbacks model save cb early stop cb epoch times cb verbose error user code usr local lib python dist packages tensorflow python keras engine training,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
64340467,2020-10-13,2020,2,Parsing information out of a pandas multi-index,"<p>From this pandas data frame, I am trying to parse out all the values corresponding to the date of '2019-1-02' for each ticker,</p>
<pre><code>                   (Dividends + Share Buyback) / FCF  ...  Price to Book Value
Ticker Date                                           ...                     
A      2007-01-03                                NaN  ...                  NaN
       2007-01-04                                NaN  ...                  NaN
       2007-01-05                                NaN  ...                  NaN
       2007-01-08                                NaN  ...                  NaN
       2007-01-09                                NaN  ...                  NaN
...                                              ...  ...                  ...
ZYXI   2019-10-07                           0.181382  ...            29.880555
       2019-10-08                           0.181382  ...            27.452610
       2019-10-09                           0.181382  ...            27.188180
       2019-10-10                           0.181382  ...            26.779516
       2019-10-11                           0.181382  ...            28.101665
</code></pre>
<p>should return:</p>
<pre><code>                   (Dividends + Share Buyback) / FCF  ...  Price to Book Value
Ticker Date                                           ...                     
A      2019-1-02                                5     ...                  6
AA     2019-1-02                                etc   ...                  etc
...    ...                                      ...   ...                  ...
</code></pre>
<p>I have tried:</p>
<pre><code>df_signals.query('Date == 2019-1-02')
</code></pre>
<p>returns:</p>
<pre><code>Empty DataFrame
Columns: [(Dividends + Share Buyback) / FCF, Asset Turnover, CapEx / (Depr + Amor), Current Ratio, Dividends / FCF, Gross Profit Margin, Interest Coverage, Log Revenue, Net Profit Margin, Quick Ratio, Return on Assets, Return on Equity, Share Buyback / FCF, Assets Growth, Assets Growth QOQ, Assets Growth YOY, Earnings Growth, Earnings Growth QOQ, Earnings Growth YOY, FCF Growth, FCF Growth QOQ, FCF Growth YOY, Sales Growth, Sales Growth QOQ, Sales Growth YOY, Earnings Yield, FCF Yield, Market-Cap, P/Cash, P/E, P/FCF, P/NCAV, P/NetNet, P/Sales, Price to Book Value]
Index: []
</code></pre>
","['python', 'python-3.x', 'pandas']",64340566,"<p>You'll want to use the <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.xs.html"" rel=""nofollow noreferrer"">DataFrame.xs(...)</a> method for this. This should work for your dataframe:</p>
<pre><code>df.xs(&quot;2019-1-02&quot;, level=&quot;Date&quot;)
</code></pre>
",Parsing information pandas multi index From pandas data frame I trying parse values corresponding date ticker Dividends Share Buyback FCF Price Book Value Ticker Date A NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ZYXI return Dividends Share Buyback FCF Price Book Value Ticker Date A AA etc etc I tried df signals query Date returns Empty DataFrame Columns Dividends Share Buyback FCF Asset Turnover CapEx Depr Amor Current Ratio Dividends FCF Gross Profit Margin Interest Coverage Log Revenue Net Profit Margin Quick Ratio Return Assets Return Equity Share Buyback FCF Assets Growth Assets Growth QOQ Assets Growth YOY Earnings Growth Earnings Growth QOQ Earnings Growth YOY FCF Growth FCF Growth QOQ FCF Growth YOY Sales Growth Sales Growth QOQ Sales Growth YOY Earnings Yield FCF Yield Market Cap P Cash P E P FCF P NCAV P NetNet P Sales Price Book Value Index,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
64370349,2020-10-15,2020,2,Pandas merge list of DFs based on grouping column value,"<p>I have a list of Pandas DFs with every DF having the same <code>columns</code>:</p>
<pre><code>df1_values = [[&quot;2001-01-01&quot;,&quot;Lime&quot;,10],[&quot;2001-01-02&quot;,&quot;Lime&quot;,20]]
df2_values = [[&quot;2001-01-01&quot;,&quot;Mango&quot;,40],[&quot;2001-01-02&quot;,&quot;Mango&quot;,50],[&quot;2001-01-03&quot;,&quot;Mango&quot;,60]]
df3_values = [[&quot;2001-01-01&quot;,&quot;Orange&quot;,30]]
df1 = pd.DataFrame(df1_values,columns=[&quot;date&quot;,&quot;fruit&quot;,&quot;value&quot;])
df2 = pd.DataFrame(df2_values,columns=[&quot;date&quot;,&quot;fruit&quot;,&quot;value&quot;])
df3 = pd.DataFrame(df3_values,columns=[&quot;date&quot;,&quot;fruit&quot;,&quot;value&quot;])
dfs = [df1,df2,df3]
</code></pre>
<p>one of the sample DFs  --&gt; DF1:</p>
<pre><code>      date     fruit    value
0   2001-01-01  Lime    10
1   2001-01-02  Lime    20
</code></pre>
<p>Trying to <code>merge</code> all the DFs in the <code>list</code> in the format below (grouped on date), EXPECTED OP:</p>
<pre><code>    date         fruit  value
  2001-01-01     Lime    10
  2001-01-01     Mango   40
  2001-01-01     Orange  30
  2001-01-02     Lime    20
  2001-01-02     Mango   50
  2001-01-03     Mango   60
</code></pre>
<p>Current iterative approach:</p>
<pre><code>date_dict={}
for each_date in [&quot;2001-01-01&quot;,&quot;2001-01-02&quot;,&quot;2001-01-03&quot;]:
   for each_df in dfs:
       if each_date in date_dict:
        #append the values for this date
       else:
           #enter the values for this date
</code></pre>
<p>It is working but it is taking a long time.</p>
<p>Pandas Approach:</p>
<pre><code>from functools import reduce
df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['fruit'],
                                        how='outer'), dfs)
</code></pre>
<p>OP:</p>
<pre><code>     date_x    fruit    value_x date_y     value_y  date    value
0   2001-01-01  Lime    10.0    NaN         NaN     NaN     NaN
1   2001-01-02  Lime    20.0    NaN         NaN     NaN     NaN
2   NaN         Mango   NaN    2001-01-01   40.0    NaN     NaN
3   NaN         Mango   NaN    2001-01-02   50.0    NaN     NaN
4   NaN         Mango   NaN    2001-01-03   60.0    NaN     NaN
5   NaN         Orange  NaN     NaN         NaN  2001-01-01 30.0
</code></pre>
<p>Any suggestions on how to correct the mistakes could be helpful.</p>
","['python', 'pandas', 'numpy', 'dataframe']",64370436,"<p>You can do <code>pandas.concat</code> followed by <code>.sort_values</code>:</p>
<pre><code>print( pd.concat(dfs).sort_values('date') )
</code></pre>
<p>Prints:</p>
<pre><code>         date   fruit  value
0  2001-01-01    Lime     10
0  2001-01-01   Mango     40
0  2001-01-01  Orange     30
1  2001-01-02    Lime     20
1  2001-01-02   Mango     50
2  2001-01-03   Mango     60
</code></pre>
",Pandas merge list DFs based grouping column value I list Pandas DFs every DF columns df values quot quot quot Lime quot quot quot quot Lime quot df values quot quot quot Mango quot quot quot quot Mango quot quot quot quot Mango quot df values quot quot quot Orange quot df pd DataFrame df values columns quot date quot quot fruit quot quot value quot df pd DataFrame df values columns quot date quot quot fruit quot quot value quot df pd DataFrame df values columns quot date quot quot fruit quot quot value quot dfs df df df one sample DFs gt DF date fruit value Lime Lime Trying merge DFs list format grouped date EXPECTED OP date fruit value Lime Mango Orange Lime Mango Mango Current iterative approach date dict date quot quot quot quot quot quot df dfs date date dict append values date else enter,"startoftags, python, pandas, numpy, dataframe, endoftags",python pandas numpy endoftags,python pandas numpy dataframe,python pandas numpy,0.87
64511457,2020-10-24,2020,2,Add a column name to a panda dataframe (multi index),"<p>I concatenate series objects, with existing column names together to a DataFrame in Pandas. The result looks like this:</p>
<pre><code>pd.concat([x, y, z], axis=1)


   X   |  Y   |   Z
  -------------------
  data | data | data
</code></pre>
<p>Now I want to insert another column name A above the column names X, Y, Z, for the whole DataFrame. This should look like this at the end:</p>
<pre><code>   A                  # New Column Name
  ------------------- 
   X   |  Y   |   Z   # Old Column Names
  -------------------
  data | data | data 
</code></pre>
<p>So far I did not find a solution how to insert a column name A above the existing columns names X, Y, Z for the complete DataFrame. I would be grateful for any help. :)</p>
","['python', 'pandas', 'dataframe']",64511787,"<p>Let's try with <code>MultiIndex.from_product</code> to create <code>MultiIndex</code> columns:</p>
<pre><code>df = pd.concat([x, y, z], axis=1)
df.columns = pd.MultiIndex.from_product([['A'], df.columns])
</code></pre>
<hr />
<pre><code>A            
X     Y     Z
data  data  data
</code></pre>
",Add column name panda dataframe multi index I concatenate series objects existing column names together DataFrame Pandas The result looks like pd concat x z axis X Y Z data data data Now I want insert another column name A column names X Y Z whole DataFrame This look like end A New Column Name X Y Z Old Column Names data data data So far I find solution insert column name A existing columns names X Y Z complete DataFrame I would grateful help,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
65340535,2020-12-17,2020,2,"How to convert a list of lists into a dataframe where first element is index, second is column name","<p>We have a list of lists:</p>
<pre><code>[['130', '2020-12-17 12:02:19', [52.1846976, 21.0525275]], ['213', '2020-12-17 12:02:22', [52.1757618, 21.2319711]]
</code></pre>
<p>and want to convert it into a dataframe as such:</p>
<pre><code>    index  2020-12-17 12:02:19          2020-12-17 12:02:22
    130    [52.1846976, 21.0525275]       NaN
    213    NaN                      [52.1757618, 21.2319711]
</code></pre>
<p>Can't figure it out.</p>
","['python', 'pandas', 'dataframe']",65341014,"<p>You can munge your list into a list of dict's and then provide the index explicitly to the construtor:</p>
<pre><code>In [1]: import pandas as pd

In [2]: data = [['130', '2020-12-17 12:02:19', [52.1846976, 21.0525275]], ['213', '2020-12-17 12:02:22', [52.1757618, 21.2319711]]]

In [3]: pd.DataFrame([{col: val} for _, col, val in data], index=[item[0] for item in data])
Out[3]:
          2020-12-17 12:02:19       2020-12-17 12:02:22
130  [52.1846976, 21.0525275]                       NaN
213                       NaN  [52.1757618, 21.2319711]
</code></pre>
",How convert list lists dataframe first element index second column name We list lists want convert dataframe index NaN NaN Can figure,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
65535591,2021-01-02,2021,4,groupby in pandas and exclude grouper column from output DataFrame,"<p>I am trying to groupby a pandas df so that it keeps the key as index but it doesn't include the key in each group.</p>
<p>Here is an example of what I mean.</p>
<ol>
<li><p>the original dataframe</p>
<p><code>ungrouped_df = pd.DataFrame({'col1':['A','A','B','C','C','C'], 'col2':[8,5,1,4,1,2], 'col3':[7,4,2,1,2,1],'col4':[1,8,0,2,0,0]})</code></p>
</li>
</ol>
<p>out:</p>
<pre><code>| index | col1 | col2 | col3 | col4 |
|-------|------|------|------|------|
| 1     |    A |    8 |    7 |    1 |
| 2     |    A |    5 |    4 |    8 |
| 3     |    B |    1 |    2 |    0 |
| 4     |    C |    4 |    1 |    2 |
| 5     |    C |    1 |    2 |    0 |
| 6     |    C |    2 |    1 |    0 |
</code></pre>
<ol start=""2"">
<li><p>now, I would like to create a numpy array from the grouped dataframe</p>
<p><code>grouped_df = ungrouped_df.groupby(by='col1', group_keys=False).apply(np.asarray)</code></p>
</li>
</ol>
<p>This is what I get</p>
<pre><code>| index | col1                                      | 
|-------|-------------------------------------------|
| A     | [[A, 8, 7, 1],[A, 5, 4, 8],[A, 8, 7, 1]]  |
| B     | [[B, 1, 2, 0]]                            |
| C     | [[C, 4, 1, 2], [C, 1, 2, 0], [C, 2, 1, 0]]|
</code></pre>
<ol start=""3"">
<li>This is what I'd like to get instead</li>
</ol>
<p>out:</p>
<pre><code>| index | col1                             | 
|-------|----------------------------------|
| A     | [[8, 7, 1],[5, 4, 8],[8, 7, 1]]  |
| B     | [[1, 2, 0]]                      |
| C     | [[4, 1, 2], [1, 2, 0], [2, 1, 0]]|
</code></pre>
<p>I can use some advice here because I am a bit lost. I thought that &quot;group_keys=False&quot; would do the trick but it doesn't. Any help is much appreciated.</p>
<p>Thanks</p>
","['python', 'pandas', 'pandas-groupby']",65535602,"<p>I generally don't recommend storing lists in columns, but the most obvious way to fix this is to ensure the unwanted column is not being grouped on.</p>
<p>You can specify that either by</p>
<ol>
<li>setting &quot;col1&quot; as the index before grouping, or</li>
<li>drop &quot;col1&quot; before grouping, or</li>
<li>selecting the columns you DO want to group</li>
</ol>
<hr />
<pre><code>df.set_index('col1').groupby(level=0).apply(np.array)

col1
A               [[8, 7, 1], [5, 4, 8]]
B                          [[1, 2, 0]]
C    [[4, 1, 2], [1, 2, 0], [2, 1, 0]]
</code></pre>
<p>OR,</p>
<pre><code>df.drop('col1', 1).groupby(df['col1']).apply(np.array)

col1
A               [[8, 7, 1], [5, 4, 8]]
B                          [[1, 2, 0]]
C    [[4, 1, 2], [1, 2, 0], [2, 1, 0]]
</code></pre>
<p>OR,</p>
<pre><code>(df.groupby('col1')[df.columns.difference(['col1'])]
   .apply(lambda x: x.values.tolist()))

col1
A               [[8, 7, 1], [5, 4, 8]]
B                          [[1, 2, 0]]
C    [[4, 1, 2], [1, 2, 0], [2, 1, 0]]
dtype: object
</code></pre>
",groupby pandas exclude grouper column output DataFrame I trying groupby pandas df keeps key index include key group Here example I mean original dataframe ungrouped df pd DataFrame col A A B C C C col col col index col col col col A A B C C C I would like create numpy array grouped dataframe grouped df ungrouped df groupby col group keys False apply np asarray This I get index col A A A A B B C C C C This I like get instead index col A B C I use advice I bit lost I thought quot group keys False quot would trick Any help much appreciated Thanks,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
65882813,2021-01-25,2021,2,groupby filter for accounts where monthly balance are all negative,"<p>Here is my sample data -</p>
<pre><code>import pandas as pd

df = pd.DataFrame({'Account': ['A1', 'A1', 'A1', 'A1', 'A2', 'A2', 'A2', 'A2', 'A3', 'A3', 'A3', 'A3'],
                   'Date': ['D1', 'D2', 'D3', 'D4', 'D1', 'D2', 'D3', 'D4', 'D1', 'D2', 'D3', 'D4'],
                   'Month': ['M1', 'M1', 'M2', 'M2', 'M1', 'M1', 'M2', 'M2', 'M1', 'M1', 'M2', 'M2'],
                   'Balance': [133, 321, 123, 234, 345, 344, 456, -765, -123, -312, 111, -766]})

print(df)

   Account Date Month  Balance
0       A1   D1    M1      133
1       A1   D2    M1      321
2       A1   D3    M2      123
3       A1   D4    M2      234
4       A2   D1    M1      345
5       A2   D2    M1      344
6       A2   D3    M2      456
7       A2   D4    M2     -765
8       A3   D1    M1     -123
9       A3   D2    M1     -312
10      A3   D3    M2      111
11      A3   D4    M2     -766
</code></pre>
<p>I want to filter out accounts from this <code>df</code> where <em>all</em> monthly balances for an account is less than 0. In the example, only <code>A3</code> should get filtered out as all monthly balances for that account is less than 0.</p>
<p>Here is what I tried which does the job -</p>
<pre><code>eod_gr = (df.groupby(['Account', 'Month'])['Balance'].mean() &lt; 0).reset_index()
tmp = eod_gr.groupby('Account')['Balance'].all().to_dict()
accounts = [i for i in tmp if not tmp[i]]
print(accounts)
df_eod = df[df['Account'].isin(accounts)]
print(df_eod)
</code></pre>
<p>The problem is I am working on a real time use case where speed is paramount. I need a way to optimize this query to attain the same results.</p>
<p><strong>Expected Output</strong></p>
<pre><code>  Account Date Month  Balance
0      A1   D1    M1      133
1      A1   D2    M1      321
2      A1   D3    M2      123
3      A1   D4    M2      234
4      A2   D1    M1      345
5      A2   D2    M1      344
6      A2   D3    M2      456
7      A2   D4    M2     -765
</code></pre>
","['python', 'pandas', 'pandas-groupby']",65882867,"<p>I think here should be improved performance omit by second groupby, here is used <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html"" rel=""nofollow noreferrer""><code>GroupBy.transform</code></a> with <code>'invert'</code> mask from <code>&lt;</code> to <code>&gt;=</code> for get all <code>accounts</code>:</p>
<pre><code>mask = df.groupby(['Account', 'Month'])['Balance'].transform('mean') &gt;= 0
accounts = df.loc[mask, 'Account']
df_eod = df[df['Account'].isin(accounts)]
print(df_eod)
  Account Date Month  Balance
0      A1   D1    M1      133
1      A1   D2    M1      321
2      A1   D3    M2      123
3      A1   D4    M2      234
4      A2   D1    M1      345
5      A2   D2    M1      344
6      A2   D3    M2      456
7      A2   D4    M2     -765
</code></pre>
<p>Another idea with aggregation <code>mean</code> and filter <code>MultiIndex Series</code> with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.get_level_values.html"" rel=""nofollow noreferrer""><code>Index.get_level_values</code></a>:</p>
<pre><code>s = df.groupby(['Account', 'Month'])['Balance'].mean()
accounts = s.index[s &gt;= 0].get_level_values(0)
df_eod = df[df['Account'].isin(accounts)]
</code></pre>
",groupby filter accounts monthly balance negative Here sample data import pandas pd df pd DataFrame Account A A A A A A A A A A A A Date D D D D D D D D D D D D Month M M M M M M M M M M M M Balance print df Account Date Month Balance A D M A D M A D M A D M A D M A D M A D M A D M A D M A D M A D M A D M I want filter accounts df monthly balances account less In example A get filtered monthly balances account less Here I tried job eod gr df groupby Account Month Balance mean lt reset index tmp eod gr groupby Account Balance dict accounts tmp tmp print accounts df eod df df Account isin accounts print,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
65934592,2021-01-28,2021,3,How to concatenate data frames with unequal number of rows and different column names,"<p>I am trying to concatenate four data frames with an unequal number of rows and different column names. Here I am placing my code and output what I am getting.</p>
<pre><code>import pandas as pd

data1 = pd.DataFrame({'Col':[33,44,55,67], 'Col1':[44,55,55,555]})
data2 = pd.DataFrame({'Col2':[33,44], 'Col3':[22,22]})
data3 = pd.DataFrame({'Col4':[33,44,44], 'Col5':[22,22,44]})
data4 = pd.DataFrame({'Col6':[33], 'Col7':[22], 'Col8':[44]})


data5 = pd.concat([data1, data2, data3, data4])
data5
</code></pre>
<p>here is my actual output</p>
<p><a href=""https://i.stack.imgur.com/NPCrk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NPCrk.png"" alt=""enter image description here"" /></a></p>
<p>what I am expecting is:</p>
<p><a href=""https://i.stack.imgur.com/ypUsg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ypUsg.png"" alt=""enter image description here"" /></a></p>
<p>Thanks in advance</p>
","['python', 'pandas', 'dataframe']",65934749,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a> with <code>axis=1</code>, but first repeat values for match maximal lengths of rows:</p>
<pre><code>dfs = [data1, data2, data3, data4]

maxlen = len(max(dfs, key=len))
print (maxlen)
4

dfs = [pd.concat([x] * int(np.ceil(maxlen / len(x))), ignore_index=True).iloc[:maxlen] 
       for x in dfs]
data5 = pd.concat(dfs, axis=1)

print (data5)
   Col  Col1  Col2  Col3  Col4  Col5  Col6  Col7  Col8
0   33    44    33    22    33    22    33    22    44
1   44    55    44    22    44    22    33    22    44
2   55    55    33    22    44    44    33    22    44
3   67   555    44    22    33    22    33    22    44
</code></pre>
",How concatenate data frames unequal number rows different column names I trying concatenate four data frames unequal number rows different column names Here I placing code output I getting import pandas pd data pd DataFrame Col Col data pd DataFrame Col Col data pd DataFrame Col Col data pd DataFrame Col Col Col data pd concat data data data data data actual output I expecting Thanks advance,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
66400783,2021-02-27,2021,3,Get the nested div contents starting from the div with a specific id,"<p>I have the following div with <code>id=&quot;participant&quot;</code> :</p>
<pre><code>&lt;div id=&quot;participant&quot; class=&quot;panel-collapse collapse in&quot; role=&quot;tabpanel&quot; aria-expanded=&quot;true&quot; aria-labelledby=&quot;headingOne&quot; style=&quot;&quot;&gt;
&lt;div class=&quot;panel-body&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
   &lt;div class=&quot;col-sm-12&quot;&gt;
      &lt;div class=&quot;question-container&quot;&gt;
         &lt;div class=&quot;question-group&quot;&gt;
            &lt;h5 class=&quot;question&quot;&gt;
               Organisation
            &lt;/h5&gt;
            &lt;div class=&quot;answer&quot;&gt;
               &lt;p&gt;Ministerio de Hacienda [Ministry of Finance]&lt;/p&gt;
               &lt;p&gt;Consejo de Contadores PÃºblicos del Paraguay (Consejo) [Council of Public Accountants of Paraguay]&lt;/p&gt;
               &lt;p&gt;Central Bank of Paraguay â Superintendence of Banks&lt;/p&gt;
               &lt;br&gt;
            &lt;/div&gt;
         &lt;/div&gt;
         &lt;div class=&quot;question-group&quot;&gt;
            &lt;h5 class=&quot;question&quot;&gt;
               Role of the organisation
            &lt;/h5&gt;
            &lt;div class=&quot;answer&quot;&gt;
               &lt;p&gt;The Ministry of Finance has authority to establish accounting standards for all entities in Paraguay other than banks and financial institutions.&amp;nbsp; &lt;/p&gt;
               &lt;p&gt;The Consejo is the professional association of public accountants in Paraguay.&amp;nbsp; The Consejo advises the Ministry of Finance with regard to accounting standards.&lt;/p&gt;
               &lt;p&gt;Accounting standards for banks and other financial institutions are established by the Central Bank of Paraguay.&lt;/p&gt;
            &lt;/div&gt;
         &lt;/div&gt;
         &lt;div class=&quot;question-group&quot;&gt;
            &lt;h5 class=&quot;question&quot;&gt;
               Website
            &lt;/h5&gt;
            &lt;div class=&quot;answer&quot;&gt;
               &lt;p&gt;Ministry of Finance: &lt;a href=&quot;http://www.hacienda.gov.py&quot; target=&quot;_blank&quot;&gt;http://www.hacienda.gov.py&lt;/a&gt;&lt;/p&gt;
               &lt;p&gt;Consejo: &lt;a href=&quot;http://www.consejo.com.py&quot; target=&quot;_blank&quot;&gt;www.consejo.com.py&lt;/a&gt;&lt;/p&gt;
               &lt;p&gt;Central Bank: &lt;a href=&quot;http://www/bcp.gov.py&quot; target=&quot;_blank&quot;&gt;http://www/bcp.gov.py&lt;/a&gt;&lt;/p&gt;
            &lt;/div&gt;
         &lt;/div&gt;
         &lt;div class=&quot;question-group&quot;&gt;
            &lt;h5 class=&quot;question&quot;&gt;
               Email contact
            &lt;/h5&gt;
            &lt;div class=&quot;answer&quot;&gt;
               &lt;p&gt;Consejo: &lt;a href=&quot;mailto:consejo@consejo.com.py&quot;&gt;consejo@consejo.com.py&lt;/a&gt;&lt;br&gt;
                  Central Bank:
               &lt;/p&gt;
               &lt;ul&gt;
                  &lt;li&gt;&lt;a href=&quot;mailto:afranco@bcp.gov.py&quot;&gt;afranco@bcp.gov.py&lt;/a&gt; and &lt;a href=&quot;hcentu@bcp.gov.py&quot;&gt;hcentu@bcp.gov.py&lt;/a&gt;&lt;/li&gt;
                  &lt;li&gt;&lt;a href=&quot;mailto:jjimenez@bcp.gov.py&quot;&gt;jjimenez@bcp.gov.py&lt;/a&gt;&lt;/li&gt;
                  &lt;li&gt;&lt;a href=&quot;mailto:hcolman@bcp.gov.py&quot;&gt;hcolman@bcp.gov.py&lt;/a&gt;&lt;/li&gt;
               &lt;/ul&gt;
            &lt;/div&gt;
         &lt;/div&gt;
      &lt;/div&gt;
   &lt;/div&gt;
&lt;/div&gt;
</code></pre>
<p>I want to get the content of each div with <code>class=&quot;question&quot;</code> and <code>class=&quot;answer&quot;</code> starting from the <code>&lt;div id=&quot;participant&quot;&gt;</code>  because I have many div with the same structure and CSS so I can distinguish between them with the <code>id</code></p>
<p>This is my expected output :</p>
<pre><code>Organisation Ministerio de Hacienda [Ministry of Finance]
             Consejo de Contadores PÃºblicos del Paraguay (Consejo) [Council of Public Accountants of Paraguay]
             Central Bank of Paraguay â Superintendence of Banks
Role of the  The Ministry of Finance has authority to establish accounting standards for all entities in Paraguay other than banks and financial institutions.
organisation The Consejo is the professional association of public accountants in Paraguay.  The Consejo advises the Ministry of Finance with regard to accounting standards.
             Accounting standards for banks and other financial institutions are established by the Central Bank of Paraguay.
Website      Ministry of Finance: http://www.hacienda.gov.py
             Consejo: www.consejo.com.py
             Central Bank: http://www/bcp.gov.py    
Emailcontact Consejo: consejo@consejo.com.py
             Central Bank:
             afranco@bcp.gov.py and hcentu@bcp.gov.py
             jjimenez@bcp.gov.py
             hcolman@bcp.gov.py          
         
</code></pre>
<p>This is my work so far :</p>
<pre><code>from bs4 import BeautifulSoup
import requests
import pandas as pd
import re
# Site URL
url = &quot;https://www.ifrs.org/use-around-the-world/use-of-ifrs-standards-by-jurisdiction/paraguay&quot;
# Make a GET request to fetch the raw HTML content
html_content = requests.get(url).text
# Parse HTML code for the entire site
soup = BeautifulSoup(html_content, &quot;lxml&quot;)
divs = soup.find_all(&quot;div&quot;, attrs={&quot;id&quot;: &quot;participant&quot;})
disp = []
d=[]
for c in divs : disp.append(c.find('div', attrs={'class': 'question-group'}))
for t in disp : d.append(t.h5.text.strip())    
</code></pre>
","['python', 'web-scraping', 'beautifulsoup']",66401055,"<p>Putting aside the final print formatting, something like this should work:</p>
<pre><code>questions = [q.text.strip() for q in soup.select('div#participant h5.question') ]
answers = [a.text.strip() for a in soup.select('div#participant div.answer')]
for q, a in zip(questions,answers):
    print(q,&quot;: &quot;,a)
    print('---')
</code></pre>
<p>Output:</p>
<pre><code>Organisation :  Ministerio de Hacienda [Ministry of Finance]
Consejo de Contadores PÃºblicos del Paraguay (Consejo) [Council of Public Accountants of Paraguay]
Central Bank of Paraguay â Superintendence of Banks
---
</code></pre>
<p>etc.</p>
",Get nested div contents starting div specific id I following div id quot participant quot lt div id quot participant quot class quot panel collapse collapse quot role quot tabpanel quot aria expanded quot true quot aria labelledby quot headingOne quot style quot quot gt lt div class quot panel body quot gt lt div class quot row quot gt lt div class quot col sm quot gt lt div class quot question container quot gt lt div class quot question group quot gt lt h class quot question quot gt Organisation lt h gt lt div class quot answer quot gt lt p gt Ministerio de Hacienda Ministry Finance lt p gt lt p gt Consejo de Contadores P blicos del Paraguay Consejo Council Public Accountants Paraguay lt p gt lt p gt Central Bank Paraguay Superintendence Banks lt p gt lt br gt lt div gt lt div,"startoftags, python, webscraping, beautifulsoup, endoftags",python arrays numpy endoftags,python webscraping beautifulsoup,python arrays numpy,0.33
66693278,2021-03-18,2021,2,Pandas Dataframe Set value in cell if condition (Element is in list),"<p>I have a dataframe containing a column called &quot;column_B&quot; with only nan-values. It contains also &quot;column_A&quot;, which is a list of strings for every cell in the dataframe.</p>
<pre><code>index   column_A    column_B  
1       ['A','B']    np.nan
2       ['B']        np.nan
3       ['C']        np.nan
</code></pre>
<p>Now I want that column_B is set to 0 if any element in  a list of column_A contains 'B'.</p>
<p>Desired result:</p>
<pre><code>   index   column_A    column_B  
    1       ['A','B']    0
    2       ['B']        0
    3       ['C']        np.nan
</code></pre>
<p>I tried</p>
<pre><code>df['column_B'][any([x == 'B' for x in df['column_A']])] = 0
</code></pre>
<p>that doesn't work.
Any ideas? :)</p>
<p>Thanks</p>
","['python', 'pandas', 'dataframe']",66693312,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>Series.map</code></a> or <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html"" rel=""nofollow noreferrer""><code>Series.apply</code></a> for test value in list by <code>in</code> and set value in <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.mask.html"" rel=""nofollow noreferrer""><code>Series.mask</code></a>:</p>
<pre><code>df['column_B'] = df['column_B'].mask(df['column_A'].map(lambda x: 'B' in x), 0)
#df['column_B'] = df['column_B'].mask(df['column_A'].apply(lambda x: 'B' in x), 0)
</code></pre>
<p>Or by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>DataFrame.loc</code></a>:</p>
<pre><code>df.loc[df['column_A'].map(lambda x: 'B' in x), 'column_B'] = 0
</code></pre>
",Pandas Dataframe Set value cell condition Element list I dataframe containing column called quot column B quot nan values It contains also quot column A quot list strings every cell dataframe index column A column B A B np nan B np nan C np nan Now I want column B set element list column A contains B Desired result index column A column B A B B C np nan I tried df column B x B x df column A work Any ideas Thanks,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
66862848,2021-03-29,2021,2,No handles with labels found to put in legend error after plotting two plots on one chart,"<p>I'm plotting some features month by month and highlighted some of them. Before I added the highlight, the legend can show automatically but now it returns <code>No handles with labels found to put in legend</code> error.</p>
<p>Example data</p>
<pre><code>df = pd.DataFrame(np.random.randn(10, 4), columns=list('ABCD'))
highlight = ['A', 'B']
</code></pre>
<pre><code>fig, ax = plt.subplots(figsize=(15, 5))
plt.plot(df.loc[:, ~df.columns.isin(highlight)], c='gray', alpha=0.5) 
plt.plot(df.loc[:, df.columns.isin(highlight)]) 
months = pd.date_range('2019-04-01','2019-08-01', freq='MS').strftime(&quot;%Y-%m&quot;).tolist()
plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
z = ax.set_xticklabels(months, rotation=45)
</code></pre>
<p><a href=""https://i.stack.imgur.com/f6amW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f6amW.png"" alt=""enter image description here"" /></a></p>
<p>I guess plotting two plots on one chart caused this but don't know how to fix it. I don't want to manually specify legends.</p>
","['python', 'pandas', 'matplotlib']",66863392,"<ul>
<li>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html"" rel=""nofollow noreferrer""><code>pandas.DataFrame.plot</code></a> instead of <code>plt.plot</code>
<ul>
<li>This ensures the column headers are correctly assigned as the labels to the correct line</li>
<li>It returns a <a href=""https://matplotlib.org/stable/api/axes_api.html#matplotlib.axes.Axes"" rel=""nofollow noreferrer""><code>matplotlib.axes.Axes</code></a> or a <code>numpy.ndarray</code> of them.</li>
</ul>
</li>
<li>This example is using <code>pandas v1.2.3</code></li>
</ul>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import matplotlib.pyplot as plt

ax = df.loc[:, df.columns.isin(highlight)].plot(figsize=(15, 5))
df.loc[:, ~df.columns.isin(highlight)].plot(c='gray', alpha=0.5, ax=ax)

plt.legend(title='column labels', loc='center left', bbox_to_anchor=(1.0, 0.5))
plt.show()
</code></pre>
<p><a href=""https://i.stack.imgur.com/QOAkb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QOAkb.png"" alt=""enter image description here"" /></a></p>
",No handles labels found put legend error plotting two plots one chart I plotting features month month highlighted Before I added highlight legend show automatically returns No handles labels found put legend error Example data df pd DataFrame np random randn columns list ABCD highlight A B fig ax plt subplots figsize plt plot df loc df columns isin highlight c gray alpha plt plot df loc df columns isin highlight months pd date range freq MS strftime quot Y quot tolist plt legend loc center left bbox anchor z ax set xticklabels months rotation I guess plotting two plots one chart caused know fix I want manually specify legends,"startoftags, python, pandas, matplotlib, endoftags",python pandas matplotlib endoftags,python pandas matplotlib,python pandas matplotlib,1.0
66867203,2021-03-30,2021,2,How to fill a pandas dataframe column using a value from another dataframe column,"<p>Firstly we can import some packages which might be useful</p>
<pre><code>import pandas as pd
import datetime
</code></pre>
<p>Say I now have a dataframe which has a date, name and age column.</p>
<pre><code>df1 = pd.DataFrame({'date': ['10-04-2020', '04-07-2019', '12-05-2015' ], 'name': ['john', 'tim', 'sam'], 'age':[20, 22, 27]})
</code></pre>
<p>Now say I have another dataframe with some random columns</p>
<pre><code>df2 = pd.DataFrame({'a': [1,2,3], 'b': [4,5,6]})
</code></pre>
<p><strong>Question:</strong></p>
<p>How can I take the age value in <code>df1</code> filtered on the date (can select this value) and populate a whole new column in <code>df2</code> with this value? Ideally this method should generalise for any number of rows in the dataframe.</p>
<p><strong>Tried</strong></p>
<p>The following is what I have tried (on a similar example) but for some reason it doesn't seem to work (it just shows nan values in the majority of column entries except for a few which randomly seem to populate).</p>
<pre><code>y = datetime.datetime(2015, 5, 12)
df2['new'] = df1[(df1['date'] == y)].age
</code></pre>
<p><em><strong>Expected Output</strong></em></p>
<p>Since I have filtered above based on sams age (date corresponds to the row with sams name) I would like the new column to be added to df2 with his age as all the entries (in this case 27 repeated 3 times).</p>
<pre><code>df2 = pd.DataFrame({'a': [1,2,3], 'b': [4,5,6], 'new': [27, 27, 27]})
</code></pre>
","['python', 'pandas', 'dataframe']",66867285,"<p>Try:</p>
<pre class=""lang-py prettyprint-override""><code>y = datetime.datetime(2015, 5, 12).strftime('%d-%m-%Y')
df2.loc[:, 'new'] = df1.loc[df1['date'] == y, &quot;age&quot;].item()

# Output
   a  b  new
0  1  4   27
1  2  5   27
2  3  6   27
</code></pre>
",How fill pandas dataframe column using value another dataframe column Firstly import packages might useful import pandas pd import datetime Say I dataframe date name age column df pd DataFrame date name john tim sam age Now say I another dataframe random columns df pd DataFrame b Question How I take age value df filtered date select value populate whole new column df value Ideally method generalise number rows dataframe Tried The following I tried similar example reason seem work shows nan values majority column entries except randomly seem populate datetime datetime df new df df date age Expected Output Since I filtered based sams age date corresponds row sams name I would like new column added df age entries case repeated times df pd DataFrame b new,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
66940333,2021-04-04,2021,2,pandas | get data from another dataframe based on condition,"<p>I have two dataframes:</p>
<p><code>products</code></p>
<pre><code>+------------+--------------------+
| item_name  | item_tags          |
+------------+--------------------+
| blue_shirt | summer,winter,blue |
|            |                    |
+------------+--------------------+
| red_skirt  | spring,summer      |
+------------+--------------------+
</code></pre>
<p>and <code>orders</code></p>
<pre><code>+------------+
| item       |
+------------+
| blue_shirt |
+------------+
| red_skirt  |
+------------+
</code></pre>
<p>and I want to create a new column in <code>orders</code>: when <code>products.item_name</code> == <code>orders.item</code>, I want to take the value of <code>products.item_tags</code> and add it to orders.</p>
<p>I've tried:</p>
<pre><code>orders['ItemTags'] = products.query(&quot;{0}=={1}&quot;.format(orders['item'], products['item_name']))['Tags']
</code></pre>
<p>But it gives me an error.</p>
","['python', 'pandas', 'dataframe']",66940428,"<p>One way we can do this is with creating a dictionary from your products table, with your item_name column as your <code>key</code> and your item_tags column as your <code>value</code>, and then <code>map</code> it onto your orders item column:</p>
<pre><code>products_dict = dict(zip(products.item_name,products.item_tags))
orders['item_tags'] = orders['item'].map(products_dict)
</code></pre>
<p>Output</p>
<pre><code>orders
Out[83]: 
         item           item_tags
0  blue_shirt  summer,winter,blue
1   red_skirt       spring,summer
</code></pre>
",pandas get data another dataframe based condition I two dataframes products item name item tags blue shirt summer winter blue red skirt spring summer orders item blue shirt red skirt I want create new column orders products item name orders item I want take value products item tags add orders I tried orders ItemTags products query quot quot format orders item products item name Tags But gives error,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
67625491,2021-05-20,2021,5,Pandas percentage change using group by,"<p>Suppose I have the following DataFrame:</p>
<pre><code>df = pd.DataFrame({'city': ['a', 'a', 'a', 'b', 'b', 'c', 'd', 'd', 'd'], 
                   'year': [2013, 2014, 2016, 2015, 2016, 2013, 2016, 2017, 2018],
                  'value': [10, 12, 16, 20, 21, 11, 15, 13, 16]})
</code></pre>
<p>And I want to find, for each city and year, what was the percentage change of value compared to the year before. My final dataframe would be:</p>
<pre><code>city  year  value
   a  2013    NaN
   a  2014   0.20
   a  2016    NaN
   b  2015    NaN
   b  2016   0.05
   c  2013    NaN
   d  2016    NaN
   d  2017  -0.14
   d  2018   0.23
</code></pre>
<p>I tried to use a group in city and then use apply but it didn't work:</p>
<pre><code>df.groupby('city').apply(lambda x: x.sort_values('year')['value'].pct_change()).reset_index()
</code></pre>
<p>It didn't work because I couldn't get the year and also because this way I was considereing that I had all years for all cities, but that is not true.</p>
<p>EDIT: I'm not very concerned with efficiency, so any solution that solves the problem is valid for me.</p>
","['python', 'pandas', 'dataframe']",67625581,"<p>Let's try lazy <code>groupby()</code>, use <code>pct_change</code> for the changes and <code>diff</code> to detect year jump:</p>
<pre><code>groups = df.sort_values('year').groupby(['city'])

df['pct_chg'] = (groups['value'].pct_change()
                    .where(groups['year'].diff()==1)
                )
</code></pre>
<p>Output:</p>
<pre><code>  city  year  value   pct_chg
0    a  2013     10       NaN
1    a  2014     12  0.200000
2    a  2016     16       NaN
3    b  2015     20       NaN
4    b  2016     21  0.050000
5    c  2013     11       NaN
6    d  2016     15       NaN
7    d  2017     13 -0.133333
8    d  2018     16  0.230769
</code></pre>
",Pandas percentage change using group Suppose I following DataFrame df pd DataFrame city b b c year value And I want find city year percentage change value compared year My final dataframe would city year value NaN NaN b NaN b c NaN NaN I tried use group city use apply work df groupby city apply lambda x x sort values year value pct change reset index It work I get year also way I considereing I years cities true EDIT I concerned efficiency solution solves problem valid,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
67825124,2021-06-03,2021,2,Add new column with specific increasing of a quarter using python,"<p>I have a dataframe, df, that has a quarters column where I would like to add an additional increased quarters column adjacent to it (increased by 2)</p>
<p>Data</p>
<pre><code>id  date    
a   Q1 2022 
a   Q1 2022 
a   Q1 2022 
a   Q1 2022 
b   Q1 2022 
b   Q1 2022 
</code></pre>
<p>Desired</p>
<pre><code>id  date      new 
a   Q1 2022   Q3 2022
a   Q1 2022   Q3 2022
a   Q1 2022   Q3 2022
a   Q1 2022   Q3 2022
b   Q1 2022   Q3 2022
b   Q1 2022   Q3 2022
</code></pre>
<p>Doing</p>
<pre><code>df['new'] = df.index + 2
</code></pre>
<p>However this is not adding 2 consistently to the entire column
I am still troubleshooting, any suggestion is appreciated</p>
","['python', 'pandas', 'numpy']",67825484,"<p>Reformat the strings in <code>date</code> in such a way that the resulting date format is <code>YearQuarter</code> so that it can be parsed into <code>PeriodIndex</code>, now add <code>2</code> to this index and <code>strftime</code> to convert back to orignal format</p>
<pre><code>s = df['date'].str.replace(r'(\S+) (\S+)', r'\2\1')
df['new'] = (pd.PeriodIndex(s, freq='Q') + 2).strftime('Q%q %Y')
</code></pre>
<hr />
<pre><code>  id     date      new
0  a  Q1 2022  Q3 2022
1  a  Q1 2022  Q3 2022
2  a  Q1 2022  Q3 2022
3  a  Q1 2022  Q3 2022
4  b  Q1 2022  Q3 2022
5  b  Q1 2022  Q3 2022
</code></pre>
",Add new column specific increasing quarter using python I dataframe df quarters column I would like add additional increased quarters column adjacent increased Data id date Q Q Q Q b Q b Q Desired id date new Q Q Q Q Q Q Q Q b Q Q b Q Q Doing df new df index However adding consistently entire column I still troubleshooting suggestion appreciated,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
67828027,2021-06-03,2021,2,Pandas counting/adding values by date and id,"<p>I want to count all orders which got paid until the date of each order.</p>
<p>Input:<br />
<a href=""https://i.stack.imgur.com/zyKya.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zyKya.png"" alt=""enter image description here"" /></a></p>
<p>Expected output:<br />
<a href=""https://i.stack.imgur.com/M1DgK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M1DgK.png"" alt=""enter image description here"" /></a></p>
<p>The following code works but is extremely slowly. Taking upwards of 10 hours for 100k+ rows. There is certainly a better way.</p>
<pre><code>orders_paid,orders_inkasso = []

for y,row in df_dated_filt.iterrows():
    x = x + 1
    orders_paid.append(df_dated_filt[(df_dated_filt[&quot;order_id&quot;] != row[&quot;order_id&quot;]) &amp; (df_dated_filt[&quot;m_order_paid&quot;] == 1) &amp; 
                      (df_dated_filt[&quot;customer_id&quot;] == row[&quot;customer_id&quot;]) &amp; 
                      (pd.to_datetime(df_dated_filt['order_date'])&lt;pd.to_datetime(row['order_date']))][&quot;order_id&quot;].count())
df_dated_filt[&quot;m_orders_paid&quot;] = orders_paid
</code></pre>
","['python', 'pandas', 'numpy']",67828287,"<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html#pandas-dataframe-sort-values"" rel=""nofollow noreferrer""><code>sort_values</code></a> to get dates in ascending order, then <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html#pandas-core-groupby-dataframegroupby-transform"" rel=""nofollow noreferrer""><code>groupby transform</code></a> with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cumsum.html#pandas-series-cumsum"" rel=""nofollow noreferrer""><code>cumsum</code></a> + <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.shift.html#pandas-series-shift"" rel=""nofollow noreferrer""><code>shift</code></a> to get total paid based up before current date:</p>
<pre><code>df['order_date'] = pd.to_datetime(df['order_date'])

df['total_paid'] = (
    df.sort_values('order_date')
        .groupby('customer_id')['order_paid']
        .transform(lambda g: g.cumsum().shift(fill_value=0))
)
</code></pre>
<p><code>df</code>:</p>
<pre><code>   customer_id  order_id order_date  order_paid  total_paid
0            1        12 2019-01-06           0           1
1            1        22 2019-01-01           1           0
2            1        31 2019-01-03           0           1
3            2        34 2018-05-08           0           0
4            2        44 2018-05-12           1           0
5            2        48 2018-05-29           1           1
6            2        55 2018-05-30           1           2
</code></pre>
<p>(Note I think that the first row should be 1 since that customer has one paid order on 2019-01-01, and row one is on 2019-01-06 which is after 2019-01-01. Additionally the provided code also puts a 1 in the first row.)</p>
<hr/>
<p>Complete Working Example:</p>
<pre><code>import pandas as pd

df = pd.DataFrame({
    'customer_id': [1, 1, 1, 2, 2, 2, 2],
    'order_id': [12, 22, 31, 34, 44, 48, 55],
    'order_date': ['2019-01-06', '2019-01-01', '2019-01-03',
                   '2018-05-08', '2018-05-12', '2018-05-29', '2018-05-30'],
    'order_paid': [0, 1, 0, 0, 1, 1, 1]
})
df['order_date'] = pd.to_datetime(df['order_date'])

df['total_paid'] = (
    df.sort_values('order_date')
        .groupby('customer_id')['order_paid']
        .transform(lambda g: g.cumsum().shift(fill_value=0))
)
print(df)
</code></pre>
<hr/>
<p>Assuming more operations depend on dates being in ascending order, it may be beneficial to sort the DataFrame by <code>customer_id</code> and <code>order_date</code>:</p>
<pre><code>df = df.sort_values(['customer_id', 'order_date'])
</code></pre>
<p>Then future operations do not need to sort:</p>
<pre><code>df['total_paid'] = (
    df.groupby('customer_id')['order_paid']
        .transform(lambda g: g.cumsum().shift(fill_value=0))
)
</code></pre>
<p>Then after all ordered date dependent operations are complete use:</p>
<pre><code>df = df.sort_values(['customer_id', 'order_id'])
</code></pre>
<p>to restore the original order of the frame.</p>
",Pandas counting adding values date id I want count orders got paid date order Input Expected output The following code works extremely slowly Taking upwards hours k rows There certainly better way orders paid orders inkasso row df dated filt iterrows x x orders paid append df dated filt df dated filt quot order id quot row quot order id quot amp df dated filt quot order paid quot amp df dated filt quot customer id quot row quot customer id quot amp pd datetime df dated filt order date lt pd datetime row order date quot order id quot count df dated filt quot orders paid quot orders paid,"startoftags, python, pandas, numpy, endoftags",python pandas numpy endoftags,python pandas numpy,python pandas numpy,1.0
67878390,2021-06-07,2021,2,Changing values in a column in a Pandas dataFrame by using another dataFrame,"<p>I have two dataFrames:</p>
<p>The first dataframe df contains the data:</p>
<pre><code>df = pd.DataFrame({'Standort': ['Vereinigte Staaten', 'Australien', 'Belgien'],
                'value': [100, 300, 150]})
</code></pre>
<p>The second dataframe Lookup_Country is a lookup table to link the column 'Standort' to column 'Land' and replace the value of 'Standort' with the value of 'Country'</p>
<pre><code>Lookup_Country = pd.DataFrame({'Land': ['Vereinigte Staaten', 'GroÃbritannien (UK)', 'Belgien'],
                'Country': ['United States', 'United Kingdom', 'Belgium']})
</code></pre>
<p>How can I replace the value of the column 'Standort' by using the dataframe Lookup_Country so that I get a third dataframe</p>
<pre><code>df3= pd.DataFrame({'Standort': ['United States', 'Australien', 'Belgium'],
                'value': [100, 300, 150]})
</code></pre>
","['python', 'pandas', 'dataframe']",67878511,"<p>You can use pd.Series.map</p>
<pre><code>df['Standort'] = df['Standort'].map(Lookup_Country.set_index('Land')['Country']).fillna(df['Standort'])

        Standort  value
0  United States    100
1     Australien    300
2        Belgium    150
</code></pre>
",Changing values column Pandas dataFrame using another dataFrame I two dataFrames The first dataframe df contains data df pd DataFrame Standort Vereinigte Staaten Australien Belgien value The second dataframe Lookup Country lookup table link column Standort column Land replace value Standort value Country Lookup Country pd DataFrame Land Vereinigte Staaten Gro britannien UK Belgien Country United States United Kingdom Belgium How I replace value column Standort using dataframe Lookup Country I get third dataframe df pd DataFrame Standort United States Australien Belgium value,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
68176808,2021-06-29,2021,2,How to add a blinking cursor in pygame?,"<p>so I was trying to add blinking cursors for my text. I don't know why the cursor only appears at y=0, not at each text box. I was trynna add a cursor for each box when active and stop blitting when not active. I suspect it goes wrong because I got my declaration wrong or the blitting part. Can anyone point out where it goes wrong/ how to fix it? Thanks</p>
<pre><code>import pygame
import datetime
import time

pygame.init()
clock = pygame.time.Clock()
pygame.font.init()

# Note
finish = 0
leftover = 0

# Font
numb_font = pygame.font.Font(Arial, 14)
text_font = pygame.font.Font(Arial, 16)

color = (233, 248, 215)
active = False

# screen resolution
Width = 800
Height = 600
#bg = pygame.image.load('opennote.png')
screen = pygame.display.set_mode((Width, Height))

# Time
time_box = pygame.Rect(250, 63, 50, 30)
date_box = pygame.Rect(221, 27, 50, 30)
# boxes numb
leftover_box = pygame.Rect(265, 105, 30, 30)
finish_box = pygame.Rect(325, 105, 30, 30)


class InputBox:

    def __init__(self, x, y, w, h, text=''):
        self.rect = pygame.Rect(x, y, w, h)
        self.color = color
        self.text = text
        self.txt_surface = text_font.render(text, True, self.color)
        self.active = False
        self.score = 1
        # Cursor declare
        self.txt_rect = self.txt_surface.get_rect()
        self.cursor = pygame.Rect(self.txt_rect.topright, (3, self.txt_rect.height + 2))

    def handle_event(self, event):
        if event.type == pygame.MOUSEBUTTONDOWN:
            # If the user clicked on the input_box rect.
            if self.rect.collidepoint(event.pos):
                # Toggle the active variable.
                self.active = not self.active
            else:
                self.active = False
        if event.type == pygame.KEYDOWN:
            if self.active:
                if event.key == pygame.K_RETURN:
                    print(self.text)
                    global leftover
                    leftover += self.score
                    self.score = 0
                    self.text = ''
                    self.active = False
                elif event.key == pygame.K_BACKSPACE:
                    self.text = self.text[:-1]
                else:
                    self.text += event.unicode
                    # Cursor

                    self.txt_rect.size = self.txt_surface.get_size()
                    self.cursor.topleft = self.txt_rect.topright

                    # Limit characters           -20 for border width
                    if self.txt_surface.get_width() &gt; self.rect.w - 15:
                        self.text = self.text[:-1]

    def draw(self, screen):
        # Blit the text.
        screen.blit(self.txt_surface, (self.rect.x + 5, self.rect.y + 10))
        # Blit the rect.
        pygame.draw.rect(screen, self.color, self.rect, 1)
        # Blit the  cursor
        if time.time() % 1 &gt; 0.5:
            pygame.draw.rect(screen, self.color, self.cursor)

    def update(self):
        # Re-render the text.
        self.txt_surface = text_font.render(self.text, True, self.color)


def main():
    clock = pygame.time.Clock()
    input_box1 = InputBox(115, 170, 250, 36)
    input_box2 = InputBox(115, 224, 250, 36)
    input_box3 = InputBox(115, 278, 250, 36)
    input_box4 = InputBox(115, 333, 250, 36)
    input_box5 = InputBox(115, 386, 250, 36)
    input_box6 = InputBox(115, 440, 250, 36)
    input_box7 = InputBox(115, 494, 250, 36)
    input_box8 = InputBox(440, 170, 250, 36)
    input_box9 = InputBox(440, 224, 250, 36)
    input_box10 = InputBox(440, 278, 250, 36)
    input_box11 = InputBox(440, 333, 250, 36)
    input_box12 = InputBox(440, 386, 250, 36)
    input_box13 = InputBox(440, 440, 250, 36)
    input_box14 = InputBox(440, 494, 250, 36)
    input_box15 = InputBox(440, 115, 250, 36)
    input_box16 = InputBox(440, 61, 250, 36)
    input_boxes = [input_box1, input_box2, input_box3, input_box4, input_box5, input_box6, input_box7, input_box8,
                   input_box9, input_box10, input_box11, input_box12, input_box13, input_box14, input_box15, input_box16]
    done = False

    while not done:
        # Background
        screen.fill((0, 0, 0))
        #screen.blit(bg, (0, 0))
        now = datetime.datetime.now()
        date_now = now.strftime(&quot;%d/%m/%Y&quot;)
        time_now = now.strftime(&quot;%H:%M:%S&quot;)
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                done = True
            for box in input_boxes:
                box.handle_event(event)

        for box in input_boxes:
            box.update()

        for box in input_boxes:
            box.draw(screen)
            # Real Time
            # Date
        pygame.draw.rect(screen, 'white', date_box, -1)
        datebox_surface = numb_font.render(date_now, True, color)
        screen.blit(datebox_surface, (date_box.x + 5, date_box.y + 5))
        # Time
        pygame.draw.rect(screen, 'white', time_box, -1)
        timebox_surface = numb_font.render(time_now, True, color)
        screen.blit(timebox_surface, (time_box.x + 5, time_box.y + 5))

        # finish &amp;Leftover
        # finish box
        pygame.draw.rect(screen, 'white', finish_box, -1)
        finishbox_surface = numb_font.render(str(finish), True, color)
        screen.blit(finishbox_surface, finish_box)
        # Leftover box
        pygame.draw.rect(screen, 'white', leftover_box, -1)
        leftover_box_surface = numb_font.render(str(leftover), True, color)
        screen.blit(leftover_box_surface, leftover_box)

        pygame.display.update()
        clock.tick(120)


if __name__ == '__main__':
    main()
    pygame.quit()
</code></pre>
","['python', 'python-3.x', 'pygame']",68181209,"<p>All you need to do is to set the position of the cursor. Get the right center  position of the bounding rectangle of the text and set the left center position of the cursor:</p>
<pre class=""lang-py prettyprint-override""><code>class InputBox:
    # [...]

    def draw(self, screen):
        # Blit the text.
        screen.blit(self.txt_surface, (self.rect.x + 5, self.rect.y + 10))
        # Blit the rect.
        pygame.draw.rect(screen, self.color, self.rect, 1)
        # Blit the  cursor
        if time.time() % 1 &gt; 0.5:

            # bounding rectangle of the text
            text_rect = self.txt_surface.get_rect(topleft = (self.rect.x + 5, self.rect.y + 10))

            # set cursor position
            self.cursor.midleft = text_rect.midright

            pygame.draw.rect(screen, self.color, self.cursor)
</code></pre>
<p><a href=""https://i.stack.imgur.com/9OEc6.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9OEc6.gif"" alt="""" /></a></p>
",How add blinking cursor pygame I trying add blinking cursors text I know cursor appears text box I trynna add cursor box active stop blitting active I suspect goes wrong I got declaration wrong blitting part Can anyone point goes wrong fix Thanks import pygame import datetime import time pygame init clock pygame time Clock pygame font init Note finish leftover Font numb font pygame font Font Arial text font pygame font Font Arial color active False screen resolution Width Height bg pygame image load opennote png screen pygame display set mode Width Height Time time box pygame Rect date box pygame Rect boxes numb leftover box pygame Rect finish box pygame Rect class InputBox def init self x w h text self rect pygame Rect x w h self color color self text text self txt surface text font render text True self color self active False self score,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
48145924,2018-01-08,2018,25,Different colors for points and line in Seaborn regplot,"<p>All examples listed in <a href=""https://seaborn.pydata.org/generated/seaborn.regplot.html"" rel=""noreferrer"">Seaborn's <code>regplot</code> documentation</a> show the same color for dots and the regression line. Changing the <code>color</code> argument changes both. How can one set a different color for the points as the line?</p>
","['python', 'matplotlib', 'seaborn']",48146987,"<p>You are right in that the <code>color</code> argument changes all the plot elements. However, if you read the last bit of the relevant sentence in the <a href=""https://seaborn.pydata.org/generated/seaborn.regplot.html"" rel=""noreferrer"">documentation</a>:</p>

<blockquote>
  <p>color : matplotlib color</p>
  
  <p>Color to apply to all plot elements; will be superseded by colors
  passed in <code>scatter_kws</code> or <code>line_kws</code>.</p>
</blockquote>

<p>Therefore, using <code>scatter_kws</code> or <code>line_kws</code> we can change the color of them individually. Taking the first example given in the documentation:</p>

<pre><code>import seaborn as sns

tips = sns.load_dataset(""tips"")
ax = sns.regplot(x=""total_bill"", y=""tip"", data=tips,
                 scatter_kws={""color"": ""black""}, line_kws={""color"": ""red""})

plt.show()
</code></pre>

<p>Gives:</p>

<p><a href=""https://i.stack.imgur.com/xXNL3.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xXNL3.png"" alt=""enter image description here""></a></p>
",Different colors points line Seaborn regplot All examples listed Seaborn regplot documentation show color dots regression line Changing color argument changes How one set different color points line,"startoftags, python, matplotlib, seaborn, endoftags",python matplotlib seaborn endoftags,python matplotlib seaborn,python matplotlib seaborn,1.0
48170490,2018-01-09,2018,2,BeautifulSoup Issue: Get exact link url and Title,"<p>This is my code. This code returns all h2 title not link</p>

<pre><code>from bs4 import BeautifulSoup
import requests
url=requests.get(""http://www.prothom-alo.com/"")
data=url.text
soup=BeautifulSoup(data ,""lxml"")
for link in soup.find_all(""h2""):
    print(link)
</code></pre>

<p>I want to get link and title like as <code>link+title</code>. I was trying many methods but I cant it properly. How can I get the exact link to the title?</p>
","['python', 'web-scraping', 'beautifulsoup']",48172706,"<p>I'm guessing you want all the titles with their corresponding links(href).  </p>

<pre><code>import requests
from bs4 import BeautifulSoup

r = requests.get('http://www.prothomalo.com/')
soup = BeautifulSoup(r.text, 'html.parser')
titles = {}
for div in soup.find_all('div', {'class': 'col col1'}):
    title = div.find('span', {'class': 'title'}).text
    link = div.find('a', {'class': 'link_overlay'}).get('href')
    titles[title] = link
</code></pre>

<p>Now, we've got a dictionary with all the titles as <code>keys</code> and their corresponding links(href) as <code>values</code>.  </p>

<p>To check what we've got</p>

<pre><code>for t in titles.items():
    print(t)
</code></pre>

<p>Output:</p>

<pre><code>('à¦¢à¦¾à¦à¦¾ à¦à¦¤à§à¦¤à¦° à¦¸à¦¿à¦à¦¿ à¦à¦ªà¦¨à¦¿à¦°à§à¦¬à¦¾à¦à¦¨à§ à¦¸à§à¦¨à¦¾ à¦à¦¾à¦¨ à¦«à¦à¦°à§à¦²', 'bangladesh/article/1405261/%E0%A6%A2%E0%A6%BE%E0%A6%95%E0%A6%BE-%E0%A6%89%E0%A6%A4%E0%A7%8D%E0%A6%A4%E0%A6%B0-%E0%A6%B8%E0%A6%BF%E0%A6%9F%E0%A6%BF-%E0%A6%89%E0%A6%AA%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A7%8D%E0%A6%AC%E0%A6%BE%E0%A6%9A%E0%A6%A8%E0%A7%87-%E0%A6%B8%E0%A7%87%E0%A6%A8%E0%A6%BE-%E0%A6%9A%E0%A6%BE%E0%A6%A8-%E0%A6%AB%E0%A6%96%E0%A6%B0%E0%A7%81%E0%A6%B2')
('à¦¸à§à¦¨à¦¾à¦° à¦­à¦°à¦¿ à§«à§¦ à¦¹à¦¾à¦à¦¾à¦° à¦à¦¾à¦à¦¾ à¦à¦¾à§à¦¾à¦²', 'economy/article/1405121/%E0%A6%B8%E0%A7%8B%E0%A6%A8%E0%A6%BE%E0%A6%B0-%E0%A6%AD%E0%A6%B0%E0%A6%BF-%E0%A7%AB%E0%A7%A6-%E0%A6%B9%E0%A6%BE%E0%A6%9C%E0%A6%BE%E0%A6%B0-%E0%A6%9F%E0%A6%BE%E0%A6%95%E0%A6%BE-%E0%A6%9B%E0%A6%BE%E0%A7%9C%E0%A6%BE%E0%A6%B2')
('à¦à§à¦·à¦¸à¦à¦à§à¦°à¦¾à¦¨à§à¦¤ à¦¬à¦à§à¦¤à¦¬à§à¦¯à§à¦° à¦à¦¨à§à¦¯ à¦¶à¦¿à¦à§à¦·à¦¾à¦®à¦¨à§à¦¤à§à¦°à§à¦° à¦ªà¦¦à¦¤à§à¦¯à¦¾à¦ à¦¦à¦¾à¦¬à¦¿', 'bangladesh/article/1405246/%E0%A6%B6%E0%A6%BF%E0%A6%95%E0%A7%8D%E0%A6%B7%E0%A6%BE%E0%A6%AE%E0%A6%A8%E0%A7%8D%E0%A6%A4%E0%A7%8D%E0%A6%B0%E0%A7%80%E0%A6%B0-%E0%A6%AA%E0%A6%A6%E0%A6%A4%E0%A7%8D%E0%A6%AF%E0%A6%BE%E0%A6%97-%E0%A6%A6%E0%A6%BE%E0%A6%AC%E0%A6%BF')
('à¦®à§à¦®à¦¿à¦¨à§à¦²à¦à§ à¦¡à¦¾à¦¬à¦² à¦¸à§à¦à§à¦à§à¦°à¦¿à¦° à¦¹à¦¾à¦¤à¦à¦¾à¦¨à¦¿', 'sports/article/1405116/%E0%A6%AE%E0%A7%81%E0%A6%AE%E0%A6%BF%E0%A6%A8%E0%A7%81%E0%A6%B2%E0%A6%95%E0%A7%87-%E0%A6%A1%E0%A6%BE%E0%A6%AC%E0%A6%B2-%E0%A6%B8%E0%A7%87%E0%A6%9E%E0%A7%8D%E0%A6%9A%E0%A7%81%E0%A6%B0%E0%A6%BF%E0%A6%B0-%E0%A6%B9%E0%A6%BE%E0%A6%A4%E0%A6%9B%E0%A6%BE%E0%A6%A8%E0%A6%BF')
('à¦¸à¦¾à¦à¦¬à¦¾à¦¦à¦¿à¦à¦¦à§à¦° à¦ªà§à¦°à¦¶à§à¦¨ à¦à§à¦¾à¦¤à§...', 'international/article/1405181/%E0%A6%B8%E0%A6%BE%E0%A6%82%E0%A6%AC%E0%A6%BE%E0%A6%A6%E0%A6%BF%E0%A6%95%E0%A6%A6%E0%A7%87%E0%A6%B0-%E0%A6%AA%E0%A7%8D%E0%A6%B0%E0%A6%B6%E0%A7%8D%E0%A6%A8-%E0%A6%8F%E0%A7%9C%E0%A6%BE%E0%A6%A4%E0%A7%87')
('à¦­à¦¾à¦°à¦¤à§ à¦¸à¦¿à¦¨à§à¦®à¦¾ à¦¹à¦²à§ à¦à¦¾à¦¤à§à§ à¦¸à¦à¦à§à¦¤ à¦¬à¦¾à¦§à§à¦¯à¦¤à¦¾à¦®à§à¦²à¦ à¦¨à§', 'international/article/1405071/%E0%A6%AD%E0%A6%BE%E0%A6%B0%E0%A6%A4%E0%A7%87-%E0%A6%B8%E0%A6%BF%E0%A6%A8%E0%A7%87%E0%A6%AE%E0%A6%BE-%E0%A6%B9%E0%A6%B2%E0%A7%87-%E0%A6%9C%E0%A6%BE%E0%A6%A4%E0%A7%80%E0%A7%9F-%E0%A6%B8%E0%A6%82%E0%A6%97%E0%A7%80%E0%A6%A4-%E0%A6%AC%E0%A6%BE%E0%A6%9C%E0%A6%BE%E0%A6%A8%E0%A7%8B-%E0%A6%AC%E0%A6%BE%E0%A6%A7%E0%A7%8D%E0%A6%AF%E0%A6%A4%E0%A6%BE%E0%A6%AE%E0%A7%82%E0%A6%B2%E0%A6%95-%E0%A6%A8%E0%A7%9F')
('à¦®à§à¦§à¦¾à¦¬à§à¦¦à§à¦° à¦à¦¤ à¦¦à¦¿à¦¨ à¦¦à§à¦°à§ à¦°à¦¾à¦à¦¬à§ à¦¦à§à¦¶?', 'durporobash/article/1405241/%E0%A6%AE%E0%A7%87%E0%A6%A7%E0%A6%BE%E0%A6%AC%E0%A7%80%E0%A6%A6%E0%A7%87%E0%A6%B0-%E0%A6%95%E0%A6%A4-%E0%A6%A6%E0%A6%BF%E0%A6%A8-%E0%A6%A6%E0%A7%82%E0%A6%B0%E0%A7%87-%E0%A6%B0%E0%A6%BE%E0%A6%96%E0%A6%AC%E0%A7%87-%E0%A6%A6%E0%A7%87%E0%A6%B6')
('à¦ªà§à¦°à¦¿à§ à¦¶à§à¦¶à¦¬, à¦¯à§à¦à¦¾à¦¨à§à¦ à¦à¦ à¦­à¦¾à¦²à§ à¦¥à§à¦à§!', 'durporobash/article/1405206/%E0%A6%AA%E0%A7%8D%E0%A6%B0%E0%A6%BF%E0%A7%9F-%E0%A6%B6%E0%A7%88%E0%A6%B6%E0%A6%AC-%E0%A6%AF%E0%A7%87%E0%A6%96%E0%A6%BE%E0%A6%A8%E0%A7%87%E0%A6%87-%E0%A6%86%E0%A6%9B-%E0%A6%AD%E0%A6%BE%E0%A6%B2%E0%A7%8B-%E0%A6%A5%E0%A7%87%E0%A6%95%E0%A7%8B')
('à¦ªà§à¦°à§à¦¸à¦¿à¦¡à§à¦¨à§à¦ à¦¨à¦¿à¦°à§à¦¬à¦¾à¦à¦¨à§ à¦à§à¦°à¦¾à¦®à§à¦ªà§à¦° à¦ªà§à¦°à¦¤à¦¿à¦¦à§à¦¬à¦¨à§à¦¦à§à¦¬à§ à¦à¦ªà¦°à¦¾à¦¹à§\u200c!', 'northamerica/article/1405231/%E0%A6%AA%E0%A7%8D%E0%A6%B0%E0%A7%87%E0%A6%B8%E0%A6%BF%E0%A6%A1%E0%A7%87%E0%A6%A8%E0%A7%8D%E0%A6%9F-%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A7%8D%E0%A6%AC%E0%A6%BE%E0%A6%9A%E0%A6%A8%E0%A7%87-%E0%A6%9F%E0%A7%8D%E0%A6%B0%E0%A6%BE%E0%A6%AE%E0%A7%8D%E0%A6%AA%E0%A7%87%E0%A6%B0-%E0%A6%AA%E0%A7%8D%E0%A6%B0%E0%A6%A4%E0%A6%BF%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A6%A8%E0%A7%8D%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%80')
('à¦à¦à§à¦ à¦¬à¦²à§ à¦­à¦¾à¦à§à¦¯!', 'northamerica/article/1405091/%E0%A6%8F%E0%A6%95%E0%A7%87%E0%A6%87-%E0%A6%AC%E0%A6%B2%E0%A7%87-%E0%A6%AD%E0%A6%BE%E0%A6%97%E0%A7%8D%E0%A6%AF')
('à¦¨à¦à¦°à§ à¦¶à§à¦¤à§à¦° à¦¹à¦¾à¦à§à¦¾', 'bangladesh/article/1405076/%E0%A6%A8%E0%A6%97%E0%A6%B0%E0%A7%87-%E0%A6%B6%E0%A7%80%E0%A6%A4%E0%A7%87%E0%A6%B0-%E0%A6%B9%E0%A6%BE%E0%A6%93%E0%A7%9F%E0%A6%BE')
</code></pre>

<p>I hope this is what you're looking for. I can't verify since I don't understand the language.</p>
",BeautifulSoup Issue Get exact link url Title This code This code returns h title link bs import BeautifulSoup import requests url requests get http www prothom alo com data url text soup BeautifulSoup data lxml link soup find h print link I want get link title like link title I trying many methods I cant properly How I get exact link title,"startoftags, python, webscraping, beautifulsoup, endoftags",python webscraping beautifulsoup endoftags,python webscraping beautifulsoup,python webscraping beautifulsoup,1.0
48409705,2018-01-23,2018,3,_tkinter.TclError: couldn&#39;t open &quot;sample.gif&quot;: no such file or directory,"<p>There's a really weird problem with tkinter.</p>

<p>I can usually show the images on tkinter.
But, I can not show any images on it if I try to load music files.</p>

<p>For example,</p>

<pre><code>import tkinter as Tk

class Frame(Tk.Frame):

def __init__(self, master=None):
    Tk.Frame.__init__(self, master)


    self.f = Tk.Frame(self)
    self.f.pack()

    self.ARTWORK = ['guthrie.gif']

    self.f_artwork = Tk.Frame(self.f)
    self.f_artwork.pack()
    self.artwork_img = Tk.PhotoImage(file=self.ARTWORK[0])

    self.artwork_la = Tk.Label(self.f_artwork, image=self.artwork_img)
    self.artwork_la.pack()

if __name__ == '__main__':

f = Frame()
f.pack()
f.mainloop()
</code></pre>

<p>I can show the image in this case, but</p>

<pre><code>import tkinter as Tk
import os
from tkinter.filedialog import askdirectory
import pygame

song_list = []

directory = askdirectory()
os.chdir(directory)

for file in os.listdir(directory):
    if file.endswith('.mp3'):
        realdir = os.path.realpath(file)
        song_list.append(file)

    pygame.mixer.init()
    pygame.mixer.music.load(song_list[0])
    pygame.mixer.music.play()




pygame.mixer.init()
pygame.mixer.music.load(song_list[0])
pygame.mixer.music.play()

class Frame(Tk.Frame):

def __init__(self, master=None):
    Tk.Frame.__init__(self, master)


    self.f = Tk.Frame(self)
    self.f.pack()

    self.ARTWORK = ['guthrie.gif']

    self.f_artwork = Tk.Frame(self.f)
    self.f_artwork.pack()
    self.artwork_img = Tk.PhotoImage(file=self.ARTWORK[0])

    self.artwork_la = Tk.Label(self.f_artwork, image=self.artwork_img)
    self.artwork_la.pack()

if __name__ == '__maim__':

f = Frame()
f.pack()
f.mainloop()
</code></pre>

<p>For this program, I cannot show the image on Tkinter.
Tkinter cannot recongnize the file.
Error message is 
_tkinter.TclError: couldn't open ""guthrie.gif"": no such file or directory</p>

<p>Can you tell me what the cause is?</p>
","['python', 'python-3.x', 'tkinter']",48409879,"<p>After running <code>os.chdir(directory)</code>, you changed the directory from where the script was ran. If you want to keep the old functionality you can capture and save the script directroy with <code>os.getcwd()</code>. Try something like this.</p>

<pre><code>import tkinter as Tk
import os
from tkinter.filedialog import askdirectory
import pygame

song_list = []

scriptDir = os.getcwd()  # directory from where script was ran
directory = askdirectory()
os.chdir(directory)

for file in os.listdir(directory):
    if file.endswith('.mp3'):
        realdir = os.path.realpath(file)
        song_list.append(file)

...

class Frame(Tk.Frame):

    def __init__(self, master=None):
        Tk.Frame.__init__(self, master)


        self.f = Tk.Frame(self)
        self.f.pack()
        os.chdir(scriptDir)  # change to the starting directory
        self.ARTWORK = ['guthrie.gif']

        ...
</code></pre>
",tkinter TclError open quot sample gif quot file directory There really weird problem tkinter I usually show images tkinter But I show images I try load music files For example import tkinter Tk class Frame Tk Frame def init self master None Tk Frame init self master self f Tk Frame self self f pack self ARTWORK guthrie gif self f artwork Tk Frame self f self f artwork pack self artwork img Tk PhotoImage file self ARTWORK self artwork la Tk Label self f artwork image self artwork img self artwork la pack name main f Frame f pack f mainloop I show image case import tkinter Tk import os tkinter filedialog import askdirectory import pygame song list directory askdirectory os chdir directory file os listdir directory file endswith mp realdir os path realpath file song list append file pygame mixer init pygame mixer music load song list pygame,"startoftags, python, python3x, tkinter, endoftags",python python3x list endoftags,python python3x tkinter,python python3x list,0.67
48487381,2018-01-28,2018,3,Update model fields based on POST data before save with Django Rest Framework,"<p>I'm using django-rest-framework and want to augment the posted data before saving it to my model as is normally achieved using the model's clean method as in this example from the django docs:</p>

<pre><code>class Article(models.Model):
...
def clean(self):
    # Don't allow draft entries to have a pub_date.
    if self.status == 'draft' and self.pub_date is not None:
        raise ValidationError(_('Draft entries may not have a publication date.'))
    # Set the pub_date for published items if it hasn't been set already.
    if self.status == 'published' and self.pub_date is None:
        self.pub_date = datetime.date.today()
</code></pre>

<p>Unfortunately a django-rest-framework Serializer does not call a model's clean method as with a standard django Form so how would I achieve this?</p>
","['python', 'django', 'django-rest-framework']",48487419,"<p>From official <a href=""http://www.django-rest-framework.org/topics/3.0-announcement/"" rel=""nofollow noreferrer"">docs</a>:</p>

<blockquote>
  <p>The one difference that you do need to note is that the .clean() method will not be called as part of serializer validation, as it would be if using a ModelForm. Use the serializer .validate() method to perform a final validation step on incoming data where required.</p>
  
  <p>There may be some cases where you really do need to keep validation logic in the model .clean() method, and cannot instead separate it into the serializer .validate(). You can do so by explicitly instantiating a model instance in the .validate() method.</p>
</blockquote>

<pre><code>def validate(self, attrs):
    instance = ExampleModel(**attrs)
    instance.clean()
    return attrs
</code></pre>
",Update model fields based POST data save Django Rest Framework I using django rest framework want augment posted data saving model normally achieved using model clean method example django docs class Article models Model def clean self Don allow draft entries pub date self status draft self pub date None raise ValidationError Draft entries may publication date Set pub date published items set already self status published self pub date None self pub date datetime date today Unfortunately django rest framework Serializer call model clean method standard django Form would I achieve,"startoftags, python, django, djangorestframework, endoftags",python django djangorestframework endoftags,python django djangorestframework,python django djangorestframework,1.0
48697381,2018-02-09,2018,2,Appending to numpy arrays,"<p>I'm trying to construct a numpy array, and then append integers and another array to it.
I tried doing this:</p>

<pre><code>xyz_list = frag_str.split()
nums = numpy.array([])
coords = numpy.array([])
for i in range(int(len(xyz_list)/4)):
    numpy.append(nums, xyz_list[i*4])
    numpy.append(coords, xyz_list[i*4+1:(i+1)*4])
print(atoms)
print(coords)
</code></pre>

<p>Printing out the output only gives my empty arrays. Why is that?
In addition, how can I rewrite <code>coords</code> in a way that allows me to have 2D arrays like this: <code>array[[0,0,0],[0,0,1],[0,0,-1]]</code>?</p>
","['python', 'arrays', 'numpy']",48697430,"<p><code>numpy.append</code>, unlike python's <code>list.append</code>, does not perform operations in place. Therefore, you need to assign the result back to a variable, as below.</p>

<pre><code>import numpy

xyz_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
nums = numpy.array([])
coords = numpy.array([])

for i in range(int(len(xyz_list)/4)):
    nums = numpy.append(nums, xyz_list[i*4])
    coords = numpy.append(coords, xyz_list[i*4+1:(i+1)*4])

print(nums)    # [ 1.  5.  9.]
print(coords)  # [  2.   3.   4.   6.   7.   8.  10.  11.  12.]
</code></pre>

<p>You can reshape <code>coords</code> as follows:</p>

<pre><code>coords = coords.reshape(3, 3)

# array([[  2.,   3.,   4.],
#        [  6.,   7.,   8.],
#        [ 10.,  11.,  12.]])
</code></pre>

<p><strong>More details on <code>numpy.append</code> behaviour</strong></p>

<p><a href=""https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.append.html"" rel=""nofollow noreferrer"">Documentation</a>:</p>

<blockquote>
  <p>Returns: A copy of arr with values appended to axis. Note that
  append does not occur in-place: a new array is allocated and filled.</p>
</blockquote>

<p>If you know the shape of your <code>numpy</code> array output beforehand, it is efficient to instantiate via <code>np.zeros(n)</code> and fill it with results later.</p>

<p>Another option: if your calculations make heavy use of inserting elements <em>to the left</em> of an array, consider using <a href=""https://docs.python.org/3/library/collections.html#collections.deque"" rel=""nofollow noreferrer""><code>collections.deque</code></a> from the standard library.</p>
",Appending numpy arrays I trying construct numpy array append integers another array I tried xyz list frag str split nums numpy array coords numpy array range int len xyz list numpy append nums xyz list numpy append coords xyz list print atoms print coords Printing output gives empty arrays Why In addition I rewrite coords way allows D arrays like array,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
48798466,2018-02-15,2018,2,Pandas: How to flag a column with groupy &amp; apply(),"<p>Right now, my code will return the rows that meet the condition in my function. How can I instead return all original rows and flag a new column ('GreaterDate) if the condition is true? </p>

<pre><code>df = pd.DataFrame({'KEY': ['100000003', '100000009', '100000009', '100000009', '100000009','100000034','100000034', '100000034'], 
          'Date1': [20120506, 20120506, 20120507,20120608,20120620,20120206,20120306,20120506],
          'Date2': [20120528, 20120507, 20120615,20120629,20120206,20120305,20120506,20120506]})

def date_compare(df):
    date_before  = df['Date1'].shift(-1)
    value = df[df['Date2'] == date_before]
    return value

dftest = df.groupby('KEY').apply(date_compare)
dftest
</code></pre>

<p>This returns two true values. </p>

<pre><code>                Date1      Date2        KEY
KEY             
100000009   1   20120506    20120507    100000009
100000034   6   20120306    20120506    100000034
</code></pre>

<p>I've tried using </p>

<pre><code>dftest['GreaterDate'] = df.groupby('KEY').apply(date_compare)
</code></pre>

<p>but that doesn't work. 
Im not so interested in the date function working, I merely used this function as an example. I am more interested in how to use apply on a groupby in order to add a new column with True or False.</p>
","['python', 'pandas', 'pandas-groupby']",48798494,"<p>Here is one way:</p>

<pre><code>def date_compare(df):
    df['dftest'] = df['Date2'] == df['Date1'].shift(-1)
    return df

dftest = pd.concat([df[df.KEY == k].pipe(date_compare) \
                   for k in set(df.KEY)], ignore_index=True)

#       Date1     Date2        KEY  dftest
# 0  20120506  20120507  100000009    True
# 1  20120507  20120615  100000009   False
# 2  20120608  20120629  100000009   False
# 3  20120620  20120206  100000009   False
# 4  20120206  20120305  100000034   False
# 5  20120306  20120506  100000034    True
# 6  20120506  20120506  100000034   False
# 7  20120506  20120528  100000003   False
</code></pre>
",Pandas How flag column groupy amp apply Right code return rows meet condition function How I instead return original rows flag new column GreaterDate condition true df pd DataFrame KEY Date Date def date compare df date df Date shift value df df Date date return value dftest df groupby KEY apply date compare dftest This returns two true values Date Date KEY KEY I tried using dftest GreaterDate df groupby KEY apply date compare work Im interested date function working I merely used function example I interested use apply groupby order add new column True False,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
48891675,2018-02-20,2018,3,skip a value in one dimension of numpy array,"<p>I have a numpy array whose shape is (72, 671). Typically I select everything across the first dimension like this:</p>

<pre><code>new_var = old_var[0:72]
</code></pre>

<p>However, for one file, I need to skip #18 across the first dimension. In other words, I want to select 0:17 and then 19:72 (or however you would write that correctly based on what is/isn't included). I have tried:</p>

<pre><code>new_var=old_var[0:18,19:72]
</code></pre>

<p>but this only selects 0:18 in the first dimension and then 19:72 in the second. at least this is what I think it's doing, since the length of the resulting variable is 18. I can't find how to correct the syntax, so any help would be appreciated.</p>
","['python', 'arrays', 'numpy']",48891728,"<p>I think you can use <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.r_.html"" rel=""nofollow noreferrer""><code>np.r_</code></a></p>

<pre><code>old_var = np.random.random((72,671))
new_var = old_var[np.r_[0:18,19:72]]
new_var.shape
</code></pre>

<p>Output:</p>

<pre><code>(71, 671)
</code></pre>
",skip value one dimension numpy array I numpy array whose shape Typically I select everything across first dimension like new var old var However one file I need skip across first dimension In words I want select however would write correctly based included I tried new var old var selects first dimension second least I think since length resulting variable I find correct syntax help would appreciated,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
48998843,2018-02-26,2018,2,"Appending to a new column the differences of current row and previous row, for multiple columns","<p>For each of the columns in my df, I want to subtract the current row from the previous row (row[n+1]-row[n]), but I am having difficulty.</p>

<p>My code is as follows:</p>

<pre><code>#!/usr/bin/python3
from pandas_datareader import data
import pandas as pd
import fix_yahoo_finance as yf
yf.pdr_override()
import os


stock_list = [""BHP.AX"", ""CBA.AX"", ""RHC.AX"", ""TLS.AX"", ""WOW.AX"", ""^AORD""]

# Function to get the closing price of the individual stocks
# from the stock_list list
def get_closing_price(stock_name, specific_close):
    symbol = stock_name
    start_date = '2010-01-01'
    end_date = '2016-06-01'
    df = data.get_data_yahoo(symbol, start_date, end_date)
    sym = symbol + "" ""
    print(sym * 10)
    df = df.drop(['Open', 'High', 'Low', 'Adj Close', 'Volume'], axis=1)
    df = df.rename(columns={'Close': specific_close})
    # https://stackoverflow.com/questions/16729483/converting-strings-to-floats-in-a-dataframe
    # df[specific_close] = df[specific_close].astype('float64')
    print(type(df[specific_close]))
    return df

# Creates a big DataFrame with all the stock's Closing
# Price returns the DataFrame
def get_all_close_prices(directory):
    count = 0
    for stock_name in stock_list:
        specific_close = stock_name.replace("".AX"", """") + ""_Close""
        if not count:
            prev_df = get_closing_price(stock_name, specific_close)
        else:
            new_df = get_closing_price(stock_name, specific_close)
            # https://stackoverflow.com/questions/11637384/pandas-join-merge-concat-two-dataframes
            prev_df = prev_df.join(new_df)
        count += 1
    prev_df.to_csv(directory)
    return prev_df

# THIS IS THE FUNCTION I NEED HELP WITH
# AS DESCRIBED IN THE QUESTION
def calculate_return(df):
    count = 0
    # for index, row in df.iterrows():
    print(df.columns[0])
    for stock in stock_list:
        specific_close = stock.replace("".AX"", """") + ""_Close""
        print(specific_close)
        # https://stackoverflow.com/questions/15891038/change-data-type-of-columns-in-pandas
        pd.to_numeric(specific_close, errors='ignore')
        df.columns[count].diff()
        count += 1
     return df


def main():
    # FINDS THE CURRENT DIRECTORY AND CREATES THE CSV TO DUMP THE DF
    csv_in_current_directory = os.getcwd() + ""/stk_output.csv""

    # FUNCTION THAT GETS ALL THE CLOSING PRICES OF THE STOCKS
    # AND RETURNS IT AS ONE COMPLETE DATAFRAME
    df = get_all_close_prices(csv_in_current_directory)

    # THIS PRINTS OUT WHAT IS IN ""OUTPUT 1""
    print(df)

    # THIS FUNCTION IS WHERE I HAVE THE PROBLEM
    df = calculate_return(df)

    # THIS SHOULD PRINT OUT WHAT IS IN ""EXPECTED OUTPUT""
    print(df)




# Main line of code
if __name__ == ""__main__"":
    main()
</code></pre>

<p><strong>Question:</strong></p>

<p>For each of the columns, I would like subtract current row from the previous row (row[n+1]-row[n]) and assign this value to a new column at the end of the dataframe as a new column as <code>stock_name + ""_Earning""</code>. My expected output (see: <em>Expected Output</em>) is that I still have the original <code>df</code> as seen in <strong>Output 1</strong>, but has 6 additional columns, with an <em>empty</em> first row, and the differences of the rows (row[n+1]-row[n]) therein in the respective column.</p>

<p><strong>Problem Faced:</strong></p>

<p>With the current code - I am getting the following error, which I have tried to get rid of</p>

<blockquote>
  <p>AttributeError: 'str' object has no attribute 'diff'</p>
</blockquote>

<p><strong>Things I Have Tried:</strong></p>

<p>Some of the things I have tried:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/15891038/change-data-type-of-columns-in-pandas"">Change data type of columns in Pandas</a></li>
<li><a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.diff.html"" rel=""nofollow noreferrer"">numpy.diff</a></li>
<li><a href=""https://stackoverflow.com/questions/38134643/data-frame-object-has-no-attribute"">Data-frame Object has no Attribute</a></li>
<li><a href=""https://stackoverflow.com/questions/39479919/how-do-i-subtract-the-previous-row-from-the-current-row-in-a-pandas-dataframe-an"">How do I subtract the previous row from the current row in a pandas dataframe and apply it to every row; without using a loop?</a></li>
<li><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.diff.html"" rel=""nofollow noreferrer"">pandas.DataFrame.diff</a></li>
</ul>

<p><strong>Expected Output:</strong></p>

<pre><code>            BHP_Close  CBA_Close  RHC_Close  TLS_Close  WOW_Close        ^AORD  BHP_Earning  CBA_Earning  RHC_Earning  TLS_Earning  WOW_Earning  ^AORD_Earning
Date
2010-01-03  40.255699  54.574299  11.240000       3.45  27.847300  4889.799805
2010-01-04  40.442600  55.399799  11.030000       3.44  27.679100  4939.500000     0.186901       0.8255        -0.21        -0.01      -0.1682   49.70020000  
</code></pre>

<p><strong>Output 1:</strong></p>

<pre><code>            BHP_Close  CBA_Close  RHC_Close  TLS_Close  WOW_Close  ^AORD_Close
Date
2010-01-03  40.255699  54.574299  11.240000       3.45  27.847300  4889.799805
2010-01-04  40.442600  55.399799  11.030000       3.44  27.679100  4939.500000
2010-01-05  40.947201  55.678299  11.180000       3.38  27.629601  4946.799805
...               ...        ...        ...        ...        ...          ...
2016-05-30  19.240000  78.180000  72.730003       5.67  22.389999  5473.600098
2016-05-31  19.080000  77.430000  72.750000       5.59  22.120001  5447.799805
2016-06-01  18.490000  76.500000  72.150002       5.52  21.799999  5395.200195
</code></pre>
","['python', 'pandas', 'numpy', 'dataframe']",49002589,"<p>Here is an easy and quick way to do what you want:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(np.arange(25).reshape(5, 5), 
                  columns=['A', 'B', 'C', 'D', 'E'])
print(df)
</code></pre>

<p>result:</p>

<pre><code>    A   B   C   D   E
0   0   1   2   3   4
1   5   6   7   8   9
2  10  11  12  13  14
3  15  16  17  18  19
4  20  21  22  23  24
</code></pre>

<p>We can use the shift member function to move the entire dataframe up (or down).
Then we just have to subtract this from the original, and rename the columns.</p>

<pre><code>df2 = df - df.shift(1, axis=0) 
df2.columns = [col + '_earning' for col in df2.columns]
print(df2)
</code></pre>

<p>result:</p>

<pre><code>   A_earning  B_earning  C_earning  D_earning  E_earning
0        NaN        NaN        NaN        NaN        NaN
1        5.0        5.0        5.0        5.0        5.0
2        5.0        5.0        5.0        5.0        5.0
3        5.0        5.0        5.0        5.0        5.0
4        5.0        5.0        5.0        5.0        5.0
</code></pre>

<p>Then just join the result with the original. </p>

<pre><code>result = pd.concat([df, df2], axis=1)
print(result)
</code></pre>

<p>result:</p>

<pre><code>    A   B   C   D   E  A_earning  B_earning  C_earning  D_earning  E_earning
0   0   1   2   3   4        NaN        NaN        NaN        NaN        NaN
1   5   6   7   8   9        5.0        5.0        5.0        5.0        5.0
2  10  11  12  13  14        5.0        5.0        5.0        5.0        5.0
3  15  16  17  18  19        5.0        5.0        5.0        5.0        5.0
4  20  21  22  23  24        5.0        5.0        5.0        5.0        5.0
</code></pre>

<p>edit: after revisiting your post, it looks like your trying to do this operation on some columns containing strings? Either filter them out or convert to a datatype which supports the '-' operator.</p>
",Appending new column differences current row previous row multiple columns For columns df I want subtract current row previous row row n row n I difficulty My code follows usr bin python pandas datareader import data import pandas pd import fix yahoo finance yf yf pdr override import os stock list BHP AX CBA AX RHC AX TLS AX WOW AX AORD Function get closing price individual stocks stock list list def get closing price stock name specific close symbol stock name start date end date df data get data yahoo symbol start date end date sym symbol print sym df df drop Open High Low Adj Close Volume axis df df rename columns Close specific close https stackoverflow com questions converting strings floats dataframe df specific close df specific close astype float print type df specific close return df Creates big DataFrame stock Closing Price returns DataFrame def get,"startoftags, python, pandas, numpy, dataframe, endoftags",python pandas dataframe endoftags,python pandas numpy dataframe,python pandas dataframe,0.87
49051969,2018-03-01,2018,6,How to assign values to multiple non existing columns in a pandas dataframe?,"<p>So what I want to do is to add columns to a dataframe and fill them (all rows respectively) with a single value. </p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(np.array([[1,2],[3,4]]), columns = [""A"",""B""])
arr = np.array([7,8])

# this is what I would like to do
df[[""C"",""D""]] = arr

# and this is what I want to achieve
#    A  B  C  D
# 0  1  2  7  8
# 1  3  4  7  8
# but it yields an ""KeyError"" sadly
# KeyError: ""['C' 'D'] not in index""
</code></pre>

<p>I do know about the assign-functionality and how I would tackle this issue if I only were to add one column at once. I just want to know whether there is a clean and simple way to do this with multiple new columns as I was not able to find one.</p>
","['python', 'pandas', 'numpy']",49052003,"<p>For me working:</p>

<pre><code>df[[""C"",""D""]] = pd.DataFrame([arr], index=df.index)
</code></pre>

<p>Or <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html"" rel=""noreferrer""><code>join</code></a>:</p>

<pre><code>df = df.join(pd.DataFrame([arr], columns=['C','D'], index=df.index))
</code></pre>

<p>Or <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html"" rel=""noreferrer""><code>assign</code></a>:</p>

<pre><code>df = df.assign(**pd.Series(arr, index=['C','D']))
</code></pre>

<hr>

<pre><code>print (df)
   A  B  C  D
0  1  2  7  8
1  3  4  7  8
</code></pre>
",How assign values multiple non existing columns pandas dataframe So I want add columns dataframe fill rows respectively single value import pandas pd import numpy np df pd DataFrame np array columns A B arr np array I would like df C D arr I want achieve A B C D yields KeyError sadly KeyError C D index I know assign functionality I would tackle issue I add one column I want know whether clean simple way multiple new columns I able find one,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
49062176,2018-03-02,2018,4,Fill NaN values from another DataFrame (with different shape),"<p>I'm looking for a faster approach to improve the performance of my solution for the following problem: a certain DataFrame has two columns with a few <em>NaN</em> values in them. The challenge is to replace these <em>NaNs</em> with values from a secondary DataFrame.</p>

<p>Below I'll share the data and code used to implement my approach. Let me explain the scenario: <code>merged_df</code> is the original DataFrame with a few columns and some of them have rows with <em>NaN</em> values:</p>

<p><a href=""https://i.stack.imgur.com/JhL3A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JhL3A.png"" alt=""enter image description here""></a></p>

<p>As you can see from the image above, columns <code>day_of_week</code> and <code>holiday_flg</code> are of particular interest. I would like to fill the <em>NaN</em> values of these columns by looking into a second DataFrame called <code>date_info_df</code>, which looks like this:</p>

<p><a href=""https://i.stack.imgur.com/uW6oG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uW6oG.png"" alt=""enter image description here""></a></p>

<p>By using the values from column <code>visit_date</code> in <code>merged_df</code> it is possible to search the second DataFrame on <code>calendar_date</code> and find equivalent matches. This method allows to get the values for <code>day_of_week</code> and <code>holiday_flg</code> from the second DataFrame.</p>

<p>The end result for this exercise is a DataFrame that looks like this:</p>

<p><a href=""https://i.stack.imgur.com/vjr68.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vjr68.png"" alt=""enter image description here""></a></p>

<p>You'll notice the approach I'm using relies on <code>apply()</code> to execute a custom function on every row of <code>merged_df</code>:</p>

<ul>
<li>For every row, search for <em>NaN</em> values in <code>day_of_week</code> and <code>holiday_flg</code>;</li>
<li>When a <em>NaN</em> is found on any or both of these columns, use the date available in from that row's <code>visit_date</code> to find an equivalent match in the second DataFrame, specifically the <code>date_info_df['calendar_date']</code> column;</li>
<li>After a successful match, the value from <code>date_info_df['day_of_week']</code> must be  copied into <code>merged_df['day_of_week']</code> and the value from <code>date_info_df['holiday_flg']</code> must also be copied into <code>date_info_df['holiday_flg']</code>.</li>
</ul>

<p>Here is a working <strong>source code</strong>:</p>

<pre><code>import math
import pandas as pd
import numpy as np
from IPython.display import display

### Data for df
data = { 'air_store_id':     [              'air_a1',     'air_a2',     'air_a3',     'air_a4' ], 
         'area_name':        [               'Tokyo',       np.nan,       np.nan,       np.nan ], 
         'genre_name':       [            'Japanese',       np.nan,       np.nan,       np.nan ], 
         'hpg_store_id':     [              'hpg_h1',       np.nan,       np.nan,       np.nan ],          
         'latitude':         [                  1234,       np.nan,       np.nan,       np.nan ], 
         'longitude':        [                  5678,       np.nan,       np.nan,       np.nan ],         
         'reserve_datetime': [ '2017-04-22 11:00:00',       np.nan,       np.nan,       np.nan ], 
         'reserve_visitors': [                    25,           35,           45,       np.nan ], 
         'visit_datetime':   [ '2017-05-23 12:00:00',       np.nan,       np.nan,       np.nan ], 
         'visit_date':       [ '2017-05-23'         , '2017-05-24', '2017-05-25', '2017-05-27' ],
         'day_of_week':      [             'Tuesday',  'Wednesday',       np.nan,       np.nan ],
         'holiday_flg':      [                     0,       np.nan,       np.nan,       np.nan ]
       }

merged_df = pd.DataFrame(data)
display(merged_df)

### Data for date_info_df
data = { 'calendar_date':     [ '2017-05-23', '2017-05-24', '2017-05-25', '2017-05-26', '2017-05-27', '2017-05-28' ], 
         'day_of_week':       [    'Tuesday',  'Wednesday',   'Thursday',     'Friday',   'Saturday',     'Sunday' ], 
         'holiday_flg':       [            0,            0,            0,            0,            1,            1 ]         
       }

date_info_df = pd.DataFrame(data)
date_info_df['calendar_date'] = pd.to_datetime(date_info_df['calendar_date']) 
display(date_info_df)

# Fix the NaN values in day_of_week and holiday_flg by inspecting data from another dataframe (date_info_df)
def fix_weekday_and_holiday(row):
    weekday = row['day_of_week']   
    holiday = row['holiday_flg']

    # search dataframe date_info_df for the appropriate value when weekday is NaN
    if (type(weekday) == float and math.isnan(weekday)):
        search_date = row['visit_date']                               
        #print('  --&gt; weekday search_date=', search_date, 'type=', type(search_date))        
        indexes = date_info_df.index[date_info_df['calendar_date'] == search_date].tolist()
        idx = indexes[0]                
        weekday = date_info_df.at[idx,'day_of_week']
        #print('  --&gt; weekday search_date=', search_date, 'is', weekday)        
        row['day_of_week'] = weekday        

    # search dataframe date_info_df for the appropriate value when holiday is NaN
    if (type(holiday) == float and math.isnan(holiday)):
        search_date = row['visit_date']                               
        #print('  --&gt; holiday search_date=', search_date, 'type=', type(search_date))        
        indexes = date_info_df.index[date_info_df['calendar_date'] == search_date].tolist()
        idx = indexes[0]                
        holiday = date_info_df.at[idx,'holiday_flg']
        #print('  --&gt; holiday search_date=', search_date, 'is', holiday)        
        row['holiday_flg'] = int(holiday)

    return row


# send every row to fix_day_of_week
merged_df = merged_df.apply(fix_weekday_and_holiday, axis=1) 

# Convert data from float to int (to remove decimal places)
merged_df['holiday_flg'] = merged_df['holiday_flg'].astype(int)

display(merged_df)
</code></pre>

<p>I did a few measurements so you can understand the struggle:</p>

<ul>
<li>On a DataFrame with <strong>6</strong> rows, <code>apply()</code> takes <strong>3.01 ms</strong>;</li>
<li>On a DataFrame with ~<strong>250000</strong> rows, <code>apply()</code> takes <strong>2min 51s</strong>.</li>
<li>On a DataFrame with ~<strong>1215000</strong> rows, <code>apply()</code> takes <strong>4min 2s</strong>.</li>
</ul>

<p><strong>How do I improve the performance of this task?</strong></p>
","['python', 'pandas', 'dataframe']",49062349,"<p>you can use <code>Index</code> to speed up the lookup, use <code>combine_first()</code> to fill NaN:</p>

<pre><code>cols = [""day_of_week"", ""holiday_flg""]
visit_date = pd.to_datetime(merged_df.visit_date)
merged_df[cols] = merged_df[cols].combine_first(
    date_info_df.set_index(""calendar_date"").loc[visit_date, cols].set_index(merged_df.index))

print(merged_df[cols])
</code></pre>

<p>the result:</p>

<pre><code> day_of_week  holiday_flg
0     Tuesday          0.0
1   Wednesday          0.0
2    Thursday          0.0
3    Saturday          1.0
</code></pre>
",Fill NaN values another DataFrame different shape I looking faster approach improve performance solution following problem certain DataFrame two columns NaN values The challenge replace NaNs values secondary DataFrame Below I share data code used implement approach Let explain scenario merged df original DataFrame columns rows NaN values As see image columns day week holiday flg particular interest I would like fill NaN values columns looking second DataFrame called date info df looks like By using values column visit date merged df possible search second DataFrame calendar date find equivalent matches This method allows get values day week holiday flg second DataFrame The end result exercise DataFrame looks like You notice approach I using relies apply execute custom function every row merged df For every row search NaN values day week holiday flg When NaN found columns use date available row visit date find equivalent match second DataFrame specifically date,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
49151683,2018-03-07,2018,2,How to find longest consecutive ocurrence of non-zero elements in 2D numpy array,"<p>I am simulating protein folding on a 2D grid where every angle is either Â±90Â° or 0Â°, and have the following problem:</p>

<p>I have an n-by-n numpy array filled with zeros, except for certain places where the value is any integer from 1 to n. Every integer appears just once. Integer k is always a nearest neighbour to k-1 <em>and</em> k + 1, except for the endpoints. The array is saved as an object in the class Grid which I have created for doing energy calculations and folding the protein. Example array, with n=5:</p>

<pre><code>&gt;&gt;&gt; from Grid import Grid
&gt;&gt;&gt; a = Grid(5)
&gt;&gt;&gt; a.show()
[[0 0 0 0 0]
 [0 0 0 0 0]
 [1 2 3 4 5]
 [0 0 0 0 0]
 [0 0 0 0 0]]
</code></pre>

<p>My goal is to find the longest consecutive line of non-zero elements withouth any bends. In the above case, the result should be 5.</p>

<p>My idea so far are something like this:     </p>

<pre><code>def getDiameter(self):
    indexes = np.zeros((self.n, 2))
    for i in range(1, self.n + 1):
        indexes[i - 1]Â = np.argwhere(self.array == i)[0]

    for i in range(self.n):
         j = 1
        currentDiameter = 1
            while indexes[0][i] == indexes[0][i + j]Â and i + j &lt;= self.n:
                currentDiameter += 1
                j += 1

        while indexes[i][0] == indexes[i + j][0]Â and i + j &lt;= self.n:
            currentDiameter += 1
            j += 1

        if currentDiameter &gt; diameter:
            diameter = currentDiameter

     return diameter
</code></pre>

<p>This has two problems: (1) it doesn't work, and (2) it is horribly inefficient if I get it to work. I am wondering if anybody has a better way of doing this. If anything is unclear, please let me know.</p>

<p>Edit:
Less trivial example</p>

<pre><code>[[ 0  0  0  0  0  0  0  0  0  0]
[ 0  0  0  0  0  0  0  0  0  0]
[ 0  0  0  0  0  0 10  0  0  0]
[ 0  0  0  0  0  0  9  0  0  0]
[ 0  0  0  0  0  0  8  0  0  0]
[ 0  0  0  4  5  6  7  0  0  0]
[ 0  0  0  3  0  0  0  0  0  0]
[ 0  0  0  2  1  0  0  0  0  0]
[ 0  0  0  0  0  0  0  0  0  0]
[ 0  0  0  0  0  0  0  0  0  0]] 
</code></pre>

<p>The correct answer here is 4 (both the longest column and the longest row have four non-zero elements).</p>
","['python', 'arrays', 'numpy']",49153300,"<p>What I understood from your question is you need to find the length of longest occurance of consecutive elements in numpy array (row by row). </p>

<p>So for this below one, the output should be <code>5</code>:</p>

<pre><code>[[1 2 3 4 0]
 [0 0 0 0 0]
 [10 11 12 13 14]
 [0 1 2 3 0]
 [1 0 0 0 0]]
</code></pre>

<p>Because <code>[10 11 12 13 14]</code> are consecutive elements and they have the longest length comparing to any consecutive elements in any other row.</p>

<p>If this is what you are expecting, consider this:</p>

<pre><code>import numpy as np
from itertools import groupby

a = np.array([[1, 2, 3, 4, 0],
 [0, 0, 0, 0, 0],
 [10, 11, 12, 13, 14],
 [0, 1, 2, 3, 0],
 [1, 0, 0, 0, 0]])

a = a.astype(float)
a[a == 0] = np.nan
b = np.diff(a)      # Calculate the n-th discrete difference. Consecutive numbers will have a difference of 1.
counter = []
for line in b:       # for each row.
    if 1 in line:    # consecutive elements differ by 1.
        counter.append(max(sum(1 for _ in g) for k, g in groupby(line) if k == 1) + 1)  # find the longest length of consecutive 1's for each row.
print(max(counter))  # find the max of list holding the longest length of consecutive 1's for each row.
# 5
</code></pre>

<p>For your particular example:</p>

<pre><code>[[0 0 0 0 0] 
[0 0 0 0 0] 
[1 2 3 4 5] 
[0 0 0 0 0] 
[0 0 0 0 0]]
# 5
</code></pre>
",How find longest consecutive ocurrence non zero elements D numpy array I simulating protein folding D grid every angle either following problem I n n numpy array filled zeros except certain places value integer n Every integer appears Integer k always nearest neighbour k k except endpoints The array saved object class Grid I created energy calculations folding protein Example array n gt gt gt Grid import Grid gt gt gt Grid gt gt gt show My goal find longest consecutive line non zero elements withouth bends In case result My idea far something like def getDiameter self indexes np zeros self n range self n indexes np argwhere self array range self n j currentDiameter indexes indexes j j lt self n currentDiameter j indexes indexes j j lt self n currentDiameter j currentDiameter gt diameter diameter currentDiameter return diameter This two problems work horribly inefficient I get work,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
49319384,2018-03-16,2018,3,Python-Calculation with pandas data frames,"<p>I am new to python. I am trying to learn pandas with below example.
I have two data frames below.</p>

<p>First one is,</p>

<pre><code>CCP_DETAILS_SID BASE_LINE
    1            1235.89
    2            369.32
    3            9863.1
</code></pre>

<p>And Second one is,</p>

<pre><code>CCP_DETAILS_SID PERIOD_SID  GROWTH
1                  601       0.1
1                  602       0.2
1                  603       0.3
2                  601       0.1
2                  602       0.2
2                  603       0.3
3                  601       0.1
3                  602       0.2
3                  603       0.3
</code></pre>

<p>by merging above two, I am trying to calculate a field called 'PROJECTION_SALES'. Formula and examples for the field i have listed below.</p>

<p>Projection_Sales=(Base_Line)*(1+Growth) and the merge or join condition between two data frames is CCP_DETAILS_SID.</p>

<p><strong>Examples</strong></p>

<pre><code>    Projection_Sales(ccp_details_sid=1 and period_sid=601)=1235.89*(1+0.1)

    Projection_Sales(ccp_details_sid=1 and period_sid=602)=1235.89*(1+0.1)*(1+0.2)

    Projection_Sales(ccp_details_sid=1 and period_sid=603)=1235.89*(1+0.1)*(1+0.2)*(1+0.3)

    Projection_Sales(ccp_details_sid=2 and period_sid=601)=369.32*(1+0.1).
</code></pre>

<p>Same way of calculation applies to other rows in the data frames. And sample output i listed below.</p>

<pre><code>CCP_DETAILS_SID PERIOD_SID  PROJECTION_SALES
1                 601        1359.479
1                 602        1631.3748
1                 603        2120.78724
2                 601        406.252
2                 602        487.5024
2                 603        633.75312
3                 601        10849.41
3                 602        13019.292
3                 603        16925.0796
</code></pre>

<p>I have tried some thing like below </p>

<pre><code>pd.merge(first,second,how='inner',on='CCP_DETAILS_SID')
</code></pre>

<p>After this step i need to extend code with the use of <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.cumprod.html"" rel=""nofollow noreferrer"">cumprod</a>. Because you can observe above examples are having cumulative product logic etc.</p>

<p>Can you people please suggest me a way to complete this calculation?.</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",49319547,"<p>Is that what you want?</p>

<pre><code>In [145]: t = d1.merge(d2)

In [146]: (t.assign(x=t.assign(x=t.GROWTH+1)
                       .groupby('CCP_DETAILS_SID')['x']
                       .cumprod())
            .eval(""Projection_Sales = BASE_LINE * x"")
            .drop('x',1))
Out[146]:
   CCP_DETAILS_SID  BASE_LINE  PERIOD_SID  GROWTH  Projection_Sales
0                1    1235.89         601     0.1        1359.47900
1                1    1235.89         602     0.2        1631.37480
2                1    1235.89         603     0.3        2120.78724
3                2     369.32         601     0.1         406.25200
4                2     369.32         602     0.2         487.50240
5                2     369.32         603     0.3         633.75312
6                3    9863.10         601     0.1       10849.41000
7                3    9863.10         602     0.2       13019.29200
8                3    9863.10         603     0.3       16925.07960
</code></pre>
",Python Calculation pandas data frames I new python I trying learn pandas example I two data frames First one CCP DETAILS SID BASE LINE And Second one CCP DETAILS SID PERIOD SID GROWTH merging two I trying calculate field called PROJECTION SALES Formula examples field listed Projection Sales Base Line Growth merge join condition two data frames CCP DETAILS SID Examples Projection Sales ccp details sid period sid Projection Sales ccp details sid period sid Projection Sales ccp details sid period sid Projection Sales ccp details sid period sid Same way calculation applies rows data frames And sample output listed CCP DETAILS SID PERIOD SID PROJECTION SALES I tried thing like pd merge first second inner CCP DETAILS SID After step need extend code use cumprod Because observe examples cumulative product logic etc Can people please suggest way complete calculation,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
49343860,2018-03-18,2018,3,How to drop duplicate values based in specific columns using pandas?,"<p>Currently, I imported the following data frame from Excel into pandas and I want to delete duplicate values based in the values of two columns.</p>

<pre><code># Python 3.5.2
# Pandas library version 0.22

import pandas as pd 

# Save the Excel workbook in a variable
current_workbook  = pd.ExcelFile('C:\\Users\\userX\\Desktop\\cost_values.xlsx')

# convert the workbook to a data frame
current_worksheet = pd.read_excel(current_workbook, index_col = 'vend_num') 

# current output
print(current_worksheet)


| vend_number |      vend_name         | quantity |  source  |
| ----------- |----------------------- | -------- | -------- | 
    CHARLS      Charlie &amp; Associates      $5,700.00   Central
    CHARLS      Charlie &amp; Associates      $5,700.00   South
    CHARLS      Charlie &amp; Associates      $5,700.00   North
    CHARLS      Charlie &amp; Associates      $5,700.00   West
    HUGHES      Hughinos                  $3,800.00   Central
    HUGHES      Hughinos                  $3,800.00   South
    FERNAS      Fernanda Industries       $3,500.00   South
    FERNAS      Fernanda Industries       $3,500.00   North
    FERNAS      Fernanda Industries       $3,000.00   West
    ....
</code></pre>

<p>What I want is to remove those duplicate values based in the columns quantity and source:</p>

<ol>
<li><p>Review the quantity and source column values: </p>

<p>1.1. If the quantity of a vendor is equal in another row from the same
vendor and source is not equal to Central then drop the repeated
rows from this vendor except the row Central.</p>

<p>1.2. Else if the quantity of a vendor is equal in another row from the same vendor and there is no source Central then drop the repeated rows.</p></li>
</ol>

<p>Desired result</p>

<pre><code>| vend_number |      vend_name         | quantity |  source  |
| ----------- |----------------------- | -------- | -------- | 
    CHARLS      Charlie &amp; Associates      $5,700.00   Central
    HUGHES      Hughinos                  $3,800.00   Central
    FERNAS      Fernanda Industries       $3,500.00   South
    FERNAS      Fernanda Industries       $3,000.00   West
    ....
</code></pre>

<p>So far, I have tried the following code but pandas is not even detecting any duplicate rows. </p>

<pre><code>print(current_worksheet.loc[current_worksheet.duplicated()])
print(current_worksheet.duplicated())
</code></pre>

<p>I have tried to figure out the solution but I am struggling quite a bit in this problem, so any help in this question is greatly appreciated. Feel free to improve the question. </p>
","['python', 'python-3.x', 'pandas']",49343949,"<p>Here is one way.</p>

<pre><code>df['CentralFlag'] = (df['source'] == 'Central')

df = df.sort_values('CentralFlag', ascending=False)\
       .drop_duplicates(['vend_name', 'quantity'])\
       .drop('CentralFlag', 1)

#   vend_number           vend_name   quantity   source
# 0      CHARLS  Charlie&amp;Associates  $5,700.00  Central
# 4      HUGHES            Hughinos  $3,800.00  Central
# 6      FERNAS  FernandaIndustries  $3,500.00    South
# 8      FERNAS  FernandaIndustries  $3,000.00     West
</code></pre>

<p><strong>Explanation</strong></p>

<ul>
<li>Create a flag column, sort by this descending, so Central is prioritised.</li>
<li>Sort by <code>vend_name</code> and <code>quantity</code>, then drop the flag column.</li>
</ul>
",How drop duplicate values based specific columns using pandas Currently I imported following data frame Excel pandas I want delete duplicate values based values two columns Python Pandas library version import pandas pd Save Excel workbook variable current workbook pd ExcelFile C Users userX Desktop cost values xlsx convert workbook data frame current worksheet pd read excel current workbook index col vend num current output print current worksheet vend number vend name quantity source CHARLS Charlie amp Associates Central CHARLS Charlie amp Associates South CHARLS Charlie amp Associates North CHARLS Charlie amp Associates West HUGHES Hughinos Central HUGHES Hughinos South FERNAS Fernanda Industries South FERNAS Fernanda Industries North FERNAS Fernanda Industries West What I want remove duplicate values based columns quantity source Review quantity source column values If quantity vendor equal another row vendor source equal Central drop repeated rows vendor except row Central Else quantity vendor equal another row,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
49444638,2018-03-23,2018,2,Python Regex Produce Words Until Character or Special Word is Found,"<p>been struggling with this for hours now, just can't seem to get my head around regex for some reason.</p>

<p>I'm looking through the strings below line by line using this pattern:</p>

<pre><code>pattern = re.compile(r""^[^&amp;,]*"")
</code></pre>

<p>The strings are kept in a dictionary so looping over them like this: </p>

<pre><code>for dct in lst:
    print(re.search(pattern, dct['artist']).group(0))

""""""
Drake
Post Malone Featuring Ty Dolla $ign
BlocBoy JB Featuring Drake
Offset &amp; Metro Boomin
Jay Rock, Kendrick Lamar, Future &amp; James Blake
""""""
</code></pre>

<p>The above gives me this as expected:</p>

<pre><code>""""""
Drake
Post Malone Featuring Ty Dolla $ign
BlockBoy JB Featuring Drake
Offset
Jay Rock 
""""""
</code></pre>

<p>But I cannot figure out how to get add that it should also stop at the string ""Featuring"", I've tried different a 100 variations of \bFeaturing\b, capital <code>B</code>, different tokens in front, back, positions in the <code>regex</code>. </p>

<p>This is the closest I've gotten, but then it only matches the lines that have ""Featuring"":</p>

<pre><code>pattern = re.compile(r""^[^&amp;,]*(?=\bFeaturing\b)"")
</code></pre>

<p>This gives me this output:</p>

<pre><code>None
&lt;_sre.SRE_Match object; span=(0, 12), match='Post Malone '&gt;
&lt;_sre.SRE_Match object; span=(0, 11), match='BlocBoy JB '&gt;
None
&lt;_sre.SRE_Match object; span=(0, 12), match='Post Malone '&gt;
None
</code></pre>

<p>I'm fairly new to this so most of what I'm doing is trial and error, but I'm on the verge of giving up. Please help me get a result like this:</p>

<pre><code>""""""
Drake
Post Malone
BlockBoy JB
Offset
Jay Rock 
""""""
</code></pre>
","['python', 'regex', 'python-3.x']",49444930,"<p>You may use</p>

<pre><code>re.findall(r'^(?:(?!\bFeaturing\b)[^&amp;,\n])*\b', s, re.M)
</code></pre>

<p>or</p>

<pre><code>re.findall(r'^.*?(?=\s*(?:\bFeaturing\b|[&amp;,]|$))', s, re.M)
</code></pre>

<p>See <a href=""https://regex101.com/r/NBqO0I/2"" rel=""nofollow noreferrer"">this regex demo</a> or <a href=""https://regex101.com/r/NBqO0I/4"" rel=""nofollow noreferrer"">another one</a>. The regexps are equivalent as far as their result is concerned.</p>

<p><strong>Details</strong></p>

<ul>
<li><code>^</code> - start of a line</li>
<li><code>(?:(?!\bFeaturing\b)[^&amp;,\n])*</code> - (see <a href=""https://stackoverflow.com/a/37343088/3832970"">more about this construct</a>) any char other than <code>&amp;</code>, <code>,</code> and a newline, as many as possible, that do not start the whole word <code>Featuring</code>.</li>
<li><p><code>\b</code> - a word boundary</p></li>
<li><p><code>.*?(?=\s*(?:\bFeaturing\b|[&amp;,]|$))</code> - matches any 0+ chars other than line break chars, as few as possible (<code>.*?</code>) up to the leftmost occurrence of 0+ whitespaces followed with...</p>

<ul>
<li><code>\bFeaturing\b</code> -  whole word <code>Featuring</code></li>
<li><code>[&amp;,]</code> - a <code>&amp;</code> or <code>,</code> char</li>
<li><code>$</code> - end of line</li>
</ul></li>
</ul>
",Python Regex Produce Words Until Character Special Word Found struggling hours seem get head around regex reason I looking strings line line using pattern pattern compile r amp The strings kept dictionary looping like dct lst print search pattern dct artist group Drake Post Malone Featuring Ty Dolla ign BlocBoy JB Featuring Drake Offset amp Metro Boomin Jay Rock Kendrick Lamar Future amp James Blake The gives expected Drake Post Malone Featuring Ty Dolla ign BlockBoy JB Featuring Drake Offset Jay Rock But I cannot figure get add also stop string Featuring I tried different variations bFeaturing b capital B different tokens front back positions regex This closest I gotten matches lines Featuring pattern compile r amp bFeaturing b This gives output None lt sre SRE Match object span match Post Malone gt lt sre SRE Match object span match BlocBoy JB gt None lt sre SRE Match object span,"startoftags, python, regex, python3x, endoftags",python arrays numpy endoftags,python regex python3x,python arrays numpy,0.33
49610295,2018-04-02,2018,2,Python: fill column based on first charakter of another columns content,"<p>I have a pandas dataframe looking like this:</p>

<pre><code>+-----+------+
| No  | type |
+-----+------+
| 123 | C01  |
| 123 | C02  |
| 123 | T01  |
| 345 | C01  |
| 345 | H12  |
| 345 | H22  |
+-----+------+
</code></pre>

<p>and a numpy array like this:</p>

<pre><code>arr = [Car, Tree, House]
</code></pre>

<p>Desired output:</p>

<pre><code>+-----+------+----------+
| No  | type | category |
+-----+------+----------+
| 123 | C01  | Car      |
| 123 | C02  | Car      |
| 123 | T01  | Tree     |
| 345 | C01  | Car      |
| 345 | H12  | House    |
| 345 | H22  | House    |
+-----+------+----------+
</code></pre>

<p>So I would like to add a column containing the element of the arr, where the first charakter matches to the first charakter of column type. </p>

<p>There is ony one element within the array for each first charakter.</p>

<p>What is the best way to achieve this? I could do this manually for each first charakter but I would like to do this within one run e.g. with apply-function.</p>

<p>Thank you,</p>

<p>MaMo</p>
","['python', 'pandas', 'numpy', 'dataframe']",49610341,"<p>Full example:</p>

<pre><code>import pandas as pd

data = '''\
No  type
123 C01
123 C02
123 T01
345 C01
345 H12
345 H22'''

df = pd.read_csv(pd.compat.StringIO(data),sep='\s+')

arr = ['Car', 'Tree', 'House']
d = {x[0]:x for x in arr}                   # Create a map
df['category'] = df['type'].str[0].map(d)   # Apply map to str[0]
</code></pre>

<p>Results in:</p>

<pre><code>    No type category
0  123  C01      Car
1  123  C02      Car
2  123  T01     Tree
3  345  C01      Car
4  345  H12    House
5  345  H22    House
</code></pre>

<p><strong>Explanation</strong>:</p>

<ol>
<li>Create dictionary by first values of <code>arr</code></li>
<li>Select first value of <code>type</code> column by <code>str[0]</code> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>map</code></a> by <code>dict</code></li>
</ol>
",Python fill column based first charakter another columns content I pandas dataframe looking like No type C C T C H H numpy array like arr Car Tree House Desired output No type category C Car C Car T Tree C Car H House H House So I would like add column containing element arr first charakter matches first charakter column type There ony one element within array first charakter What best way achieve I could manually first charakter I would like within one run e g apply function Thank MaMo,"startoftags, python, pandas, numpy, dataframe, endoftags",python pandas dataframe endoftags,python pandas numpy dataframe,python pandas dataframe,0.87
50051799,2018-04-26,2018,3,"Check if a row with certain values exist, and create if it does not exist","<p>I have a large table with a couple of thousands of origin points (column A) and a couple of thousands of destination points (column B) and another columns with a value. I need an efficient algorithm in python to make sure that there is a row for each pair of origin to destination, and if not, create it. </p>

<p>For example, imagine I have three origin points (1,2,3) and Three destination points (1,2,3). Currently, my data does not have a row for each possible pair between origins (column A) and destinations (column B) and is like:</p>

<pre><code>Index  A      B     Value
0      1      1     V11
1      1      3     V13
2      2      1     V21
3      2      2     V22
4      2      3     V23
5      3      1     V31
6      3      3     V33
</code></pre>

<p>I want a python script to make it look like:</p>

<pre><code>Index  A      B     Value
0      1      1     V11
1      1      2     NA
2      1      3     V13
3      2      1     V21
4      2      2     V22
5      2      3     V23
6      3      1     V31
7      3      2     NA
8      3      3     V33
</code></pre>
","['python', 'pandas', 'dataframe']",50051914,"<p>This is one way using <code>itertools.product</code>.</p>

<p>The idea is to calculate the full set of combinations, remove combinations that already exist, then add the remainder to the dataframe.</p>

<pre><code>from itertools import product

maxval = df[['A', 'B']].max().max()

prod = set(product(range(1, maxval+1), range(1, maxval+1)))
existing = set(map(tuple, df[['A', 'B']].values))

additional = pd.DataFrame(np.array(list(prod - existing)), columns=['A', 'B'])

res = pd.concat([df.set_index('Index'), additional], axis=0)\
        .sort_values(['A', 'B'])\
        .reset_index(drop=True)\
        .reset_index()

print(res)

   index  A  B Value
0      0  1  1   V11
1      1  1  2   NaN
2      2  1  3   V13
3      3  2  1   V21
4      4  2  2   V22
5      5  2  3   V23
6      6  3  1   V31
7      7  3  2   NaN
8      8  3  3   V33
</code></pre>
",Check row certain values exist create exist I large table couple thousands origin points column A couple thousands destination points column B another columns value I need efficient algorithm python make sure row pair origin destination create For example imagine I three origin points Three destination points Currently data row possible pair origins column A destinations column B like Index A B Value V V V V V V V I want python script make look like Index A B Value V NA V V V V V NA V,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
50190720,2018-05-05,2018,3,How to index into a list of dictionaries?,"<p>I am a beginner. I have this list:</p>

<pre><code>players = [{'name': 'John', 'points': '27', 'Played': '6'},
{'name': 'Emil', 'points': '43', 'Played' : '13'},
{'name': 'Susan', 'points': '11', 'Played': '2'},
{'name': 'Peter', 'points': '4', 'Played': '3'}]
</code></pre>

<p>What I would like to do was to be able to say:</p>

<pre><code>players[""John""][""score""]=newScore
players[""john""][""Played""] = players[""john""][""Played""]+1
</code></pre>

<p>This list represents like a list of objects, where the name is the primary key and then there's parameters for each object.</p>

<p>But of course that doesn't work, I'm able to touch elements by, for example by doing:</p>

<pre><code>print (players[0][""score""])
</code></pre>

<p>and this would then print John's score alright, but with this way I would need to go through all players[x] to first compare if the name is the same, and then I can access it, it just doesn't seem so Pythonic to me.</p>

<p>How would you go about this in a Pythonic way? </p>
","['python', 'python-3.x', 'dictionary']",50190837,"<p>You could wrap your current data with a class that could provide the specialized update methods you specified, without changing the entire structure of your original data to begin with:</p>

<pre><code>class Row:
   def __init__(self, row):
      self.row = row
   def __setitem__(self, _t, _val):
      self.row[_t] = str(_val)
   def __getitem__(self, _stat):
      return int(self.row[_stat])
   def __repr__(self):
      return '{}({})'.format(self.__class__.__name__, str(self.row))

class Players:
   def __init__(self, data={}):
      self.data = {i['name']:Row(i) for i in data}
   def __getitem__(self, name):
      return self.data[name]
   def __setitem__(self, _name, _data):
      self.data[_name] = Row(_data)
   def __repr__(self):
      return '{}({})'.format(self.__class__.__name__, str(self.data))

players = [{'points': '27', 'name': 'John', 'Played': '6'}, {'points': '43', 'name': 'Emil', 'Played': '13'}, {'points': '11', 'name': 'Susan', 'Played': '2'}, {'points': '4', 'name': 'Peter', 'Played': '3'}]
d = Players(players)
d['John']['points'] = 30
d[""John""][""Played""] = d[""John""][""Played""]+1
print(d['John'])
d['Bob'] = {'points': '2', 'Played': '5'}
</code></pre>

<p>Output:</p>

<pre><code>Row({'points': '30', 'name': 'John', 'Played': '7'})
</code></pre>
",How index list dictionaries I beginner I list players name John points Played name Emil points Played name Susan points Played name Peter points Played What I would like able say players John score newScore players john Played players john Played This list represents like list objects name primary key parameters object But course work I able touch elements example print players score would print John score alright way I would need go players x first compare name I access seem Pythonic How would go Pythonic way,"startoftags, python, python3x, dictionary, endoftags",python python3x list endoftags,python python3x dictionary,python python3x list,0.67
50249653,2018-05-09,2018,2,Pandas merging 2 dataframes on their similar columns(which is the index),"<p>I have two dataframes, which in both I happened to set 'timeStamp' as the index. <code>df_1.set_index('timeStamp', inplace=True)</code>. </p>

<p><strong>df_1</strong></p>

<pre><code>                     value
timeStamp                 
2016-11-23 20:00:00  37.21
2016-11-23 21:00:00  37.79
2016-11-23 22:00:00  33.99
2016-11-23 23:00:00  32.66
2016-11-24 00:00:00  31.61
</code></pre>

<p><strong>df_2</strong></p>

<pre><code>                     value
timeStamp                 
2016-11-23 23:00:00  32.92
2016-11-24 00:00:00  31.54
2016-11-24 01:00:00  29.14
</code></pre>

<p>I wanted to make a dataframe comparing both values when the time is shared.  I tried <code>combined_df= pd.merge(df_real, df_fc, on='timeStamp', how='inner')</code> and got a <code>key error</code>. </p>

<p>So instead of merging two dataframes on an index, I kept the dataframes without 'timeStamp' as their index. For example.</p>

<p><strong>df I used instead for merging</strong></p>

<pre><code>             timeStamp  value
0  2016-11-23 20:00:00  37.21
1  2016-11-23 21:00:00  37.79
2  2016-11-23 22:00:00  33.99
3  2016-11-23 23:00:00  32.66
13 2016-11-24 00:00:00  31.61
</code></pre>

<p>Then I was able to merge and my new df was set(shown below). I also then set the index to timestamp, later on.</p>

<pre><code>            timeStamp  value_x  value_y 
0  2016-11-23 23:00:00    32.66    32.92 
</code></pre>

<p><strong>my question</strong> Why couldn't I merge on the column name that was specified as an index? I wanted to set that merge to a new dataframe... </p>
","['python', 'pandas', 'dataframe']",50249732,"<p>I believe that you CAN merge on an index. You just seem to have used the wrong syntax. Instead of specifying <code>on</code> you should try using <code>left_index</code> and <code>right_index</code>. </p>

<p>See the <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer"">documentation for merges here</a></p>
",Pandas merging dataframes similar columns index I two dataframes I happened set timeStamp index df set index timeStamp inplace True df value timeStamp df value timeStamp I wanted make dataframe comparing values time shared I tried combined df pd merge df real df fc timeStamp inner got key error So instead merging two dataframes index I kept dataframes without timeStamp index For example df I used instead merging timeStamp value Then I able merge new df set shown I also set index timestamp later timeStamp value x value question Why I merge column name specified index I wanted set merge new dataframe,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
50263851,2018-05-10,2018,3,Performing operations on grouped rows in python,"<p>I have a dataframe where pic_code value may repeat. If it repeats, I want to set the variable ""keep"" to ""t"" for the pic_code that is closest to its mpe_wgt.</p>

<p>For example, the second pic_code has ""keep"" set to t since it has the ""weight"" closest to its corresponding ""mpe_weight"". My code results in ""keep"" staying 'f' for all and ""diff"" staying ""100"" for all.</p>

<pre><code>df['keep']='f'
df['diff']=100

def cln_df(data):
    if pd.unique(data['mpe_wgt']).shape==(1,):
        data['keep'][0:1]='t'
    elif pd.unique(data['mpe_wgt']).shape!=(1,): 
        data['diff']=abs(data['weight']-(data['mpe_wgt']/100))
        data['keep'][data['diff']==min(data['diff'])]='t'
    return data

df=df.groupby('pic_code').apply(cln_df)
</code></pre>

<p>df before</p>

<pre><code>  pic_code      weight      mpe_wgt    keep    diff
  1234          45          34         f       100
  1234          32          23         f       100
  45344         54          35         f       100
  234           76          98         f       100
  234           65          12         f       100
</code></pre>

<p>df output should be</p>

<pre><code>  pic_code      weight      mpe_wgt    keep    diff
  1234          45          34         f       11
  1234          32          23         t       9
  45344         54          35         t       100
  234           76          98         t       22
  234           65          12         f       53
</code></pre>

<p>I'm fairly new to python so please keep the solutions as simple as possible. I really want to make my method work so please don't get too fancy. Thanks in advance for your help.</p>
","['python', 'pandas', 'dataframe']",50263912,"<p>This is one way. Note I am using Boolean values <code>True</code> / <code>False</code> in place of strings <code>""t""</code> and <code>""f""</code>. This is just good practice.</p>

<p>Note that all the below operations are vectorised, while <code>groupby.apply</code> with a custom function certainly is not.</p>

<p><strong>Setup</strong></p>

<pre><code>print(df)

   pic_code  weight  mpe_wgt
0      1234      45       34
1      1234      32       23
2     45344      54       35
3       234      76       98
4       234      65       12
</code></pre>

<p><strong>Solution</strong></p>

<pre><code># calculate difference
df['diff'] = (df['weight'] - df['mpe_wgt']).abs()

# sort by pic_code, then by diff
df = df.sort_values(['pic_code', 'diff'])

# define keep column as True only for non-duplicates by pic_code
df['keep'] = ~df.duplicated('pic_code')
</code></pre>

<p><strong>Result</strong></p>

<pre><code>print(df)

   pic_code  weight  mpe_wgt  diff   keep
3       234      76       98    22   True
4       234      65       12    53  False
1      1234      32       23     9   True
0      1234      45       34    11  False
2     45344      54       35    19   True
</code></pre>
",Performing operations grouped rows python I dataframe pic code value may repeat If repeats I want set variable keep pic code closest mpe wgt For example second pic code keep set since weight closest corresponding mpe weight My code results keep staying f diff staying df keep f df diff def cln df data pd unique data mpe wgt shape data keep elif pd unique data mpe wgt shape data diff abs data weight data mpe wgt data keep data diff min data diff return data df df groupby pic code apply cln df df pic code weight mpe wgt keep diff f f f f f df output pic code weight mpe wgt keep diff f f I fairly new python please keep solutions simple possible I really want make method work please get fancy Thanks advance help,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
50282540,2018-05-10,2018,2,Count number of values in nested lists inside a dictionary,"<p>I have the following dictionary:</p>

<pre><code>d= {
'2018-01-01': [[1.0],        [2.0],        [3.0]],
'2018-01-02': [[4.0],        [5.0],        [6.0]],
'2018-01-03': [[7.0],        [8.0],        [9.0]],
'2018-01-04': [[10.0],       [11.0],       [12.0]],
'2018-01-31': [[13.0, 14.0], [15.0, 16.0], [17.0]]
}
</code></pre>

<p>My aim is to count the number of values (floats in this case), assuming some of these 'cells' might contain more than one value (e.g.: values for <code>'2018-01-31'</code>). In other words, I need: </p>

<pre><code>result=17
</code></pre>

<p>So far, I was able to count the number of 'cells' (sublists), but I can't find out how can I count several values inside a single 'cell'. I tried:</p>

<pre><code>cols=len(d[[k for k in d.keys() if d[k]==max(d.values(),key=len)][0]])
cells = [[1 for j in range(cols)] for i in range(len(d))]
result = len([item for sublist in cells for item in sublist])

print(str(result))
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>15
</code></pre>

<p>Thank you very much in advance.</p>
","['python', 'list', 'dictionary']",50282570,"<p><strong>Option 1</strong><br>
100% flattening + <code>len</code></p>

<pre><code>&gt;&gt;&gt; len([k for i in d.values() for j in i for k in j])
17
</code></pre>

<hr>

<p><strong>Option 2</strong><br>
<code>sum</code> + <code>len</code> + partial flattening<br>
Use a <code>sum</code> with a generator comprehension. 3-list form:</p>

<pre><code>&gt;&gt;&gt; sum(len([*a, *b, *c]) for (a, b, c) in d.values())
17
</code></pre>

<p>For a the generic solution (with more/less than 3 sublists), use <code>itertools.chain</code>:</p>

<pre><code>&gt;&gt;&gt; from itertools import chain
&gt;&gt;&gt; sum(len(list(chain.from_iterable(v))) for v in d.values())
17
</code></pre>
",Count number values nested lists inside dictionary I following dictionary My aim count number values floats case assuming cells might contain one value e g values In words I need result So far I able count number cells sublists I find I count several values inside single cell I tried cols len k k keys k max values key len cells j range cols range len result len item sublist cells item sublist print str result Output Thank much advance,"startoftags, python, list, dictionary, endoftags",python python3x list endoftags,python list dictionary,python python3x list,0.67
50296097,2018-05-11,2018,8,How to initialize a SimpleNamespace from a dict,"<p>I'm sure this must be simple, but I could not find the answer.</p>

<p>I have a dictionary like:</p>

<pre><code>d = {'a': 1, 'b':2}
</code></pre>

<p>I'd like to access that via dot notation, like: <code>d.a</code></p>

<p>The <a href=""https://docs.python.org/3/library/types.html"" rel=""noreferrer""><code>SimpleNamespace</code></a> is designed for this, but I cannot just pass the dict into the SimpleNamespace constructor. 
I get the error: <code>TypeError: no positional arguments expected</code></p>

<p>How do I initialize the SimpleNamespace from a dictionary?</p>
","['python', 'python-3.x', 'dictionary']",50296116,"<p>Pass in the dictionary using the <a href=""https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists"" rel=""noreferrer""><code>**kwargs</code> call syntax</a> to unpack your dictionary into separate arguments:</p>

<pre><code>SimpleNamespace(**d)
</code></pre>

<p>This applies each key-value pair in <code>d</code> as a separate keyword argument. </p>

<p>Conversely, the closely releated <a href=""https://docs.python.org/3/tutorial/controlflow.html#keyword-arguments"" rel=""noreferrer""><code>**kwargs</code> parameter definition</a> in the <code>__init__</code> method of the class definition <a href=""https://docs.python.org/3/library/types.html#types.SimpleNamespace"" rel=""noreferrer"">shown in the Python documentation</a> captures all keyword arguments passed to the class into a single dictionary again.</p>

<p>Demo:</p>

<pre><code>&gt;&gt;&gt; from types import SimpleNamespace
&gt;&gt;&gt; d = {'a': 1, 'b':2}
&gt;&gt;&gt; sn = SimpleNamespace(**d)
&gt;&gt;&gt; sn
namespace(a=1, b=2)
&gt;&gt;&gt; sn.a
1
</code></pre>
",How initialize SimpleNamespace dict I sure must simple I could find answer I dictionary like b I like access via dot notation like The SimpleNamespace designed I cannot pass dict SimpleNamespace constructor I get error TypeError positional arguments expected How I initialize SimpleNamespace dictionary,"startoftags, python, python3x, dictionary, endoftags",python python3x list endoftags,python python3x dictionary,python python3x list,0.67
50485492,2018-05-23,2018,3,combine DataFrame MultiIndex to string column,"<p>I have following DataFrame:</p>

<pre><code>df = pd.DataFrame([[1,2,3], [11,22,33]], columns = ['A', 'B', 'C'])
df.set_index(['A', 'B'], inplace=True)

        C
A  B     
1  2    3
11 22  33
</code></pre>

<p>How I make additional 'text' column that will be string combination of the MultiIndex.</p>

<p>Without removing my index!</p>

<p>For example:</p>

<pre><code>        C    D
A  B            
1  2    3    1_2
11 22  33  11_22
</code></pre>
","['python', 'pandas', 'dataframe']",50486304,"<p>Perhaps a simple list comprehension might help i.e </p>

<pre><code>df['new'] = ['_'.join(map(str,i)) for i in df.index.tolist()]

        C    new
A  B            
1  2    3    1_2
11 22  33  11_22
</code></pre>
",combine DataFrame MultiIndex string column I following DataFrame df pd DataFrame columns A B C df set index A B inplace True C A B How I make additional text column string combination MultiIndex Without removing index For example C D A B,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
50577585,2018-05-29,2018,2,Python Django get distinct queryset by month from a DateField,"<pre><code>class MyModel(models.Model):
    TRANSACTION_TYPE_CHOICES = (
        ('p', 'P'),
        ('c', 'C'),
    )
    status = models.CharField(max_length=50, choices=TRANSACTION_TYPE_CHOICES, default='c')
    user = models.ForeignKey(User, db_index=True, on_delete=models.CASCADE,related_name='user_wallet')
    date = models.DateField(auto_now=True)
    amount = models.FloatField(null=True, blank=True)


    def __unicode__(self):
        return str(self.id)
</code></pre>

<p>I am a fresher in <code>Python django</code> and have a little knowledge in <code>Django Rest Framework</code>.</p>

<p>I have a model like above and I want to filter the date field by month and  get distinct queryset by month.....Is there any default way to do this...</p>

<p>Thanks in advance</p>
","['python', 'django', 'django-rest-framework']",50577708,"<p>you can use <a href=""https://docs.djangoproject.com/en/2.0/ref/models/database-functions/#django.db.models.functions.TruncMonth"" rel=""nofollow noreferrer"">TruncMonth</a> with <a href=""https://docs.djangoproject.com/en/2.0/topics/db/aggregation/#aggregating-annotations"" rel=""nofollow noreferrer"">annotations</a></p>

<pre><code>from django.db.models.functions import TruncMonth

MyModel.objects.annotate(
    month=TruncMonth('date')
).filter(month=YOURVALUE).values('month').distinct()
</code></pre>

<p>or if you need only filter date by month with distinct you can use <a href=""https://docs.djangoproject.com/en/2.0/ref/models/querysets/#month"" rel=""nofollow noreferrer"">__month</a> option</p>

<pre><code>MyModel.objects.filter(date__month=YOURVALUE).distinct()
</code></pre>

<p><strong>Older django</strong>
you can use <a href=""https://docs.djangoproject.com/en/2.0/ref/models/querysets/#django.db.models.query.QuerySet.extra"" rel=""nofollow noreferrer"">extra</a>, example for postgres </p>

<pre><code>MyModel.objects.extra(
    select={'month': ""EXTRACT(month FROM date)""},
    where=[""EXTRACT(month FROM date)=%s""],
    params=[5]
    # CHANGE 5 on you value
    ).values('month').distinct()
</code></pre>
",Python Django get distinct queryset month DateField class MyModel models Model TRANSACTION TYPE CHOICES p P c C status models CharField max length choices TRANSACTION TYPE CHOICES default c user models ForeignKey User db index True delete models CASCADE related name user wallet date models DateField auto True amount models FloatField null True blank True def unicode self return str self id I fresher Python django little knowledge Django Rest Framework I model like I want filter date field month get distinct queryset month Is default way Thanks advance,"startoftags, python, django, djangorestframework, endoftags",python django djangorestframework endoftags,python django djangorestframework,python django djangorestframework,1.0
50708959,2018-06-05,2018,2,How to merge two columns from a dataframe,"<p>I'm having a csv file that I did manipulations on it, I merged two different files using <code>pandas.merge</code>. But now, I have some columns that are called for example column_x and column_y.
So I want to merge these two columns to obtain one column.
By knowing that the two columns can be represented like this:</p>

<pre><code>Column_x     Column_y
       2          2.1 
                    3
   4.322            4
       5
</code></pre>

<p>And then obtain either one of these two results:</p>

<pre><code>Column  
     2  
     3    
 4.322 
     5
</code></pre>

<p>Or:</p>

<pre><code>Column
     2.1 
     3
     4
     5
</code></pre>

<p>Either of these two results will satisfy me</p>
","['python', 'pandas', 'csv']",50709029,"<pre><code>df.replace('', np.nan).Column_x.fillna(df.Column_y)
</code></pre>

<p>or </p>

<pre><code>df.replace('', np.nan).Column_y.fillna(df.Column_x)
</code></pre>

<p>Respectively, these output:</p>

<pre><code>0    2.000
1    3.000
2    4.322
3    5.00
</code></pre>

<p>and </p>

<pre><code>0    2.1
1    3.0
2    4.0
3    5.0
</code></pre>

<p>If your blank ""cells"" in your dataframe are already <code>NaN</code>, you can go ahead and omit the <code>.replace('', np.nan)</code></p>
",How merge two columns dataframe I csv file I manipulations I merged two different files using pandas merge But I columns called example column x column So I want merge two columns obtain one column By knowing two columns represented like Column x Column And obtain either one two results Column Or Column Either two results satisfy,"startoftags, python, pandas, csv, endoftags",python pandas dataframe endoftags,python pandas csv,python pandas dataframe,0.67
50709396,2018-06-05,2018,2,KeyError When Selecting a Column,"<p>I'm trying to call a field and getting an error. </p>

<p>Calling any field in this table gets the same error. </p>

<pre><code>df_ret = pd.read_csv('Retention Data.csv', na_values=['.'])
print(df_ret[""Cohorts Retention Rate""])
</code></pre>

<p>This is what my data looks like:</p>

<p><a href=""https://i.stack.imgur.com/2fyR8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2fyR8.png"" alt=""enter image description here""></a></p>

<p>This is the error I get:</p>

<blockquote>
  <p>KeyError: 'Cohorts Retention Rate'</p>
</blockquote>

<p>Using:</p>

<pre><code>2.7.13 |Anaconda, Inc.| (default, Sep 21 2017, 17:38:20) 
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]
</code></pre>
","['python', 'pandas', 'dataframe']",50709591,"<p>There appears to be whitespace in your column names. You can remove whitespace as follows:</p>

<pre><code>df_ret.columns = df_ret.columns.str.strip()
</code></pre>

<p>You can then access the series as expected:</p>

<pre><code>print(df_ret['Cohorts Retention Rate'])
</code></pre>
",KeyError When Selecting Column I trying call field getting error Calling field table gets error df ret pd read csv Retention Data csv na values print df ret Cohorts Retention Rate This data looks like This error I get KeyError Cohorts Retention Rate Using Anaconda Inc default Sep GCC Compatible Clang tags RELEASE final,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
50789490,2018-06-11,2018,2,Pandas Plot Grouped Bar Chart by Time,"<p>I have a table below that I would like to plot in a grouped bar chart. I want the x-axis to be <code>time_period</code> and the y-axis to be <code>death_licenses</code> and I want to be categorized by <code>civic_centre</code>. As you can see, for every distinct <code>time_period</code>, there is four categorical options in <code>civic_centre</code>.</p>

<pre><code>+-------------+--------------+----------------+
| time_period | civic_centre | death_licenses |
+-------------+--------------+----------------+
| 2011-01-01  | ET           |            410 |
| 2011-01-01  | NY           |            681 |
| 2011-01-01  | SC           |            674 |
| 2011-01-01  | TO           |            297 |
| 2011-02-01  | ET           |            307 |
| 2011-02-01  | NY           |            388 |
| 2011-02-01  | SC           |            407 |
| 2011-02-01  | TO           |            223 |
| 2011-03-01  | ET           |            349 |
| 2011-03-01  | NY           |            655 |
| 2011-03-01  | SC           |            400 |
| 2011-03-01  | TO           |            185 |
| 2011-04-01  | ET           |            373 |
| 2011-04-01  | NY           |            640 |
| 2011-04-01  | SC           |            457 |
| 2011-04-01  | TO           |             42 |
+-------------+--------------+----------------+
</code></pre>

<p>Here's the work that I did so far:</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

class Utility:

@staticmethod
def read_csv(csv, number_columns=[], categorical_columns=[], date_columns=[], drop_columns_if_empty=[], drop_duplicate_columns=[]):
    df = pd.read_csv(csv, na_values=['--', ''])
    df.rename(columns=lambda x: x.strip().replace('""', '').replace(' ', '_').replace('__', '_').lower(),
              inplace=True)
    df[number_columns] = df[number_columns].astype(str).replace({'[\$,)]': '', ' ': '', '[(]': '-'}, regex=True)
    for col in number_columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    for col in date_columns:
        df[col] = pd.to_datetime(df[col], errors='coerce')

    df.dropna(subset=drop_columns_if_empty, how='any', inplace=True)
    df = df.applymap(lambda x: x.strip() if type(x) is str else x)
    if (len(drop_duplicate_columns) &gt; 1):
        df = df.drop_duplicates(drop_duplicate_columns, keep='last')
    for col in categorical_columns:
        df[col] = pd.Categorical(df[col])

    return df

df = Utility.read_csv('http://opendata.toronto.ca/clerk/registry.service/death.csv', number_columns=['death_licenses'], categorical_columns=['place_of_death', 'civic_centre'], date_columns=['time_period'])
df.sort_values(['time_period', 'civic_centre'], ascending=[True, False])
df2 = df.groupby(['time_period', 'civic_centre'])['death_licenses'].agg('sum').reset_index()
</code></pre>

<p>I want to do something like this:
<a href=""https://i.stack.imgur.com/jYUYN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jYUYN.png"" alt=""grouped bar graph""></a></p>
","['python', 'pandas', 'matplotlib']",50790183,"<p>here are a couple of plotting options (if I have understood you correctly), I prefer the first myself.</p>

<pre><code>% matplotlib inline

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from pandas import Series, DataFrame

civics = ([i for i in ['ET', 'NY', 'SC', 'TO']] * 4)
civics.sort()

data = DataFrame({
    'time_period': Series([pd.to_datetime('2011-0{}-01'.format(i)) for i in 
range(1, 5)] * 4),
    'civic_centre': Series(civics),
    'death_licenses': Series(np.random.randint(400, 500, 16))
})

# As four series.

pd.pivot_table(data, index = 'time_period', columns = 'civic_centre', values 
= 'death_licenses').plot();

# As a grouped bar plot.

pd.pivot_table(data, index = 'civic_centre', columns = 'time_period', values 
= 'death_licenses').plot(kind = 'bar')
</code></pre>

<p>Gives these two plots:</p>

<p><a href=""https://i.stack.imgur.com/P6WWa.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/P6WWa.png"" alt=""enter image description here""></a></p>
",Pandas Plot Grouped Bar Chart Time I table I would like plot grouped bar chart I want x axis time period axis death licenses I want categorized civic centre As see every distinct time period four categorical options civic centre time period civic centre death licenses ET NY SC TO ET NY SC TO ET NY SC TO ET NY SC TO Here work I far import numpy np import pandas pd import matplotlib pyplot plt class Utility staticmethod def read csv csv number columns categorical columns date columns drop columns empty drop duplicate columns df pd read csv csv na values df rename columns lambda x x strip replace replace replace lower inplace True df number columns df number columns astype str replace regex True col number columns df col pd numeric df col errors coerce col date columns df col pd datetime df col errors coerce df dropna,"startoftags, python, pandas, matplotlib, endoftags",python pandas matplotlib endoftags,python pandas matplotlib,python pandas matplotlib,1.0
50818352,2018-06-12,2018,2,How to create a new column with distinct Id and different values of two other columns?,"<p>I have dtaframe which looks like this:</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({'ID': [332, 332, 332, 315, 315, 315, 315, 315, 310, 310], \
                    'Name': ['Alex', 'Alex', 'Alex', 'Sara', 'Sara', 'Sara', 'Sara', 'Sara', 'Franck','Franck'], \
                    'Shift': ['Day', 'Day', 'Night', 'Day', 'Night', 'Night', 'Day', 'Day', 'Night', 'Night']})
df

Output
    ID  Name    Shift
0   332 Alex    Day
1   332 Alex    Day
2   332 Alex    Night
3   315 Sara    Day
4   315 Sara    Night
5   315 Sara    Night
6   315 Sara    Day
7   315 Sara    Day
8   310 Franck  Night
9   310 Franck  Night
</code></pre>

<p>And I want to add a new column like this:</p>

<pre><code>    ID  Name    Shift   Two_Shift
0   332 Alex    Day     Yes
1   332 Alex    Day     Yes
2   332 Alex    Night   Yes
3   315 Sara    Day     Yes
4   315 Sara    Night   Yes
5   315 Sara    Night   Yes
6   315 Sara    Day     Yes
7   315 Sara    Day     Yes
8   310 Franck  Night   No
9   310 Franck  Night   No
</code></pre>

<p>For this I have tried this code:</p>

<pre><code>df['Two_Shift'] = np.where((df['ID'] == df['ID']) &amp; (df['Shift'] != df['Shift']), 'Yes', 'No')
</code></pre>

<p>but it does not work. </p>

<p>Thanks!</p>
","['python', 'pandas', 'numpy']",50818442,"<p>IIUC:</p>

<pre><code>df['Two_Shift'] = np.where(df.groupby('ID')['Shift'].transform('nunique') == 2,'Yes','No')
</code></pre>

<p>Output:</p>

<pre><code>    ID    Name  Shift Two_Shift
0  332    Alex    Day       Yes
1  332    Alex    Day       Yes
2  332    Alex  Night       Yes
3  315    Sara    Day       Yes
4  315    Sara  Night       Yes
5  315    Sara  Night       Yes
6  315    Sara    Day       Yes
7  315    Sara    Day       Yes
8  310  Franck  Night        No
9  310  Franck  Night        No
</code></pre>
",How create new column distinct Id different values two columns I dtaframe looks like import pandas pd import numpy np df pd DataFrame ID Name Alex Alex Alex Sara Sara Sara Sara Sara Franck Franck Shift Day Day Night Day Night Night Day Day Night Night df Output ID Name Shift Alex Day Alex Day Alex Night Sara Day Sara Night Sara Night Sara Day Sara Day Franck Night Franck Night And I want add new column like ID Name Shift Two Shift Alex Day Yes Alex Day Yes Alex Night Yes Sara Day Yes Sara Night Yes Sara Night Yes Sara Day Yes Sara Day Yes Franck Night No Franck Night No For I tried code df Two Shift np df ID df ID amp df Shift df Shift Yes No work Thanks,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
51135148,2018-07-02,2018,2,Make bot automatically quit after it has done its work,"<p>I'd like to use my bot not as a daemon that runs forever, but as a kind of ""shell script"". So it should automatically quit after it has done its work.</p>

<p>My approach so far in Python 3.5 / Python 3.6:</p>

<pre><code>import discord
TOKEN = '&lt;redacted&gt;'
client = discord.Client()

@client.event
async def on_ready():
    for member in client.get_all_members():
        print(member.display_name)
    # please, please quit now!

client.run(TOKEN)
</code></pre>

<p>What I want here is, that the script should just quit after printing all the members' display_names, so I can further process them elsewhere. At the moment it DOES print the desired information, but continues to run forever.</p>
","['python', 'discord', 'discord.py']",51135462,"<p>Answering my question myself:</p>

<p>Just replace the line <code># please, please quit now!</code> with <code>await client.logout()</code> does the job.</p>
",Make bot automatically quit done work I like use bot daemon runs forever kind shell script So automatically quit done work My approach far Python Python import discord TOKEN lt redacted gt client discord Client client event async def ready member client get members print member display name please please quit client run TOKEN What I want script quit printing members display names I process elsewhere At moment DOES print desired information continues run forever,"startoftags, python, discord, discordpy, endoftags",python discord discordpy endoftags,python discord discordpy,python discord discordpy,1.0
51190740,2018-07-05,2018,4,Reduce multi-dimensional array of strings along axis in Numpy,"<p>Does Numpy implement some function for reducing multi-dimensional array of strings? I know it offers some features for string concatenation of multiple arrays, but I haven't found anything about string reduction.</p>

<p>Let's say I have a 2-D array of strings:</p>

<pre><code>np.array([['a', 'b', 'c'],['e','f','g']])
</code></pre>

<p>And I want to convert it to:</p>

<pre><code>np.array(['a b c','e f g'])
</code></pre>

<p>Is there better way, than using a for loop, such as:</p>

<pre><code>old_strings = np.array([['a', 'b', 'c'],['e','f','g']])
new_strings = np.array([])
for s in old_strings:
    new_strings = np.append(new_strings, (' '.join(s)))
</code></pre>
","['python', 'arrays', 'numpy']",51192426,"<p>Using regular string operations is better than using <code>np.char.join</code>.</p>

<pre><code>&gt;&gt;&gt; arr = np.array([['a', 'b', 'c'],['e','f','g']])
&gt;&gt;&gt; np.array([' '.join(i) for i in arr])
array(['a b c', 'e f g'], dtype='&lt;U5')
</code></pre>

<p>Will be faster than <code>np.char.join</code></p>

<pre><code>%timeit np.array([' '.join(i) for i in arr])
8.69 Âµs Â± 30 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)

%timeit np.char.join(' ', arr)
14.6 Âµs Â± 86.1 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
</code></pre>

<p>On a much larger array:</p>

<pre><code>arr = np.repeat(arr, 10000).reshape(-1, 3)

%timeit np.array([' '.join(i) for i in arr])
54.2 ms Â± 596 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

%timeit np.char.join(' ', arr)
72.3 ms Â± 2.36 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
</code></pre>
",Reduce multi dimensional array strings along axis Numpy Does Numpy implement function reducing multi dimensional array strings I know offers features string concatenation multiple arrays I found anything string reduction Let say I D array strings np array b c e f g And I want convert np array b c e f g Is better way using loop old strings np array b c e f g new strings np array old strings new strings np append new strings join,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
51352415,2018-07-15,2018,2,Django delete models from db,"<p>I have such issue, I created models, than deleted them. I did make migrations and migrate, but when I want to create Group of Users in the Available permissions I see this models and permissions for them. </p>

<p>Models Group, Person, Membership<br>
<a href=""https://i.stack.imgur.com/AJ5cB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AJ5cB.png"" alt=""models Group, Person, Membership""></a></p>

<p>How to remove them?</p>
","['python', 'django', 'django-models']",51352630,"<p>Delete also from <code>auth_permission</code> table. As you see in the picture there are permissions. You should remove from this table. <a href=""https://i.stack.imgur.com/Ynegt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ynegt.png"" alt=""enter image description here""></a></p>
",Django delete models db I issue I created models deleted I make migrations migrate I want create Group Users Available permissions I see models permissions Models Group Person Membership How remove,"startoftags, python, django, djangomodels, endoftags",python django djangorestframework endoftags,python django djangomodels,python django djangorestframework,0.67
51398604,2018-07-18,2018,3,Updating the values of one column in a pandas dataframe based on input proportion,"<p>Some column of pandas dataframe having no. unique value (say 4). Those value will have some initial proportion across all rows. I need to change that giving input of desired proportion. Let's say I have 100 rows and a Column name <code>city</code> having values in given proportion.</p>

<pre><code>Mumbai  30%
Kolkata 40%
Chennai 10%
Delhi   20%
</code></pre>

<p>Now i need to change the values across the column so that I get my desired proportion (or structure of data).</p>

<pre><code>Mumbai  20%
Kolkata 50%
Chennai 20%
Delhi   10%
</code></pre>

<p>While doing this <strong>I want to make sure that when changing the value of rows having city <code>Mumbai</code> from 25% to 20% I should keep 20% of them same as before and only alter the rest 5% i.e. not to clear all the values and populate according to new proportion</strong> I am trying to do this in pandas dataframe. Any help is appreciated. </p>

<p>Edit: So say my column looks like this with 10 rows. </p>

<pre><code>1   Mumbai
2   Mumbai 
3   Mumbai
4   Kolkata
5   Kolkata
6   Kolkata
7   Kolkata
8   Chennai
9   Delhi
10  Delhi
</code></pre>

<p>Now i would like it to be changed some thing like given the above change. </p>

<pre><code>1   Mumbai
2   Mumbai 
3   Kolkata
4   Kolkata
5   Kolkata
6   Kolkata
7   Kolkata
8   Chennai
9   Chennai
10  Delhi
</code></pre>

<p>I didn't made it random. new rows having Mumbai are subset of the last one.</p>
","['python', 'pandas', 'dataframe']",51415651,"<pre><code>from collections import Counter 
import pandas as pd

def set_proportion(df, column, new_proportion):
    proportion = (df[column].value_counts() / df.shape[0]).to_dict()
    prop_diff = {key: new_proportion[key] - proportion[key] for key in new_proportion}
    prop_diff_cnt = {key: int(round(value * df.shape[0])) for key, value in prop_diff.items()}
    to_add = {key: diff for key, diff in prop_diff_cnt.items() if diff &gt; 0}
    to_remove = {key: diff for key, diff in prop_diff_cnt.items() if diff &lt; 0}
    to_add = sum(([key] * diff for key, diff in to_add.items()), [])
    to_remove = sum(([key] * -diff for key, diff in to_remove.items()), [])
    # group to counter to do updates to the dataframe in bulk, one update per each *unique* replacement pair
    counter = Counter(list(zip(to_remove, to_add)))
    for (remove, add), count in counter.items():
        df.loc[df[df[column] == remove].iloc[-count:].index, column] = add    

df = pd.DataFrame([""Mumbai""] * 3 + [""Kolkata""] * 4 + [""Chennai""] + [""Delhi""] * 2, columns=['city']) 
print df 
    city
0   Mumbai
1   Mumbai 
2   Mumbai
3   Kolkata
4   Kolkata
5   Kolkata
6   Kolkata
7   Chennai
8   Delhi
9   Delhi    

set_proportion(df, 'city', {'Mumbai': 0.2, 'Kolkata': 0.5, 'Chennai': 0.2, 'Delhi': 0.1})
print df 
      city
0  Mumbai 
1  Mumbai 
2  Chennai
3  Kolkata
4  Kolkata
5  Kolkata
6  Kolkata
7  Chennai
8  Delhi  
9  Kolkata

# set_proportion modifies the original dataframe, so we need to reinitialize it
df = pd.DataFrame([""Mumbai""] * 3 + [""Kolkata""] * 4 + [""Chennai""] + [""Delhi""] * 2, columns=['city']) 

set_proportion(df, 'city', {'Mumbai': 0.2, 'Kolkata': 0.1, 'Chennai': 0.3, 'Delhi': 0.4})
print df 

      city
0  Mumbai 
1  Mumbai 
2  Delhi  
3  Kolkata
4  Delhi  
5  Chennai
6  Chennai
7  Chennai
8  Delhi  
9  Delhi  
</code></pre>
",Updating values one column pandas dataframe based input proportion Some column pandas dataframe unique value say Those value initial proportion across rows I need change giving input desired proportion Let say I rows Column name city values given proportion Mumbai Kolkata Chennai Delhi Now need change values across column I get desired proportion structure data Mumbai Kolkata Chennai Delhi While I want make sure changing value rows city Mumbai I keep alter rest e clear values populate according new proportion I trying pandas dataframe Any help appreciated Edit So say column looks like rows Mumbai Mumbai Mumbai Kolkata Kolkata Kolkata Kolkata Chennai Delhi Delhi Now would like changed thing like given change Mumbai Mumbai Kolkata Kolkata Kolkata Kolkata Kolkata Chennai Chennai Delhi I made random new rows Mumbai subset last one,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
51452773,2018-07-21,2018,2,"Pandas , Sum Values of two columns from 2 DFs","<p>I have two data-frames as shown below (sample data) : </p>

<p>df1:</p>

<pre><code>Key1    Val1    Val2    Val3
A       2       1       3
B       2       2       2
C       3       3       4
D       4       4       2
E       5       6       5
</code></pre>

<p>df2:</p>

<pre><code>Key1    Seasionality
A       1.2
B       -1
C       1
D       1.2
E       1.5
</code></pre>

<p>How can we add <code>Seasonality</code> to <code>df1 (val1, val2 , val3) where df1.key1 = df2.key1</code>. </p>

<p>Desired Output:</p>

<pre><code>Key1    Val1    Val2    Vale
A       3.2     2.2     4.2
B       1       1       1
C       4       4       5
D       5.2     5.2     3.2
E       6.5     7.5     6.5
</code></pre>

<p>Previously i had similar requirement where i need to add columns from two different df with same keys but they had similar column names, so i was able to use :</p>

<pre><code>pd.concat([df1,df2]).groupby(level=0).sum()
</code></pre>

<p>But can not think of a solution to above problem. </p>
","['python', 'python-3.x', 'pandas']",51452808,"<p>One possible solution is to align the keys temporarily:</p>

<pre><code>df1.set_index('Key1', inplace=True)
df2.set_index('Key1', inplace=True)
df1 = df1.add(df2['Seasionality'], axis=0)
df1.reset_index(inplace=True)
</code></pre>
",Pandas Sum Values two columns DFs I two data frames shown sample data df Key Val Val Val A B C D E df Key Seasionality A B C D E How add Seasonality df val val val df key df key Desired Output Key Val Val Vale A B C D E Previously similar requirement need add columns two different df keys similar column names able use pd concat df df groupby level sum But think solution problem,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
51459489,2018-07-21,2018,2,Python fill missing data,"<p>So I have a table that is kind of data like this</p>

<pre><code>theta    phi    x   y

0       0      1   2
0       1      2   3
--------------------
90      360    4   5
</code></pre>

<p>theta values runs from 0 to 90 and for each theta phi runs from 0 to 360,
but some of the phi s are missing here and there in the table and I have to fill those 
values. I'm trying to use pandas for this job like this,</p>

<pre><code>import pandas as pd
cols=['theta','phi','x','y']
data = pd.read_csv('data.dat', sep="" |\t"", header=None,names=cols,engine='python')


def fill_up(i):
    df=data[data['theta']==i]
    df.set_index('phi',inplace=True)
    df= df.reindex(range(0,361)).reset_index()
    df=df[cols]
    df.interpolate(inplace=True)
    return df

df=pd.concat([fill_up(i) for i in xrange(0,91)])
df.to_csv(""new.txt"",sep=' ', index=False, header=False)
</code></pre>

<p>as you can see I'm creating a seperate data frame for each theta values and concatnating them finally. Is there any way to achieve this without creating a different dataframe or achieving this more efficiently? Also should I take care for any memory overflow as the datafile can be several mb?</p>
","['python', 'pandas', 'dataframe']",51459639,"<p>I think you can do it by setting 'theta' and 'phi' as index with <code>set_index</code>, then <code>reindex</code> with <code>pd.MultiIndex.from_product</code> with all the values of 'theta' and 'phi' you expect, fill nan values with <code>interpolate</code> and finally <code>reset_index</code> such as:</p>

<pre><code>new_data = (data.set_index(['theta','phi'])
                 .reindex(pd.MultiIndex.from_product( [range(91),range(361)],
                                                      names=['theta','phi']))
                 .interpolate().reset_index())
</code></pre>

<p>Note the <code>range(91)</code> and <code>range(361)</code> to create all the values for theta and phi.</p>

<p>EDIT for blank line, you can do:</p>

<pre><code>new_data = (data.set_index(['theta','phi'])
                 .reindex(pd.MultiIndex.from_product( [range(91),range(362)],
                                                      names=['theta','phi']))
                 .interpolate().reset_index())
new_data.loc[new_data['phi'] ==361] = ''
</code></pre>

<p>Note that the range for phi is <code>range(362)</code> to add a row that you make ""blank"" with the next command line, replacing all the rows where <code>phi = 361</code> by blank</p>

<p>Or, with the <code>new_data</code> as in the original solution, you can create a blank dataframe with specific indexes that you can <code>concat</code> to new_data and <code>sort_index</code> after</p>

<pre><code>blanck_frame = pd.DataFrame(data='', columns=new_data.columns
                            index=new_data.loc[new_data['phi'] ==360].index+0.5)
new_data = pd.concat([new_data,blanck_frame]).sort_index()
# you can add  .reset_index(drop=True) at the end if you want integer indexes
</code></pre>
",Python fill missing data So I table kind data like theta phi x theta values runs theta phi runs phi missing table I fill values I trying use pandas job like import pandas pd cols theta phi x data pd read csv data dat sep header None names cols engine python def fill df data data theta df set index phi inplace True df df reindex range reset index df df cols df interpolate inplace True return df df pd concat fill xrange df csv new txt sep index False header False see I creating seperate data frame theta values concatnating finally Is way achieve without creating different dataframe achieving efficiently Also I take care memory overflow datafile several mb,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
51485353,2018-07-23,2018,3,Get count of duplicated values per category/group in pandas python,"<p>I have a df1 like this:</p>

<pre><code>Type     Name     Identifier     Number     Amount
 A        xx          0001         12        0.89
          xx          0001         56        0.78
          zz          0002         33        0.56
          yy          0020         44        0.45
          yy          0020         67        0.45
 B        ww          0300         12        0.34
          ww          0300         54        0.1
          kk          0900         43        0.2
</code></pre>

<p>I want to get the count of the duplicated identifier per type such that the resulting dataframe now looks like</p>

<pre><code> Type     Count_Dups      Ave. Amount  
  A         2                2.345  
  B         1                0.44
</code></pre>

<p>where ave. amount is the sum of all duplicated values / count of duplicated values. (example: A = (0.89+0.78+0.45+0.45)/2)</p>

<p>should I use a for loop? is groupby enough?</p>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",51485549,"<p>IIUC, you can use this method.  Filter the dataframe down to the duplicates, then groupby with nunique and sum, lastly, divide the two columns.</p>

<pre><code>df_out = df1[df1.duplicated(subset=['Type','Identifier'], keep=False)]\
             .groupby('Type')['Identifier','Amount']\
             .agg({'Identifier':'nunique','Amount':'sum'})\
             .rename(columns={'Identifier':'Count_Dups'})

df_out['Ave. Amount'] = df_out['Amount']  / df_out['Count_Dups']

print(df_out.reset_index())
</code></pre>

<p>Output:</p>

<pre><code>  Type  Count_Dups  Amount  Ave. Amount
0    A           2    2.57        1.285
1    B           1    0.44        0.440
</code></pre>
",Get count duplicated values per category group pandas python I df like Type Name Identifier Number Amount A xx xx zz yy yy B ww ww kk I want get count duplicated identifier per type resulting dataframe looks like Type Count Dups Ave Amount A B ave amount sum duplicated values count duplicated values example A I use loop groupby enough,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas dataframe pandasgroupby,python pandas dataframe,0.87
51554553,2018-07-27,2018,2,Sum up a field in the queryset in Django,"<p>models.py</p>

<pre><code>class QaCommission(models.Model):
    user = models.ForeignKey(PlUser, on_delete=models.CASCADE, blank=True, null=True, related_name='user_commission')
    ref = models.ForeignKey(PlUser, on_delete=models.CASCADE, blank=True, null=True, related_name='ref_commission')
    price = models.FloatField(blank=True, null=True)
    pct = models.FloatField(blank=True, null=True)
    commission = models.FloatField(blank=True, null=True)
    status = models.IntegerField(blank=True, null=True, default=0)
</code></pre>

<p>serializers.py</p>

<pre><code>class QaCommissionSerializer(serializers.ModelSerializer):
    class Meta:
        model = QaCommission
        fields = '__all__'
</code></pre>

<p>views.py</p>

<pre><code>class QaCommissionList(viewsets.ModelViewSet):
    queryset = QaCommission.objects.all()
    serializer_class = QaCommissionSerializer
</code></pre>

<p>If we are filtering ref=60 in this view, results show like this:</p>

<pre><code>{
""count"": 18,
""next"": ""http://127.0.0.1:8008/api/qacommission/?ref=60&amp;page=2"",
""previous"": null,
""results"": [
    {
        ""id"": 1,
        ""price"": 20.0,
        ""pct"": 0.1,
        ""commission"": 2.0,
        ""status"": 1,
        ""user"": 7,
        ""ref"": 60
    },
    {
        ""id"": 2,
        ""price"": 10.0,
        ""pct"": 0.1,
        ""commission"": 1.0,
        ""status"": 1,
        ""user"": 7,
        ""ref"": 60
    },
    ......
    ......
    ......
    {
        ""id"": 10,
        ""price"": 15.0,
        ""pct"": 0.1,
        ""commission"": 1.5,
        ""status"": 1,
        ""user"": 7,
        ""ref"": 60
    }
]
}
</code></pre>

<p>I wanna sum up all the ""commission"" field in the results and attach the sum to the original queryset (maybe next to ""count"":18), as shown above, there are 18 commission need to be count.</p>

<p>How could I implement this? Need your help, thanks!</p>
","['python', 'django', 'django-rest-framework']",51554829,"<p>Try to override <code>list()</code> method of <code>ModelViewset</code> as,</p>

<pre><code>class QaCommissionList(viewsets.ModelViewSet):
    queryset = QaCommission.objects.all()
    serializer_class = QaCommissionSerializer

    <b>def list(self, request, *args, **kwargs):
        response = super().list(request, *args, **kwargs)
        response.data['sum'] = sum([data.get('commission', 0) for data in response.data['results']])
        return response</b></code></pre>

<p>This answer sum-up the <code>commision</code> <strong>from particular page</strong> and displaying it
<br><br>
<strong>UPDATE</strong></p>

<pre><code><b>from django.db.models import Sum</b>


class QaCommissionList(viewsets.ModelViewSet):
    queryset = QaCommission.objects.all()
    serializer_class = QaCommissionSerializer

    <b>def list(self, request, *args, **kwargs):
        response = super().list(request, *args, **kwargs)
        if 'ref' in request.GET and request.GET['ref']:
            response.data['sum'] = QaCommission.objects.filter(ref=int(request.GET['ref'])
                                                               ).aggregate(sum=Sum('commission'))['sum']
        return response</b></code></pre>

<p>The abouve answer will return the Whole sum of <code>commission</code> column w.r.t the filter (irrespective of the pagination)<br><br>
Thanks <a href=""https://stackoverflow.com/users/41316/bruno-desthuilliers"">@bruno</a> for mentioning such a valid point</p>
",Sum field queryset Django models py class QaCommission models Model user models ForeignKey PlUser delete models CASCADE blank True null True related name user commission ref models ForeignKey PlUser delete models CASCADE blank True null True related name ref commission price models FloatField blank True null True pct models FloatField blank True null True commission models FloatField blank True null True status models IntegerField blank True null True default serializers py class serializers ModelSerializer class Meta model QaCommission fields views py class QaCommissionList viewsets ModelViewSet queryset QaCommission objects serializer class If filtering ref view results show like count next http api qacommission ref amp page previous null results id price pct commission status user ref id price pct commission status user ref id price pct commission status user ref I wanna sum commission field results attach sum original queryset maybe next count shown commission need count How could I implement,"startoftags, python, django, djangorestframework, endoftags",python django djangorestframework endoftags,python django djangorestframework,python django djangorestframework,1.0
51571350,2018-07-28,2018,4,proper way to replace NaN value from another dataframe on column match in pandas,"<p>I'm newbie to pandas, and trying to  replace a column value (NaN) in df1 with df2 with column value match. And facing the following error. </p>

<pre><code>df1
unique_col  |  Measure
944537          NaN
7811403         NaN 
8901242114307     1 

df2
unique_col  |  Measure
944537           18
7811403          12 
8901242114307    17.5



df1.loc[(df1.unique_col.isin(df2.unique_col) &amp;
                       df1.Measure.isnull()), ['Measure']] = df2[['Measure']]
</code></pre>

<p>I have a two dataframes with 3 million records and on performing below operation facing the following error:  </p>

<blockquote>
  <p>ValueError: cannot reindex from a duplicate axis</p>
</blockquote>
","['python', 'pandas', 'numpy']",51571565,"<p>You way to easily fill nans is to use <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html"" rel=""noreferrer""><code>fillna</code></a> function. In your case, if you have the dfs as (notice the indexes)</p>

<pre><code>    unique_col      Measure
0   944537          NaN
1   7811403         NaN
2   8901242114307   1.0


    unique_col      Measure
0   944537          18.0
1   7811403         12.0
2   8901242114307   17.5
</code></pre>

<p>You can simply</p>

<pre><code>&gt;&gt;&gt; df.fillna(df2)


    unique_col       Measure
0   944537           18.0
1   7811403          12.0
2   8901242114307    1.0
</code></pre>

<p>If indexes are <em>not</em> the same as the above, you can <em>set them to be the same</em> and use the same function</p>

<pre><code>df = df.set_index('unique_col')
df.fillna(df2.set_index('unique_col'))
</code></pre>
",proper way replace NaN value another dataframe column match pandas I newbie pandas trying replace column value NaN df df column value match And facing following error df unique col Measure NaN NaN df unique col Measure df loc df unique col isin df unique col amp df Measure isnull Measure df Measure I two dataframes million records performing operation facing following error ValueError cannot reindex duplicate axis,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
51580878,2018-07-29,2018,3,Python pandas add multiple columns to dataframe with map,"<p>I have a Pandas dataframe that looks as follows.</p>

<pre><code>player  count1  count2  text
A       1       1       X   
A       2       1       Y   
A       3       1       Z   
A       4       2       D   
A       5       2       E   
B       1       1       F   
B       2       2       G   
B       3       2       H   
B       4       2       J   
</code></pre>

<p>Column <code>player</code> contains names, <code>count1</code> is a cumulative sum, column <code>count2</code> contains other counts, and column <code>text</code> contains some text.</p>

<p>I now want to create 2 new columns that contain the values of <code>count1</code> and <code>text</code> where the column <code>count2</code> first contains the value <code>2</code>.</p>

<p>Hence, the result should look like this:</p>

<pre><code>player  count1  count2  text    new new2
A       1       1       X       4   D
A       2       1       Y       4   D
A       3       1       Z       4   D
A       4       2       D       4   D
A       5       2       E       4   D
B       1       1       F       2   G
B       2       2       G       2   G
B       3       2       H       2   G
B       4       2       J       2   G
</code></pre>

<p>I already asked a similar question, but where only, one new column should be added [here][1].</p>

<p>The answer was to use <code>map</code> by <code>Series</code>.</p>

<pre><code>s = df[df['count2'] == 2].drop_duplicates(['player']).set_index('player')['count1']
df['new'] = df['player'].map(s)
</code></pre>

<p>However, when I try to apply this approach to two columns, it does not work.</p>

<p>I try it like this:</p>

<pre><code>s = df[df['count2'] == 2].drop_duplicates(['player']).set_index('player')[['count1', 'text']]
df[['new', 'new2']] = df['player'].map(s)
</code></pre>

<p>This yields the following error:</p>

<blockquote>
  <p>TypeError: 'DataFrame' object is not callable</p>
</blockquote>

<p>How can I get this to work?</p>
","['python', 'pandas', 'dataframe']",51581114,"<p>You can filter on count2 == 2, drop duplicates by player, then merge the result back to your original DF on player, eg:</p>

<pre><code>new = df.merge(
    df.loc[df.count2 == 2, ['player', 'count1', 'text']]
    .drop_duplicates(subset=['player']), 
    on='player'
)
</code></pre>

<p>Which gives you:</p>

<pre><code>  player  count1_x  count2 text_x  count1_y text_y
0      A         1       1      X         4      D
1      A         2       1      Y         4      D
2      A         3       1      Z         4      D
3      A         4       2      D         4      D
4      A         5       2      E         4      D
5      B         1       1      F         2      G
6      B         2       2      G         2      G
7      B         3       2      H         2      G
8      B         4       2      J         2      G
</code></pre>
",Python pandas add multiple columns dataframe map I Pandas dataframe looks follows player count count text A X A Y A Z A D A E B F B G B H B J Column player contains names count cumulative sum column count contains counts column text contains text I want create new columns contain values count text column count first contains value Hence result look like player count count text new new A X D A Y D A Z D A D D A E D B F G B G G B H G B J G I already asked similar question one new column added The answer use map Series df df count drop duplicates player set index player count df new df player map However I try apply approach two columns work I try like df df count drop duplicates player set index player count,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
51675612,2018-08-03,2018,4,Pygame Snake Eating Itself,"<p>I've recently started using pygame and I'm following TheNewBostons's youtube tutorial. Here's my main game loop:</p>

<pre><code>def game_loop():
    global direction
    global tdirection

    lead_x=display_width/2
    lead_y=display_height/2

    block_size=10
    change_x=10
    change_y=0

    game_exit=False
    GameOver=False
    main_menu=False


    snakelist=[]
    snakelength=1

    applethickness=20

    RandAppleX=round(random.randrange(0, display_width-applethickness )/10.0)*10.0
    RandAppleY=round(random.randrange(0, display_height-applethickness )/10.0)*10.0



    while not game_exit:


        while GameOver==True:
            game_display.fill(white)
            message_screen('You lost, Press Q to quit or C to retry.', red)
            pygame.display.update()
            for event in pygame.event.get():
                if event.type==pygame.KEYDOWN:
                    if event.key==pygame.K_q:
                        loss.play()
                        game_exit=True
                        GameOver= False


                    elif event.key==pygame.K_c:
                        direction='right'
                        game_loop()


                if event.type==pygame.QUIT:
                    game_exit=True
                    GameOver= False




        for event in pygame.event.get():


            if event.type==pygame.QUIT:
                game_exit=True

            if event.type==pygame.KEYDOWN:
                if event.key==pygame.K_a:

                    change_x=-block_size
                    direction='left'

                    change_x=-block_size                        
                    change_y=0
                elif event.key==pygame.K_d:
                    direction='right'

                    change_x=block_size
                    change_y=0  
                elif event.key==pygame.K_w:
                    direction='up'

                    change_y=-block_size
                    change_x=0
                elif event.key==pygame.K_s:
                    direction='down'

                    change_y=block_size
                    change_x=0

                elif event.key==pygame.K_ESCAPE:
                    pause()





        if lead_x&gt;=display_width or lead_x&lt;0 or lead_y&gt;=display_height or lead_y&lt;0:
            loss.play()
            GameOver=True



        lead_x+=change_x
        lead_y+=change_y




        clock.tick(FPS)





        snakehead=[]

        snakehead.append(lead_x)
        snakehead.append(lead_y)

        snakelist.append(snakehead)


        if len(snakelist)&gt;snakelength:
            del snakelist[0]

        for segment in snakelist[:-1]:
            if segment==snakehead:
                loss.play()
                GameOver=True






        game_display.fill(white)
        game_display.blit(aimg, (RandAppleX, RandAppleY))   

        snake(snakelist, block_size)
        score(snakelength-1)




        if lead_x&gt;= RandAppleX and lead_x&lt;= RandAppleX+applethickness and lead_y&lt;= RandAppleY+applethickness and lead_y&gt;= RandAppleY:


            RandAppleX=round(random.randrange(0, display_width-applethickness )/10.0)*10.0
            RandAppleY=round(random.randrange(0, display_height-applethickness )/10.0)*10.0
            snakelength+=1
            effect.play()





        pygame.display.update()




    pygame.quit()
    quit()
</code></pre>

<p>Now I need to prevent the snake from eating itself when its moving in a certain direction. For example then its moving to the right, pressing the A button on my keyboard would instantly make it eat itself and if its moving up pressing the S button would do the same thing. Is there a way to prevent this from happening?</p>
","['python', 'python-3.x', 'pygame']",51675958,"<p>An easy solution could be to prevent the snake from backing into itself by checking that the direction it is moving in is not opposite the requested direction. This would involve a few extra checks. Something like this:</p>

<pre><code>        if event.type==pygame.KEYDOWN:
            if event.key==pygame.K_a and direction != 'right':

                change_x=-block_size
                direction='left'

                change_x=-block_size                        
                change_y=0
            elif event.key==pygame.K_d and direction != 'left':
                direction='right'

                change_x=block_size
                change_y=0  
            elif event.key==pygame.K_w and direction != 'down':
                direction='up'

                change_y=-block_size
                change_x=0
            elif event.key==pygame.K_s and direction != 'right':
                direction='down'

                change_y=block_size
                change_x=0
</code></pre>
",Pygame Snake Eating Itself I recently started using pygame I following TheNewBostons youtube tutorial Here main game loop def game loop global direction global tdirection lead x display width lead display height block size change x change game exit False GameOver False main menu False snakelist snakelength applethickness RandAppleX round random randrange display width applethickness RandAppleY round random randrange display height applethickness game exit GameOver True game display fill white message screen You lost Press Q quit C retry red pygame display update event pygame event get event type pygame KEYDOWN event key pygame K q loss play game exit True GameOver False elif event key pygame K c direction right game loop event type pygame QUIT game exit True GameOver False event pygame event get event type pygame QUIT game exit True event type pygame KEYDOWN event key pygame K change x block size direction left change x block,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
51817919,2018-08-13,2018,3,Pandas iloc returns different range than loc,"<p>I am a bit confused of the iloc function of pandas, because I want to select a range of columns and the output is different than expected. The same will happen to row selection, so I wrote a little example:</p>

<pre><code>template = pd.DataFrame(
    {'Headline': ['Subheading', '', 'Animal', 'Tiger', 'Bird', 'Lion'],
     'Headline2': ['', 'Weight', 2017, 'group1', 'group2', 'group3'],
     'Headline3': ['', '', 2018, 'group1', 'group2', 'group3']
     })

     Headline Headline2 Headline3
0  Subheading                    
1                Weight          
2      Animal      2017      2018
3       Tiger    group1    group1
4        Bird    group2    group2
5        Lion    group3    group3
</code></pre>

<p>I want to select line 1 to line 2 with <code>print(template.loc[1:2])</code> the result is what I have expected:</p>

<pre><code>  Headline Headline2 Headline3
1             Weight          
2   Animal      2017      2018
</code></pre>

<p>If I do this <code>print(template.iloc[1:2])</code> I would think that I get the same result, but no:</p>

<pre><code>  Headline Headline2 Headline3
1             Weight          
</code></pre>

<p>I am a bit confused, because I expected the same behavior for both functions, but the output of both functions differ if I select a range (FROM:TO).<br>
It seems like using iloc needs to have the TO value +1 in order to have the same result as loc <code>print(template.iloc[1:3])</code>:</p>

<pre><code>  Headline Headline2 Headline3
1             Weight          
2   Animal      2017      2018
</code></pre>

<p>Can someone put some light on it?</p>
","['python', 'pandas', 'dataframe']",51818001,"<p>As it mentioned in <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer"">docs</a> for <code>loc</code>:</p>

<blockquote>
  <p>Warning: Note that contrary to usual python slices, both the start and
  the stop are included</p>
</blockquote>

<p>On the other hand, <code>iloc</code> do selects based on integer-location based indexing, so it doesn't include stop index.</p>
",Pandas iloc returns different range loc I bit confused iloc function pandas I want select range columns output different expected The happen row selection I wrote little example template pd DataFrame Headline Subheading Animal Tiger Bird Lion Headline Weight group group group Headline group group group Headline Headline Headline Subheading Weight Animal Tiger group group Bird group group Lion group group I want select line line print template loc result I expected Headline Headline Headline Weight Animal If I print template iloc I would think I get result Headline Headline Headline Weight I bit confused I expected behavior functions output functions differ I select range FROM TO It seems like using iloc needs TO value order result loc print template iloc Headline Headline Headline Weight Animal Can someone put light,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
51898826,2018-08-17,2018,7,Converting object column in pandas dataframe to datetime,"<p>I have an object column in a pandas dataframe in the format dd/mm/yyyy, that I want to convert with to_datetime.</p>

<p>I tried to convert it to datetime using the below:</p>

<pre><code>df['Time stamp'] = pd.to_datetime(df['Time stamp'], format= '%d/%m/%Y')
</code></pre>

<p>I get the following errors:</p>

<pre><code>TypeError: Unrecognized value type: &lt;class 'str'&gt;
ValueError: unconverted data remains:  
</code></pre>

<p>Does this mean that there is a blank row somewhere, I have checked the original csv and I cannot see one.</p>
","['python', 'pandas', 'datetime']",51899360,"<p>It means you have an extra space. Though <code>pd.to_datetime</code> is very good at parsing dates normally without any format specified, when you actually specify a format, it has to match EXACTLY.</p>
<p>You can likely solve your issue by adding <code>.str.strip()</code> to remove the extra whitespace before converting.</p>
<pre><code>import pandas as pd
df['Time stamp'] = pd.to_datetime(df['Time stamp'].str.strip(), format='%d/%m/%Y')
</code></pre>
<p>Alternatively, you can take advantage of its ability to parse various formats of dates by using the <code>dayfirst=True</code> argument</p>
<pre><code>df['Time stamp'] = pd.to_datetime(df['Time stamp'], dayfirst=True)
</code></pre>
<hr />
<h3>Example:</h3>
<pre><code>import pandas as pd
df = pd.DataFrame({'Time stamp': ['01/02/1988', '01/02/1988 ']})

pd.to_datetime(df['Time stamp'], format= '%d/%m/%Y')
</code></pre>
<blockquote>
<p>ValueError: unconverted data remains:</p>
</blockquote>
<pre><code>pd.to_datetime(df['Time stamp'].str.strip(), format='%d/%m/%Y')
#0   1988-02-01
#1   1988-02-01
#Name: Time stamp, dtype: datetime64[ns]

pd.to_datetime(df['Time stamp'], dayfirst=True)
#0   1988-02-01
#1   1988-02-01
#Name: Time stamp, dtype: datetime64[ns]
</code></pre>
",Converting object column pandas dataframe datetime I object column pandas dataframe format dd mm yyyy I want convert datetime I tried convert datetime using df Time stamp pd datetime df Time stamp format Y I get following errors TypeError Unrecognized value type lt class str gt ValueError unconverted data remains Does mean blank row somewhere I checked original csv I cannot see one,"startoftags, python, pandas, datetime, endoftags",python pandas numpy endoftags,python pandas datetime,python pandas numpy,0.67
52035467,2018-08-27,2018,3,fillna doesn&#39;t give the desired result,"<p>I'm trying to substitute NaTs in a pandas dataframe.</p>

<pre><code>orders.PAID_AT

0                       NaT
1                       NaT
2                       NaT
3                       NaT
4                       NaT
6                       NaT
7                       NaT
8                       NaT
9                       NaT
10                      NaT
11      2018-08-04 16:19:10
12      2018-08-04 16:19:10
13                      NaT
14                      NaT
15      2018-08-04 13:49:08
16      2018-08-04 13:49:08
18                      NaT
19                      NaT
20                      NaT
21      2018-08-04 12:41:48
</code></pre>

<p>The rows 0..10 need to be filled with value of row 11 etc. Somehow I can't get it right with:</p>

<pre><code>orders.PAID_AT.fillna(method='bfill', inplace=True)
</code></pre>

<p>I'm getting the same result as above. What am I missing here?</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",52035529,"<p>For avoid <a href=""https://stackoverflow.com/questions/21463589/pandas-chained-assignments/21463854#21463854"">chained assignments</a> assign back:</p>

<pre><code>orders.PAID_AT = orders.PAID_AT.bfill()
</code></pre>
",fillna give desired result I trying substitute NaTs pandas dataframe orders PAID AT NaT NaT NaT NaT NaT NaT NaT NaT NaT NaT NaT NaT NaT NaT NaT The rows need filled value row etc Somehow I get right orders PAID AT fillna method bfill inplace True I getting result What I missing,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
52048171,2018-08-27,2018,5,keras&#39;s binary_crossentropy loss function range,"<p>When I use keras's <code>binary_crossentropy</code> as the <a href=""https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L3270"" rel=""nofollow noreferrer"">loss function</a> (that calls <a href=""https://github.com/tensorflow/tensorflow/blob/600caf99897e82cd0db8665acca5e7630ec1a292/tensorflow/python/ops/nn_impl.py#L107"" rel=""nofollow noreferrer"">tensorflow's sigmoid_cross_entropy</a>, it seems to produce loss values only between <code>[0, 1]</code>. However, the equation itself </p>

<pre><code># The logistic loss formula from above is
#   x - x * z + log(1 + exp(-x))
# For x &lt; 0, a more numerically stable formula is
#   -x * z + log(1 + exp(x))
# Note that these two expressions can be combined into the following:
#   max(x, 0) - x * z + log(1 + exp(-abs(x)))
# To allow computing gradients at zero, we define custom versions of max and
# abs functions.
zeros = array_ops.zeros_like(logits, dtype=logits.dtype)
cond = (logits &gt;= zeros)
relu_logits = array_ops.where(cond, logits, zeros)
neg_abs_logits = array_ops.where(cond, -logits, logits)
return math_ops.add(
    relu_logits - logits * labels,
    math_ops.log1p(math_ops.exp(neg_abs_logits)), name=name)
</code></pre>

<p>implies that the range is from <code>[0, infinity)</code>. So is Tensorflow doing some sort of clipping that I'm not catching? Moreover, since it's doing <code>math_ops.add()</code> I'd assume it'd be for sure greater than 1. Am I right to assume that loss range can definitely exceed 1?</p>
","['python', 'tensorflow', 'machine-learning', 'keras', 'deep-learning']",52057662,"<p>The cross entropy function is indeed not bounded upwards. However it will only take on large values if the predictions are very wrong. Let's first look at the behavior of a randomly initialized network.</p>

<p>With random weights, the many units/layers will usually compound to result in the network outputing approximately uniform predictions. That is, in a classification problem with <code>n</code> classes you will get probabilities of around <code>1/n</code> for each class (0.5 in the two-class case). In this case, the cross entropy will be around the entropy of an n-class uniform distribution, which is <code>log(n)</code>, <em>under certain assumptions</em> (see below).</p>

<p>This can be seen as follows: The cross entropy for a single data point is <code>-sum(p(k)*log(q(k)))</code> where <code>p</code> are the true probabilities (labels), <code>q</code> are the predictions, <code>k</code> are the different classes and the sum is over the classes. Now, with hard labels (i.e. one-hot encoded) only a single <code>p(k)</code> is 1, all others are 0. Thus, the term reduces to <code>-log(q(k))</code> where <code>k</code> is now the correct class. If with a randomly initialized network <code>q(k) ~ 1/n</code>, we get <code>-log(1/n) = log(n)</code>.</p>

<p>We can also go of the definition of the cross entropy which is generally <code>entropy(p) + kullback-leibler divergence(p,q)</code>. If <code>p</code> and <code>q</code> are the same distributions (e.g. <code>p</code> is uniform when we have the same number of examples for each class, and <code>q</code> is around uniform for random networks) then the KL divergence becomes 0 and we are left with <code>entropy(p)</code>.</p>

<p>Now, since the training objective is usually to <em>reduce</em> cross entropy, we can think of <code>log(n)</code> as a kind of worst-case value. If it ever gets higher, there is probably something wrong with your model. Since it looks like you only have two classes (0 and 1), <code>log(2) &lt; 1</code> and so your cross entropy will generally be quite small.</p>
",keras binary crossentropy loss function range When I use keras binary crossentropy loss function calls tensorflow sigmoid cross entropy seems produce loss values However equation The logistic loss formula x x z log exp x For x lt numerically stable formula x z log exp x Note two expressions combined following max x x z log exp abs x To allow computing gradients zero define custom versions max abs functions zeros array ops zeros like logits dtype logits dtype cond logits gt zeros relu logits array ops cond logits zeros neg abs logits array ops cond logits logits return math ops add relu logits logits labels math ops log p math ops exp neg abs logits name name implies range infinity So Tensorflow sort clipping I catching Moreover since math ops add I assume sure greater Am I right assume loss range definitely exceed,"startoftags, python, tensorflow, machinelearning, keras, deeplearning, endoftags",python tensorflow keras endoftags,python tensorflow machinelearning keras deeplearning,python tensorflow keras,0.77
52059974,2018-08-28,2018,3,How to delete or destroy Label in tkinter?,"<p>This Tkinter code doesn't have a widget, just a label so it displays just a text on the screen so I want to destroy or delete the label after a certain time !. How can I do this when method label.after(1000 , label.destroy) doesn't work?</p>
<pre><code>import tkinter, win32api, win32con, pywintypes

label = tkinter.Label(text='Text on the screen', font=('Times New Roman','80'), fg='black', bg='white')
label.master.overrideredirect(True)
label.master.geometry(&quot;+250+250&quot;)
label.master.lift()
label.master.wm_attributes(&quot;-topmost&quot;, True)
label.master.wm_attributes(&quot;-disabled&quot;, True)
label.master.wm_attributes(&quot;-transparentcolor&quot;, &quot;white&quot;)

hWindow = pywintypes.HANDLE(int(label.master.frame(), 16))
exStyle = win32con.WS_EX_COMPOSITED | win32con.WS_EX_LAYERED | win32con.WS_EX_NOACTIVATE | win32con.WS_EX_TOPMOST | win32con.WS_EX_TRANSPARENT
win32api.SetWindowLong(hWindow, win32con.GWL_EXSTYLE, exStyle)

label.pack()

label.after(1000 , lambda: label.destroy())   #doesn't work anyway..

label.mainloop()
</code></pre>
","['python', 'python-3.x', 'tkinter']",52064361,"<p>In the code you have provided I believe the fix you are looking for is to change this:</p>

<pre><code>label.after(1000 , lambda: label.destroy())
</code></pre>

<p>To this:</p>

<pre><code>label.after(1000, label.master.destroy)
</code></pre>

<p>You need to destroy <code>label.master</code> (I am guessing this is actually a root window) because if you do not then you end up with a big box on the screen that is not transparent.</p>

<p>That said I am not sure why you are writing your app in this way. I guess it works and I was not actually aware you could do this but still I personally would write it using a root window to work with.</p>

<pre><code>import tkinter as tk

root = tk.Tk()


label = tk.Label(root, text='Text on the screen',
                 font=('Times New Roman','80'), fg='black', bg='white')
label.pack()

root.overrideredirect(True)
root.geometry(""+250+250"")
root.wm_attributes(""-topmost"", True)
root.wm_attributes(""-disabled"", True)
root.wm_attributes(""-transparentcolor"", ""white"")

root.after(1000, root.destroy)

root.mainloop()
</code></pre>
",How delete destroy Label tkinter This Tkinter code widget label displays text screen I want destroy delete label certain time How I method label label destroy work import tkinter win api win con pywintypes label tkinter Label text Text screen font Times New Roman fg black bg white label master overrideredirect True label master geometry quot quot label master lift label master wm attributes quot topmost quot True label master wm attributes quot disabled quot True label master wm attributes quot transparentcolor quot quot white quot hWindow pywintypes HANDLE int label master frame exStyle win con WS EX COMPOSITED win con WS EX LAYERED win con WS EX NOACTIVATE win con WS EX TOPMOST win con WS EX TRANSPARENT win api SetWindowLong hWindow win con GWL EXSTYLE exStyle label pack label lambda label destroy work anyway label mainloop,"startoftags, python, python3x, tkinter, endoftags",python python3x list endoftags,python python3x tkinter,python python3x list,0.67
52065501,2018-08-28,2018,3,Add column identifying original data frame when using pd.concat,"<p>I have a dictionary of data frames like the following:</p>

<pre><code>test = {'df1':pd.DataFrame({'col1':[3, 5, 1, 4], 'col2':[3, 5, 1, 4]}), 'df2':pd.DataFrame({'col1':[3, 5, 1, 4], 'col2':[3, 5, 1, 4]}), 'df3':pd.DataFrame({'col1':[3, 5, 1, 4], 'col2':[3, 5, 1, 4]}), 'df4':pd.DataFrame({'col1':[3, 5, 1, 4], 'col2':[3, 5, 1, 4]})]
</code></pre>

<p>I want to concatenate these data frames, but add a new column which gives 'identity' (dictionary key name) of which data frame the value came from. How can this be done? If I do <code>pd.concat(test.values())</code>, I get the concatenation which I want but no identity column.</p>

<p>Thanks,
Jack</p>
","['python', 'python-3.x', 'pandas']",52065592,"<p>Using <code>concat</code> with <code>keys</code></p>

<pre><code>pd.concat(test.values(),keys=test.keys())
Out[261]: 
       col1  col2
df1 0     3     3
    1     5     5
    2     1     1
    3     4     4
df2 0     3     3
    1     5     5
    2     1     1
    3     4     4
df3 0     3     3
    1     5     5
    2     1     1
    3     4     4
df4 0     3     3
    1     5     5
    2     1     1
    3     4     4
</code></pre>
",Add column identifying original data frame using pd concat I dictionary data frames like following test df pd DataFrame col col df pd DataFrame col col df pd DataFrame col col df pd DataFrame col col I want concatenate data frames add new column gives identity dictionary key name data frame value came How done If I pd concat test values I get concatenation I want identity column Thanks Jack,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
52122750,2018-08-31,2018,2,Using Text qualifier in pandas,"<p>I have a csv file having ^ as the text qualifier and , as delimiter. I can read it in MS access but unable to do so in pandas/python.</p>

<p>What is the option to provide text qualifier in pandas read_csv?</p>

<pre><code>col_list=pd.read_csv(""abc.csv"",nrows=1,sep=',').columns.tolist()

['^Run_Time^',
 '^Run_Desc^',
 '^As_Of_Date^']
</code></pre>
","['python', 'pandas', 'csv']",52122791,"<p><code>pd.read_csv(""abc.csv"",quotechar='^',nrows=1, sep=',')</code></p>
",Using Text qualifier pandas I csv file text qualifier delimiter I read MS access unable pandas python What option provide text qualifier pandas read csv col list pd read csv abc csv nrows sep columns tolist Run Time Run Desc As Of Date,"startoftags, python, pandas, csv, endoftags",python pandas numpy endoftags,python pandas csv,python pandas numpy,0.67
52132913,2018-09-01,2018,2,Discord.py --&gt; channel.mention,"<p>I'm trying to make a bot for a discord server that simply listens for specific messages, deletes them and then refers the user to a different text channel (in a clickable link by mentioning it)</p>

<p>Here's what I have now:</p>

<pre><code>import Discord
import asyncio


client = discord.Client()


@client.event
async def on_message(message):
    msg = '{0.author.mention}\nWrong text channel\nUse '.format(message)
    if message.content.startswith('!p'):
        await client.delete_message(message)
        await client.send_message(message.channel, msg)
    return


client.run('')
</code></pre>

<p>Ideally, I'd also want to search through a list with <code>startswith()</code> instead of just <code>('!p')</code> &amp; to ignore all messages from a specific text channel as well but I'm not sure how to do those either</p>
","['python', 'discord', 'discord.py']",52133159,"<p>Sure, just add <code>text_channel = client.get_channel('1234567890')</code> and reference its mention with <code>text_channel.mention</code> (where <code>1234567890</code> is the id of the channel you want to link to)</p>

<p>So the code would end up looking something like this </p>

<pre><code>@client.event
async def on_message(message):
  text_channel = client.get_channel('1234567890')
  msg = '{0.author.mention}\nWrong text channel\nUse {1.mention}'.format(message,text_channel)
  if message.content.startswith('!p'):
      await client.delete_message(message)
      await client.send_message(message.channel, msg)
  return
</code></pre>

<p>Regarding your second question, you could do something like this</p>

<pre><code>  arr = ['!p','!a','!b']
  for a in arr:
    if message.content.startswith(a):
      break
  else:
    return
</code></pre>

<p>and remove the <code>if message.content.startswith('!p'):</code> altogether</p>

<p>To ignore a specific channel just do <code>if message.channel.id == ""9876543210"":</code> at the top of the function (<code>9876543210</code> is the id of the channel you want to ignore commands from)<br>
With those changes the code looks like this</p>

<pre><code>@client.event
async def on_message(message):
  if message.channel.id == ""9876543210"":
    return
  arr = ['!p','!a','!b']
  for a in arr:
    if message.content.startswith(a):
      break
  else:
    return
  text_channel = client.get_channel('1234567890')
  msg = '{0.author.mention}\nWrong text channel\nUse {1.mention}'.format(message,text_channel)
  await client.delete_message(message)
  await client.send_message(message.channel, msg)
  return
</code></pre>
",Discord py gt channel mention I trying make bot discord server simply listens specific messages deletes refers user different text channel clickable link mentioning Here I import Discord import asyncio client discord Client client event async def message message msg author mention nWrong text channel nUse format message message content startswith p await client delete message message await client send message message channel msg return client run Ideally I also want search list startswith instead p amp ignore messages specific text channel well I sure either,"startoftags, python, discord, discordpy, endoftags",python discord discordpy endoftags,python discord discordpy,python discord discordpy,1.0
52213220,2018-09-06,2018,2,Subtract two columns in pandas dataframe,"<p>I have two columns in pandas dataframe that represent hour of the day in 24 hour format, i.e., <code>18:00:00</code>. Both of them are in object datatype and I want to find the difference in hours of the two columns. For eg. difference between <code>18:00:00</code> and <code>17:00:00</code> should come out as 1. I tried using <code>to_timedelta</code> function but it returns 'no units specified' error even after I specify unit as 'h'.</p>

<pre><code>d = {'col1': ['18:00:00', '19:00:00'], 'col2': ['17:00:00', '17:00:00']}
df = pd.DataFrame(data=d)
df

df['col1']-df['col2']
</code></pre>

<p>Can anyone assist in this? Thanks in Advance.</p>
","['python', 'pandas', 'dataframe']",52213360,"<p>You have to convert <code>to_datetime</code> first:</p>

<pre><code>df.col1 = pd.to_datetime(df.col1)
df.col2 = pd.to_datetime(df.col2)

df.col1.sub(df.col2)
</code></pre>

<p></p>

<pre><code>0   01:00:00
1   02:00:00
dtype: timedelta64[ns]
</code></pre>

<p>If you just want the result in hours, divide by another <code>Timedelta</code>:</p>

<pre><code>df.col1.sub(df.col2).div(pd.Timedelta('1h'))
</code></pre>

<p></p>

<pre><code>0    1.0
1    2.0
dtype: float64
</code></pre>
",Subtract two columns pandas dataframe I two columns pandas dataframe represent hour day hour format e Both object datatype I want find difference hours two columns For eg difference come I tried using timedelta function returns units specified error even I specify unit h col col df pd DataFrame data df df col df col Can anyone assist Thanks Advance,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
52298695,2018-09-12,2018,2,Finding the mean of adjacent elements (row and column) in a DataFrame,"<p>I have an image from which I have created a DataFrame based on its pixel intensity. From this, I would like to create a grid where I find the mean intensity within each square in that grid, represented by 2x2 pixels. This will be done in order to capture larger areas of intensities, in order to distinguish them from background noise. (I thought it would be good to include this in order to provide context.)</p>

<p>In the DataFrame, this would translate into finding the mean of 4 values from a group of 2 adjacent rows and columns. </p>

<p>So, to illustrate the problem, let's say we have the following DataFrame:</p>

<pre><code>df=pd.DataFrame({'A':(np.linspace(1,4,num=4)),'B':(np.linspace(5,8,num=4)),'C':(np.linspace(9,12,num=4)), 'D':(np.linspace(13,16,num=4))})
</code></pre>

<p>From that, we want to create a DataFrame that corresponds to the mean of each square. In this case, it would correspond to the following (i.e. for example, 3 would be the mean of the 2x2 square with the values (1,5,2,6), 11.5 would be the mean of (9, 13, 10, 14):</p>

<pre><code>df_mean=DataFrame({'A':pd.Series([3,11.5]),'B':pd.Series([5.5,13.5])})
</code></pre>

<p>If the question is still unclear, imagine taking the original DataFrame and drawing a vertical line and a horizontal line across the middle. This would thus yield 4 boxes. Within each of the 4 boxes you will find 4 values. I would like to calculate the mean of each box and insert it onto a new DataFrame that consists of the means of the boxes.</p>

<p>PS: Unfortunately, I do not yet know how to display the DataFrames themselves, rather than just the code. The print function didn't work. I hope that is not too bothersome.</p>

<p>Thank you very much!</p>
","['python', 'pandas', 'dataframe']",52298826,"<p>You can do this very efficiently using the underlying <code>numpy</code> array:</p>

<pre><code>def square_mean(arr, y, x):
    yy, xx = arr.shape
    vals = arr.reshape(y, yy//y, x, xx//x).mean((1,3))
    return vals

pd.DataFrame(square_mean(df.values, 2, 2))
</code></pre>

<p></p>

<pre><code>     0     1
0  3.5  11.5
1  5.5  13.5
</code></pre>

<hr>

<p>This solution works all because of some clever reshaping of the array, here is how the reshaping works:</p>

<pre><code>yy, xx = arr.shape
vals = arr.reshape(2, yy//2, 2, xx//2)
print(vals)
</code></pre>

<p></p>

<pre><code>[[[[ 1.  5.]
   [ 9. 13.]]

  [[ 2.  6.]
   [10. 14.]]]


 [[[ 3.  7.]
   [11. 15.]]

  [[ 4.  8.]
   [12. 16.]]]]
</code></pre>

<p>As you can see, the array has been reshaped into chunks, that we can then use to calculate the mean.</p>

<hr>

<p>This solution will scale to all input sizes, simply select <code>x</code> as the number of chunks along the x-axis, and <code>y</code> for the number of chunks along the y-axis:</p>

<pre><code>df = pd.DataFrame(np.random.randint(1, 5, (10, 10)))

   0  1  2  3  4  5  6  7  8  9
0  1  3  4  2  3  3  3  2  1  2
1  3  3  4  1  3  4  4  4  1  3
2  2  3  2  2  4  4  1  1  1  1
3  1  2  1  2  1  3  1  1  2  3
4  2  2  3  4  3  2  4  3  4  2
5  3  3  1  4  2  1  2  3  1  3
6  2  1  3  4  3  2  3  4  3  4
7  2  3  4  2  1  1  1  1  3  2
8  4  3  2  2  2  2  2  1  3  3
9  3  2  1  2  1  3  4  2  4  4
</code></pre>

<p></p>

<p>We can divide into any number of chunks:</p>

<pre><code>square_mean(df.values, 2, 2)
</code></pre>

<p></p>

<pre><code>array([[2.44, 2.4 ],
       [2.4 , 2.48]])

square_mean(df.values, 5, 5)
</code></pre>

<p></p>

<pre><code>array([[2.5 , 2.75, 3.25, 3.25, 1.75],
       [2.  , 1.75, 3.  , 1.  , 1.75],
       [2.5 , 3.  , 2.  , 3.  , 2.5 ],
       [2.  , 3.25, 1.75, 2.25, 3.  ],
       [3.  , 1.75, 2.  , 2.25, 3.5 ]])
</code></pre>
",Finding mean adjacent elements row column DataFrame I image I created DataFrame based pixel intensity From I would like create grid I find mean intensity within square grid represented x pixels This done order capture larger areas intensities order distinguish background noise I thought would good include order provide context In DataFrame would translate finding mean values group adjacent rows columns So illustrate problem let say following DataFrame df pd DataFrame A np linspace num B np linspace num C np linspace num D np linspace num From want create DataFrame corresponds mean square In case would correspond following e example would mean x square values would mean df mean DataFrame A pd Series B pd Series If question still unclear imagine taking original DataFrame drawing vertical line horizontal line across middle This would thus yield boxes Within boxes find values I would like calculate mean box insert onto new,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
52327944,2018-09-14,2018,8,Django Rest Framework model serializer: Set all fields to read only except one,"<p>The model that I'm using has a lot of fields. I want to be able to set all the fields to be read only except for one i.e. I want to allow only one particular field to be writable. Is there a shortcut to do this?
I'm only aware of using ""read_only_fields=('x','y') and I really don't want to type out all the fields especially if I'm going to make changes to the models later. ""exclude ="" also doesn't apply in this case.</p>
","['python', 'django', 'django-rest-framework']",52328292,"<p>Try to override serializer's <code>__init__</code> method:</p>

<pre><code>def __init__(self, *args, **kwargs):
    super(UserSerializer, self).__init__(*args, **kwargs)
    for field in self.fields:
        if field != 'some_required_filed':
            self.fields[field].read_only = True
</code></pre>
",Django Rest Framework model serializer Set fields read except one The model I using lot fields I want able set fields read except one e I want allow one particular field writable Is shortcut I aware using read fields x I really want type fields especially I going make changes models later exclude also apply case,"startoftags, python, django, djangorestframework, endoftags",python django djangorestframework endoftags,python django djangorestframework,python django djangorestframework,1.0
52494999,2018-09-25,2018,3,Pandas - count since last transaction,"<p>I have a dataframe (call it <code>txn_df</code>) that contains monetary transaction records, here are the significant columns in this problem:</p>

<pre><code>txn_year    txn_month   custid  withdraw    deposit
2011        4           123     0.0         100.0
2011        5           123     0.0         0.0
2011        6           123     0.0         0.0
2011        7           123     50.1        0.0
2011        8           123     0.0         0.0
</code></pre>

<p>Assume also that we have multiple customers here. <code>withdraw</code> and <code>deposit</code> 0.0 value for both means no transaction has taken place. What I want to do is to produce a new column that indicates how many months has occurred since there was a transaction.  Something similar to this:</p>

<pre><code>txn_year    txn_month   custid  withdraw    deposit     num_months_since_last_txn
2011        4           123     0.0         100.0       0
2011        5           123     0.0         0.0         1
2011        6           123     0.0         0.0         2           
2011        7           123     50.1        0.0         3
2011        8           123     0.0         0.0         1
</code></pre>

<p>The only solution so far that I can think of is to produce a new column <code>has_txn</code> (which is either 1/0 or True/False) when either one of <code>withdraw</code> and <code>deposit</code> has value > 0.0 but I can't continue from there.</p>
","['python', 'pandas', 'dataframe']",52495492,"<p>one way to solve this problem,</p>

<pre><code>df['series'] =  df[['withdraw','deposit']].ne(0).sum(axis=1)
m = df['series']&gt;=1
</code></pre>

<p>As @Chris A commented,</p>

<pre><code>m = df[['withdraw','deposit']].gt(0).any(axis=1) #replacement for above snippet,

df['num_months_since_last_txn'] = df.groupby(m.cumsum()).cumcount()
df.loc[df['num_months_since_last_txn']==0,'num_months_since_last_txn']=(df['num_months_since_last_txn']+1).shift(1).fillna(0)
print df
</code></pre>

<p>Output:</p>

<pre><code>   txn_year  txn_month  custid  withdraw  deposit
0      2011          4     123       0.0    100.0
1      2011          5     123       0.0      0.0
2      2011          6     123       0.0      0.0
3      2011          7     123      50.1      0.0
4      2011          8     123       0.0      0.0
   txn_year  txn_month  custid  withdraw  deposit  num_months_since_last_txn
0      2011          4     123       0.0    100.0                        0.0
1      2011          5     123       0.0      0.0                        1.0
2      2011          6     123       0.0      0.0                        2.0
3      2011          7     123      50.1      0.0                        3.0
4      2011          8     123       0.0      0.0                        1.0
</code></pre>

<p>Explanation:</p>

<ol>
<li>To get transaction happened or not use <code>ne</code> and sum to get values in binary.</li>
<li>when transaction is 1 create the series from 0,1,2...n using <code>groupby</code>, <code>cumsum</code>, <code>cumcount</code>. </li>
<li>rearrange the value for <code>0</code> using <code>.loc</code></li>
</ol>

<p>Note: May be I have added more complex to solving this problem. But It will give you an idea and approach to solve this problem.</p>

<p>Solution for considering customer Id,</p>

<pre><code>df=df.sort_values(by=['custid','txn_month'])
mask=~df.duplicated(subset=['custid'],keep='first')
m = df[['withdraw','deposit']].gt(0).any(axis=1)
df['num_months_since_last_txn'] = df.groupby(m.cumsum()).cumcount()
df.loc[df['num_months_since_last_txn']==0,'num_months_since_last_txn']=(df['num_months_since_last_txn']+1).shift(1)
df.loc[mask,'num_months_since_last_txn']=0
</code></pre>

<p>Sample Input:</p>

<pre><code>   txn_year  txn_month  custid  withdraw  deposit
0      2011          4     123       0.0    100.0
1      2011          5     123       0.0      0.0
2      2011          4    1245       0.0    100.0
3      2011          5    1245       0.0      0.0
4      2011          6     123       0.0      0.0
5      2011          7    1245      50.1      0.0
6      2011          7     123      50.1      0.0
7      2011          8     123       0.0      0.0
8      2011          6    1245       0.0      0.0
9      2011          8    1245       0.0      0.0
</code></pre>

<p>Sample Output:</p>

<pre><code>   txn_year  txn_month  custid  withdraw  deposit  num_months_since_last_txn
0      2011          4     123       0.0    100.0                        0.0
1      2011          5     123       0.0      0.0                        1.0
4      2011          6     123       0.0      0.0                        2.0
6      2011          7     123      50.1      0.0                        3.0
7      2011          8     123       0.0      0.0                        1.0
2      2011          4    1245       0.0    100.0                        0.0
3      2011          5    1245       0.0      0.0                        1.0
8      2011          6    1245       0.0      0.0                        2.0
5      2011          7    1245      50.1      0.0                        3.0
9      2011          8    1245       0.0      0.0                        1.0
</code></pre>

<p>Explanation for considering Customer ID,</p>

<ol>
<li>The above code works based on the interval between [1,1]. So to make the same format, sort df by cust_id and txn_month, For future you could add txn_year. </li>
<li>fillna(0), won't be work here, Because shift will not create NaN for next customer. To reset into 0 Find duplication of customer Id and take first value replace that into 0. </li>
</ol>
",Pandas count since last transaction I dataframe call txn df contains monetary transaction records significant columns problem txn year txn month custid withdraw deposit Assume also multiple customers withdraw deposit value means transaction taken place What I want produce new column indicates many months occurred since transaction Something similar txn year txn month custid withdraw deposit num months since last txn The solution far I think produce new column txn either True False either one withdraw deposit value I continue,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
52498848,2018-09-25,2018,2,Split Columns in pandas with str.split and keep values,"<p>So I am stuck with a problem here:</p>

<p>I have a pandas dataframe which looks like the following:</p>

<pre><code>ID Name    Value
0  Peter   21,2
1  Frank   24
2  Tom     23,21/23,60 
3  Ismael  21,2/ 21,54
4  Joe     23,1

and so on...
</code></pre>

<p>What I am trying to is to split the ""Value"" column by the slash forward (/) but keep all the values, which do not have this kind of pattern.</p>

<p>Like here:</p>

<pre><code>ID Name    Value
0  Peter   21,2
1  Frank   24
2  Tom     23,21
3  Ismael  21,2
4  Joe     23,1
</code></pre>

<p>How can I achieve this? I tried the str.split method but it's not giving me the solution I want. Instead, it returns NaN as can be seen in the following.</p>

<pre><code>My Code: df['Value']=df['value'].str.split('/', expand=True)[0]

Returns:

ID Name    Value
0  Peter   NaN
1  Frank   NaN
2  Tom     23,21
3  Ismael  21,2
4  Joe     Nan
</code></pre>

<p>All I need is the very first Value before the '/' is coming. </p>

<p>Appreciate any kind of help!</p>
","['python', 'pandas', 'dataframe']",52498876,"<p>Remove <code>expand=True</code> for return lists and add <code>str[0]</code> for select first value:</p>

<pre><code>df['Value'] = df['Value'].str.split('/').str[0]
print (df)
   ID    Name  Value
0   0   Peter   21,2
1   1   Frank     24
2   2     Tom  23,21
3   3  Ismael   21,2
4   4     Joe   23,1
</code></pre>

<p>If performance is important use list comprehension:</p>

<pre><code>df['Value'] = [x.split('/')[0] for x in df['Value']]
</code></pre>
",Split Columns pandas str split keep values So I stuck problem I pandas dataframe looks like following ID Name Value Peter Frank Tom Ismael Joe What I trying split Value column slash forward keep values kind pattern Like ID Name Value Peter Frank Tom Ismael Joe How I achieve I tried str split method giving solution I want Instead returns NaN seen following My Code df Value df value str split expand True Returns ID Name Value Peter NaN Frank NaN Tom Ismael Joe Nan All I need first Value coming Appreciate kind help,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
52579935,2018-09-30,2018,3,Remove duplicates from array and elements in matching positions in another array,"<p>I have two numpy array, I want to remove duplicate values from the first array (including the original value) and remove the items in the matching positions in the second array.</p>

<p>For example:</p>

<pre><code>a = [1, 2, 2, 3]
b = ['a', 'd', 'f', 'c']
</code></pre>

<p>Becomes:</p>

<pre><code>a = [1, 3]
b = ['a', 'c']
</code></pre>

<p>I need to do this efficiently and not use the naive solution which is time consuming </p>
","['python', 'arrays', 'numpy']",52580001,"<p>Here's one with <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html"" rel=""nofollow noreferrer""><code>np.unique</code></a> -</p>

<pre><code>unq,idx,c = np.unique(a, return_index=True, return_counts=True)
unq_idx = np.sort(idx[c==1])
a_out = a[unq_idx]
b_out = b[unq_idx]
</code></pre>

<p>Sample run -</p>

<pre><code>In [34]: a
Out[34]: array([1, 2, 2, 3])

In [35]: b
Out[35]: array(['a', 'd', 'f', 'c'], dtype='|S1')

In [36]: unq,idx,c = np.unique(a, return_index=1, return_counts=1)
    ...: unq_idx = idx[c==1]
    ...: a_out = a[unq_idx]
    ...: b_out = b[unq_idx]

In [37]: a_out
Out[37]: array([1, 3])

In [38]: b_out
Out[38]: array(['a', 'c'], dtype='|S1')
</code></pre>
",Remove duplicates array elements matching positions another array I two numpy array I want remove duplicate values first array including original value remove items matching positions second array For example b f c Becomes b c I need efficiently use naive solution time consuming,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
52701688,2018-10-08,2018,18,Pandas ImportError: matplotlib is required for plotting,"<p>Pandas does not recognize installed matplotlib library</p>

<h1>here is the code</h1>

<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))
ts = ts.cumsum()
ts.plot()
</code></pre>

<h1>error is</h1>

<pre><code>c:\users\xxxxx\appdata\local\programs\python\python36\lib\site-packages\pandas\plotting\_core.py in _raise_if_no_mpl()
     55     # TODO(mpl_converter): remove once converter is explicit
     56     if not _HAS_MPL:
---&gt; 57         raise ImportError(""matplotlib is required for plotting."")
     58 
     59    
ImportError: matplotlib is required for plotting.
</code></pre>
","['python', 'pandas', 'matplotlib']",52705465,"<p>Installing matplotlib before installing pandas again made it work.</p>
",Pandas ImportError matplotlib required plotting Pandas recognize installed matplotlib library code import pandas pd import numpy np import matplotlib pyplot plt matplotlib inline ts pd Series np random randn index pd date range periods ts ts cumsum ts plot error c users xxxxx appdata local programs python python lib site packages pandas plotting core py raise mpl TODO mpl converter remove converter explicit HAS MPL gt raise ImportError matplotlib required plotting ImportError matplotlib required plotting,"startoftags, python, pandas, matplotlib, endoftags",python pandas matplotlib endoftags,python pandas matplotlib,python pandas matplotlib,1.0
52976404,2018-10-24,2018,4,Replace all occurrences matching regular expression in the first word of a line,"<p>I would like to parse an ASCII file and escape ""."" characters in the first word of each line. Here is an example:</p>

<pre><code>line='DXa0.Xa1.a2 p1 p2 deviceName  a=157.585p b=54.46u $x=106.124 $y=107.996 $a=0'
</code></pre>

<p>I would like to produce the following</p>

<pre><code>DXa0\.Xa1\.a2 p1 p2 deviceName  a=157.585p b=54.46u $x=106.124 $y=107.996 $a=0
</code></pre>

<p>I could run the following for this example: </p>

<pre><code>re.sub(r""\."", '\\.', line, count=2 )
</code></pre>

<p>But that assumes 2 ""."" characters in the word, which is not guaranteed. The workaround I found for now is </p>

<pre><code>re.sub(r""\."", '\\.', line.split(' ', 1)[0]) + ' ' + line.split(' ', 1)[1]
</code></pre>

<p>It produces the expected results but that's not pretty, I'm sure there is a better way.</p>
","['python', 'regex', 'python-3.x']",52976588,"<p>You may achieve that with <code>re</code> using a regex to match the first word up to the first whitespace after the first streak of non-whitespace chars and a lambda expression as the replacement argument:</p>

<pre><code>re.sub(r""^\s*\S+"", lambda x: x.group().replace('.', r'\.'), s)
</code></pre>

<p>Here, <code>^\s*\S+</code> matches any 0+ whitespaces and then 1+ non-whitespaces at the start of the string, and then all <code>.</code> in that match (<code>x.group()</code>) are replaced with <code>\.</code> char sequences.</p>

<p>See the <a href=""https://rextester.com/FWUUT43712"" rel=""nofollow noreferrer"">Python demo</a>.</p>

<p>If you install the <a href=""https://pypi.org/project/regex/"" rel=""nofollow noreferrer"">PyPi <code>regex</code> module</a>, you will be able to achieve what you need with a single <code>regex.sub</code> call:</p>

<pre><code>import regex
s = 'DXa0.Xa1.a2 p1 p2 deviceName  a=157.585p b=54.46u $x=106.124 $y=107.996 $a=0'
rx = r'\G[^\s.]*\K\.'
print(regex.sub(rx, r'\\.', s))
</code></pre>

<p>See the <a href=""https://rextester.com/AGDJ15542"" rel=""nofollow noreferrer"">Python demo</a>. Also, see <a href=""https://regex101.com/r/64teGb/1"" rel=""nofollow noreferrer"">this regex demo</a>.</p>

<p><strong>Details</strong></p>

<ul>
<li><code>\G</code> - start of a string or the end of the previous successful match</li>
<li><code>[^\s.]*</code> - 0 or more chars other than a whitespace (<code>\s</code>) and a dot (a dot inside square brackets only matches a <code>.</code> char)</li>
<li><code>\K</code> - match reset operator discarding all the text matched so far in the current iteration</li>
<li><code>\.</code> - a dot.</li>
</ul>

<p>You may do without a <code>\K</code> if you use capturing/backreference:</p>

<pre><code>regex.sub(r'\G([^\s.]*)\.', r'\1\\.', s)
</code></pre>
",Replace occurrences matching regular expression first word line I would like parse ASCII file escape characters first word line Here example line DXa Xa p p deviceName p b u x I would like produce following DXa Xa p p deviceName p b u x I could run following example sub r line count But assumes characters word guaranteed The workaround I found sub r line split line split It produces expected results pretty I sure better way,"startoftags, python, regex, python3x, endoftags",python django djangorestframework endoftags,python regex python3x,python django djangorestframework,0.33
53153703,2018-11-05,2018,4,Groupby count only when a certain value is present in one of the column in pandas,"<p>I have a dataframe similar to the below mentioned database:</p>

<p><code>+------------+-----+--------+
 |    time    | id  | status |
 +------------+-----+--------+
 | 1451606400 | id1 | Yes    |
 | 1451606400 | id1 | Yes    |
 | 1456790400 | id2 | No     |
 | 1456790400 | id2 | Yes    |
 | 1456790400 | id2 | No     |
 +------------+-----+--------+</code></p>

<p>I'm grouping by all the columns mentioned above and i'm able to get the count in a different column named <code>'count'</code> successfully using the below command:</p>

<p><code>df.groupby(['time','id', 'status']).size().reset_index(name='count')</code></p>

<p>But I want the count in the above dataframe only in those rows with <code>status = 'Yes'</code> and rest should be <code>'0'</code></p>

<p>Desired Output:</p>

<p><code>+------------+-----+--------+---------+
 |    time    | id  | status | count   |
 +------------+-----+--------+---------+
 | 1451606400 | id1 | Yes    |       2 |
 | 1456790400 | id2 | Yes    |       1 |
 | 1456790400 | id2 | No     |       0 |
 +------------+-----+--------+---------+</code></p>

<p>I tried to count for <code>status = 'Yes'</code> with the below code:</p>

<p><code>df[df['status']== 'Yes'].groupby(['time','id','status']).size().reset_index(name='count')</code></p>

<p>which obviously gives me those rows with <code>status = 'Yes'</code> and discarded the rest. I want the discarded ones with <code>count = 0</code></p>

<p>Is there any way to get the result?</p>

<p>Thanks in advance!</p>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",53153736,"<p>Use lambda function with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.apply.html"" rel=""noreferrer""><code>apply</code></a> and for count <code>sum</code> boolena <code>True</code> values proccesses like <code>1</code>:</p>

<pre><code>df1 = (df.groupby(['time','id','status'])
         .apply(lambda x: (x['status']== 'Yes').sum())
         .reset_index(name='count'))
</code></pre>

<p>Or create new column and aggregate <code>sum</code>:</p>

<pre><code>df1 = (df.assign(A=df['status']=='Yes')
         .groupby(['time','id','status'])['A']
         .sum()
         .astype(int)
         .reset_index(name='count'))
</code></pre>

<p>Very similar solution with no new column, but worse readable a bit:</p>

<pre><code>df1 = ((df['status']=='Yes')
        .groupby([df['time'],df['id'],df['status']])
        .sum()
        .astype(int)
        .reset_index(name='count'))

print (df)
         time   id status  count
0  1451606400  id1    Yes      2
1  1456790400  id2     No      0
2  1456790400  id2    Yes      1
</code></pre>
",Groupby count certain value present one column pandas I dataframe similar mentioned database time id status id Yes id Yes id No id Yes id No I grouping columns mentioned able get count different column named count successfully using command df groupby time id status size reset index name count But I want count dataframe rows status Yes rest Desired Output time id status count id Yes id Yes id No I tried count status Yes code df df status Yes groupby time id status size reset index name count obviously gives rows status Yes discarded rest I want discarded ones count Is way get result Thanks advance,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas dataframe pandasgroupby,python pandas dataframe,0.87
53269061,2018-11-12,2018,2,pandas dataframe sum date range of another DataFrame,"<p>I have two dataframes. I want to sum an ""amount"" column in the 2nd, for each record in the first datafame.</p>

<p>So for each </p>

<pre><code>df1.Date = sum(df2.amount WHERE df1.Date &lt;= df2.Date AND df1.yearAgo &gt;= df2.Date)

df1 = pd.DataFrame({'Date':['2018-10-31','2018-10-30','2018-10-29','2018-10-28'],'yearAgo':['2017-10-31','2017-10-30','2017-10-29','2017-10-28']})

df2 = pd.DataFrame({'Date':['2018-10-30','2018-7-30','2018-4-30','2018-1-30','2017-10-30'],'amount':[1.0,1.0,1.0,1.0,0.75]})
</code></pre>

<p>desired results:</p>

<pre><code>df1.Date     yearToDateTotalAmount
2018-10-31        3.0
2018-10-30        4.75
2018-10-29        3.75
2018-10-28        3.75
</code></pre>
","['python', 'pandas', 'dataframe']",53269428,"<p>IIUC, your expected output should have <code>4</code> in first row.</p>

<p>You can achieve this very efficiently using <code>numpy</code>'s feature of <a href=""https://docs.scipy.org/doc/numpy-1.10.4/reference/generated/numpy.ufunc.outer.html"" rel=""nofollow noreferrer""><code>outer</code></a> comparison, since <code>less_equal</code> and <code>greater_equal</code> are <code>ufunc</code>s.</p>

<p>Notice that</p>

<pre><code>&gt;&gt;&gt; np.greater_equal.outer(df1.Date, df2.Date)

array([[ True,  True,  True,  True,  True],
       [ True,  True,  True,  True,  True],
       [False,  True,  True,  True,  True],
       [False,  True,  True,  True,  True]])
</code></pre>

<p>So you can get your mask by</p>

<pre><code>mask = np.greater_equal.outer(df1.Date, df2.Date) &amp; 
       np.less_equal.outer(df1.yearAgo, df2.Date)
</code></pre>

<p>And use <a href=""https://docs.scipy.org/doc/numpy-1.10.4/reference/generated/numpy.ufunc.outer.html"" rel=""nofollow noreferrer""><code>outer multiplication</code></a> + summing along <code>axis=1</code></p>

<pre><code>&gt;&gt;&gt; np.sum(np.multiply(mask, df2.amount.values), axis=1)

Out[49]:
array([4.  , 4.75, 3.75, 3.75])
</code></pre>

<p>In the end, just assign back</p>

<pre><code>&gt;&gt;&gt; df1['yearToDateTotalAmount'] = np.sum(np.multiply(mask, df2.amount.values), axis=1)

    Date        yearAgo     yearToDateTotalAmount
0   2018-10-31  2017-10-31  4.00
1   2018-10-30  2017-10-30  4.75
2   2018-10-29  2017-10-29  3.75
3   2018-10-28  2017-10-28  3.75
</code></pre>
",pandas dataframe sum date range another DataFrame I two dataframes I want sum amount column nd record first datafame So df Date sum df amount WHERE df Date lt df Date AND df yearAgo gt df Date df pd DataFrame Date yearAgo df pd DataFrame Date amount desired results df Date,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
53418077,2018-11-21,2018,2,how to solve this changing dataframe problem,"<p>let say i have a dataframe that consist of these two columns. </p>

<pre><code>User_id hotel_cluster 
   1     0
   2     2
   3     2
   3     3 
   3     0
   4     2
</code></pre>

<p>i want to change it into something like this. Do i need to write a function or is there a pandas way to do it?</p>

<pre><code>User_id hotel_cluster_0 hotel_cluster_1 hotel_cluster_2 hotel_cluster_3
  1          1                  0             0              0
  2          0                  0             1              0
  3          1                  0             1              1
  4          0                  0             1              0
</code></pre>

<p>Please help! Sorry if i am not posting the question in the right format
Thank you!</p>
","['python', 'pandas', 'dataframe']",53418157,"<p><a href=""https://stackoverflow.com/q/47152691/2336654"">SEE ALSO</a></p>

<hr>

<p>IIUC:</p>

<h3>Option 1</h3>

<p>First change <code>'hotel_cluster'</code> to a categorical that includes categories that don't exist</p>

<pre><code>col = 'hotel_cluster'
df[col] = pd.Categorical(df[col], categories=[0, 1, 2, 3])
pd.crosstab(*map(df.get, df)).add_prefix(f""{col}_"")

hotel_cluster  hotel_cluster_0  hotel_cluster_1  hotel_cluster_2  hotel_cluster_3
User_id                                                                          
1                            1                0                0                0
2                            0                0                1                0
3                            1                0                1                1
4                            0                0                1                0
</code></pre>

<hr>

<h3>Option 2</h3>

<p>Reindex after <code>crosstab</code></p>

<pre><code>pd.crosstab(*map(df.get, df)).reindex(
    columns=range(4), fill_value=0
).add_prefix('hotel_cluster_')

hotel_cluster  hotel_cluster_0  hotel_cluster_1  hotel_cluster_2  hotel_cluster_3
User_id                                                                          
1                            1                0                0                0
2                            0                0                1                0
3                            1                0                1                1
4                            0                0                1                0
</code></pre>
",solve changing dataframe problem let say dataframe consist two columns User id hotel cluster want change something like Do need write function pandas way User id hotel cluster hotel cluster hotel cluster hotel cluster Please help Sorry posting question right format Thank,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
53419940,2018-11-21,2018,3,How to remove spaces in between characters without removing ALL spaces in a dataframe?,"<p>Lets say I  have a dataframe like this:</p>

<pre><code>ID    Name       Description
0     Manny      V e  r y calm
1     Joey       Keen and a n a l y t i c a l
2     Lisa       R a s h and careless
3     Ash        Always joyful
</code></pre>

<p>I want to remove all the spaces between each letter in the <code>Description</code> column without completely removing all the necessary spaces between words.</p>

<p>Is there a simple way to this in Pandas?</p>
","['python', 'pandas', 'dataframe']",53420840,"<p>This is a tricky problem, but one approach that may get you most of the way there is to use negative and positive lookbehinds/lookaheads to encode a few basic rules.</p>

<p>The following example would likely work well enough given what you've described. It will incorrectly combine characters from consecutive ""real"" words that have been exploded into separated characters, but if that's rare this will probably be fine. You could add additional rules to cover more edge cases.</p>

<pre><code>import re
import pandas as pd

s = pd.Series(['V e  r y calm', 'Keen and a n a l y t i c a l',
'R a s h and careless', 'Always joyful'])

regex = re.compile('(?&lt;![a-zA-Z]{2})(?&lt;=[a-zA-Z]{1}) +(?=[a-zA-Z] |.$)')
s.str.replace(regex, '')

0              Very calm
1    Keen and analytical
2      Rash and careless
3          Always joyful
dtype: object
</code></pre>

<p>This regex effectively says:</p>

<p>Look for sequences of spaces and replace spaces, but only if there is one letter before them. If there are two letters, don't do anything (i.e., a 2-letter word). But more specifically, actually only replace a space if there is a letter after the last space in the sequence, or any character that terminates the string.</p>
",How remove spaces characters without removing ALL spaces dataframe Lets say I dataframe like ID Name Description Manny V e r calm Joey Keen n l c l Lisa R h careless Ash Always joyful I want remove spaces letter Description column without completely removing necessary spaces words Is simple way Pandas,"startoftags, python, pandas, dataframe, endoftags",python python3x list endoftags,python pandas dataframe,python python3x list,0.33
53424798,2018-11-22,2018,2,Python pandas: map and return Nan,"<p>I have two data frame, the first one is:</p>

<pre><code>id code
1   2
2   3
3   3
4   1
</code></pre>

<p>and the second one is:</p>

<pre><code>id code  name
1    1   Mary
2    2   Ben
3    3   John
</code></pre>

<p>I would like to map the data frame 1 so that it looks like:</p>

<pre><code>id code  name
1   2    Ben
2   3    John
3   3    John
4   1    Mary
</code></pre>

<p>I try to use this code:</p>

<pre><code>mapping = dict(df2[['code','name']].values)
df1['name'] = df1['code'].map(mapping)
</code></pre>

<p>My mapping is correct, but the mapping value are all NAN:</p>

<pre><code>mapping = {1:""Mary"", 2:""Ben"", 3:""John""}

id code  name
1   2    NaN
2   3    NaN
3   3    NaN
4   1    NaN
</code></pre>

<p>Can anyone know why an how to solve? </p>
","['python', 'pandas', 'dataframe']",53424837,"<p>Problem is different type of values in column <code>code</code> so necessary converting to integers or strings by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html"" rel=""nofollow noreferrer""><code>astype</code></a> for same types in both:</p>

<pre><code>print (df1['code'].dtype)
object

print (df2['code'].dtype)
int64
</code></pre>



<pre><code>print (type(df1.loc[0, 'code']))
&lt;class 'str'&gt;

print (type(df2.loc[0, 'code']))
&lt;class 'numpy.int64'&gt;
</code></pre>

<hr>

<pre><code>mapping = dict(df2[['code','name']].values)
#same dtypes - integers
df1['name'] = df1['code'].astype(int).map(mapping)
</code></pre>



<pre><code>#same dtypes - object (obviously strings)
df2['code'] = df2['code'].astype(str)
mapping = dict(df2[['code','name']].values)
df1['name'] = df1['code'].map(mapping)
</code></pre>

<hr>

<pre><code>print (df1)
   id code  name
0   1    2   Ben
1   2    3  John
2   3    3  John
3   4    1  Mary
</code></pre>
",Python pandas map return Nan I two data frame first one id code second one id code name Mary Ben John I would like map data frame looks like id code name Ben John John Mary I try use code mapping dict df code name values df name df code map mapping My mapping correct mapping value NAN mapping Mary Ben John id code name NaN NaN NaN NaN Can anyone know solve,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
53554737,2018-11-30,2018,2,Python3 Pandas Dataframe KeyError Issue,"<p>I have an Dataframe crawls, like following:
<a href=""https://i.stack.imgur.com/VoD8V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VoD8V.png"" alt=""enter image description here""></a></p>

<p>When I run this code</p>

<pre><code>crawl_stats = (
crawls['updated']
    .groupby(crawls.index.get_level_values('url'))
    .agg({
        'number of crawls': 'count', 
        'proportion of updates': 'mean', 
        'number of updates': 'sum'
    })
</code></pre>

<p>It shows the error:</p>

<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-62-180f1041465d&gt; in &lt;module&gt;
      8 crawl_stats = (
      9     crawls['updated']
---&gt; 10         .groupby(crawls.index.get_level_values('url'))
     11         # .groupby('url')
     12         .agg({

/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexes/base.py in _get_level_values(self, level)
   3155         """"""
   3156 
-&gt; 3157         self._validate_index_level(level)
   3158         return self
   3159 

/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexes/base.py in _validate_index_level(self, level)
   1942         elif level != self.name:
   1943             raise KeyError('Level %s must be same as name (%s)' %
-&gt; 1944                            (level, self.name))
   1945 
   1946     def _get_level_number(self, level):

KeyError: 'Level url must be same as name (None)'
</code></pre>

<p>And I tried this modified code:</p>

<pre><code>crawl_stats = (
crawls['updated']
    # .groupby(crawls.index.get_level_values('url'))
    .groupby('url')
    .agg({
        'number of crawls': 'count', 
        'proportion of updates': 'mean', 
        'number of updates': 'sum'
    })
</code></pre>

<p>It also shows error:</p>

<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-63-8c5f0f6f7c86&gt; in &lt;module&gt;
      9     crawls['updated']
     10         # .groupby(crawls.index.get_level_values('url'))
---&gt; 11         .groupby('url')
     12         .agg({
     13             'number of crawls': 'count',       
3293             # Add key to exclusions

    KeyError: 'url'
</code></pre>

<p>I have already tried to do as other guidance in stack overflow before, but it still doesn't work yet.
Could anybody help me fix it? Thanks!</p>

<p>Here is my code to create the Dataframe crawls.</p>

<pre><code>def make_crawls_dataframe(crawl_json_records):
    """"""Creates a Pandas DataFrame from the given list of JSON records.

    The DataFrame corresponds to the following relation:

        crawls(primary key (url, hour), updated)

    Each hour in which a crawl happened for a page (regardless of
    whether it found a change) should be represented.  `updated` is
    a boolean value indicating whether the check for that hour found
    a change.

    The result is sorted by URL in ascending order and **further**
    sorted by hour in ascending order among the rows for each URL.

    Args:
      crawl_json_records (list): A list of JSON objects such as the
                                 crawl_json variable above.

    Returns:
      DataFrame: A table whose schema (and sort order) is described
                 above.
    """"""
    url = []
    hour = []
    updated = []


    # To get the 1000 url, number of checks and positive checks
    for i in range(len(crawl_json_records)):
        temp_url = [crawl_json_records[i]['url']]
        temp_len = crawl_json_records[i][""number of checks""]
        temp_checks = crawl_json_records[i][""positive checks""]

        # url.append(temp_url*temp_len)
        for item0 in temp_url*temp_len:
            url.append(item0)
        # hour.append(list(range(1,temp_len+1)))
        for item1 in list(range(1,temp_len+1)):
            hour.append(item1)
        temp_updated = [0]*temp_len

        for item in temp_checks:
            temp_updated[item-1] = 1
            # updated.append(temp_updated)
        for item2 in temp_updated:
            updated.append(item2)

    # print('len(url):',len(url))
    # 521674
    # print('len(hour):',len(hour))
    # print('len(updated):',len(updated))
    # Above 3 is 521674
    #print(type(temp_len))
    #print(temp_len)
    #print(temp_url*temp_len)

    columns = ['url','hour','updated']
    data = np.array((url,hour,updated)).T
    df = pd.DataFrame(data=data, columns=columns)
    df.index += 1
    # df.index = df['url']
    return df.sort_values(by=['url','hour'], ascending=True)

crawls = make_crawls_dataframe(crawl_json)
crawls.head(50)  # crawls shows as the image
</code></pre>
","['python', 'python-3.x', 'pandas']",53554801,"<p>There are 2 problems - need grouping by column <code>url</code> and also define list of tuples for new columns names with aggregated functions:</p>

<pre><code>crawls = pd.DataFrame({
    'url': ['a','a','a','a','b','b','b'],
    'updated': list(range(7))
})
print (crawls)
  url  updated
0   a        0
1   a        1
2   a        2
3   a        3
4   b        4
5   b        5
6   b        6

d = [('number of crawls', 'count'), 
     ('proportion of updates', 'mean'), 
     ('number of updates', 'sum')]
crawl_stats = crawls.groupby('url')['updated'].agg(d)
print (crawl_stats)
     number of crawls  proportion of updates  number of updates
url                                                            
a                   4                    1.5                  6
b                   3                    5.0                 15
</code></pre>

<p>EDIT:</p>

<p>Problem with son numeric column should be converting to numpy array, better is create dict and pass to DataFrame construcor:</p>

<p>Change:</p>

<pre><code>columns = ['url','hour','updated']
data = np.array((url,hour,updated)).T
df = pd.DataFrame(data=data, columns=columns)
</code></pre>

<p>to:</p>

<pre><code>columns = ['url','hour','updated']
df = pd.DataFrame({'url':url, 'hour':hour,'updated':updated}, columns=columns)
</code></pre>
",Python Pandas Dataframe KeyError Issue I Dataframe crawls like following When I run code crawl stats crawls updated groupby crawls index get level values url agg number crawls count proportion updates mean number updates sum It shows error KeyError Traceback recent call last lt ipython input f gt lt module gt crawl stats crawls updated gt groupby crawls index get level values url groupby url agg Library Frameworks Python framework Versions lib python site packages pandas core indexes base py get level values self level gt self validate index level level return self Library Frameworks Python framework Versions lib python site packages pandas core indexes base py validate index level self level elif level self name raise KeyError Level must name gt level self name def get level number self level KeyError Level url must name None And I tried modified code crawl stats crawls updated groupby crawls index get,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
53636253,2018-12-05,2018,2,Discord bot adding reactions to a message discord.py (no custom emojis),"<p>I've been trying to make a bot using discord.py add a reaction to a message using discord.py after reading <a href=""https://stackoverflow.com/questions/48982061/how-do-you-have-a-bot-add-a-reaction-using-a-custom-emoji"">this</a> (which is not what I wanted because I am not using custom emojis) but it ends up giving this error:</p>

<p><code>discord.ext.commands.errors.CommandInvokeError: Command raised an exception: InvalidArgument: message argument must be a Message</code> </p>

<p>I tried this code</p>

<pre><code>@commands.command(pass_context=True)
async def emoji(ctx):
    msg = ""working""
    await bot.say(msg)
    reactions = ['dart']
    for emoji in reactions: await bot.add_reaction(msg, emoji)
</code></pre>

<p>Any other related questions here on discord.py are just not helpful for this
Any ideas on how to implement this</p>
","['python', 'discord', 'discord.py']",53637950,"<p>The error message is telling you what the error is: &quot;<code>message argument must be a Message</code>&quot;</p>
<p>In the line</p>
<pre><code>await bot.add_reaction(msg, emoji)
</code></pre>
<p><code>msg</code> is a string, not a <code>Message</code> object.  You need to capture the message you send and then add the reactions to that message:</p>
<pre><code>@commands.command(pass_context=True)
async def emoji(ctx):
    msg = await bot.say(&quot;working&quot;)
    reactions = ['dart']
    for emoji in reactions: 
        await bot.add_reaction(msg, emoji)
</code></pre>
<hr />
<p>Note that in later versions of <code>discord.py</code>, <a href=""https://discordpy.readthedocs.io/en/latest/api.html?highlight=add_reaction#discord.Message.add_reaction"" rel=""nofollow noreferrer""><code>add_reaction</code></a> has changed from <code>bot.add_reaction(msg, emoji)</code> to <code>await msg.add_reaction(emoji)</code>.</p>
",Discord bot adding reactions message discord py custom emojis I trying make bot using discord py add reaction message using discord py reading I wanted I using custom emojis ends giving error discord ext commands errors CommandInvokeError Command raised exception InvalidArgument message argument must Message I tried code commands command pass context True async def emoji ctx msg working await bot say msg reactions dart emoji reactions await bot add reaction msg emoji Any related questions discord py helpful Any ideas implement,"startoftags, python, discord, discordpy, endoftags",python discord discordpy endoftags,python discord discordpy,python discord discordpy,1.0
53643260,2018-12-06,2018,3,Match individual words in a string to dictionary keys,"<p>Hi I'm trying to input a string and then have the string split into individual words. The unique words that are in the string and that are also in the dictionary keys of ""contents"" retrieve the corresponding values from the dictionary ""files"". </p>

<p>How do I split the input string to check individual words against the dictionary ""concept"" keys and if possible return the words in the string, not the dictionary keys? </p>

<p>I tried to split the string into a list and then pass the list values directly into the dictionary but I got lost real quick (those are the variables commented out at the top. Any help is appreciated. Thank you</p>

<pre><code>def concept(word):

# convert var(word) to list
#my_string_list=[str(i) for i in word]

# join list(my_string_list) back to string
#mystring = ''.join(my_string_list)

# use this to list python files
files    = {1:""file0001.txt"",
            2:""file0002.txt"",
            3:""file0003.txt"",
            4:""file0004.txt"",
            5:""file0005.txt"",
            6:""file0006.txt"",
            7:""file0007.txt"",    
            8:""file0008.txt"",
            9:""file0009.txt""}

# change keys to searchable simple keyword phrases. 
concepts = {'GAMES':[1,2,4,3,3],
            'BLACKJACK':[5,3,5,3,5],
            'MACHINE':[4,9,9,9,4],
            'DATABASE':[5,3,3,3,5],
            'LEARNING':[4,9,4,9,4]}

# convert to uppercase, search var(mystring) in dict 'concepts', if not found return not found""
if word.upper() not in concepts:
    print(""{}: Not Found in Database"" .format(word)) not in concepts
    return

# for matching keys in dict 'concept' list values in dict 'files'
for pattern in concepts[word.upper()]:
    print(files[pattern])


# return input box at end of query        
while True:
    concept(input(""Enter Concept Idea: ""))
    print(""\n"")
</code></pre>
","['python', 'python-3.x', 'dictionary']",53643334,"<p>Assuming the input is a list of words separated by spaces you could do:</p>

<pre><code>def concept(phrase):

    words = phrase.split()

    # use this to list python files
    files = {1: ""file0001.txt"",
             2: ""file0002.txt"",
             3: ""file0003.txt"",
             4: ""file0004.txt"",
             5: ""file0005.txt"",
             6: ""file0006.txt"",
             7: ""file0007.txt"",
             8: ""file0008.txt"",
             9: ""file0009.txt""}

    # change keys to searchable simple keyword phrases.
    concepts = {'GAMES': [1, 2, 4, 3, 3],
                'BLACKJACK': [5, 3, 5, 3, 5],
                'MACHINE': [4, 9, 9, 9, 4],
                'DATABASE': [5, 3, 3, 3, 5],
                'LEARNING': [4, 9, 4, 9, 4]}

    for word in words:
        # convert to uppercase, search var(mystring) in dict 'concepts', if not found return not found""
        if word.upper() not in concepts:
            print(""{}: Not Found in Database"".format(word))
        else:
            # for matching keys in dict 'concept' list values in dict 'files'
            for pattern in concepts[word.upper()]:
                print(files[pattern])

concept(""games blackjack foo"")
</code></pre>

<p><strong>Output</strong></p>

<pre><code>file0001.txt
file0002.txt
file0004.txt
file0003.txt
file0003.txt
file0005.txt
file0003.txt
file0005.txt
file0003.txt
file0005.txt
foo: Not Found in Database
</code></pre>

<p>The line <code>words = phrase.split()</code> split the string phrase on spaces. To check if a word is in the dictionary you need to do it one at the time, hence the loop <code>for word in words</code> iterating over the words of phrase.</p>

<p><strong>Further</strong></p>

<ol>
<li><a href=""https://stackoverflow.com/questions/3845362/how-can-i-check-if-a-key-exists-in-a-dictionary"">How can I check if a key exists in a dictionary?</a></li>
<li><a href=""https://stackoverflow.com/questions/3475251/split-a-string-by-a-delimiter-in-python"">Split a string by a delimiter in python</a></li>
</ol>
",Match individual words string dictionary keys Hi I trying input string string split individual words The unique words string also dictionary keys contents retrieve corresponding values dictionary files How I split input string check individual words dictionary concept keys possible return words string dictionary keys I tried split string list pass list values directly dictionary I got lost real quick variables commented top Any help appreciated Thank def concept word convert var word list string list str word join list string list back string mystring join string list use list python files files file txt file txt file txt file txt file txt file txt file txt file txt file txt change keys searchable simple keyword phrases concepts GAMES BLACKJACK MACHINE DATABASE LEARNING convert uppercase search var mystring dict concepts found return found word upper concepts print Not Found Database format word concepts return matching keys dict concept list values,"startoftags, python, python3x, dictionary, endoftags",python django djangorestframework endoftags,python python3x dictionary,python django djangorestframework,0.33
53655430,2018-12-06,2018,2,Transform multiple CSV rows in Pandas into one,"<p>I'm just starting out with Pandas and I'm trying to make a data file I have into something I can export and read. The CSV I have is in this form:</p>

<pre><code>time    |   parameter   |   value
------------------------------------
1       |       a       |   21
2       |       a       |   21
3       |       a       |   21
1       |       b       |   19
2       |       b       |   19
3       |       b       |   19
1       |       c       |   17
2       |       c       |   17
3       |       c       |   17
</code></pre>

<p>I want to transform it in the following form:</p>

<pre><code>time    |   a   |   b   |   c   
------------------------------------
1       |   21  |   19  |   17  
2       |   21  |   19  |   17  
3       |   21  |   19  |   17  
1       |   21  |   19  |   17  
2       |   21  |   19  |   17  
3       |   21  |   19  |   17  
1       |   21  |   19  |   17  
2       |   21  |   19  |   17  
3       |   21  |   19  |   17  
</code></pre>

<p>Of course my data have different values, but the example above should be sufficient. It's weather data, like temperature and wind speed, and each row has the timestamp of the measurement, the param name and the value.</p>

<p>I want to transform it into a single row with 3 columns (or more if there are more parameters) for each timestamp, where the column name is the param name.</p>

<p>I know that I have to group my data by the time column so I've done
<code>df.groupby('time')</code></p>

<p>However, I cannot figure out how to execute an apply method that will give me the results I want. Any hints are appreciated!</p>
","['python', 'pandas', 'csv']",53656261,"<p>You can try using <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html"" rel=""nofollow noreferrer"">pivot table</a>:</p>

<pre><code>pd.pivot_table(df, index='time', columns='parameter', values='value')
</code></pre>

<p></p>

<pre><code>parameter   a   b   c
time                 
1          21  19  17
2          21  19  17
3          21  19  17
</code></pre>
",Transform multiple CSV rows Pandas one I starting Pandas I trying make data file I something I export read The CSV I form time parameter value b b b c c c I want transform following form time b c Of course data different values example sufficient It weather data like temperature wind speed row timestamp measurement param name value I want transform single row columns parameters timestamp column name param name I know I group data time column I done df groupby time However I cannot figure execute apply method give results I want Any hints appreciated,"startoftags, python, pandas, csv, endoftags",python pandas numpy endoftags,python pandas csv,python pandas numpy,0.67
53657210,2018-12-06,2018,5,Pandas concat does not handle Timestamp columns correctly?,"<p>When two dataframes are concatenated (using concat) by default concat creates a new dataframe with the union of the columns of both, setting the values of any missing columns in the result with nan.  For example...</p>

<pre><code>import pandas as pd
a = pd.DataFrame({'A':range(5), 'B':range(5)})
b = pd.DataFrame({'A':range(5)})
pd.concat([a , b], sort=False)

    A   B
0   0   0.0
1   1   1.0
...
3   3   NaN
4   4   NaN
</code></pre>

<p>But if the missing column in one of the dataframes contains timestamps this breaks...</p>

<pre><code>a = pd.DataFrame({'A':range(5), 'B':[pd.Timestamp.utcnow() for _ in range(5)]})
b = pd.DataFrame({'A':range(5)})
pd.concat([a , b], sort=False)
</code></pre>

<p>Throws ""AttributeError: 'NoneType' object has no attribute '_can_consolidate'"".</p>

<p>Python 3.6.5; Pandas 0.23; Windows 7 x64</p>

<p>Is this is known issue?<br>
Are they're any workarounds known?</p>
","['python', 'python-3.x', 'pandas']",53657537,"<p>As explained in the comments, this is a known issue (see <a href=""https://github.com/pandas-dev/pandas/issues/22796"" rel=""nofollow noreferrer"">GH22796</a>) and is fixed for version 0.24. In the meantime, there are two possible workarounds.</p>

<p>One is to convert to string:</p>

<pre><code>df = pd.concat([a.assign(B=a.B.astype(str)), b], sort=False) 
df['B'] = pd.to_datetime(df['B'], errors='coerce')
df

   A                          B
0  0 2018-12-06 18:21:35.363477
1  1 2018-12-06 18:21:35.363728
2  2 2018-12-06 18:21:35.363740
3  3 2018-12-06 18:21:35.363748
4  4 2018-12-06 18:21:35.363756
0  0                        NaT
1  1                        NaT
2  2                        NaT
3  3                        NaT
4  4                        NaT
</code></pre>

<p>The other, as @root mentioned, is to initialise an empty column in <code>b</code>:</p>

<pre><code>pd.concat([a, b.assign(B=pd.NaT)], sort=False)

   A                                 B
0  0  2018-12-06 18:21:35.363477+00:00
1  1  2018-12-06 18:21:35.363728+00:00
2  2  2018-12-06 18:21:35.363740+00:00
3  3  2018-12-06 18:21:35.363748+00:00
4  4  2018-12-06 18:21:35.363756+00:00
0  0                               NaT
1  1                               NaT
2  2                               NaT
3  3                               NaT
4  4                               NaT
</code></pre>
",Pandas concat handle Timestamp columns correctly When two dataframes concatenated using concat default concat creates new dataframe union columns setting values missing columns result nan For example import pandas pd pd DataFrame A range B range b pd DataFrame A range pd concat b sort False A B NaN NaN But missing column one dataframes contains timestamps breaks pd DataFrame A range B pd Timestamp utcnow range b pd DataFrame A range pd concat b sort False Throws AttributeError NoneType object attribute consolidate Python Pandas Windows x Is known issue Are workarounds known,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
54006298,2019-01-02,2019,16,Select rows of a dataframe based on another dataframe in Python,"<p>I have the following dataframe:</p>
<pre><code>import pandas as pd
import numpy as np
df1 = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),
                   'B': 'one one two three two two one three'.split(),
                   'C': np.arange(8), 'D': np.arange(8) * 2})
print(df1)

    A      B   C   D
0  foo    one  0   0
1  bar    one  1   2
2  foo    two  2   4
3  bar  three  3   6
4  foo    two  4   8
5  bar    two  5  10
6  foo    one  6  12
7  foo  three  7  14
</code></pre>
<p>I hope to select rows in <code>df1</code> by the <code>df2</code> as follows:</p>
<pre><code>df2 = pd.DataFrame({'A': 'foo bar'.split(),
                   'B': 'one two'.split()
                   })
print(df2)

     A    B
0  foo  one
1  bar  two
</code></pre>
<p>Here is what I have tried in Python, but I just wonder if there is another method. Thanks.</p>
<pre><code>df = df1.merge(df2, on=['A','B'])
print(df)
</code></pre>
<p>This is the output expected.</p>
<pre><code>    A      B   C   D
0  foo    one  0   0
1  bar    two  5  10
2  foo    one  6  12
</code></pre>
<p><a href=""https://stackoverflow.com/questions/13937022/using-pandas-to-select-rows-using-two-different-columns-from-dataframe"">Using pandas to select rows using two different columns from dataframe?</a></p>
<p><a href=""https://stackoverflow.com/questions/52163161/select-columns-of-a-dataframe-based-on-another-dataframe"">Select Columns of a DataFrame based on another DataFrame</a></p>
","['python', 'pandas', 'dataframe']",54006502,"<p>Simpliest is use <code>merge</code> with inner join.</p>

<p>Another solution with filtering:</p>

<pre><code>arr = [np.array([df1[k] == v for k, v in x.items()]).all(axis=0) for x in df2.to_dict('r')]
df = df1[np.array(arr).any(axis=0)]
print(df)
     A    B  C   D
0  foo  one  0   0
5  bar  two  5  10
6  foo  one  6  12
</code></pre>

<p>Or create <code>MultiIndex</code> and filter with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.isin.html"" rel=""noreferrer""><code>Index.isin</code></a>:</p>

<pre><code>df = df1[df1.set_index(['A','B']).index.isin(df2.set_index(['A','B']).index)]
print(df)
     A    B  C   D
0  foo  one  0   0
5  bar  two  5  10
6  foo  one  6  12
</code></pre>
",Select rows dataframe based another dataframe Python I following dataframe import pandas pd import numpy np df pd DataFrame A foo bar foo bar foo bar foo foo split B one one two three two two one three split C np arange D np arange print df A B C D foo one bar one foo two bar three foo two bar two foo one foo three I hope select rows df df follows df pd DataFrame A foo bar split B one two split print df A B foo one bar two Here I tried Python I wonder another method Thanks df df merge df A B print df This output expected A B C D foo one bar two foo one Using pandas select rows using two different columns dataframe Select Columns DataFrame based another DataFrame,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
54041899,2019-01-04,2019,3,Average of consecutive days in dataframe,"<p>I have a pandas dataframe <code>df</code> as:</p>

<pre><code>Date         Val    WD
1/3/2019     2.65   Thursday
1/4/2019     2.51   Friday
1/5/2019     2.95   Saturday
1/6/2019     3.39   Sunday
1/7/2019     3.39   Monday
1/12/2019    2.23   Saturday
1/13/2019    2.50   Sunday
1/14/2019    3.62   Monday
1/15/2019    3.81   Tuesday
1/16/2019    3.75   Wednesday
1/17/2019    3.69   Thursday
1/18/2019    3.47   Friday
</code></pre>

<p>I need to get the following <code>df2</code> from above:</p>

<pre><code>Date         Val    WD
1/3/2019     2.65   Thursday
1/4/2019     2.51   Friday
1/5/2019     3.24   Saturday
1/6/2019     3.24   Sunday
1/7/2019     3.24   Monday
1/12/2019    2.78   Saturday
1/13/2019    2.78   Sunday
1/14/2019    2.78   Monday
1/15/2019    3.81   Tuesday
1/16/2019    3.75   Wednesday
1/17/2019    3.69   Thursday
1/18/2019    3.47   Friday
</code></pre>

<p>Where the df2 values are updated to have average of consecutive Sat, Sun and Mon values.</p>

<p>i.e. average of  <code>2.95, 3.39, 3.39</code> for dates <code>1/5/2019, 1/6/2019, 1/7/2019</code> in df is 3.24 and hence in df2 I have replaced the <code>1/5/2019, 1/6/2019, 1/7/2019</code> values with 3.24. </p>

<p>The trick has been finding the consecutive Saturday, Sunday and Monday. Not sure how to approach this.</p>
","['python', 'python-3.x', 'pandas']",54042811,"<p>You can use <code>CustomBusinessDay</code> with <code>pd.grouper</code> to create a group col:</p>

<pre><code># if you want to only find the mean if all three days are found
from pandas.tseries.offsets import CustomBusinessDay
days = CustomBusinessDay(weekmask='Tue Wed Thu Fri Sat')

df['group_col'] = df.groupby(pd.Grouper(key='Date', freq=days)).ngroup()
df.update(df[df.groupby('group_col')['Val'].transform('size').eq(3)].groupby('group_col').transform('mean'))

    Date          Val          WD     group_col
0   2019-01-03  2.650000    Thursday    0
1   2019-01-04  2.510000    Friday      1
2   2019-01-05  3.243333    Saturday    2
3   2019-01-06  3.243333    Sunday      2
4   2019-01-07  3.243333    Monday      2
5   2019-01-12  2.783333    Saturday    7
6   2019-01-13  2.783333    Sunday      7
7   2019-01-14  2.783333    Monday      7
8   2019-01-15  3.810000    Tuesday     8
9   2019-01-16  3.750000    Wednesday   9
10  2019-01-17  3.690000    Thursday    10
11  2019-01-18  3.470000    Friday      11
</code></pre>

<p>or if you want to find the mean of any combination of sat sun mon in the same week</p>

<pre><code>days = CustomBusinessDay(weekmask='Tue Wed Thu Fri Sat')

df['group_col'] = df.groupby(pd.Grouper(key='Date', freq=days)).ngroup()
df['Val'] = df.groupby('group_col')['Val'].transform('mean')
</code></pre>
",Average consecutive days dataframe I pandas dataframe df Date Val WD Thursday Friday Saturday Sunday Monday Saturday Sunday Monday Tuesday Wednesday Thursday Friday I need get following df Date Val WD Thursday Friday Saturday Sunday Monday Saturday Sunday Monday Tuesday Wednesday Thursday Friday Where df values updated average consecutive Sat Sun Mon values e average dates df hence df I replaced values The trick finding consecutive Saturday Sunday Monday Not sure approach,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
54043086,2019-01-04,2019,5,conditional fill in pandas dataframe,"<p>I have a dataframe <code>df</code> with float values in column <code>A</code>. I want to add another column <code>B</code> such that:</p>

<ol>
<li><p><code>B[0] = A[0]</code></p>

<p>for <code>i &gt; 0</code>...</p></li>
<li><code>B[i] = if(np.isnan(A[i])) then A[i] else Step3</code></li>
<li><code>B[i] = if(abs((B[i-1] - A[i]) / B[i-1]) &lt; 0.3) then B[i-1] else A[i]</code></li>
</ol>

<p>Sample dataframe <code>df</code> can be generated as given below</p>

<pre><code>import numpy as np
import pandas as pd
df = pd.DataFrame(1000*(2+np.random.randn(500, 1)), columns=list('A'))
df.loc[1, 'A'] = np.nan
df.loc[15, 'A'] = np.nan
df.loc[240, 'A'] = np.nan
df.loc[241, 'A'] = np.nan
</code></pre>
","['python', 'pandas', 'dataframe']",54043218,"<p>This can be done fairly efficiently with <a href=""http://numba.pydata.org/"" rel=""nofollow noreferrer"">Numba</a>. If you are not able to use Numba, just omit <code>@njit</code> and your logic will run as a Python-level loop.</p>

<pre><code>import numpy as np
import pandas as pd
from numba import njit

np.random.seed(0)
df = pd.DataFrame(1000*(2+np.random.randn(500, 1)), columns=['A'])
df.loc[1, 'A'] = np.nan
df.loc[15, 'A'] = np.nan
df.loc[240, 'A'] = np.nan

@njit
def recurse_nb(x):
    out = x.copy()
    for i in range(1, x.shape[0]):
        if not np.isnan(x[i]) and (abs(1 - x[i] / out[i-1]) &lt; 0.3):
            out[i] = out[i-1]
    return out

df['B'] = recurse_nb(df['A'].values)

print(df.head(10))

             A            B
0  3764.052346  3764.052346
1          NaN          NaN
2  2978.737984  2978.737984
3  4240.893199  4240.893199
4  3867.557990  4240.893199
5  1022.722120  1022.722120
6  2950.088418  2950.088418
7  1848.642792  1848.642792
8  1896.781148  1848.642792
9  2410.598502  2410.598502
</code></pre>
",conditional fill pandas dataframe I dataframe df float values column A I want add another column B B A gt B np isnan A A else Step B abs B A B lt B else A Sample dataframe df generated given import numpy np import pandas pd df pd DataFrame np random randn columns list A df loc A np nan df loc A np nan df loc A np nan df loc A np nan,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
54119634,2019-01-09,2019,2,Slicing strings in a data frame,"<p>I have a data frame that looks like this (before).  </p>

<pre><code>BEFORE:
string
Oct 05 
190103  
</code></pre>

<p>How can I make it look like this (after)?</p>

<pre><code>AFTER:
string                                  the_date
Oct 05                                  181005
190103                                  190103
</code></pre>
","['python', 'python-3.x', 'pandas']",54119901,"<p>You can use a regular expression to match the last continuous sequence of numbers between the last space of a string and the last period of a string.  Use:</p>

<pre><code>\s[^\s]+?(\d+)\.[^\.]+?$
</code></pre>

<hr>

<h3><code>str.extract</code></h3>

<pre><code>df['string'].str.extract(r'\s[^\s]+?(\d+)\.[^\.]+?$')
</code></pre>

<p></p>

<pre><code>        0
0  181004
1  181004
2  181004
3  181106
4  181106
5  190102
6  190103
7   51811
</code></pre>

<hr>

<p>As has been noted in the comments your last line should be <code>51811</code>, or else you are not using a consistent rule throughout your DataFrame.</p>

<hr>

<p><strong><em>Regex Explanation</em></strong></p>

<pre><code>\s                    # match a whitespace character
[^\s]+?               # match a non whitespace character between 1 and unlimited times, lazy
(                     # start of matching group 1
  \d+                 # match 1 or more digits          
)         
\.                    # match a period character
[^\.]+?               # match a non period character one to unlimited times, lazy
$                     # assert position at end of line
</code></pre>
",Slicing strings data frame I data frame looks like BEFORE string Oct How I make look like AFTER string date Oct,"startoftags, python, python3x, pandas, endoftags",python python3x pandas endoftags,python python3x pandas,python python3x pandas,1.0
54297406,2019-01-21,2019,2,Finding a specific sequential pattern,"<p>I have a dataframe like this:</p>

<pre><code>Id  Seq Event
1     2    A 
1     3    B 
1     5    c 
1     6    A 
2     1    A 
2     2    B 
2     4    A 
2     6    B
</code></pre>

<p>I want to find how many times a specific pattern appears. Let's say ""AB"" . The output should be.</p>

<pre><code>Id  Pattern_Count
1    1
2    2 
</code></pre>

<p>I tried using Event + Event.shift() and searching for the specific pattern. It's a tedious task when I have to search for a longer pattern like ""ABCDE"" and I don't want to shift it 4 times. Is there any alternative way to do this?</p>
","['python', 'pandas', 'numpy', 'dataframe']",54297459,"<p>You can do this with <code>groupby</code>, <code>agg</code>, and <code>str.count</code>:</p>

<pre><code>(df.groupby('Id')['Event']
   .agg(''.join)
   .str.count('AB')
   .reset_index(name='Pattern_Count'))

   Id  Pattern_Count
0   1              1
1   2              2
</code></pre>

<p>Note that <code>str.count</code> will work for simple substring matches only, regex patterns are not supported directly.</p>
",Finding specific sequential pattern I dataframe like Id Seq Event A B c A A B A B I want find many times specific pattern appears Let say AB The output Id Pattern Count I tried using Event Event shift searching specific pattern It tedious task I search longer pattern like ABCDE I want shift times Is alternative way,"startoftags, python, pandas, numpy, dataframe, endoftags",python arrays numpy endoftags,python pandas numpy dataframe,python arrays numpy,0.58
54389331,2019-01-27,2019,3,Adjust different transparency for different class in seaborn scatter plot,"<p>I want different alpha value (transparency) for Different Class in scatter plot.</p>

<pre><code>sns.scatterplot(x=""BorrowerAPR"", y=""LoanOriginalAmount"", data=df_new, 
                alpha=0.03, hue=""LoanStatus"")
</code></pre>

<p>Expecting Class 1 alpha to be 0.2.</p>

<p><a href=""https://i.stack.imgur.com/Ltbk1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ltbk1.png"" alt="" Current Picture ""></a></p>
","['python', 'matplotlib', 'seaborn']",54963996,"<p>One way is to plot them separately, though you'll get different hues if not specified. Here's an example from the built-in <code>tips</code> dataset with different <code>alpha</code> values for smokers and non-smokers:</p>

<pre><code>import seaborn as sns
import numpy as np

tips = sns.load_dataset(""tips"")
tips[""alpha""] = np.where(tips.smoker == ""Yes"", 1.0, 0.5)

ax = sns.scatterplot(x=""total_bill"", y=""tip"",
                     data=tips[tips.alpha == 0.5], alpha=0.5)
sns.scatterplot(x=""total_bill"", y=""tip"", data=tips[tips.alpha == 1.0], 
                alpha=1.0, ax=ax)
</code></pre>

<p><a href=""https://i.stack.imgur.com/x5xuo.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/x5xuo.png"" alt=""enter image description here""></a></p>

<p>This also stacks the higher-alpha points atop the lower ones.</p>

<p>More generally for multiple <code>alpha</code> categories:</p>

<pre><code>alphas = tips.alpha.sort_values().unique()
ax = sns.scatterplot(x=""total_bill"", y=""tip"",
                     data=tips[tips.alpha == alphas[0]], alpha=alphas[0])
for alpha in alphas[1:]:
    sns.scatterplot(x=""total_bill"", y=""tip"",
                    data=tips[tips.alpha == alpha], alpha=alpha, ax=ax)
</code></pre>
",Adjust different transparency different class seaborn scatter plot I want different alpha value transparency Different Class scatter plot sns scatterplot x BorrowerAPR LoanOriginalAmount data df new alpha hue LoanStatus Expecting Class alpha,"startoftags, python, matplotlib, seaborn, endoftags",python pandas datetime endoftags,python matplotlib seaborn,python pandas datetime,0.33
54422122,2019-01-29,2019,3,Pandas upsampling using groupby and resample,"<p>I have grouped timeseries with gaps. I wan't to fill the gaps, respecting the groupings.</p>

<p><code>date</code> is unique within each <code>id</code>.</p>

<p>The following works but gives me zero's where I wan't NaN's</p>

<pre><code>data.groupby('id').resample('D', on='date').sum()\
    .drop('id', axis=1).reset_index()
</code></pre>

<p>The following do not work for some reason</p>

<pre><code>data.groupby('id').resample('D', on='date').asfreq()\
    .drop('id', axis=1).reset_index()

data.groupby('id').resample('D', on='date').fillna('pad')\
    .drop('id', axis=1).reset_index()
</code></pre>

<p>I get the following error:
<code>Upsampling from level= or on= selection is not supported, use .set_index(...) to explicitly set index to datetime-like</code></p>

<p>I've tried to use the <code>pandas.Grouper</code> with <code>set_index</code> multilevel index or single but it do not seems to upsample my date column so i get continous dates or it do not respect the <code>id</code> column.</p>

<p>Pandas is version 0.23</p>

<p><strong>Try it yourself:</strong></p>

<pre><code>data = pd.DataFrame({
'id': [1,1,1,2,2,2],
'date': [
    datetime(2018, 1, 1),
    datetime(2018, 1, 5),
    datetime(2018, 1, 10),
    datetime(2018, 1, 1),
    datetime(2018, 1, 5),
    datetime(2018, 1, 10)],
'value': [100, 110, 90, 50, 40, 60]})

# Works but gives zeros
data.groupby('id').resample('D', on='date').sum()
# Fails
data.groupby('id').resample('D', on='date').asfreq()
data.groupby('id').resample('D', on='date').fillna('pad')
</code></pre>
","['python', 'pandas', 'pandas-groupby']",54422138,"<p>Create <code>DatetimeIndex</code> and remove parameter <code>on</code> from <code>resample</code>:</p>

<pre><code>print (data.set_index('date').groupby('id').resample('D').asfreq())
                id
id date           
1  2018-01-01  1.0
   2018-01-02  NaN
   2018-01-03  NaN
   2018-01-04  NaN
   2018-01-05  1.0
   2018-01-06  NaN
   2018-01-07  NaN
   2018-01-08  NaN
   2018-01-09  NaN
   2018-01-10  1.0
2  2018-01-01  2.0
   2018-01-02  NaN
   2018-01-03  NaN
   2018-01-04  NaN
   2018-01-05  2.0
   2018-01-06  NaN
   2018-01-07  NaN
   2018-01-08  NaN
   2018-01-09  NaN
   2018-01-10  2.0
</code></pre>

<hr>

<pre><code>print (data.set_index('date').groupby('id').resample('D').fillna('pad'))
#alternatives
#print (data.set_index('date').groupby('id').resample('D').ffill())
#print (data.set_index('date').groupby('id').resample('D').pad())
               id
id date          
1  2018-01-01   1
   2018-01-02   1
   2018-01-03   1
   2018-01-04   1
   2018-01-05   1
   2018-01-06   1
   2018-01-07   1
   2018-01-08   1
   2018-01-09   1
   2018-01-10   1
2  2018-01-01   2
   2018-01-02   2
   2018-01-03   2
   2018-01-04   2
   2018-01-05   2
   2018-01-06   2
   2018-01-07   2
   2018-01-08   2
   2018-01-09   2
   2018-01-10   2
</code></pre>

<p>EDIT:</p>

<p>If want use <code>sum</code> with missing values need <code>min_count=1</code> parameter - <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sum.html"" rel=""noreferrer""><code>sum</code></a>:</p>

<blockquote>
  <p><strong>min_count</strong> : int, default 0
  The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.</p>
  
  <p>New in version 0.22.0: Added with the default being 0. This means the sum of an all-NA or empty Series is 0, and the product of an all-NA or empty Series is 1.</p>
</blockquote>

<pre><code>print (data.groupby('id').resample('D', on='date').sum(min_count=1))
</code></pre>
",Pandas upsampling using groupby resample I grouped timeseries gaps I wan fill gaps respecting groupings date unique within id The following works gives zero I wan NaN data groupby id resample D date sum drop id axis reset index The following work reason data groupby id resample D date asfreq drop id axis reset index data groupby id resample D date fillna pad drop id axis reset index I get following error Upsampling level selection supported use set index explicitly set index datetime like I tried use pandas Grouper set index multilevel index single seems upsample date column get continous dates respect id column Pandas version Try data pd DataFrame id date datetime datetime datetime datetime datetime datetime value Works gives zeros data groupby id resample D date sum Fails data groupby id resample D date asfreq data groupby id resample D date fillna pad,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
54488538,2019-02-01,2019,2,Fill in missing dates in a groupby with a defined frequency with multiple columns,"<p>Imagine I have a dataframe that looks like:</p>

<pre><code>ID      DATE         VALUE_1   Value_2 ...
1    31-01-2006        5         ""USD""  
1    31-01-2007        5         ""USD""
1    31-01-2008        10        ""USD""
1    31-01-2011        11        ""USD""
2    31-12-2006        5         ""USD""
2    31-12-2007        5         ""USD""
2    31-12-2008        5         ""USD""
2    31-12-2009        5         ""USD""
</code></pre>

<p>With X more columns.</p>

<p>As you can see this is panel data with multiple entries on the same date for different IDs. What I want to do is fill in missing dates for each ID. You can see that for ID ""1"" there is a jump in months between the second and third entry. </p>

<p>I would like a dataframe that looks like the one below - keep in mind that I am looking for a solution that works for dataframes with many value columns +30 and many ID's (1000+), and still is efficient. I.e there should NOT be any data filling for ID's that are already ""complete"", meaning, that they already have a  frequency as specified by the data. In this case, a yearly frequency. Keep in mind though, that even though they have a yearly frequency, they don't always follow the calendar year. </p>

<pre><code>ID      DATE         VALUE_1   Value_2 ...
1    31-01-2006        5         ""USD""  
1    31-01-2007        5         ""USD""
1    31-01-2008        10        ""USD""
1    31-01-2009        NA          NA
1    31-01-2010        NA          NA
1    31-01-2011        11        ""USD""
2    31-12-2006        5         ""USD""
2    31-12-2007        5         ""USD""
2    31-12-2008        5         ""USD""
2    31-12-2009        5         ""USD""
</code></pre>
","['python', 'pandas', 'pandas-groupby']",54489048,"<p>Here is a fully flexible solution:</p>

<pre><code>def resample_custom_freq(data):
    """""" Resample datetime using different time offsets """"""

    # Compute the offsets
    month = data['Month'][0] - 1
    day = data['Day'][0] - 1

    # Modify data
    data = data.resample('AS').last().drop('ID', axis=1).reset_index().reset_index()
    data.loc[:, 'DATE'] += pd.offsets.MonthOffset(month)
    data.loc[:, 'DATE'] += pd.offsets.DateOffset(day)
    return data

df['DATE'] =  pd.to_datetime(df['DATE'])
df['Month'] = df['DATE'].dt.month
df['Day'] = df['DATE'].dt.day
df.set_index('DATE', inplace=True, drop=True)
df_1 = df.groupby('ID').apply(resample_custom_freq).reset_index().drop(['level_1', 'index', 'Month', 'Day'], axis=1)

df_1
Out[264]: 
   ID       DATE  VALUE_1 Value_2
0   1 2006-01-31      5.0   ""USD""
1   1 2007-01-31      5.0   ""USD""
2   1 2008-01-31     10.0   ""USD""
3   1 2009-01-31      NaN     NaN
4   1 2010-01-31      NaN     NaN
5   1 2011-01-31     11.0   ""USD""
6   2 2006-12-31      5.0   ""USD""
7   2 2007-12-31      5.0   ""USD""
8   2 2008-12-31      5.0   ""USD""
9   2 2009-12-31      5.0    ""USD
</code></pre>
",Fill missing dates groupby defined frequency multiple columns Imagine I dataframe looks like ID DATE VALUE Value USD USD USD USD USD USD USD USD With X columns As see panel data multiple entries date different IDs What I want fill missing dates ID You see ID jump months second third entry I would like dataframe looks like one keep mind I looking solution works dataframes many value columns many ID still efficient I e NOT data filling ID already complete meaning already frequency specified data In case yearly frequency Keep mind though even though yearly frequency always follow calendar year ID DATE VALUE Value USD USD USD NA NA NA NA USD USD USD USD USD,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
54651268,2019-02-12,2019,2,specify dtypes when saving pandas dataframe to a binary file,"<p>I have a pandas DataFrame I want to write to a binary file, however the df contains mixed dtypes. If I used <code>df.values.tofile()</code> I cannot specify different dtypes (even when specifying <code>astype('f4, f4, i4, i4').tofile()</code> in below example). Workaround at the moment is to use <code>struct</code> but is very slow!</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(data=np.random.random(size=(10, 4)) * 10, columns=['f1', 'f2', 'i1', 'i2'])
df.i1 = df.i1.astype(int)
df.i2 = df.i2.astype(int)

with open('tmp', 'w') as ply:    

    for ix, row in df.iterrows():

        ply.write(struct.pack('&lt;ffii', *row.values))
</code></pre>

<p>I am creating a <code>.ply</code> file which requires the data to be formatted correctly.</p>
","['python', 'pandas', 'numpy']",54836027,"<p>The solution seems to be <code>df.to_records(index=False).tobytes()</code></p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(data=np.random.random(size=(10, 4)) * 10, columns=['f1', 'f2', 'i1', 'i2'])
df.i1 = df.i1.astype(int)
df.i2 = df.i2.astype(int)

with open('test.dat', 'w') as fh:

    fh.write(df.to_records(index=False).tobytes())

arr = np.fromfile('test.dat', dtype='f8, f8, i8, i8')
df2 = pd.DataFrame(arr)
df2.columns = ['f1', 'f2', 'i1', 'i2'] # strange but if columns specified above then df2 == df is False :\
print np.all(df2 == df)
</code></pre>
",specify dtypes saving pandas dataframe binary file I pandas DataFrame I want write binary file however df contains mixed dtypes If I used df values tofile I cannot specify different dtypes even specifying astype f f tofile example Workaround moment use struct slow import pandas pd import numpy np df pd DataFrame data np random random size columns f f df df astype int df df astype int open tmp w ply ix row df iterrows ply write struct pack lt ffii row values I creating ply file requires data formatted correctly,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
54670877,2019-02-13,2019,2,How to apply single condition to a list of columns in a dataframe and add value to 4th column without using multiple OR&#39;s,"<p>I  need to apply a single condition to 3 columns of a dataframe and change value of 4th without using or statement .  </p>

<p>I can do with <code>np.where</code> but if the no of columns is big it's going to take a lot of time </p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'a':[1,2,3,4],'b':[1,3,6,7],'c':[4,6,4,1], 'd':['p','f','p','u'],'e':['a','a','b','c']})

df['d'] = np.where(df.a &gt; 4 | df.b &gt; 4 | df.c &gt; 4 , 'p',df['d']) 

df = pd.DataFrame({'a':[1,2,3,4,5],'b':[1,3,6,7],'c':[4,6,4,1], 'd':['p','f','p','f']})
df['d']=np.where(df.a &gt; 4 | df.b &gt; 4 | df.c &gt; 4 , 'p','f') 
</code></pre>

<p>I need someway of implementing same condition <code>&gt; , &lt;</code> to list of columns without using or for each.  </p>
","['python', 'pandas', 'dataframe']",54684381,"<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.gt.html#pandas.DataFrame.gt"" rel=""nofollow noreferrer"">DataFrame.gt</a> along with <code>np.where</code>:</p>

<pre><code>import numpy as np
import pandas as pd 

df = pd.DataFrame({'a':[1,2,3,4],'b':[1,3,6,7],'c':[4,6,4,1], 'd':['p','f','p','u'],'e':['a','a','b','c']})

# create a subset of a dataframe on which you want to check condition
new_df = df[['a','b','c']]
mask = new_df.gt(4).any(axis=1)  # check if any value is greater than 4

df['d'] = np.where(mask, 'p','f')

print(df)
</code></pre>

<p>Output:</p>

<pre><code>   a  b  c  d  e                                                                                                                      
0  1  1  4  f  a                                                                                                                      
1  2  3  6  p  a                                                                                                                      
2  3  6  4  p  b                                                                                                                      
3  4  7  1  p  c  
</code></pre>
",How apply single condition list columns dataframe add value th column without using multiple OR I need apply single condition columns dataframe change value th without using statement I np columns big going take lot time import pandas pd import numpy np df pd DataFrame b c p f p u e b c df np df gt df b gt df c gt p df df pd DataFrame b c p f p f df np df gt df b gt df c gt p f I need someway implementing condition gt lt list columns without using,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
54673261,2019-02-13,2019,2,Shift every values of a dataframe without changing columns,"<p>I have the <code>pandas DataFrame</code> below:</p>

<pre><code>pd.DataFrame(
    list(range(16,0,-1)),
    index=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P']
)
</code></pre>

<p>I would like to know if there is a <code>pandas function</code> that can allow me to shift by a number <code>x</code> the values of this dataframe in a way that: instance, if <code>x=3</code>, I get: </p>

<pre><code> D    E    F    G    H  I    J    K   L   M   N  O   P 
16   15   14   13   12  11   10   9   8   7   6  5   4
</code></pre>

<p>and makes sure that the last columns keeps the same order</p>
","['python', 'pandas', 'numpy']",54673669,"<p>Though <a href=""https://stackoverflow.com/questions/54673261/shift-every-values-of-a-dataframe-without-changing-columns#comment96136385_54673261"">@Chris</a>'s answer is intuitive and likely what you should use, I'll add my 2 cents.</p>

<h3><code>dropna</code> with <code>astype</code></h3>

<p>I don't like the conversion of integers to floats when shifting.  Also notice that I use <code>df.dtypes</code> in the <code>astype</code>.  This keeps it agnostic of what the types start out as.</p>

<pre><code>df.shift(3).dropna().astype(df.dtypes).T

    D   E   F   G   H   I   J  K  L  M  N  O  P
0  16  15  14  13  12  11  10  9  8  7  6  5  4
</code></pre>

<p>This is problematic if there are pre-existing NA in the data.  In that case, I'd include an <code>iloc</code> to trim the first 3 rows explicitly.  But if that is the case then just use the next solution</p>

<hr>

<h3><code>iloc</code> with <code>set_index</code></h3>

<pre><code>df.iloc[3:].set_index(df.index[:-3]).T

    D   E   F   G   H   I   J  K  L  M  N  O  P
0  16  15  14  13  12  11  10  9  8  7  6  5  4
</code></pre>

<hr>

<h3><code>pd.DataFrame</code></h3>

<pre><code>pd.DataFrame(df.values[:-3].T, df.columns, df.index[3:])

    D   E   F   G   H   I   J  K  L  M  N  O  P
0  16  15  14  13  12  11  10  9  8  7  6  5  4
</code></pre>
",Shift every values dataframe without changing columns I pandas DataFrame pd DataFrame list range index A B C D E F G H I J K L M N O P I would like know pandas function allow shift number x values dataframe way instance x I get D E F G H I J K L M N O P makes sure last columns keeps order,"startoftags, python, pandas, numpy, endoftags",python python3x pandas endoftags,python pandas numpy,python python3x pandas,0.67
54721914,2019-02-16,2019,2,"Difference of datetimes in hours, excluding the weekend","<p>I currently have a dataframe, where an uniqueID has multiple dates in another column.  I want extract the hours between each date, but ignore the weekend if the next date is after the weekend.  For example, if today is friday at 12 pm,
and the following date is tuesday at 12 pm then the difference in hours between these two dates would be 48 hours. </p>

<p>Here is my dataset with the expected output:</p>

<pre><code>df = pd.DataFrame({""UniqueID"": [""A"",""A"",""A"",""B"",""B"",""B"",""C"",""C""],""Date"":
[""2018-12-07 10:30:00"",""2018-12-10 14:30:00"",""2018-12-11 17:30:00"",
""2018-12-14 09:00:00"",""2018-12-18 09:00:00"",
""2018-12-21 11:00:00"",""2019-01-01 15:00:00"",""2019-01-07 15:00:00""],
""ExpectedOutput"": [""28.0"",""27.0"",""Nan"",""48.0"",""74.0"",""NaN"",""96.0"",""NaN""]})

df[""Date""] = df[""Date""].astype(np.datetime64)
</code></pre>

<p>This is what I have so far, but it includes the weekends:</p>

<pre><code>df[""date_diff""] = df.groupby([""UniqueID""])[""Date""].apply(lambda x: x.diff() 
/ np.timedelta64(1 ,'h')).shift(-1)
</code></pre>

<p>Thanks!</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",54722763,"<p>Idea is floor datetimes for remove <code>times</code> and get number of business days between start day + one day and shifted day to <code>hours3</code> column by <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.busday_count.html"" rel=""nofollow noreferrer""><code>numpy.busday_count</code></a> and then create <code>hour1</code> and <code>hour2</code> columns for start and end hours if not weekends hours. Last sum all hours columns together:</p>

<pre><code>df[""Date""] = pd.to_datetime(df[""Date""])
df = df.sort_values(['UniqueID','Date'])

df[""shifted""] = df.groupby([""UniqueID""])[""Date""].shift(-1)
df[""hours1""] = df[""Date""].dt.floor('d') 
df[""hours2""] = df[""shifted""].dt.floor('d') 

mask = df['shifted'].notnull()
f = lambda x: np.busday_count(x['hours1'] + pd.Timedelta(1, unit='d'), x['hours2'])
df.loc[mask, 'hours3'] = df[mask].apply(f, axis=1) * 24

mask1 = df['hours1'].dt.dayofweek &lt; 5
hours1 = df['hours1'] + pd.Timedelta(1, unit='d') - df['Date']
df['hours1'] = np.where(mask1, hours1, np.nan) / np.timedelta64(1 ,'h')

mask1 = df['hours2'].dt.dayofweek &lt; 5
df['hours2'] = np.where(mask1, df['shifted']-df['hours2'], np.nan) / np.timedelta64(1 ,'h')

df['date_diff'] = df['hours1'].fillna(0) + df['hours2'] + df['hours3']
</code></pre>

<hr>

<pre><code>print (df)
  UniqueID                Date ExpectedOutput             shifted  hours1  \
0        A 2018-12-07 10:30:00           28.0 2018-12-10 14:30:00    13.5   
1        A 2018-12-10 14:30:00           27.0 2018-12-11 17:30:00     9.5   
2        A 2018-12-11 17:30:00            Nan                 NaT     6.5   
3        B 2018-12-14 09:00:00           48.0 2018-12-18 09:00:00    15.0   
4        B 2018-12-18 09:00:00           74.0 2018-12-21 11:00:00    15.0   
5        B 2018-12-21 11:00:00            NaN                 NaT    13.0   
6        C 2019-01-01 15:00:00           96.0 2019-01-07 15:00:00     9.0   
7        C 2019-01-07 15:00:00            NaN                 NaT     9.0   

   hours2  hours3  date_diff  
0    14.5     0.0       28.0  
1    17.5     0.0       27.0  
2     NaN     NaN        NaN  
3     9.0    24.0       48.0  
4    11.0    48.0       74.0  
5     NaN     NaN        NaN  
6    15.0    72.0       96.0  
7     NaN     NaN        NaN  
</code></pre>

<p>First solution was removed with 2 reasons - was not accurate and slow:</p>

<pre><code>np.random.seed(2019)

dates = pd.date_range('2015-01-01','2018-01-01', freq='H')
df = pd.DataFrame({""UniqueID"": np.random.choice(list('ABCDEFGHIJ'), size=100),
                   ""Date"": np.random.choice(dates, size=100)})
print (df)
</code></pre>

<hr>

<pre><code>def old(df):
    df[""Date""] = pd.to_datetime(df[""Date""])
    df = df.sort_values(['UniqueID','Date'])

    df[""shifted""] = df.groupby([""UniqueID""])[""Date""].shift(-1)

    def f(x):
        a = pd.date_range(x['Date'],  x['shifted'], freq='T')
        return ((a.dayofweek &lt; 5).sum() / 60).round()


    mask = df['shifted'].notnull()
    df.loc[mask, 'date_diff'] = df[mask].apply(f, axis=1)  
    return df
</code></pre>

<hr>

<pre><code>def new(df):
    df[""Date""] = pd.to_datetime(df[""Date""])
    df = df.sort_values(['UniqueID','Date'])

    df[""shifted""] = df.groupby([""UniqueID""])[""Date""].shift(-1)
    df[""hours1""] = df[""Date""].dt.floor('d') 
    df[""hours2""] = df[""shifted""].dt.floor('d') 

    mask = df['shifted'].notnull()
    f = lambda x: np.busday_count(x['hours1'] + pd.Timedelta(1, unit='d'), x['hours2'])
    df.loc[mask, 'hours3'] = df[mask].apply(f, axis=1) * 24

    mask1 = df['hours1'].dt.dayofweek &lt; 5
    hours1 = df['hours1'] + pd.Timedelta(1, unit='d') - df['Date']
    df['hours1'] = np.where(mask1, hours1, np.nan) / np.timedelta64(1 ,'h')

    mask1 = df['hours2'].dt.dayofweek &lt; 5
    df['hours2'] = np.where(mask1, df['shifted'] - df['hours2'], np.nan) / np.timedelta64(1 ,'h')

    df['date_diff'] = df['hours1'].fillna(0) + df['hours2'] + df['hours3']
    return df
print (new(df))
print (old(df))
</code></pre>

<hr>

<pre><code>In [44]: %timeit (new(df))
22.7 ms Â± 115 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

In [45]: %timeit (old(df))
1.01 s Â± 8.03 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
</code></pre>
",Difference datetimes hours excluding weekend I currently dataframe uniqueID multiple dates another column I want extract hours date ignore weekend next date weekend For example today friday pm following date tuesday pm difference hours two dates would hours Here dataset expected output df pd DataFrame UniqueID A A A B B B C C Date ExpectedOutput Nan NaN NaN df Date df Date astype np datetime This I far includes weekends df date diff df groupby UniqueID Date apply lambda x x diff np timedelta h shift Thanks,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas numpy endoftags,python python3x pandas dataframe,python pandas numpy,0.58
54723409,2019-02-16,2019,2,How to find url multiple parameters and change the value?,"<p>How i can change multiple parameters value in this url: <a href=""https://google.com/?test=sadsad&amp;again=tesss&amp;dadasd=asdaas"" rel=""nofollow noreferrer"">https://google.com/?test=sadsad&amp;again=tesss&amp;dadasd=asdaas</a></p>

<p>You can see my code: i can just change 2 value!</p>

<p>This is the response <a href=""https://google.com/?test=aaaaa&amp;dadasd=howwww"" rel=""nofollow noreferrer"">https://google.com/?test=aaaaa&amp;dadasd=howwww</a></p>

<p><code>again</code> parameter not in the response! how i can change the value and add it to the url?</p>

<pre><code>def between(value, a, b):
    pos_a = value.find(a)
    if pos_a == -1: return """"
    pos_b = value.rfind(b)
    if pos_b == -1: return """"
    adjusted_pos_a = pos_a + len(a)
    if adjusted_pos_a &gt;= pos_b: return """"
    return value[adjusted_pos_a:pos_b]

def before(value, a):
    pos_a = value.find(a)
    if pos_a == -1: return """"
    return value[0:pos_a]

def after(value, a):
    pos_a = value.rfind(a)
    if pos_a == -1: return """"
    adjusted_pos_a = pos_a + len(a)
    if adjusted_pos_a &gt;= len(value): return """"
    return value[adjusted_pos_a:]


test = ""https://google.com/?test=sadsad&amp;again=tesss&amp;dadasd=asdaas""
if ""&amp;"" in test:
    print(test.replace(between(test, ""="", ""&amp;""), 'aaaaa').replace(after(test, ""=""), 'howwww'))
else:
    print(test.replace(after(test, ""=""), 'test'))
</code></pre>

<p>Thanks!</p>
","['python', 'python-3.x', 'python-2.7']",54723956,"<p>From your code it seems like you are probably fairly new to programming, so first of all congratulations on having attempted to solve your problem.</p>

<p>As you might expect, there are language features you may not know about yet that can help with problems like this. (There are also libraries specifically for parsing URLs, but point you to those wouldn't help your progress in Python quite as much - if you are just trying to get some job done they might be a godsend).</p>

<p>Since the question lacks a little clarity (don't worry - I can only speak and write English, so you are ahead of me there), I'll try to explain a simpler approach to your problem. From the last block of your code I understand your intent to be:</p>

<blockquote>
  <p>""If there are multiple parameters, replace the value of the first with <code>'aaaaa'</code> and the others with <code>'howwww'</code>. If there is only one, replace its value with <code>'test'</code>.""</p>
</blockquote>

<p>Your code is a fair attempt (at what I think you want to do). I hope the following discussion will help you. First, set <code>url</code> to your example initially.</p>

<pre><code>&gt;&gt;&gt; url = ""https://google.com/?test=sadsad&amp;again=tesss&amp;dadasd=asdaas""
</code></pre>

<p>While the code deals with multiple arguments or one, it doesn't deal with no arguments at all. This may or may not matter, but I like to <a href=""https://en.wikipedia.org/wiki/Defensive_programming"" rel=""nofollow noreferrer"">program defensively</a>, having made too many silly mistakes in the past. Further, detecting that case early simplifies the remaining logic by eliminating an ""edge case"" (something the general flow of your code does not handle). If I were writing a function (good when you want to repeat actions) I'd start it with something like</p>

<pre><code> if ""?"" not in url:
    return url
</code></pre>

<p>I skipped this here because I know what the sample string is and I'm not writing a function. Once you know there <em>are</em> arguments, you can split them out quite easily with</p>

<pre><code>&gt;&gt;&gt; stuff, args = url.split(""?"", 1)
</code></pre>

<p>The second argument to <code>split</code> is another defensive measure, telling it to ignore all but the first question mark. Since we know there <em>is</em> at least one, this guarantees there will always be two elements in the result, and Python won't complain about a different number of names as values in that assignment. Let's confirm their values:</p>

<pre><code>&gt;&gt;&gt; stuff, args
('https://google.com/', 'test=sadsad&amp;again=tesss&amp;dadasd=asdaas')
</code></pre>

<p>Now we have the arguments alone, we can split them out into a list:</p>

<pre><code>&gt;&gt;&gt; key_vals = args.split(""&amp;"")
&gt;&gt;&gt; key_vals
['test=sadsad', 'again=tesss', 'dadasd=asdaas']
</code></pre>

<p>Now you can create a list of key,value pairs:</p>

<pre><code>&gt;&gt;&gt; kv_pairs = [kv.split(""="", 1) for kv in key_vals]
&gt;&gt;&gt; kv_pairs
[['test', 'sadsad'], ['again', 'tesss'], ['dadasd', 'asdaas']]
</code></pre>

<p>At this point you can do whatever is appropriate do the keys and values - deleting elements, changing values, changing keys, and so on. You could create a dictionary from them, but beware repeated keys. I assume you can change <code>kv_pairs</code> to reflect the final URL you want.</p>

<p>Once you have made the necessary changes, putting the return value back together is relatively simple: we have to put an <code>""=""</code> between each key and value, then a ""&amp;"" between each resulting string, then join the stuff back up with a ""?"". One step at a time:</p>

<pre><code>&gt;&gt;&gt; [f""{k}={v}"" for (k, v) in kv_pairs]
['test=sadsad', 'again=tesss', 'dadasd=asdaas']

&gt;&gt;&gt; ""&amp;"".join(f""{k}={v}"" for (k, v) in kv_pairs)
'test=sadsad&amp;again=tesss&amp;dadasd=asdaas'

&gt;&gt;&gt; stuff + ""?"" + ""&amp;"".join(f""{k}={v}"" for (k, v) in kv_pairs)
'https://google.com/?test=sadsad&amp;again=tesss&amp;dadasd=asdaas'
</code></pre>
",How find url multiple parameters change value How change multiple parameters value url https google com test sadsad amp tesss amp dadasd asdaas You see code change value This response https google com test aaaaa amp dadasd howwww parameter response change value add url def value b pos value find pos return pos b value rfind b pos b return adjusted pos pos len adjusted pos gt pos b return return value adjusted pos pos b def value pos value find pos return return value pos def value pos value rfind pos return adjusted pos pos len adjusted pos gt len value return return value adjusted pos test https google com test sadsad amp tesss amp dadasd asdaas amp test print test replace test amp aaaaa replace test howwww else print test replace test test Thanks,"startoftags, python, python3x, python27, endoftags",python arrays numpy endoftags,python python3x python27,python arrays numpy,0.33
54810815,2019-02-21,2019,2,Getting data from span class in python,"<p>I am new to data scraping and I am using <code>BeautifulSoup</code> to grap some data from a webpage.</p>

<p>What I have done is the following</p>

<pre><code>all = soup.find_all(""span"",{""class"":""compare-property""})

arg=all[0]

print(arg)
</code></pre>

<p>The output is:</p>

<pre><code>&lt; span class=""compare-property"" data-placement=""top"" data-propid=""1858251""    data-toggle=""tooltip"" id=""compare-link-1858251"" title=""Bera saman""&gt;
&lt; i class=""fa fa-plus""&gt;&lt;/i&gt;
&lt; /span&gt;'
</code></pre>

<p>Now I need the number called <code>data-propid</code>, which is 1858251 in the example
How can I get that number?</p>
","['python', 'web-scraping', 'beautifulsoup']",54810909,"<p>You should get it with </p>

<pre><code>all[0]['data-propid']
</code></pre>

<p>Greetings
Kai Dannies</p>
",Getting data span class python I new data scraping I using BeautifulSoup grap data webpage What I done following soup find span class compare property arg print arg The output lt span class compare property data placement top data propid data toggle tooltip id compare link title Bera saman gt lt class fa fa plus gt lt gt lt span gt Now I need number called data propid example How I get number,"startoftags, python, webscraping, beautifulsoup, endoftags",python arrays numpy endoftags,python webscraping beautifulsoup,python arrays numpy,0.33
54832721,2019-02-22,2019,2,How to do complex selection in pandas?,"<p>I have a df like below:</p>

<pre><code>President   Start Date  End Date
B Clinton   1992-01-01  1999-12-31
G Bush      2000-01-01  2007-12-31
B Obama     2008-01-01  2015-12-31
D Trump     2016-01-01  2019-12-31 # not too far away!!
</code></pre>

<p>I want to create another df, something like this</p>

<pre><code>timestamp   President
1992-01-01  B Clinton
1992-01-02  B Clinton
...
2000-01-01  G Bush
...
</code></pre>

<p>Basically I want to create a dataframe which its index is time stamp and then its content is selected based on the condition on the two columns of another df.</p>

<p>I feel there is a way within pandas to do this, but I am not sure how. I tried to use <code>np.piecewise</code> but seems generating the conditions will be very hard for me. How could I do this?</p>
","['python', 'pandas', 'dataframe']",54833277,"<p>This is another <a href=""https://stackoverflow.com/a/53218939/7964527"">unnesting</a> problem </p>

<pre><code>df['New']=[pd.date_range(x,y).tolist() for x , y in zip (df.StartDate,df.EndDate)]

unnesting(df,['New'])
</code></pre>

<hr>

<p>FYI I have pasted the function here </p>

<pre><code>def unnesting(df, explode):
    idx=df.index.repeat(df[explode[0]].str.len())
    df1=pd.concat([pd.DataFrame({x:np.concatenate(df[x].values)} )for x in explode],axis=1)
    df1.index=idx
    return df1.join(df.drop(explode,1),how='left')
</code></pre>
",How complex selection pandas I df like President Start Date End Date B Clinton G Bush B Obama D Trump far away I want create another df something like timestamp President B Clinton B Clinton G Bush Basically I want create dataframe index time stamp content selected based condition two columns another df I feel way within pandas I sure I tried use np piecewise seems generating conditions hard How could I,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
54973175,2019-03-03,2019,2,retrieving values in one row of dataframes based on value in other,"<p>I have mutiple DataFrames, each containing a row called 'location' and another row called 'value' (both make up the index). for example, suppose i have the following 2:</p>

<pre><code>df1 = pd.DataFrame(np.array([[-4,2,5],['nyc','sf','chi']]), columns=['col1','col2','col3'], index=['value','location'])

df2 = pd.DataFrame(np.array([[5,0,-3],['nyc','sf','chi']]), columns=['col1','col2','col3'], index=['value','location'])
</code></pre>

<p>the DataFrames will be housed in a dictionary that I can iterate through. Ultimately, I want to retrieve the list of 'value's for each 'location' in a separate DataFrame. so the desired output would look like:</p>

<p><a href=""https://i.stack.imgur.com/ayL93.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ayL93.png"" alt=""enter image description here""></a></p>

<p>this is a toy example, while my real one will have many more DataFrames and the source DataFrames will have other rows besides the 2 key ones I am interested in</p>
","['python', 'pandas', 'dataframe']",54973209,"<p>I would recommend <code>set_index</code> and <code>concat</code>:</p>

<pre><code>(pd.concat([df.T.set_index('location')['value'] for df in [df1, df2]], axis=1)
   .T
   .reset_index(drop=True))

location nyc sf chi
0         -4  2   5
1          5  0  -3
</code></pre>
",retrieving values one row dataframes based value I mutiple DataFrames containing row called location another row called value make index example suppose following df pd DataFrame np array nyc sf chi columns col col col index value location df pd DataFrame np array nyc sf chi columns col col col index value location DataFrames housed dictionary I iterate Ultimately I want retrieve list value location separate DataFrame desired output would look like toy example real one many DataFrames source DataFrames rows besides key ones I interested,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
55020741,2019-03-06,2019,5,math operations between column in multiindex dataframe,"<p>I have a dataframe with column multiindex that I need to slice and perform math operations between the slices.</p>

<pre><code># sample df
idx=pd.IndexSlice
np.random.seed(123)
tuples = list(zip(*[['one', 'one', 'two', 'two', 'three', 'three'],['foo', 'bar', 'foo', 'bar', 'foo', 'bar']]))
index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])
df = pd.DataFrame(np.random.randn(3, 6), index=['A', 'B', 'C'], columns=index)
</code></pre>

<p>If I wanted to perform say addition/subtraction between individual columns, I could use index slice and do it like this:</p>

<pre><code>df.loc[:,idx['three','foo']] - df.loc[:,idx['two','foo']]
</code></pre>

<p>However, if I want to use higher level slice it doesn't work and return NaNs:</p>

<pre><code># not working
df.loc[:,idx['three',:]] - df.loc[:,idx['two',:]]
</code></pre>

<p>Is there an easy way to use higher level slices of the df and add/subtract corresponding columns only?  My dataframe potentially contains hundreds of columns in multiindex. Thanks</p>
","['python', 'pandas', 'dataframe']",55020842,"<p>If need MultiIndex in output use <code>rename</code> for same level od MultiIndex:</p>

<pre><code>df = df.loc[:,idx['three',:]] - df.loc[:,idx['two',:]].rename(columns={'two':'three'})
print (df)
first      three          
second       foo       bar
A      -0.861579  3.157731
B      -1.944822  0.772031
C       2.649912  2.621137
</code></pre>

<p>Advantage is possible rename both levels to new index names and join to original:</p>

<pre><code>df = (df.join(df.loc[:,idx['three',:]].rename(columns={'three':'four'}) - 
              df.loc[:,idx['two',:]].rename(columns={'two':'four'})))
print (df)
first        one                 two               three                four  \
second       foo       bar       foo       bar       foo       bar       foo   
A      -1.085631  0.997345  0.282978 -1.506295 -0.578600  1.651437 -0.861579   
B      -2.426679 -0.428913  1.265936 -0.866740 -0.678886 -0.094709 -1.944822   
C       1.491390 -0.638902 -0.443982 -0.434351  2.205930  2.186786  2.649912   

first             
second       bar  
A       3.157731  
B       0.772031  
C       2.621137  
</code></pre>

<p>If not necessary, use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.xs.html"" rel=""noreferrer""><code>DataFrame.xs</code></a>:</p>

<pre><code>df1 = df.xs('three', axis=1, level=0) - df.xs('two', axis=1, level=0)
print (df1)
second       foo       bar
A      -0.861579  3.157731
B      -1.944822  0.772031
C       2.649912  2.621137
</code></pre>

<p>If need first level one possible solution is <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.MultiIndex.from_product.html"" rel=""noreferrer""><code>MultiIndex.from_product</code></a>:</p>

<pre><code>df1 = df.xs('three', axis=1, level=0) - df.xs('two', axis=1, level=0)
df1.columns = pd.MultiIndex.from_product([['new'], df1.columns], 
                                         names=['first','second'])
print (df1)
first        new          
second       foo       bar
A      -0.861579  3.157731
B      -1.944822  0.772031
C       2.649912  2.621137
</code></pre>
",math operations column multiindex dataframe I dataframe column multiindex I need slice perform math operations slices sample df idx pd IndexSlice np random seed tuples list zip one one two two three three foo bar foo bar foo bar index pd MultiIndex tuples tuples names first second df pd DataFrame np random randn index A B C columns index If I wanted perform say addition subtraction individual columns I could use index slice like df loc idx three foo df loc idx two foo However I want use higher level slice work return NaNs working df loc idx three df loc idx two Is easy way use higher level slices df add subtract corresponding columns My dataframe potentially contains hundreds columns multiindex Thanks,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
55149738,2019-03-13,2019,2,Pandas replace values with NaN at random,"<p>I am testing the performance of a machine learning algorithm, specifically how it handles missing data and what kind of performance degrades are experienced when variables are missing.</p>

<p>For example when 20% of variable x is missing the accuracy of the model goes down by a certain %. In order to do this I would like to simulate the missing data by replacing 20% of the rows in a dataframe column.</p>

<p>Is there an existing way to do this?</p>

<p>starting df:</p>

<pre><code>d = {'var1': [1, 2, 3, 4], 'var2': [5, 6, 7, 8]}
df = pd.DataFrame(data=d)
df
    var1   var2
0     1     5
1     2     6
2     3     7
3     4     8
</code></pre>

<p>end result:
drop 50% of column 'var1' at random</p>

<pre><code>df
    var1   var2
0    nan    5
1     2     6
2    nan    7
3     4     8
</code></pre>
","['python', 'pandas', 'dataframe']",55149944,"<p>Reassign using the <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.sample.html"" rel=""noreferrer""><code>sample</code></a> method, and pandas will introduce <code>NaN</code> values due to auto-alignment:</p>

<pre><code>df['var1'] = df['var1'].sample(frac=0.5)
</code></pre>

<p>Interactively:</p>

<pre><code>In [1]: import pandas as pd
   ...: d = {'var1': [1, 2, 3, 4], 'var2': [5, 6, 7, 8]}
   ...: df = pd.DataFrame(data=d)
   ...: df
   ...:
Out[1]:
   var1  var2
0     1     5
1     2     6
2     3     7
3     4     8

In [2]: df['var1'] = df['var1'].sample(frac=0.5)

In [3]: df
Out[3]:
   var1  var2
0   1.0     5
1   NaN     6
2   3.0     7
3   NaN     8
</code></pre>
",Pandas replace values NaN random I testing performance machine learning algorithm specifically handles missing data kind performance degrades experienced variables missing For example variable x missing accuracy model goes certain In order I would like simulate missing data replacing rows dataframe column Is existing way starting df var var df pd DataFrame data df var var end result drop column var random df var var nan nan,"startoftags, python, pandas, dataframe, endoftags",python pandas datetime endoftags,python pandas dataframe,python pandas datetime,0.67
55248600,2019-03-19,2019,3,Pandas specifying custom holidays,"<p>I'm trying to specify business days in a foreign country, but I can't get the pandas function <code>pd.bdate_range()</code> to recognize holidays. My code is as follows:</p>

<pre><code>import pandas as pd
import datetime

weekmask = ""Mon Tue Wed Thu Fri""

holidays = [datetime.datetime(2017, 1, 9), datetime.datetime(2017, 3, 20),
            datetime.datetime(2017, 4, 13)]

BdaysCol2017 = pd.bdate_range(start = pd.datetime(2017, 1, 1), 
                              end = pd.datetime(2017, 12, 31), 
                              weekmask = weekmask, 
                              holidays = holidays)
</code></pre>

<p>But I get the following error on the <code>holidays</code> parameter:</p>

<pre><code>ValueError: a custom frequency string is required when holidays or weekmask are passed, got frequency B
</code></pre>

<p>Why is this? How can I specify custom holidays? Is there a better way to do this? </p>

<p>Thank you</p>
","['python', 'pandas', 'datetime']",55250895,"<p>as the docs specify about weekmask and holidays:</p>

<blockquote>
  <p>only used when custom frequency strings are passed</p>
</blockquote>

<p>so you need:</p>

<pre><code>BdaysCol2017 = pd.bdate_range(start = pd.datetime(2017, 1, 1), 
                          end = pd.datetime(2017, 12, 31),
                          freq='C',
                          weekmask = weekmask,
                          holidays=holidays)
</code></pre>
",Pandas specifying custom holidays I trying specify business days foreign country I get pandas function pd bdate range recognize holidays My code follows import pandas pd import datetime weekmask Mon Tue Wed Thu Fri holidays datetime datetime datetime datetime datetime datetime BdaysCol pd bdate range start pd datetime end pd datetime weekmask weekmask holidays holidays But I get following error holidays parameter ValueError custom frequency string required holidays weekmask passed got frequency B Why How I specify custom holidays Is better way Thank,"startoftags, python, pandas, datetime, endoftags",python pandas matplotlib endoftags,python pandas datetime,python pandas matplotlib,0.67
55262665,2019-03-20,2019,3,finding groups that meet a condition in pandas groupby,"<p>This is my dataframe:</p>

<pre><code>df = pd.DataFrame({'a':list('xxxyyzz'), 'b':[10,20,30,5,3,1,2]})
</code></pre>

<p>I group them:</p>

<pre><code>groups = df.groupby('a')
</code></pre>

<p>I want to print the groups that has at least one <code>b</code> above 20. In this case I want to print <code>x</code>.
This is my desired outcome:</p>

<pre><code>x
   a   b
0  x  10
1  x  20
2  x  30
</code></pre>
","['python', 'pandas', 'pandas-groupby']",55262697,"<p>Compare values by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.gt.html"" rel=""noreferrer""><code>Series.gt</code></a>, grouping by <code>a</code> column like <code>Series</code> - <code>df['a']</code> and use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.transform.html"" rel=""noreferrer""><code>GroupBy.transform</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.any.html"" rel=""noreferrer""><code>GroupBy.any</code></a> for test at least one <code>True</code> per groups:</p>

<pre><code>df1 = df[df['b'].gt(20).groupby(df['a']).transform('any')]
print (df1)
   a   b
0  x  10
1  x  20
2  x  30
</code></pre>
",finding groups meet condition pandas groupby This dataframe df pd DataFrame list xxxyyzz b I group groups df groupby I want print groups least one b In case I want print x This desired outcome x b x x x,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
55592217,2019-04-09,2019,3,Removing duplicates based on two columns while deleting inconsistent data,"<p>I have a pandas dataframe like this:</p>

<pre><code>   a  b  c
0  1  1  1
1  1  1  0
2  2  4  1    
3  3  5  0
4  3  5  0
</code></pre>

<p>where the first 2 columns ('a' and 'b') are IDs while the last one ('c') is a validation (0 = neg, 1 = pos). I do know how to remove duplicates based on the values of the first 2 columns, however in this case I would also like to get rid of inconsistent data i.e. duplicated data validated both as positive and negative. So for example the first 2 rows are duplicated but inconsistent hence I should remove the entire record, while the last 2 rows are both duplicated and consistent so I'd keep one of the records. The expected result sholud be:</p>

<pre><code>   a  b  c
0  2  4  1
1  3  5  0
</code></pre>

<p>The real dataframe can have more than two duplicates per group and
as you can see also the index has been changed. Thanks.</p>
","['python', 'python-3.x', 'pandas']",55592283,"<p>First filter rows by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.transform.html"" rel=""nofollow noreferrer""><code>GroupBy.transform</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.SeriesGroupBy.nunique.html"" rel=""nofollow noreferrer""><code>SeriesGroupBy.nunique</code></a> for get only unique values groups with <a href=""http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer""><code>boolean indexing</code></a> and then <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html"" rel=""nofollow noreferrer""><code>DataFrame.drop_duplicates</code></a>:</p>

<pre><code>df = (df[df.groupby(['a','b'])['c'].transform('nunique').eq(1)]
           .drop_duplicates(['a','b'])
           .reset_index(drop=True))
print (df)
   a  b  c
0  2  4  1
1  3  5  0
</code></pre>

<p><strong>Detail</strong>:</p>

<pre><code>print (df.groupby(['a','b'])['c'].transform('nunique'))
0    2
1    2
2    1
3    1
4    1
Name: c, dtype: int64
</code></pre>
",Removing duplicates based two columns deleting inconsistent data I pandas dataframe like b c first columns b IDs last one c validation neg pos I know remove duplicates based values first columns however case I would also like get rid inconsistent data e duplicated data validated positive negative So example first rows duplicated inconsistent hence I remove entire record last rows duplicated consistent I keep one records The expected result sholud b c The real dataframe two duplicates per group see also index changed Thanks,"startoftags, python, python3x, pandas, endoftags",python python3x pandas endoftags,python python3x pandas,python python3x pandas,1.0
55702440,2019-04-16,2019,3,How to append the df to a another df in a process of loop,"<p>This code gets the data and the data is made into a loop and that runs until the loops gets completed. </p>

<p>So i need to append the data to a df that stores the data after every process complete</p>

<p>code :</p>

<pre><code>a = ""SELECT id FROM USER WHERE time &gt;'2018-03-01'""
dataa = pd.read_sql_query(a, con=engine)
print(dataa)

for userid in dataa:
   x=f""SELECT idbody FROM col1 WHERE user_id='{userid}'""
   data = pd.read_sql_query(x,con = engine)
</code></pre>

<p>so here data gets is processed and  data every time  produced is different need to append the data to a df that stores all the data that gets processed</p>
","['python', 'python-3.x', 'pandas']",55702757,"<p>In loop or by list comprehension append values to <code>list</code> and only once use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a>:</p>

<pre><code>a = ""SELECT id FROM USER WHERE time &gt;'2018-03-01'""
dataa = pd.read_sql_query(a, con=engine)

dfs = []
for userid in dataa:
    x=f""SELECT idbody FROM col1 WHERE user_id='{userid}'""
    data = pd.read_sql_query(x,con = engine)
    dfs.append(data)

df = pd.concat(dfs, ignore_index=True)
</code></pre>

<hr>

<pre><code>dfs = [pd.read_sql_query(f""SELECT idbody FROM col1 WHERE user_id='{userid}'"",con = engine) 
       for userid in dataa]

df = pd.concat(dfs, ignore_index=True)
</code></pre>
",How append df another df process loop This code gets data data made loop runs loops gets completed So need append data df stores data every process complete code SELECT id FROM USER WHERE time gt dataa pd read sql query con engine print dataa userid dataa x f SELECT idbody FROM col WHERE user id userid data pd read sql query x con engine data gets processed data every time produced different need append data df stores data gets processed,"startoftags, python, python3x, pandas, endoftags",python python3x pandas endoftags,python python3x pandas,python python3x pandas,1.0
55742925,2019-04-18,2019,2,How to fill column with values based on string contains condition,"<p>I have the following data:</p>

<pre><code>data = {
    'employee'  : ['Emp1', 'Emp2', 'Emp3', 'Emp4', 'Emp5'],
    'code'      : ['2018_1', '2018_3', '2019_1', '2019_2', '2017_1'],
}

old_salary_bonus = 3000

new_salary_bonus = {
    '2019_1': 1000,
    '2019_2': 980,
}

df = pd.DataFrame(data)
</code></pre>

<p>Task: Add df['salary_bonus'] column based on the following condition:
If employee's code contains '2019', use code value to retrieve salary bonus value from new_salary_bonus, else use old_salary_bonus value.</p>

<p>Expected Output:</p>

<pre><code>   employee  code     salary_bonus
0  Emp1      2018_1   3000
1  Emp2      2018_3   3000
2  Emp3      2019_1   1000
3  Emp4      2019_2   980
4  Emp5      2017_1   3000
</code></pre>

<p>Please help.</p>
","['python', 'python-3.x', 'pandas']",55742952,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>Series.map</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.fillna.html"" rel=""nofollow noreferrer""><code>Series.fillna</code></a> for repplace non matched values:</p>

<pre><code>import pandas as pd

data = {
    'employee'  : ['Emp1', 'Emp2', 'Emp3', 'Emp4', 'Emp5'],
    'code'      : ['2018_1', '2018_3', '2019_1', '2019_2', '2017_1'],
}

old_salary_bonus = 3000

new_salary_bonus = {
    '2019_1': 1000,
    '2019_2': 980,
}

df = pd.DataFrame(data)
</code></pre>

<hr>

<pre><code>df['salary_bonus'] = df['code'].map(new_salary_bonus).fillna(old_salary_bonus)
print (df)
  employee    code  salary_bonus
0     Emp1  2018_1        3000.0
1     Emp2  2018_3        3000.0
2     Emp3  2019_1        1000.0
3     Emp4  2019_2         980.0
4     Emp5  2017_1        3000.0
</code></pre>

<p>Another solution with <code>get</code> with default value if not matched:</p>

<pre><code>df['salary_bonus'] = df['code'].map(lambda x: new_salary_bonus.get(x, old_salary_bonus))
</code></pre>
",How fill column values based string contains condition I following data data employee Emp Emp Emp Emp Emp code old salary bonus new salary bonus df pd DataFrame data Task Add df salary bonus column based following condition If employee code contains use code value retrieve salary bonus value new salary bonus else use old salary bonus value Expected Output employee code salary bonus Emp Emp Emp Emp Emp Please help,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
55800911,2019-04-22,2019,2,conditionally merge cells&#39; contents in a column,"<p>Looking for a pandanic way to turn the following df:</p>

<pre><code>    name    desc
0   A       a
1   NaN     aa
2   NaN     aaa
3   B       b
4   NaN     bb
</code></pre>

<p>into:</p>

<pre><code>    name    desc
0   A       a
            aa
            aaa
3   B       b
            bb

# strings in desc are concat-ed together with end of line char
</code></pre>

<p>I am thinking of the general directions of either itertuple or backfill+groupby, but both of those approaches require some juggling.</p>

<p>here is the starting point:</p>

<pre><code>import pandas as pd
import numpy as np
nan = np.nan

df = pd.DataFrame(
    {'name': ['A', nan, nan, 'B', nan],
    'desc': ['a', 'aa', 'aaa', 'b', 'bb']}
)
</code></pre>
","['python', 'pandas', 'dataframe']",55801020,"<p>I think you want a combination of <code>fillna(method='ffill')</code> and <code>groupby</code>. </p>

<p>How does this look?</p>

<pre><code>import pandas as pd
import numpy as np
nan = np.nan

df = pd.DataFrame(
    {'name': ['A', nan, nan, 'B', nan],
    'desc': ['a', 'aa', 'aaa', 'b', 'bb']}
)

df['name'] = df['name'].fillna(method='ffill')

df = df.groupby('name')['desc'].apply(lambda d: '\n'.join(d)).reset_index()
print df
</code></pre>

<p>prints</p>

<pre><code>  name        desc
0    A  a\naa\naaa
1    B       b\nbb
</code></pre>
",conditionally merge cells contents column Looking pandanic way turn following df name desc A NaN aa NaN aaa B b NaN bb name desc A aa aaa B b bb strings desc concat ed together end line char I thinking general directions either itertuple backfill groupby approaches require juggling starting point import pandas pd import numpy np nan np nan df pd DataFrame name A nan nan B nan desc aa aaa b bb,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
55844330,2019-04-25,2019,2,"expected dense_input to have shape (7,) but got array with shape (1,)","<p>I'm trying to train a model on features i extracted from some images, model trains fine, but when i try model.predict it gives me this error.
"" expected dense_input to have shape (7,) but got array with shape (1,)""
i have the knowledge about the shape of the input but the error is just weird. it makes no sense to me right now i tried to print the shape of the input i am giving to model.predict and its fine.</p>

<pre><code>import numpy as np

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation

trainX = np.array(train_set)

trainY = np.array(train_labels)

model = Sequential()

model.add(Dense(8, input_dim=7, activation='relu'))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(trainX, trainY, nb_epoch=1200, batch_size=2, verbose=2)
model.save('my_model.h5') 

for i in np.array(test_set):
    print(i.shape)
    dataPrediction = model.predict(i)
    print (dataPrediction, '&lt;--- Predicted number')
    print (test_labels[i],' &lt;-- Correct answer \n')
</code></pre>

<p>print(i.shape) gives me (7,)
yet it gives me error
Error when checking input: expected dense_input to have shape (7,) but got array with shape (1,)</p>
","['python', 'tensorflow', 'keras']",55844388,"<p>That's because your model expects an array of samples, but you're giving it a single sample at a time. </p>

<p>It therefore treated each feature in your sample as an individual sample of shape <code>(1,)</code>, which didn't make sense to it since you presumably had 7 features and it therefore expected a sample of shape <code>(7,)</code>.</p>

<p>You can just do <code>model.predict(np.array(test_set))</code>.</p>
",expected dense input shape got array shape I trying train model features extracted images model trains fine try model predict gives error expected dense input shape got array shape knowledge shape input error weird makes sense right tried print shape input giving model predict fine import numpy np tensorflow keras models import Sequential tensorflow keras layers import Dense Activation trainX np array train set trainY np array train labels model Sequential model add Dense input dim activation relu model add Dense model compile loss mean squared error optimizer adam model fit trainX trainY nb epoch batch size verbose model save model h np array test set print shape dataPrediction model predict print dataPrediction lt Predicted number print test labels lt Correct answer n print shape gives yet gives error Error checking input expected dense input shape got array shape,"startoftags, python, tensorflow, keras, endoftags",python django djangorestframework endoftags,python tensorflow keras,python django djangorestframework,0.33
55916061,2019-04-30,2019,4,No legends Seaborn lineplot,"<p>When I make a lineplot with Seaborn with multiple lineplots on the same axis, no legends are created. Even if I supply the argument ""brief"" or ""full"" for legend in sns.lineplot, nothing shows, and calling ax.get_legend_handles_labels() returns two empty lists.</p>

<p>How can I add legends in a box on the right hand side, linking the color of a line to a name?</p>

<p><a href=""https://i.stack.imgur.com/oSHXn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oSHXn.png"" alt=""enter image description here""></a></p>

<pre><code>import seaborn as sns
import matplotlib.pyplot as plt
import random

fig1 = plt.figure(figsize=(12, 12))
ax = fig1.add_subplot(1, 1, 1)

x = range(10)

series = list()
for i in range(3):
    y_i = list()
    for j in range(10):
        y_i.append(random.randint(0, 50))

    sns.lineplot(x=x, y=y_i, ax=ax, legend='brief')

plt.show()
</code></pre>
","['python', 'matplotlib', 'seaborn']",55916268,"<p>That's because you applied simple lists as x- and y-data. They have no metadata like names or labels. You can add a label by the <code>label</code>-kwarg though, e.g.:</p>

<pre><code>sns.lineplot(x=x, y=y_i, ax=ax, legend='brief', label=str(i))
</code></pre>

<p>Or you decide to use a pandas series object or a dataframe for your data, which are able to provide column names.</p>
",No legends Seaborn lineplot When I make lineplot Seaborn multiple lineplots axis legends created Even I supply argument brief full legend sns lineplot nothing shows calling ax get legend handles labels returns two empty lists How I add legends box right hand side linking color line name import seaborn sns import matplotlib pyplot plt import random fig plt figure figsize ax fig add subplot x range series list range list j range append random randint sns lineplot x x ax ax legend brief plt show,"startoftags, python, matplotlib, seaborn, endoftags",python arrays numpy endoftags,python matplotlib seaborn,python arrays numpy,0.33
56139551,2019-05-14,2019,2,Duplicating rows where a cell contains multiple pieces of data,"<p>I would like to take a dataframe and duplicate certain rows. 
One column, called <code>name</code>, may have multiple names.
An example dataframe is contructed below:</p>

<pre><code>data = [
    ['Joe', '17-11-2018', '2'],
    ['Karen', '17-11-2018', '4'],
    ['Bill, Avery', '17-11-2018', '6'],
    ['Sam', '18-11-2018', '4'],
    ['Alex, Frank', '18-11-2018', '6'],
    ['Chris', '18-11-2018', '8'],
]
df = pd.DataFrame(data, columns = ['name','date','number'])
</code></pre>

<p>This yields the following dataframe:</p>

<pre><code>          name        date number
0          Joe  17-11-2018      2
1        Karen  17-11-2018      4
2  Bill, Avery  17-11-2018      6
3          Sam  18-11-2018      4
4  Alex, Frank  18-11-2018      6
5        Chris  18-11-2018      8
</code></pre>

<p>I would like to take all rows where there are multiple names (comma-separated) and duplicate them for each individual name. The resulting dataframe should look like this:</p>

<pre><code>    name        date number
0    Joe  17-11-2018      2
1  Karen  17-11-2018      4
2   Bill  17-11-2018      6
3  Avery  17-11-2018      6
4    Sam  18-11-2018      4
5   Alex  18-11-2018      6
6  Frank  18-11-2018      6
7  Chris  18-11-2018      8
</code></pre>
","['python', 'python-3.x', 'pandas']",56139590,"<p>After <code>str.split</code> , it become a <a href=""https://stackoverflow.com/questions/53218931/how-to-unnest-explode-a-column-in-a-pandas-dataframe/53218939#53218939""><code>unnest</code></a> problem </p>

<pre><code>df['name']=df.name.str.split(',')

unnesting(df,['name'])
Out[97]: 
     name        date number
0     Joe  17-11-2018      2
1   Karen  17-11-2018      4
2    Bill  17-11-2018      6
2   Avery  17-11-2018      6
3     Sam  18-11-2018      4
4    Alex  18-11-2018      6
4   Frank  18-11-2018      6
5   Chris  18-11-2018      8
</code></pre>

<hr>

<pre><code>def unnesting(df, explode):
    idx = df.index.repeat(df[explode[0]].str.len())
    df1 = pd.concat([
        pd.DataFrame({x: np.concatenate(df[x].values)}) for x in explode], axis=1)
    df1.index = idx
    return df1.join(df.drop(explode, 1), how='left')
</code></pre>
",Duplicating rows cell contains multiple pieces data I would like take dataframe duplicate certain rows One column called name may multiple names An example dataframe contructed data Joe Karen Bill Avery Sam Alex Frank Chris df pd DataFrame data columns name date number This yields following dataframe name date number Joe Karen Bill Avery Sam Alex Frank Chris I would like take rows multiple names comma separated duplicate individual name The resulting dataframe look like name date number Joe Karen Bill Avery Sam Alex Frank Chris,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
56195891,2019-05-18,2019,2,Speed up datetime formatting when reading csv,"<p>I have a csv file with 4+m records.</p>

<p>I import it using pd.read_csv('big_file.csv', dtype=object)</p>

<p>This file has 2 columns with dates in the following format: 
'yyyy-mm-ddThh:mm:ss.nsTZ' e.g.'2018-05-05T04:39:09.447Z'</p>

<p>I need to transform them to 
'yyyy-mm-dd H:M:S' e.g. '2018-09-23 06:03:12'</p>

<p>I use the following code to do so:</p>

<pre class=""lang-py prettyprint-override""><code>df['created'] = pd.to_datetime(arg=df.created).dt.strftime('%Y-%m-%d %H:%M:%S')
df['lastLogin'] = pd.to_datetime(arg=df.lastLogin).dt.strftime('%Y-%m-%d %H:%M:%S')
df['lastUpdated'] = pd.to_datetime(arg=df.lastUpdated).dt.strftime('%Y-%m-%d %H:%M:%S')
df['created'] = pd.to_datetime(arg=df.created)
df['lastLogin'] = pd.to_datetime(arg=df.lastLogin)
df['lastUpdated'] = pd.to_datetime(arg=df.lastUpdated)
</code></pre>

<p>This process is extremely slow:</p>

<pre><code>CPU times: user 1min 48s, sys: 1.19 s, total: 1min 49s
Wall time: 1min 49s
</code></pre>

<p>Is there a way to speed it up?</p>
","['python', 'python-3.x', 'pandas']",56312179,"<p>From the comment, I understand that you don't need the meaning of the dates but you just want to change the aesthetic appearance of the strings representing the dates. Then, you can just treat the data simply as strings. So, I did it like this.</p>

<pre><code>#!/usr/bin/python3

import numpy as np
import sys

def gen_sample(numdata, outfname):
    yy=np.random.randint(1905, 2018, 2*numdata)
    mm=np.random.randint(   1,   13, 2*numdata)
    dd=np.random.randint(   1,   29, 2*numdata)
    hhh=np.random.randint(   0,   25, 2*numdata)
    mmm=np.random.randint(   0,   61, 2*numdata)
    sss=np.random.randint(   0,   61, 2*numdata)
    baboon=np.random.randint(   0, 1000, 2*numdata)
    with open(outfname, 'w') as outf:
        for jj in range(numdata):
            outf.write('%4.4i-%2.2i-%2.2iT%2.2i:%2.2i:%2.2i.%3.3iZ,%4.4i-%2.2i-%2.2iT%2.2i:%2.2i:%2.2i.%3.3iZ\n'
                       %(yy[2*jj],   mm[2*jj],   dd[2*jj],
                         hhh[2*jj],  mmm[2*jj],  sss[2*jj],  baboon[2*jj],
                         yy[2*jj+1], mm[2*jj+1], dd[2*jj+1],
                         hhh[2*jj+1], mmm[2*jj+1], sss[2*jj+1], baboon[2*jj+1]))


def convert(infname,outfname):

    data=np.loadtxt(infname, dtype=np.str, delimiter=',', ndmin=2)
    with open(outfname,'w') as outf:
        for jr in range(data.shape[0]):
            outf.write('%s %s,%s %s\n'%(
                data[jr,0][0:10],
                data[jr,0][11:19],
                data[jr,1][0:10],
                data[jr,1][11:19] ))


if __name__=='__main__':
    sample_fname= 'daa.csv'
    out_fname= 'daadaa.csv'
    if len(sys.argv)&gt;1:
        numdata=int(sys.argv[1])
        gen_sample(numdata, sample_fname)
    else:
        convert(sample_fname, out_fname)
</code></pre>

<p>and it took me about 15 seconds for 4M*2 data on my computer. Please see this</p>

<pre><code>#!/bin/bash

for jj in 0 1 2
do
  echo ""generating sample..""
  ./main.py 4000000
  echo ""loading, converting, and writing..""
  echo ""----""
  /usr/bin/time ./main.py 
  echo ""----""
done
</code></pre>

<p>and this</p>

<pre><code>$ ./run.sh 
generating sample..
loading, converting, and writing..
----
14.96user 0.94system 0:15.05elapsed 105%CPU (0avgtext+0avgdata 818724maxresident)k
8inputs+312504outputs (0major+315787minor)pagefaults 0swaps
----
generating sample..
loading, converting, and writing..
----
14.91user 0.93system 0:14.99elapsed 105%CPU (0avgtext+0avgdata 818848maxresident)k
16inputs+312504outputs (0major+315864minor)pagefaults 0swaps
----
generating sample..
loading, converting, and writing..
----
15.39user 0.95system 0:15.52elapsed 105%CPU (0avgtext+0avgdata 818736maxresident)k
8inputs+312504outputs (0major+315857minor)pagefaults 0swaps
----
</code></pre>

<p>Input file is like</p>

<pre><code>$ head daa.csv 
2016-10-05T08:07:03.214Z,1973-10-01T12:36:21.367Z
1961-08-24T02:08:57.436Z,1953-03-06T00:56:12.486Z
1986-09-07T17:15:60.322Z,1952-11-19T19:02:56.159Z
1939-08-17T05:13:19.659Z,1920-12-15T16:46:52.628Z
2004-11-09T02:29:25.905Z,1925-02-07T10:37:49.142Z
2011-12-12T10:46:38.583Z,1992-02-10T08:58:60.284Z
1968-01-23T05:05:05.151Z,1935-09-17T07:12:49.392Z
1916-04-05T18:55:35.281Z,1919-10-12T10:05:10.249Z
1970-10-04T21:45:16.751Z,1951-01-08T16:58:51.190Z
1910-01-19T22:12:04.088Z,2006-03-08T09:26:45.690Z
</code></pre>

<p>Output file was like</p>

<pre><code>$ head daadaa.csv 
2016-10-05 08:07:03,1973-10-01 12:36:21
1961-08-24 02:08:57,1953-03-06 00:56:12
1986-09-07 17:15:60,1952-11-19 19:02:56
1939-08-17 05:13:19,1920-12-15 16:46:52
2004-11-09 02:29:25,1925-02-07 10:37:49
2011-12-12 10:46:38,1992-02-10 08:58:60
1968-01-23 05:05:05,1935-09-17 07:12:49
1916-04-05 18:55:35,1919-10-12 10:05:10
1970-10-04 21:45:16,1951-01-08 16:58:51
1910-01-19 22:12:04,2006-03-08 09:26:45
</code></pre>

<p>If you don't need to write the converted data back to a file, 
maybe the run time will be faster.
You will need to make the function more robust for the variation of your own data, but I hope the idea is delivered.</p>
",Speed datetime formatting reading csv I csv file records I import using pd read csv big file csv dtype object This file columns dates following format yyyy mm ddThh mm ss nsTZ e g T Z I need transform yyyy mm dd H M S e g I use following code df created pd datetime arg df created dt strftime Y H M S df lastLogin pd datetime arg df lastLogin dt strftime Y H M S df lastUpdated pd datetime arg df lastUpdated dt strftime Y H M S df created pd datetime arg df created df lastLogin pd datetime arg df lastLogin df lastUpdated pd datetime arg df lastUpdated This process extremely slow CPU times user min sys total min Wall time min Is way speed,"startoftags, python, python3x, pandas, endoftags",python pandas datetime endoftags,python python3x pandas,python pandas datetime,0.67
56431408,2019-06-03,2019,2,Adding rows to DataFrame conditioned on values,"<p>I have the following Pandas DataFrame:</p>

<pre><code>     start_timestamp_milli  end_timestamp_milli       name  rating
1            1555414708025        1555414723279    Valence       2   
2            1555414708025        1555414723279    Arousal       6   
3            1555414708025        1555414723279  Dominance       2   
4            1555414708025        1555414723279    Sadness       1
5            1555414708025        1555414723279    Happiness     0
6            1555414708025        1555414723279    Anger         0
7            1555414708025        1555414723279    Surprise      0
8            1555414708025        1555414723279    Stress        0
9            1555414813304        1555414831795    Valence       3   
10           1555414813304        1555414831795    Arousal       5   
11           1555414813304        1555414831795  Dominance       2   
12           1555414813304        1555414831795    Sadness       0
13           1555414813304        1555414831795    Happiness     0
14           1555414813304        1555414831795    Anger         0
15           1555414708025        1555414723279    Surprise      0
16           1555414708025        1555414723279    Stress        0   
17           1555414921819        1555414931382    Valence       1   
18           1555414921819        1555414931382    Arousal       7   
19           1555414921819        1555414931382  Dominance       2   
20           1555414921819        1555414931382    Sadness       1 
21           1555414921819        1555414931382   Happiness      0  
22           1555414921819        1555414931382    Anger         1
23           1555414708025        1555414723279    Surprise      0
24           1555414708025        1555414723279    Stress        1 
</code></pre>

<p>Now, for each block with the same <code>start_timestamp_milli</code> and <code>end_timestamp_milli</code>, I would like to insert an additional row with name ""Neutral"" and a rating of 1 if the rating of Sadness, Happiness, Anger, Surprise and Stress is 0 and otherwise 0. The <code>start_timestamp_milli</code> and <code>end_timestamp_milli</code> of the new row should be set to the values of that block.</p>

<p>The resulting DataFrame should look like this:</p>

<pre><code>     start_timestamp_milli  end_timestamp_milli       name  rating
1            1555414708025        1555414723279    Valence       2   
2            1555414708025        1555414723279    Arousal       6   
3            1555414708025        1555414723279  Dominance       2   
4            1555414708025        1555414723279    Sadness       1
5            1555414708025        1555414723279    Happiness     0
6            1555414708025        1555414723279    Anger         0
7            1555414708025        1555414723279    Surprise      0
8            1555414708025        1555414723279    Stress        0
9            1555414708025        1555414723279    Neutral       0
10           1555414813304        1555414831795    Valence       3   
11           1555414813304        1555414831795    Arousal       5   
12           1555414813304        1555414831795  Dominance       2   
13           1555414813304        1555414831795    Sadness       0
14           1555414813304        1555414831795    Happiness     0
15           1555414813304        1555414831795    Anger         0
16           1555414708025        1555414723279    Surprise      0
17           1555414708025        1555414723279    Stress        0
18           1555414708025        1555414723279    Neutral       1   
19           1555414921819        1555414931382    Valence       1   
20           1555414921819        1555414931382    Arousal       7   
21           1555414921819        1555414931382  Dominance       2   
22           1555414921819        1555414931382    Sadness       1 
23           1555414921819        1555414931382   Happiness      0  
24           1555414921819        1555414931382    Anger         1
25           1555414708025        1555414723279    Surprise      0
26           1555414708025        1555414723279    Stress        1 
27           1555414708025        1555414723279    Neutral       0
</code></pre>

<p>How can this be done?</p>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",56431697,"<p>You can do with filter before <code>groupby</code> <code>agg</code> + <code>all</code> , then <code>concat</code> back the result </p>

<pre><code>s=df.loc[df.name.isin(['Sadness', 'Happiness', 'Anger', 'Surprise' , 'Stress']),'rating'].\
       eq(0).\
            groupby([df['start_timestamp_milli'],df['end_timestamp_milli']]).\
                 agg('all').reset_index().assign(name='Neutral')
df=pd.concat([df,s],sort=False).sort_values(['start_timestamp_milli','end_timestamp_milli'])
df
Out[66]: 
    start_timestamp_milli  end_timestamp_milli       name  rating
1           1555414708025        1555414723279    Valence       2
2           1555414708025        1555414723279    Arousal       6
3           1555414708025        1555414723279  Dominance       2
4           1555414708025        1555414723279    Sadness       1
5           1555414708025        1555414723279  Happiness       0
6           1555414708025        1555414723279      Anger       0
7           1555414708025        1555414723279   Surprise       0
8           1555414708025        1555414723279     Stress       0
15          1555414708025        1555414723279   Surprise       0
16          1555414708025        1555414723279     Stress       0
23          1555414708025        1555414723279   Surprise       0
24          1555414708025        1555414723279     Stress       1
0           1555414708025        1555414723279    Neutral       0
9           1555414813304        1555414831795    Valence       3
10          1555414813304        1555414831795    Arousal       5
11          1555414813304        1555414831795  Dominance       2
12          1555414813304        1555414831795    Sadness       0
13          1555414813304        1555414831795  Happiness       0
14          1555414813304        1555414831795      Anger       0
1           1555414813304        1555414831795    Neutral       1
17          1555414921819        1555414931382    Valence       1
18          1555414921819        1555414931382    Arousal       7
19          1555414921819        1555414931382  Dominance       2
20          1555414921819        1555414931382    Sadness       1
21          1555414921819        1555414931382  Happiness       0
22          1555414921819        1555414931382      Anger       1
2           1555414921819        1555414931382    Neutral       0
</code></pre>
",Adding rows DataFrame conditioned values I following Pandas DataFrame start timestamp milli end timestamp milli name rating Valence Arousal Dominance Sadness Happiness Anger Surprise Stress Valence Arousal Dominance Sadness Happiness Anger Surprise Stress Valence Arousal Dominance Sadness Happiness Anger Surprise Stress Now block start timestamp milli end timestamp milli I would like insert additional row name Neutral rating rating Sadness Happiness Anger Surprise Stress otherwise The start timestamp milli end timestamp milli new row set values block The resulting DataFrame look like start timestamp milli end timestamp milli name rating Valence Arousal Dominance Sadness Happiness Anger Surprise Stress Neutral Valence Arousal Dominance Sadness Happiness Anger Surprise Stress Neutral Valence Arousal Dominance Sadness Happiness Anger Surprise Stress Neutral How done,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas dataframe pandasgroupby,python pandas dataframe,0.87
56691002,2019-06-20,2019,2,Marking &quot;True&quot; rows that contain a keyword (keyword being the column name) in pandas in one iteration,"<p>I want to do a keyword search on pandas dataframe, adding each keyword as a column to the dataset, and marking ""True"" rows of data that contain the keyword. </p>

<p>A piece of code that does it nicely is this: </p>

<pre class=""lang-py prettyprint-override""><code>stocks = ['Microsoft','Apple', 'Amazon']
for stock in stocks:
    df[stock] = df.astype(str).sum(axis=1).str.contains(stock)
</code></pre>

<p>However, this loops through the entire dataset once for each keyword. I would like to do the same thing in one iteration (i.e. each row is checked for the presence of the keywords only once), because my dataset is large. </p>

<p>The expected result is this:</p>

<p><a href=""https://i.stack.imgur.com/xXggw.png"" rel=""nofollow noreferrer"">Expected dataframe</a></p>

<p>Any help will be appreciated</p>

<p>EDIT: I am getting a Memory Error. </p>
","['python', 'pandas', 'dataframe']",56698682,"<p>I can't imagine there is something that does what you want without iterating over the three keywards. </p>

<p>Still, I have two suggestions:</p>

<ol>
<li>Do it just once: It might be a long operation, but then you dump it to a file, and next time you need it, is already there: load the file and you're good to go.</li>
</ol>

<p>You can use something simple like csv:</p>

<pre><code>my_df.to_csv(""my_file.csv"")
</code></pre>

<p>or if you want to be faster use the <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_hdf.html"" rel=""nofollow noreferrer"">hdf5</a> format</p>

<ol start=""2"">
<li>Define your ""text"" column as string-type outside the loop. For what I see from the picture, you don't need to sum(axis=1), but if you alctually must, do it also outside the loop: create a series and then loop over it.</li>
</ol>

<p>For example:</p>

<pre><code>text_series = df[stock].astype(str).sum(axis=1)
for stock in stocks:
   df[stock] = text_series.str.contains(stock)
</code></pre>

<ol start=""3"">
<li>Compute the operation in batches.</li>
</ol>

<p>For example:</p>

<pre><code>batch_size = df.shape[0]/10 # e.g. divide your df in 10 chunks
for i in range(0,df.shape[0],batch_size): #batch_size is the step
   for stock in stocks:
      df[i:i+batch_size][stock] = text_series[i:i+batch_size].str.contains(stock)
</code></pre>

<p>N.B. your database has to be divisible by 10 otherwise choose another divisor</p>
",Marking quot True quot rows contain keyword keyword column name pandas one iteration I want keyword search pandas dataframe adding keyword column dataset marking True rows data contain keyword A piece code nicely stocks Microsoft Apple Amazon stock stocks df stock df astype str sum axis str contains stock However loops entire dataset keyword I would like thing one iteration e row checked presence keywords dataset large The expected result Expected dataframe Any help appreciated EDIT I getting Memory Error,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
56802883,2019-06-28,2019,2,Deriving multiple columns from single column data in python 3 pandas,"<p>Source Data:</p>

<pre><code>20  7369    CLERK
30  7499    SALESMAN
30  7521    SALESMAN
20  7566    MANAGER
30  7654    SALESMAN
30  7698    MANAGER
10  7782    MANAGER
20  7788    ANALYST
10  7839    PRESIDENT
30  7844    SALESMAN
20  7876    CLERK
30  7900    CLERK
20  7902    ANALYST
</code></pre>

<p>Requirement:
<code>012345678901234567890123456789</code></p>

<p>Hi All,<br></p>

<p>I am reading this .dat file data into python pandas successfully.
Left to right length of the data in a row is 30 (012345678901234567890123456789)
My requirement is,
I need to derive 3 columns</p>

<pre><code>From left to right: 1 to 4 (length 4) spaces as DEPTNO 
From left to right: 5 to 13 (length 9) spaces as EMPNO 
From left to right: 14 to 30 (length 9) spaces as EMPNO 
</code></pre>

<p>I tried this code:</p>

<pre><code>import pandas as pd    
with open('Emp.dat','r') as f:
    next(f) # skip first row
    df = pd.DataFrame(l.rstrip().split() for l in f)
</code></pre>

<p>Required Output:</p>

<pre><code>DEPTNO  EMPNO   JOB
20      7369    CLERK
30      7499    SALESMAN
30      7521    SALESMAN
20      7566    MANAGER
30      7654    SALESMAN
30      7698    MANAGER
10      7782    MANAGER
20      7788    ANALYST
10      7839    PRESIDENT
30      7844    SALESMAN
20      7876    CLERK
30      7900    CLERK
20      7902    ANALYST
</code></pre>
","['python', 'python-3.x', 'pandas']",56803007,"<p>Maybe use a <code>columns</code> argument:</p>

<pre><code>import pandas as pd    
with open('Emp.dat','r') as f:
    next(f) # skip first row
    df = pd.DataFrame((l.rstrip().split() for l in f), columns=['DEPTNO', 'EMPNO', 'JOB'])
</code></pre>

<p>Output:</p>

<pre><code>   DEPTNO EMPNO        JOB
0      20  7369      CLERK
1      30  7499   SALESMAN
2      30  7521   SALESMAN
3      20  7566    MANAGER
4      30  7654   SALESMAN
5      30  7698    MANAGER
6      10  7782    MANAGER
7      20  7788    ANALYST
8      10  7839  PRESIDENT
9      30  7844   SALESMAN
10     20  7876      CLERK
11     30  7900      CLERK
12     20  7902    ANALYST
</code></pre>
",Deriving multiple columns single column data python pandas Source Data CLERK SALESMAN SALESMAN MANAGER SALESMAN MANAGER MANAGER ANALYST PRESIDENT SALESMAN CLERK CLERK ANALYST Requirement Hi All I reading dat file data python pandas successfully Left right length data row My requirement I need derive columns From left right length spaces DEPTNO From left right length spaces EMPNO From left right length spaces EMPNO I tried code import pandas pd open Emp dat r f next f skip first row df pd DataFrame l rstrip split l f Required Output DEPTNO EMPNO JOB CLERK SALESMAN SALESMAN MANAGER SALESMAN MANAGER MANAGER ANALYST PRESIDENT SALESMAN CLERK CLERK ANALYST,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
56830117,2019-07-01,2019,3,Get item of list in dataframe column using loop,"<p>I have dataframe <code>df</code> like this :</p>

<pre><code>    date        item
    2019-03-29  [book,pencil]
    ...
</code></pre>

<p>I want to get every item  in list using loop. this is what I've tried:</p>

<pre><code>    for i in range(len(df)):
        for x in df['attributeName'][i]:
            print(x)
</code></pre>

<p>But i got every single character. The desired output is:</p>

<pre><code>    book
    pencil
</code></pre>

<p>How do I solve this problem?</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",56830152,"<pre><code>df2['item'].apply(pd.Series).unstack().reset_index(drop=True)
</code></pre>

<p>A better alternative.. <a href=""https://stackoverflow.com/questions/54432583/when-should-i-ever-want-to-use-pandas-apply-in-my-code"">A Common Pitfall: Exploding Columns of Lists</a></p>

<pre><code>pd.DataFrame(df2['item'].tolist()).unstack().reset_index(drop=True)
</code></pre>

<p><strong>Output</strong></p>

<pre><code>0      book
1    pencil
dtype: object
</code></pre>
",Get item list dataframe column using loop I dataframe df like date item book pencil I want get every item list using loop I tried range len df x df attributeName print x But got every single character The desired output book pencil How I solve problem,"startoftags, python, python3x, pandas, dataframe, endoftags",python python3x pandas endoftags,python python3x pandas dataframe,python python3x pandas,0.87
57043695,2019-07-15,2019,3,pandas - downsample a more frequent DataFrame to the frequency of a less frequent DataFrame,"<p>I have two DataFrames that have different data measured at different frequencies, as in those csv examples:</p>

<p>df1:</p>

<pre><code>i,m1,m2,t
0,0.556529,6.863255,43564.844
1,0.5565576199999884,6.86327749999999,43564.863999999994
2,0.5565559400000003,6.8632764,43564.884
3,0.5565699799999941,6.863286799999996,43564.903999999995
4,0.5565570200000007,6.863277200000001,43564.924
5,0.5565316400000097,6.863257100000007,43564.944
...
</code></pre>

<p>df2:</p>

<pre><code>i,m3,m4,t
0,306.81162500000596,-1.2126870045404683,43564.878125
1,306.86175000000725,-1.1705838272666433,43564.928250000004
2,306.77552454544787,-1.1240195386446195,43564.97837499999
3,306.85900545454086,-1.0210345363692084,43565.0285
4,306.8354250000052,-1.0052431772666657,43565.078625
5,306.88397499999286,-0.9468344809917896,43565.12875
...
</code></pre>

<p>I would like to obtain a single df that have all the measures of both dfs at the times of the first one (which get data less frequently).</p>

<p>I tried to do that with a for loop averaging over the df2 measures between two timestamps of df1 but it was <strong>extremely slow</strong>.</p>
","['python', 'pandas', 'dataframe']",57043933,"<p>IIUC,  <code>i</code> is index column and you want to put <code>df2['t']</code> in bins and averaging the other columns. So you can use <code>pd.cut</code>:</p>

<pre><code>groups =pd.cut(df2.t, bins= list(df1.t) + [np.inf],
               right=False,
               labels=df1['t'])

# cols to copy
cols = [col for col in df2.columns if col != 't']

# groupby and get the average
new_df = (df2[cols].groupby(groups)
                   .mean()
                   .reset_index()
         )
</code></pre>

<p>Then <code>new_df</code> is:</p>

<pre><code>           t          m3        m4
0  43564.844         NaN       NaN
1  43564.864  306.811625 -1.212687
2  43564.884         NaN       NaN
3  43564.904         NaN       NaN
4  43564.924  306.861750 -1.170584
5  43564.944  306.838482 -1.024283
</code></pre>

<p>which you can merge with <code>df1</code> on <code>t</code>:</p>

<pre><code>df1.merge(new_df, on='t', how='left')
</code></pre>

<p>and get:</p>

<pre><code>         m1        m2        t          m3        m4
0  0.556529  6.863255  43564.8         NaN       NaN
1  0.556558  6.863277  43564.9  306.811625 -1.212687
2  0.556556  6.863276  43564.9         NaN       NaN
3  0.556570  6.863287  43564.9         NaN       NaN
4  0.556557  6.863277  43564.9  306.861750 -1.170584
5  0.556532  6.863257  43564.9  306.838482 -1.024283
</code></pre>
",pandas downsample frequent DataFrame frequency less frequent DataFrame I two DataFrames different data measured different frequencies csv examples df df I would like obtain single df measures dfs times first one get data less frequently I tried loop averaging df measures two timestamps df extremely slow,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
57077024,2019-07-17,2019,2,How do you create rows for every categories in a column?,"<p>Say that I have the following data. Like how many times my kids opened the fridge for each hour from 1 PM to 3 PM.</p>

<pre><code>| ----- | ----- | ----- |
|  Name |  Hour |  Open |
| ----- | ----- | ----- |
|  Bob  |     1 |     4 |
| ----- | ----- | ----- |
|  Bob  |     3 |     2 |
| ----- | ----- | ----- |
|  Jane |     1 |     1 |
| ----- | ----- | ----- |
|  Jane |     2 |     7 |
| ----- | ----- | ----- |
</code></pre>

<p>If I call this with pandas, how do I fill the missing hours so I could have the following dataframe?</p>

<pre><code>| ----- | ----- | ----- |
|  Name |  Hour |  Open |
| ----- | ----- | ----- |
|  Bob  |     1 |     4 |
| ----- | ----- | ----- |
|  Bob  |     2 |  None |    &lt;&lt;-- New row with Null or 0 for 'Open' column.
| ----- | ----- | ----- |
|  Bob  |     3 |     2 |
| ----- | ----- | ----- |
|  Jane |     1 |     1 |
| ----- | ----- | ----- |
|  Jane |     2 |     7 |
| ----- | ----- | ----- |
|  Jane |     3 |  None |    &lt;&lt;-- New row with Null or 0 for 'Open' column.
| ----- | ----- | ----- |
</code></pre>

<p>Obviously, I kinda need it to be automatic so I could use it for some real data. So I can't just insert a row. The index or value sorting is not important.</p>
","['python', 'pandas', 'dataframe']",57077096,"<p>Idea is use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>DataFrame.reindex</code></a> by all possible combinations created by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.MultiIndex.from_product.html"" rel=""nofollow noreferrer""><code>MultiIndex.from_product</code></a>:</p>

<pre><code>mux = pd.MultiIndex.from_product([df['Name'].unique(), 
                                  range(1, df['Hour'].max() + 1)], names=['Name','Hour'])
df1 = (df.set_index(['Name','Hour'])
         .reindex(mux)
         .reset_index())
print (df1)
   Name  Hour  Open
0   Bob     1   4.0
1   Bob     2   NaN
2   Bob     3   2.0
3  Jane     1   1.0
4  Jane     2   7.0
5  Jane     3   NaN
</code></pre>

<p>If use pandas 0.24+ is possible use <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html"" rel=""nofollow noreferrer"">Nullable Integer Data Type</a>:</p>

<pre><code>df1 = (df.set_index(['Name','Hour'])
         .reindex(mux).astype('Int64')
         .reset_index())
print (df1)
   Name  Hour  Open
0   Bob     1     4
1   Bob     2   NaN
2   Bob     3     2
3  Jane     1     1
4  Jane     2     7
5  Jane     3   NaN
</code></pre>

<p>And for replace non exist values to <code>0</code> add <code>fill_value</code> parameter:</p>

<pre><code>df1 = (df.set_index(['Name','Hour'])
         .reindex(mux, fill_value=0)
         .reset_index())
print (df1)
   Name  Hour  Open
0   Bob     1     4
1   Bob     2     0
2   Bob     3     2
3  Jane     1     1
4  Jane     2     7
5  Jane     3     0
</code></pre>
",How create rows every categories column Say I following data Like many times kids opened fridge hour PM PM Name Hour Open Bob Bob Jane Jane If I call pandas I fill missing hours I could following dataframe Name Hour Open Bob Bob None lt lt New row Null Open column Bob Jane Jane Jane None lt lt New row Null Open column Obviously I kinda need automatic I could use real data So I insert row The index value sorting important,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
57120458,2019-07-19,2019,2,Pivot datetime index to a start and end column,"<p>I'm having a problem with switching this data: </p>

<pre><code>datetime               transmission #
2019-07-12 00:03:06    124
2019-07-12 00:04:56    124
2019-07-12 00:20:10    125
2019-07-12 00:21:33    125
</code></pre>

<p>to a format like this using the python module pandas:</p>

<pre><code>transmission #   start                  end
124              2019-07-12 00:03:06    2019-07-12 00:04:56
125              2019-07-12 00:20:10    2019-07-12 00:21:33
</code></pre>

<p>At first, I thought I could do this with a pivot where the index was transmission # and the values were <code>datetime</code> but I can't seem to make it work. </p>

<pre><code>print(df.pivot(index = 'ConnectDisconnect', columns=['start', 'end'], values='data_point_time'))
</code></pre>

<p>ConnectDisconnect is the <code>transmission#</code>. I thought this would work but it just outputs </p>

<pre><code>Traceback (most recent call last):
  File ""data.py"", line 28, in &lt;module&gt;
    print(df.pivot(index = 'ConnectDisconnect', columns=['start', 'end'], values='data_point_time'))
  File ""C:\Program Files (x86)\Python37-32\lib\site-packages\pandas\core\generic.py"", line 5067, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'Series' object has no attribute 'pivot'
</code></pre>

<p>if anyone could help me out with this, it would be much appreciated</p>
","['python', 'pandas', 'datetime']",57121222,"<pre><code>df = pd.read_csv('test.csv', sep=r'\s{2,}', engine='python')
n = len(df.index)//2
x = df['datetime']
# drop the column of 'datetime'
df = df.drop('datetime', axis=1)
# Remove the duplicated row
df = df.drop_duplicates()
df.index=range(n) 
start = x[0::2]
start.index=range(n)
end = x[1::2]
end.index = range(n)
df['start'] =start 
df['end']=end 
print(df.to_string(index=False)) 
</code></pre>

<p>Output is </p>

<pre><code>transmission #                start                  end
           124  2019-07-12 00:03:06  2019-07-12 00:04:56
           125  2019-07-12 00:20:10  2019-07-12 00:21:33
</code></pre>
",Pivot datetime index start end column I problem switching data datetime transmission format like using python module pandas transmission start end At first I thought I could pivot index transmission values datetime I seem make work print df pivot index ConnectDisconnect columns start end values data point time ConnectDisconnect transmission I thought would work outputs Traceback recent call last File data py line lt module gt print df pivot index ConnectDisconnect columns start end values data point time File C Program Files x Python lib site packages pandas core generic py line getattr return object getattribute self name AttributeError Series object attribute pivot anyone could help would much appreciated,"startoftags, python, pandas, datetime, endoftags",python pandas numpy endoftags,python pandas datetime,python pandas numpy,0.67
57143942,2019-07-22,2019,4,Group list while recursive looping,"<p>This is my sample input and output:</p>

<pre><code>l = [
    ['random_str0', 'random_str', 'random_str'],
    ['random_str1', '', 'random_str'],
    ['random_str2', '', ''],
    ['random_str3', 'random_str', 'random_str'],
    ['random_str4', '', ''],
    ['random_str5', '', ''],
    ['random_str6', 'random_str', ''],
    ['random_str7', 'random_str', 'random_str'],
    ['random_str8', '', ''],
    ['random_str9', '', ''],
    ['random_str10', '', ''],
    ['random_str11', '', ''],
]


out = [ # something like this. data structure and type and order are not important
    ['random_str0', 'random_str', 'random_str'],
    [
        ['random_str1', '', 'random_str']
        ['random_str2', '', '']
    ],
    [
        ['random_str3', 'random_str', 'random_str'],
        ['random_str4', '', ''],
        ['random_str5', '', '']
    ],
    ['random_str6', 'random_str', ''],
    [
        ['random_str7', 'random_str', 'random_str'],
        ['random_str8', '', ''],
        ['random_str9', '', ''],
        ['random_str10', '', ''],
        ['random_str11', '', '']
    ]
]
</code></pre>

<p>The idea if any inner list is having either index 1 or 2 value is followed by one or more list having missing index 1 and 2 value, it forms a group. (My actual code is more complex and have other conditions as well but for brevity, it is omitted as it is not part of the actual question.</p>

<p>This is how I tried.</p>

<pre><code>for n in reversed(range(1, 5)):
    for i in range(len(l)-n):
        group = [l[i+j] for j in range(n+1)]
        if (
            (group[0][1] or group[0][2]) and
            all([not (g[1] and g[2]) for g in group[1:]])
        ):
            print(group)

Out: # not desired as it is overlapping.
[
    ['random_str7', 'random_str', 'random_str'],
    ['random_str8', '', ''],
    ['random_str9', '', ''],
    ['random_str10', '', ''],
    ['random_str11', '', '']
]
[
    ['random_str7', 'random_str', 'random_str'],
    ['random_str8', '', ''],
    ['random_str9', '', ''],
    ['random_str10', '', '']
]
[
    ['random_str3', 'random_str', 'random_str'],
    ['random_str4', '', ''],
    ['random_str5', '', '']
]
[
    ['random_str7', 'random_str', 'random_str'],
    ['random_str8', '', ''],
    ['random_str9', '', '']
]
[
    ['random_str1', 'random_str', 'random_str'],
    ['random_str2', '', '']
]
[
    ['random_str3', 'random_str', 'random_str'],
    ['random_str4', '', '']
]
[
    ['random_str7', 'random_str', 'random_str'],
    ['random_str8', '', '']
]
</code></pre>

<p><strong>The Question is</strong> how can I track record such that the grouping doesn't overlap. I think recursive looping will help but I don't know how to accomplish that.</p>

<p>the final data structure is not needed to be a list. I tried it with dicts but the code becomes more complicated.</p>

<p><strong>for more clarification, I created step by step pastebin</strong>
<a href=""https://pastebin.com/qeWbxheK"" rel=""nofollow noreferrer"">https://pastebin.com/qeWbxheK</a></p>
","['python', 'python-3.x', 'list']",57144320,"<p>With single loop:</p>

<pre><code>import pprint

res = []
for sub_l in lst:   # lst is your initial list
    if sub_l[1] or sub_l[2]:
        res.append(sub_l)   # add as a base item of the group
    elif not sub_l[1] and not sub_l[2] and res:
        # check if last item is not a 2-dimensional list yet
        if not isinstance(res[-1][0], list): res[-1] = [res[-1]]
        res[-1].append(sub_l)

pprint.pprint(res)
</code></pre>

<p>The output:</p>

<pre><code>[['random_str0', 'random_str', 'random_str'],
 [['random_str1', '', 'random_str'], ['random_str2', '', '']],
 [['random_str3', 'random_str', 'random_str'],
  ['random_str4', '', ''],
  ['random_str5', '', '']],
 ['random_str6', 'random_str', ''],
 [['random_str7', 'random_str', 'random_str'],
  ['random_str8', '', ''],
  ['random_str9', '', ''],
  ['random_str10', '', ''],
  ['random_str11', '', '']]]
</code></pre>
",Group list recursive looping This sample input output l random str random str random str random str random str random str random str random str random str random str random str random str random str random str random str random str random str random str random str random str something like data structure type order important random str random str random str random str random str random str random str random str random str random str random str random str random str random str random str random str random str random str random str random str The idea inner list either index value followed one list missing index value forms group My actual code complex conditions well brevity omitted part actual question This I tried n reversed range range len l n group l j j range n group group g g g group print group Out desired overlapping random,"startoftags, python, python3x, list, endoftags",python django djangomodels endoftags,python python3x list,python django djangomodels,0.33
57181931,2019-07-24,2019,3,Converting dtype: period[M] to string format,"<p>I have converted my dates in to an Dtype M format as I don't want anything to do with the dates. Unfortunately I cannot plot with this format so I want now convert this in to strings.</p>

<p>So I need to group my data so I can print out some graphs by months.
But I keep getting a serial JSON error when my data is in <code>dtype:Mperiod</code>
so I want to convert it to strings.</p>

<pre><code>df['Date_Modified'] = pd.to_datetime(df['Collection_End_Date']).dt.to_period('M')  
#Add a new column called Date Modified to show just month and year

df = df.groupby([""Date_Modified"", ""Entity""]).sum().reset_index()
#Group the data frame by the new column and then Company and sum the values

df[""Date_Modified""].index = df[""Date_Modified""].index.strftime('%Y-%m')
</code></pre>

<p>It returns a string of numbers, but I need it to return a string output.</p>
","['python', 'pandas', 'numpy']",57181957,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.strftime.html"" rel=""nofollow noreferrer""><code>Series.dt.strftime</code></a> for set <code>Series</code> to strings in last step:</p>

<pre><code>df[""Date_Modified""]= df[""Date_Modified""].dt.strftime('%Y-%m')
</code></pre>

<p>Or set it before <code>groupby</code>, then converting to month period is not necessary:</p>

<pre><code>df['Date_Modified'] = pd.to_datetime(df['Collection_End_Date']).dt.strftime('%Y-%m')
df = df.groupby([""Date_Modified"", ""Entity""]).sum().reset_index()
</code></pre>
",Converting dtype period M string format I converted dates Dtype M format I want anything dates Unfortunately I cannot plot format I want convert strings So I need group data I print graphs months But I keep getting serial JSON error data dtype Mperiod I want convert strings df Date Modified pd datetime df Collection End Date dt period M Add new column called Date Modified show month year df df groupby Date Modified Entity sum reset index Group data frame new column Company sum values df Date Modified index df Date Modified index strftime Y It returns string numbers I need return string output,"startoftags, python, pandas, numpy, endoftags",python pandas numpy endoftags,python pandas numpy,python pandas numpy,1.0
57257033,2019-07-29,2019,2,How to add a new column to a pandas df that returns the smallest value that is greater in the same group from another dataframe,"<p>Hi I have the following two pandas dataframes: df1 and df2.</p>

<p>I want to create a new dataframe, df3 such that it is the same as df1 but with one extra column called ""new price"".</p>

<p>The way I want new price to be populated is to return the first price with the same code from df2 that is greater than or equal to the price in df1.</p>

<p>Here are the dataframes:</p>

<p>df1:</p>

<pre><code>Code    Price
X        4.3    
X        2.5    
X        4  
X        1.5    
X        0.24   
X        1  
X        1.3    
Y        3.9    
Y        2.6    

</code></pre>

<p>df2:</p>

<pre><code>Code Price
X   0.5
X   1
X   1.5
X   2
X   2.5
X   3
X   3.5
X   4
X   4.5
X   5
X   5.5
Y   0.5
Y   1
Y   1.5
Y   2
Y   2.5
Y   3
Y   3.5
Y   4
Y   4.5
Y   5
Y   5.5

</code></pre>

<p>So as an example let is consider the first entry in df1</p>

<pre><code>Code Price
X    4.3
</code></pre>

<p>So the column new price should look at all prices with code X in df2 and return the smallest price from df2 that is greater than or equal to 4.3.</p>

<p>In this case it is 4.5.</p>

<p>Repeat this for each line to get </p>

<p>df3:</p>

<pre><code>Code    Price   New Price
X        4.3       4.5
X        2.5       2.5
...
Y         2.6       3
</code></pre>

<p>Does anyone know how to achieve this, I have tried pandas merge but that didn't work.</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",57257470,"<p>You can do a cross join and then <code>query</code> , finally <code>groupby().first()</code>:</p>

<pre><code>m=(df1.assign(key=1).merge(df2.assign(key=1),on='key',suffixes=('','_y')).drop('key', 1)
                                            .query(""(Code==Code_y)&amp;(Price&lt;=Price_y)""))
m.groupby(['Code','Price'],sort=False)['Price_y'].first().reset_index(name='New Price')
</code></pre>

<hr>

<pre><code>  Code  Price  New Price
0    X   4.30        4.5
1    X   2.50        2.5
2    X   4.00        4.0
3    X   1.50        1.5
4    X   0.24        0.5
5    X   1.00        1.0
6    X   1.30        1.5
7    Y   3.90        4.0
8    Y   2.60        3.0
</code></pre>
",How add new column pandas df returns smallest value greater group another dataframe Hi I following two pandas dataframes df df I want create new dataframe df df one extra column called new price The way I want new price populated return first price code df greater equal price df Here dataframes df Code Price X X X X X X X Y Y df Code Price X X X X X X X X X X X Y Y Y Y Y Y Y Y Y Y Y So example let consider first entry df Code Price X So column new price look prices code X df return smallest price df greater equal In case Repeat line get df Code Price New Price X X Y Does anyone know achieve I tried pandas merge work,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
57293506,2019-07-31,2019,7,Find index of the first and/or last value in a column that is not NaN,"<p>I am dealing with sub-surface measurements from a borehole where each measurement type covers a different range of depths.  Depth is being used as the index in this case.  </p>

<p>I need to find the depth (index) of the first and/or last occurrence of data (non-NaN value) for each measurement type.</p>

<p>Getting the depth (index) of the first or last row of the dataframe is easy: <code>df.index[0]</code> or <code>df.index[-1]</code>.  The trick is in finding the index of the first or last non-NaN occurrence of any given column.</p>

<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame([[500, np.NaN, np.NaN,     25],
                   [501, np.NaN, np.NaN,     27],
                   [502, np.NaN,     33,     24],
                   [503,      4,     32,     18],
                   [504,     12,     45,      5],
                   [505,      8,     38, np.NaN]])
df.columns = ['Depth','x1','x2','x3']
df.set_index('Depth')
</code></pre>

<p><a href=""https://i.stack.imgur.com/Y7taF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y7taF.png"" alt=""enter image description here""></a></p>

<p>The ideal solution would produce an index (depth) of 503 for the first occurrence of x1, 502 for the first occurrence of x2, and 504 for the last occurrence of x3.</p>
","['python', 'pandas', 'numpy', 'dataframe']",57294505,"<p>Let's try this, if I understand you correctly:</p>

<pre><code>pd.concat([df.apply(pd.Series.first_valid_index),
           df.apply(pd.Series.last_valid_index)], 
           axis=1, 
           keys=['Min_Depth', 'Max_Depth'])
</code></pre>

<p>Output:</p>

<pre><code>      Min_Depth   Max_Depth
x1          503         505
x2          502         505
x3          500         504
</code></pre>

<p>Or Transpose output:</p>

<pre><code>pd.concat([df.apply(pd.Series.first_valid_index),
           df.apply(pd.Series.last_valid_index)], 
           axis=1, 
           keys=['Min_Depth', 'Max_Depth']).T
</code></pre>

<p>Output:</p>

<pre><code>            x1   x2   x3
Min_Depth  503  502  500
Max_Depth  505  505  504
</code></pre>

<hr>

<p>Using apply with a list of func:</p>

<pre><code>df.apply([pd.Series.first_valid_index, pd.Series.last_valid_index])
</code></pre>

<p>Output:</p>

<pre><code>                    x1   x2   x3
first_valid_index  503  502  500
last_valid_index   505  505  504
</code></pre>

<p>With a little renaming:</p>

<pre><code>df.apply([pd.Series.first_valid_index, pd.Series.last_valid_index])\
  .set_axis(['Min_Depth', 'Max_Depth'], axis=0, inplace=False)
</code></pre>

<p>Output:</p>

<pre><code>            x1   x2   x3
Min_Depth  503  502  500
Max_Depth  505  505  504
</code></pre>
",Find index first last value column NaN I dealing sub surface measurements borehole measurement type covers different range depths Depth used index case I need find depth index first last occurrence data non NaN value measurement type Getting depth index first last row dataframe easy df index df index The trick finding index first last non NaN occurrence given column df pd DataFrame np NaN np NaN np NaN np NaN np NaN np NaN df columns Depth x x x df set index Depth The ideal solution would produce index depth first occurrence x first occurrence x last occurrence x,"startoftags, python, pandas, numpy, dataframe, endoftags",python pandas numpy endoftags,python pandas numpy dataframe,python pandas numpy,0.87
57386455,2019-08-07,2019,2,"How to import csv with same filename to data frame, apply some program, then merge?","<p>I have two types of file with same file name in two different folder, contains different information that I need to preprocess then merge. I've been doing it manually using:</p>

<pre><code>a = './location/ID01.csv'

df1 = pd.read_csv(a)

# and rest of codes to preprocess a
</code></pre>

<p>and for other file</p>

<pre><code>b = './log/ID01.csv'

df2 = pd.read_csv(b)

# and rest of codes to preprocess b 
</code></pre>

<p>then I manually merge each using</p>

<pre><code>new_df = df2.merge(df1, on=['hour'], how='outer')
new_df.to_csv('merged.csv')
</code></pre>

<p>but of course it is time consuming. How can I do it in a loop so that all files in both folder can be processed in one time?</p>
","['python', 'pandas', 'csv']",57386562,"<p>You can do something like:</p>

<pre><code>import os
import pandas as pd


files_in_log = set(os.listdir('log'))
files_in_location = set(os.listdir('location'))
os.mkdir('results')
for filename in files_in_log &amp; files_in_location:
    df1 = pd.read_csv(os.path.join('log', filename))    
    df2 = pd.read_csv(os.path.join('location', filename))    
    new_df = df2.merge(df1, on=['hour'], how='outer')    
    new_df.to_csv(os.path.join('results', filename))
</code></pre>
",How import csv filename data frame apply program merge I two types file file name two different folder contains different information I need preprocess merge I manually using location ID csv df pd read csv rest codes preprocess file b log ID csv df pd read csv b rest codes preprocess b I manually merge using new df df merge df hour outer new df csv merged csv course time consuming How I loop files folder processed one time,"startoftags, python, pandas, csv, endoftags",python pandas matplotlib endoftags,python pandas csv,python pandas matplotlib,0.67
57435869,2019-08-09,2019,6,Break up a list of strings in a pandas dataframe column into new columns based on first word of each sentence,"<p>So I have roughly 40,000 rows of people and their complaints.  I am attempting to sort them into their respective columns for analysis, and for other analysts 
 at my company who use other tools can use this data.  </p>

<p><strong>DataFrame Example:</strong></p>

<pre><code>df = pd.DataFrame({""person"": [1, 2, 3], 
                   ""problems"": [""body: knee hurts(bad-pain), toes hurt(BIG/MIDDLE); mind: stressed, tired"", 
                                ""soul: missing; mind: can't think; body: feels great(lifts weights), overweight(always bulking), missing a finger"", 
                                ""none""]})
df     
âââââ¦âââââââââ¦âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
â   â person â                                                     problems                                                     â
â ââââ¬âââââââââ¬âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ£
â 0 â      1 â body: knee hurts(bad-pain), toes hurt(BIG/MIDDLE); mind: stressed, tired                                         â
â 1 â      2 â soul: missing; mind: can't think; body: feels great(lifts weights), overweight(always bulking), missing a finger â
â 2 â      3 â none                                                                                                             â
âââââ©âââââââââ©âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
</code></pre>

<p><strong>Desired Output:</strong></p>

<pre><code>âââââ¦âââââââââ¦âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¦âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¦ââââââââââââââââââââââââ¦ââââââââââââââââ
â   â person â                                                     problems                                                     â                                      body                                      â         mind          â     soul      â
â ââââ¬âââââââââ¬âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¬âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¬ââââââââââââââââââââââââ¬ââââââââââââââââ£
â 0 â      1 â body: knee hurts(bad-pain), toes hurt(BIG/MIDDLE); mind: stressed, tired                                         â body: knee hurts(bad-pain), toes hurt(BIG/MIDDLE)                              â mind: stressed, tired â NaN           â
â 1 â      2 â soul: missing; mind: can't think; body: feels great(lifts weights), overweight(always bulking), missing a finger â body: feels great(lifts weights), overweight(always bulking), missing a finger â mind: can't think     â soul: missing â
â 2 â      3 â none                                                                                                             â NaN                                                                            â NaN                   â NaN           â
âââââ©âââââââââ©âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©ââââââââââââââââââââââââ©ââââââââââââââââ
</code></pre>

<p><strong>Things I've tried / where I'm at:</strong></p>

<p>So I've been able to at least separate these with a regex statement that seems to do the job with my real data. </p>

<pre><code>df.problems.str.extractall(r""(\b(?!(?: \b))[\w\s.()',:/-]+)"")


+---+-------+--------------------------------------------------------------------------------+
|   |       |                                       0                                        |
+---+-------+--------------------------------------------------------------------------------+
|   | match |                                                                                |
| 0 | 0     | body: knee hurts(bad-pain), toes hurt(BIG/MIDDLE)                              |
|   | 1     | mind: stressed, tired                                                          |
| 1 | 0     | soul: missing                                                                  |
|   | 1     | mind: can't think                                                              |
|   | 2     | body: feels great(lifts weights), overweight(always bulking), missing a finger |
| 2 | 0     | none                                                                           |
+---+-------+--------------------------------------------------------------------------------+
</code></pre>

<p>I'm a regex beginner, so I expect this could probably be done better.  My original regex pattern was <code>r'([^;]+)'</code>, but I was trying to exclude the space after the semi-colons.</p>

<p>So I'm at a loss.  I played with:</p>

<p><code>df.problems.str.extractall(r""(\b(?!(?: \b))[\w\s.()',:/-]+)"").unstack()</code>, which ""works""(doesn't error out) with my example here.</p>

<p>But with my real data, I get an error: <code>""ValueError: Index contains duplicate entries, cannot reshape""</code>  </p>

<p>Even if it worked with my real data, I'd still have to figure out how to get these 'categories'(body, mind, soul) into assigned columns.  </p>

<p>I'd probably have better luck if I could word this question better.  I'm trying to really self-learn here, so I'll appreciate any leads even if they're not a complete solution.</p>

<p>I'm kind of sniffing a trail that maybe I can do this somehow with a groupby or multiIndex know-how.  Kind of new to programming, so I'm still feeling my way around in the dark.  I would appreciate any tips or ideas anyone has to offer.  Thank you!</p>

<p>EDIT:  I just want to come back and mention the error I was getting in my real data <code>""ValueError: Index contains duplicate entries, cannot reshape""</code> when using @WeNYoBen's solution: </p>

<pre><code>(df.problems.str.extractall(r""(\b(?!(?: \b))[\w\s.()',:/-]+)"")[0]
.str.split(':',expand=True)
.set_index(0,append=True)[1]
.unstack()
.groupby(level=0)
.first())
</code></pre>

<p>It turned out I had some groups with multiple colons.  For example: </p>

<pre><code>df = pd.DataFrame({""person"": [1, 2, 3], 
                   ""problems"": [""body: knee hurts(bad-pain), toes hurt(BIG/MIDDLE); mind: stressed, energy: tired"", 
                                ""soul: missing; mind: can't think; body: feels great(lifts weights), overweight(always bulking), missing a finger"", 
                                ""none""]})




âââââ¦âââââââââ¦âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
â   â person â                                                     problems                                                     â
â ââââ¬âââââââââ¬âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ£
â 0 â      1 â body: knee hurts(bad-pain), toes hurt(BIG/MIDDLE); mind: stressed, energy: tired                                 â
â 1 â      2 â soul: missing; mind: can't think; body: feels great(lifts weights), overweight(always bulking), missing a finger â
â 2 â      3 â none                                                                                                             â
âââââ©âââââââââ©âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
</code></pre>

<p>See the first row update reflecting the edge case I discovered <code>; mind: stressed, energy: tired</code>.</p>

<p>I was able to fix this by altering my regex to say the beginning of the match must be the beginning of the string or be preceded with a semi-colon. </p>

<pre><code>splits = [r'(^)(.+?)[:]', r'(;)(.+?)[:]']
str.split('|'.join(splits)
</code></pre>

<p>After that I just had to re-tweak the set_index portion to get @WeNYoBen's helpful solution to work, so I'll stick with this one.  </p>
","['python', 'regex', 'pandas']",57436454,"<p>It's not elegant but it gets the job done:</p>

<pre><code>df['split'] = df.problems.str.split(';')
df['mind'] = df.split.apply(
    lambda x: ''.join([category for category in x if 'mind' in category]))
df['body'] = df.split.apply(
    lambda x: ''.join([category for category in x if 'body' in category]))
df['soul'] = df.split.apply(
    lambda x: ''.join([category for category in x if 'soul' in category]))
df.drop('split', inplace=True)
</code></pre>

<p>You could probably wrap </p>

<pre><code>df[cat] = df.split.apply(lambda x: ''.join([category for category in x if cat in category])) 
</code></pre>

<p>in a function and run it on your dataframe for each <code>cat</code> (e.g. <code>cats=['mind', 'body', 'soul', 'whathaveyou', 'etc.']</code>.</p>

<hr>

<p><strong>Edit</strong>:</p>

<p>As @ifly6 has pointed out, there may be intersections of keywords in the strings that users enter. To be safe, the function should be altered to</p>

<pre><code>df[cat] = df.split.apply(lambda x: ''.join([category for category in x if category.startswith(cat)])) 
</code></pre>
",Break list strings pandas dataframe column new columns based first word sentence So I roughly rows people complaints I attempting sort respective columns analysis analysts company use tools use data DataFrame Example df pd DataFrame person problems body knee hurts bad pain toes hurt BIG MIDDLE mind stressed tired soul missing mind think body feels great lifts weights overweight always bulking missing finger none df person problems body knee hurts bad pain toes hurt BIG MIDDLE mind stressed tired soul missing mind think body feels great lifts weights overweight always bulking missing finger none Desired Output person problems body mind soul body knee hurts bad pain toes hurt BIG MIDDLE mind stressed tired body knee hurts bad pain toes hurt BIG MIDDLE mind stressed tired NaN soul missing mind think body feels great lifts weights overweight always bulking missing finger body feels great lifts weights overweight always bulking missing finger,"startoftags, python, regex, pandas, endoftags",python pandas dataframe endoftags,python regex pandas,python pandas dataframe,0.67
57526116,2019-08-16,2019,4,How do I display a subset of a pandas dataframe?,"<p>I have a dataframe <code>df</code> that contains datetimes for every hour of a day between 2003-02-12 to 2017-06-30 and I want to delete all datetimes between 24th Dec and 1st Jan of EVERY year.
An extract of my data frame is:</p>

<pre><code>...
7505,2003-12-23 17:00:00
7506,2003-12-23 18:00:00
7507,2003-12-23 19:00:00
7508,2003-12-23 20:00:00
7509,2003-12-23 21:00:00
7510,2003-12-23 22:00:00
7511,2003-12-23 23:00:00
7512,2003-12-24 00:00:00
7513,2003-12-24 01:00:00
7514,2003-12-24 02:00:00
7515,2003-12-24 03:00:00
7516,2003-12-24 04:00:00
7517,2003-12-24 05:00:00
7518,2003-12-24 06:00:00
...
7723,2004-01-01 19:00:00
7724,2004-01-01 20:00:00
7725,2004-01-01 21:00:00
7726,2004-01-01 22:00:00
7727,2004-01-01 23:00:00
7728,2004-01-02 00:00:00
7729,2004-01-02 01:00:00
7730,2004-01-02 02:00:00
7731,2004-01-02 03:00:00
7732,2004-01-02 04:00:00
7733,2004-01-02 05:00:00
7734,2004-01-02 06:00:00
7735,2004-01-02 07:00:00
...
</code></pre>

<p>and my expected output is:</p>

<pre><code>...
7505,2003-12-23 17:00:00
7506,2003-12-23 18:00:00
7507,2003-12-23 19:00:00
7508,2003-12-23 20:00:00
7509,2003-12-23 21:00:00
7510,2003-12-23 22:00:00
7511,2003-12-23 23:00:00
...
7728,2004-01-02 00:00:00
7729,2004-01-02 01:00:00
7730,2004-01-02 02:00:00
7731,2004-01-02 03:00:00
7732,2004-01-02 04:00:00
7733,2004-01-02 05:00:00
7734,2004-01-02 06:00:00
7735,2004-01-02 07:00:00
...
</code></pre>
","['python', 'pandas', 'datetime']",57526377,"<p>Sample dataframe:</p>

<pre><code>                dates
0 2003-12-23 23:00:00
1 2003-12-24 05:00:00
2 2004-12-27 05:00:00
3 2003-12-13 23:00:00
4 2002-12-23 23:00:00
5 2004-01-01 05:00:00
6 2014-12-24 05:00:00
</code></pre>

<p><hr></p>

<h1>Solution:</h1>

<p>If you want it for every year between the following dates excluded, then extract the month and dates first:</p>

<pre><code>df['month'] = df['dates'].dt.month
df['day'] = df['dates'].dt.day
</code></pre>

<p>And now put the condition check:</p>

<pre><code>dec_days = [24, 25, 26, 27, 28, 29, 30, 31]  
## if the month is dec, then check for these dates 
## if the month is jan, then just check for the day to be 1 like below
df = df[~(((df.month == 12) &amp; (df.day.isin(dec_days))) | ((df.month == 1) &amp; (df.day == 1)))]
</code></pre>

<p>Sample output:</p>

<pre><code>                dates  month  day
0 2003-12-23 23:00:00     12   23
3 2003-12-13 23:00:00     12   13
4 2002-12-23 23:00:00     12   23
</code></pre>
",How I display subset pandas dataframe I dataframe df contains datetimes every hour day I want delete datetimes th Dec st Jan EVERY year An extract data frame expected output,"startoftags, python, pandas, datetime, endoftags",python pandas dataframe endoftags,python pandas datetime,python pandas dataframe,0.67
57546413,2019-08-18,2019,2,How to &#39;skip&#39; specific words using regex in python?,"<p>I am writing a program which takes the explanation of a German idiom from Wikipedia, for example, and catches the idiom, its meaning and any additional information. </p>

<p>Example, the bolded text is supposed to be matched:</p>

<blockquote>
  <p>** Sich wie ein Backfisch benehmen â <strong>albern bzw. unreif sein</strong>. Zur Etymologie des Wortes âBackfischâ fÃ¼r unreife MÃ¤dchen siehe dort. (Sprichwort um 1900: âMit 14 Jahrân und sieben Wochen ist der Backfisch ausgekrochen.â[6]).</p>
</blockquote>

<p>Basically, the phrase starts after the dash <strong>-</strong> and ends before the first full stop, i.e. it is only one sentence. <strong>However, i want to skip abbreviations such as <em>bzw.</em>, <em>z. B.</em>, <em>u. A.</em>,</strong> etc, since they do not mark the end of the sentence.</p>

<p>I am unsure how to skip the word, but still match it. Also, as I said, I want to skip frequently used abbreviations in German such as the aforementioned ones in italics.</p>

<p>I already tried matching a structure beginning with <strong>-</strong> and ending with <strong>.</strong>, whereas the <strong>.</strong> should not be preceded by <em>bzw</em>. However, I did not succeed in doing that.</p>
","['python', 'regex', 'python-3.x']",57547962,"<p>Use a <strong>non-capturing group</strong>. Take a look:</p>

<pre><code>(?&lt;=â )(?:.+)?(?:bzw\.|Z\. b\.|u\. a\.)[^\.]+
</code></pre>

<p><a href=""https://i.stack.imgur.com/ueejR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ueejR.png"" alt=""enter image description here""></a></p>

<p><a href=""https://regex101.com/r/P7aonE/6"" rel=""nofollow noreferrer"">Regex Demo</a> - top right you can see description of individual regex components.</p>

<pre><code>(?&lt;=â )                    start after â character + whitespace, but not match
(?:.+)?                    add any text before abbreviation into non-capturing group.
(?:bzw\.|Z\. b\.|u\. a\.)  add abbreviations into non-capturing group. Escape the dots via \. 
[^\.]+                     match anything until fullstop
</code></pre>

<p>Essentially the idea is to start with the â character + whitespace, but not match it. Then capture any following text, abbreviation and capture till the first dot <code>.</code>, but without capturing the abbreviations group (notice the <code>?:</code>). Since the abbreviation dot is part of the non-capturing group, we ""skip"" it and continue until the dot that ends the sentence. You can expand the abbreviations list by adding more abbreviations via the <code>|</code> symbol. </p>

<p><strong>Bonus:</strong></p>

<p>If you are anticipating that you will not always start with the <code>â</code> sequence, you can do the following:</p>

<pre><code>(?:â |: )((?:.+)?(?:bzw\.|Z\. b\.|u\. a\.)[^\.]+)
</code></pre>

<p>This will allow the regex to work also with <code>:</code> character instead of <code>â</code>, for example, but you will need to retrieve the result as group 1.</p>

<p><a href=""https://regex101.com/r/P7aonE/7"" rel=""nofollow noreferrer"">Regex Demo</a></p>
",How skip specific words using regex python I writing program takes explanation German idiom Wikipedia example catches idiom meaning additional information Example bolded text supposed matched Sich wie ein Backfisch benehmen albern bzw unreif sein Zur Etymologie des Wortes Backfisch f r unreife M dchen siehe dort Sprichwort um Mit Jahr n und sieben Wochen ist der Backfisch ausgekrochen Basically phrase starts dash ends first full stop e one sentence However want skip abbreviations bzw z B u A etc since mark end sentence I unsure skip word still match Also I said I want skip frequently used abbreviations German aforementioned ones italics I already tried matching structure beginning ending whereas preceded bzw However I succeed,"startoftags, python, regex, python3x, endoftags",python arrays numpy endoftags,python regex python3x,python arrays numpy,0.33
57719181,2019-08-30,2019,2,How to change the array entries with order = &#39;F&#39; in numpy,"<p>I am trying to replace some entries of an array. I have to use the order='F' for compatibility reasons. The array I am working with is big, yet to reproduce the problem try the following. </p>

<p>This works:  </p>

<pre><code>a = numpy.array([[1, 2], [4, 5]])

a.reshape((4, 1), order = 'C')[2] = 8

a = array([[1, 2],
           [8, 5]])
</code></pre>

<p>The following doesn't work:</p>

<pre><code>a = numpy.array([[1, 2], [4, 5]])

a.reshape((4, 1), order = 'F')[2] = 8

a = array([[1, 2],
           [4, 5]])
</code></pre>

<p>Is there a way around this?</p>
","['python', 'arrays', 'numpy']",57719331,"<p>For the <code>'C'</code> type array this will get you a new 4x1 array that references all the same elements as the original array:</p>

<pre><code>a = numpy.array([[1, 2], [4, 5]])
a.reshape((4, 1), order = 'C')[2] = 8
</code></pre>

<p>So, the result is the same as doing this:</p>

<pre><code>a = numpy.array([[1, 2], [4, 5]])
b = a.reshape((4, 1), order = 'C')
b[2] = 8  # this line also changes a, because it references the same elements
print(a)
print(b)
</code></pre>

<p>Result:</p>

<pre><code>[[1 2]
 [8 5]] 
[[1]
 [4]
 [8]
 [5]]
</code></pre>

<p>However, this doesn't work in the same way:</p>

<pre><code>a = numpy.array([[1, 2], [4, 5]])
a.reshape((4, 1), order = 'F')[2] = 8
</code></pre>

<p>Because <code>a.reshape((4, 1), order = 'F')</code> does give you a reshaped array, but it's an entirely new copy of the same array with different dimensions and thus changing it won't change the original array:</p>

<pre><code>a = numpy.array([[1, 2], [4, 5]])
b = a.reshape((4, 1), order = 'F')  # you get a copy, order 'F'
b[2] = 8  # this line does not change a, because it references different elements.
print(a)
print(b)
</code></pre>

<p>Result:</p>

<pre><code>[[1 2]
 [4 5]] 
[[1]
 [4]
 [8]
 [5]]
</code></pre>

<p>However, if you define <code>a</code> as an <code>F</code> order array as well, <code>a</code> and <code>b</code> can share their data, but the result may be surprisingly different (this is the whole point of the different ordering):</p>

<pre><code>a = numpy.array([[1, 2], [4, 5]], order = 'F')
b = a.reshape((4, 1), order = 'F')
b[2] = 8  # this line does change a, they share their data, both order 'F'.
print(a)
print(b)
</code></pre>

<p>Result:</p>

<pre><code>[[1 8]
 [4 5]]
[[1]
 [4]
 [8]
 [5]]
</code></pre>

<p>Note the result as printed for <code>a</code>!</p>
",How change array entries order F numpy I trying replace entries array I use order F compatibility reasons The array I working big yet reproduce problem try following This works numpy array reshape order C array The following work numpy array reshape order F array Is way around,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
57747406,2019-09-01,2019,2,python dictionary keyError,"<p>New to python and what looks like simple doable piece of code yielding <code>KeyError</code>:</p>

<pre><code>patt=list('jkasb')

dict={}
for i in patt:
    dict[i]= 1 if dict[i] is None else dict[i]+1 # This line throws error
</code></pre>

<blockquote>
  <p>error: KeyError: 'j'</p>
</blockquote>
","['python', 'python-3.x', 'dictionary']",57747799,"<p>In your case, the KeyError is occurring because you are trying to access a key which is not in the dictionary. Initially, the dictionary is empty. So, none of the keys exist in it. </p>

<p>This may seem strange if you are coming from a C++ background as C++ maps give default values for keys that don't exist yet. You can get the same behavior in python by using <code>collections.defaultdict</code>. The modified code is given below. I took the liberty of converting the defaultdict to a regular dictionary at the end of the code:</p>

<pre><code>from collections import defaultdict
patt='jkasb'

my_default_dict=defaultdict(int)
for i in patt:
    my_default_dict[i]+=1

my_dict = dict(my_default_dict) # converting the defaultdict to a regular dictionary
</code></pre>

<p>You can also solve this problem in a number of other ways. I am showing some of them below:</p>

<ul>
<li><p><strong>By checking if the key exists in the dictionary:</strong></p>

<pre><code>patt='jkasb'

my_dict={}
for i in patt:
    my_dict[i]= 1 if i not in my_dict else my_dict[i]+1 # checking if i exists in dict
</code></pre></li>
<li><p><strong>Using <code>dict.get()</code> without default return values:</strong></p>

<pre><code>patt='jkasb'

my_dict={}
for i in patt:
    my_dict[i]= 1 if my_dict.get(i) is None else my_dict[i]+1 # using dict.get
print(my_dict)
</code></pre></li>
<li><p><strong>Using <code>dict.get()</code> with default return values:</strong></p>

<pre><code>patt='jkasb'

my_dict={}
for i in patt:
    my_dict[i]= my_dict.get(i, 0)+1 # using dict.get with default return value 0
</code></pre></li>
<li><p><strong>As your code is actually just counting the frequency of each character, you can also use <code>collections.Counter</code> and then convert it to a dictionary:</strong></p>

<pre><code>from collections import Counter
patt='jkasb'

character_counter = Counter(patt)

my_dict = dict(character_counter)
</code></pre></li>
</ul>

<p>Also, as dict is a built-in data type and I used dict to convert the defaultdict and Counter to a normal dictionary, I changed the name of the dictionary from dict to my_dict.</p>
",python dictionary keyError New python looks like simple doable piece code yielding KeyError patt list jkasb dict patt dict dict None else dict This line throws error error KeyError j,"startoftags, python, python3x, dictionary, endoftags",python python3x list endoftags,python python3x dictionary,python python3x list,0.67
57937690,2019-09-14,2019,2,How to find customer satisfaction using pandas?,"<p>I have two columns such as agent_email and effortscore. In effortscore, Y means dissatisfied and N means satisfied. </p>

<p>columns look like this:</p>

<pre><code>agent_email effortscore.
ab           1
ab           0
xy           1
xy           0
</code></pre>

<hr>

<pre><code>formula=(total 1's / total response)*100.
</code></pre>

<p>I want the output to be like</p>

<pre><code>ab 50% csat
xy 100% csat
</code></pre>
","['python', 'python-3.x', 'pandas', 'dataframe']",57937724,"<p>I believe you need aggregate <code>mean</code>, it working, because only <code>1</code> and <code>0</code> values and number of <code>1</code> divided by total is <code>mean</code> formula:</p>

<pre><code>df = pd.DataFrame({'agent_email':['ab@gmail.com','ab@gmail.com','xy@gmail.com'],
                   'effortscore':[1,0,1]})

df1 = df.groupby('agent_email')['effortscore'].mean().mul(100).reset_index()
print (df1)
    agent_email  effortscore
0  ab@gmail.com         50.0
1  xy@gmail.com        100.0
</code></pre>

<p>Because data in question are different, need compare by <code>eq</code> for <code>==</code> and aggregate <code>mean</code>:</p>

<pre><code>print (df)
  agent_email effortscore
0          ab           Y
1          ab           N
2          xy           Y
3          xy           N

df1 = df['effortscore'].eq('Y').groupby(df['agent_email']).mean().mul(100).reset_index()
print (df1)
  agent_email  effortscore
0          ab         50.0
1          xy         50.0
</code></pre>
",How find customer satisfaction using pandas I two columns agent email effortscore In effortscore Y means dissatisfied N means satisfied columns look like agent email effortscore ab ab xy xy formula total total response I want output like ab csat xy csat,"startoftags, python, python3x, pandas, dataframe, endoftags",python python3x pandas endoftags,python python3x pandas dataframe,python python3x pandas,0.87
58353594,2019-10-12,2019,2,Duplicate Quantity as new columns,"<p>This is my table</p>

<p><a href=""https://i.stack.imgur.com/XMyDl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XMyDl.png"" alt=""enter image description here""></a></p>

<p>I want it to be the following, i.e., by duplicating <code>Quantity</code> of <code>(shopID, productID)</code> <code>Quantity</code> of other difference <code>(shopID, productID)</code> as new columns, <code>Quantity_shopID_productID</code>.
<a href=""https://i.stack.imgur.com/9NfKL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9NfKL.png"" alt=""enter image description here""></a></p>

<p>Following is my code:</p>

<pre><code>from datetime import date 
import pandas as pd
df=pd.DataFrame({""Date"":[date(2019,10,1),date(2019,10,2),date(2019,10,1),date(2019,10,2),date(2019,10,1),date(2019,10,2),date(2019,10,1),date(2019,10,2)],
                ""ShopID"":[1,1,1,1,2,2,2,2],
                ""ProductID"":[1,1,2,2,1,1,2,2],
                ""Quantity"":[3,3,4,4,5,5,6,6]})


for sid in df.ShopID.unique():
    for pid in df.ProductID.unique():
        col_name='Quantity{}_{}'.format(sid,pid)
        print(col_name)
        df1=df[(df.ShopID==sid) &amp; (df.ProductID==pid)][['Date','Quantity']]
        df1.rename(columns={'Quantity':col_name}, inplace=True)
        display(df1)
        df=df.merge(df1, how=""left"",on=""Date"")
        df.loc[(df.ShopID==sid) &amp; (df.ProductID==pid),col_name]=None       

print(df)
</code></pre>

<p>The problem is, it works very slow as I have over 108 different <code>(shopID, productID)</code> combinations over 3 years period. Is there anyway to make it more efficient?</p>
","['python', 'python-3.x', 'pandas']",58353967,"<p>This is a <code>pivot</code> and <code>merge</code> problem with a little extra:</p>

<pre><code># somehow merge only works with pandas datetime
df['Date'] = pd.to_datetime(df['Date'])

# define the new column names
df['new_col'] = 'Quantity_'+df['ShopID'].astype(str) + '_' + df['ProductID'].astype(str)

# new data to merge:
pivot = df.pivot_table(index='Date', 
                        columns='new_col', 
                        values='Quantity')

# merge
new_df = df.merge(pivot, left_on='Date', right_index=True)

# mask
mask = new_df['new_col'].values[:,None] == pivot.columns.values

# adding the None the values:
new_df[pivot.columns] = new_df[pivot.columns].mask(mask)
</code></pre>

<p>Output:</p>

<pre><code>    Date                   ShopID    ProductID    Quantity  new_col         Quantity_1_1    Quantity_1_2    Quantity_2_1    Quantity_2_2
--  -------------------  --------  -----------  ----------  ------------  --------------  --------------  --------------  --------------
 0  2019-10-01 00:00:00         1            1           3  Quantity_1_1             nan               4               5               6
 1  2019-10-02 00:00:00         1            1           3  Quantity_1_1             nan               4               5               6
 2  2019-10-01 00:00:00         1            2           4  Quantity_1_2               3             nan               5               6
 3  2019-10-02 00:00:00         1            2           4  Quantity_1_2               3             nan               5               6
 4  2019-10-01 00:00:00         2            1           5  Quantity_2_1               3               4             nan               6
 5  2019-10-02 00:00:00         2            1           5  Quantity_2_1               3               4             nan               6
 6  2019-10-01 00:00:00         2            2           6  Quantity_2_2               3               4               5             nan
 7  2019-10-02 00:00:00         2            2           6  Quantity_2_2               3               4               5             nan
</code></pre>

<hr>

<p>Test data with similar size to your actual data:</p>

<pre><code># 3 years dates
dates = pd.date_range('2015-01-01', '2018-12-31', freq='D')

# 12 Shops and 9 products
idx = pd.MultiIndex.from_product((dates, range(1,13), range(1,10)),
                                 names=('Date','ShopID', 'ProductID'))

# the test data
np.random.seed(1)
df = pd.DataFrame({'Quantity':np.random.randint(0,10, len(idx))},
                  index=idx).reset_index()
</code></pre>

<p>The above code tooks about 10 seconds on an i5 laptop :-)</p>
",Duplicate Quantity new columns This table I want following e duplicating Quantity shopID productID Quantity difference shopID productID new columns Quantity shopID productID Following code datetime import date import pandas pd df pd DataFrame Date date date date date date date date date ShopID ProductID Quantity sid df ShopID unique pid df ProductID unique col name Quantity format sid pid print col name df df df ShopID sid amp df ProductID pid Date Quantity df rename columns Quantity col name inplace True display df df df merge df left Date df loc df ShopID sid amp df ProductID pid col name None print df The problem works slow I different shopID productID combinations years period Is anyway make efficient,"startoftags, python, python3x, pandas, endoftags",python python3x pandas endoftags,python python3x pandas,python python3x pandas,1.0
58383992,2019-10-14,2019,4,Remove characters from the end of each element in a list of strings based on another list of strings (e.g. blacklist strings),"<p>I have a dictionary which contains a number of unique string values for a key ""sample"". I'm converting this key ""sample"" into a list for plotting, however I want to generate another list with an equal number of elements that strip certain strings at the end of each element to generate a ""clean"" list that can then group certain samples together for plotting. For example, my blacklist looks like:</p>

<pre><code>blacklist = ['_001', '_002', '_003', '_004', '_005', '_006', '_007', '_008', '_009', \
                       '_01', '_02', '_03', '_04', '_05', '_06', '_07', '_08', '_09', \
                       '_1', '_2', '_3', '_4', '_5', '_6', '_7', '_8', '_9']
</code></pre>

<p>which I want to remove from each item in this example list generated from my dictionary:</p>

<pre><code>sample = [(d['sample']) for d in my_stats]
sample
['sample_A', 'sample_A_001', 'sample_A_002', 'my_long_sample_B_1', 'other_sample_C_08', 'sample_A_03', 'sample1_D_07']
</code></pre>

<p>with the desired result of a new list:</p>

<pre><code>sample
['sample_A', 'sample_A', 'sample_A', 'my_long_sample_B', 'other_sample_C', 'sample_A', 'sample1_D']
</code></pre>

<p>For context, I understand there will be some elements that will then be the same -- I want to use this list to compile a dataframe in conjunction with lists with an equal number of values generated other keys from this dictionary that will be used as an id in plotting (i.e. such that I can use it to group/color all of those values the same). Note that there may be various numbers of underscores and there may be elements in my list of strings that do not contain any values from the blacklist (which is why I can't use some variant of split on the last underscore for example).</p>

<p>This is similar to this issue:
<a href=""https://stackoverflow.com/questions/14215338/how-can-i-remove-multiple-characters-in-a-list/14215379#14215379"">How can I remove multiple characters in a list?</a></p>

<p>but I don't want it to be so generalized/greedy and would ideally like to remove it from only the end as the user may have an input file with parts of these strings (e.g. the 1 in sample1_D) internally. I don't necessarily need to use a blacklist if there's another solution, it just seemed like that might be the easiest way.</p>
","['python', 'regex', 'python-3.x']",58384146,"<p>Here you go, see if this fits your requirements.</p>

<p>Basically, you're just splitting on the <code>'_'</code> character and testing if the last split in the list is in your blacklist.  If <code>True</code>, then drop it, if <code>False</code> put the string back together; and build a new list from the results.</p>

<pre><code>blacklist = ['_001', '_002', '_003', '_004', '_005', '_006', '_007', '_008',
             '_01', '_02', '_03', '_04', '_05', '_06', '_07', '_08', '_09',
             '_1', '_2', '_3', '_4', '_5', '_6', '_7', '_8', '_9']
sample = ['sample_A', 'sample_A_001', 'sample_A_002', 'my_long_sample_B_1',
          'other_sample_C_08', 'sample_A_03', 'sample1_D_07']
results = []

for i in sample:
    splt = i.split('_')
    value = '_'.join(splt[:-1]) if '_{}'.format(splt[-1:][0]) in blacklist else '_'.join(splt)
    results.append(value)

print(results)
</code></pre>

<h3>Output:</h3>

<pre><code>['sample_A', 'sample_A', 'sample_A', 'my_long_sample_B', 'other_sample_C', 'sample_A', 'sample1_D']
</code></pre>
",Remove characters end element list strings based another list strings e g blacklist strings I dictionary contains number unique string values key sample I converting key sample list plotting however I want generate another list equal number elements strip certain strings end element generate clean list group certain samples together plotting For example blacklist looks like blacklist I want remove item example list generated dictionary sample sample stats sample sample A sample A sample A long sample B sample C sample A sample D desired result new list sample sample A sample A sample A long sample B sample C sample A sample D For context I understand elements I want use list compile dataframe conjunction lists equal number values generated keys dictionary used id plotting e I use group color values Note may various numbers underscores may elements list strings contain values blacklist I use variant split last underscore,"startoftags, python, regex, python3x, endoftags",python python3x list endoftags,python regex python3x,python python3x list,0.67
58391709,2019-10-15,2019,2,How to assign value to rows which are between two rows with specific string in column in dataframe?,"<p>I have dataframe with time series where one column has strings: <code>Normal Value</code> and <code>Wrong Value</code>. I would like to find all rows which are just between rows with <code>Wrong Value</code> and assign them 0 to new column. Rows which have <code>Normal Value</code> and are not between rows with <code>Wrong Value</code> should have value 1. <code>Value</code> column represents high peaks in time series. </p>

<p>Sample dataframe:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'Date': ['2019-01-01','2019-01-02','2019-01-03','2019-01-04','2019-01-05','2019-01-06','2019-01-07','2019-01-08','2019-01-09', '2019-01-10'],
...                    'Value': [-0.011295, -0.013431, 580944.426061, 0.000000, 0.000000, -0.999998, 0.000000, 0.000000, 712327.147257, -0.999999],
...                    'String': ['Normal Value', 'Normal Value', 'Wrong Value', 'Normal Value', 'Normal Value', 'Wrong Value', 'Normal Value', 'Normal Value', 'Wrong Value', 'Wrong Value']})
&gt;&gt;&gt; df
         Date          Value        String
0  2019-01-01      -0.011295  Normal Value
1  2019-01-02      -0.013431  Normal Value
2  2019-01-03  580944.426061   Wrong Value
3  2019-01-04       0.000000  Normal Value
4  2019-01-05       0.000000  Normal Value
5  2019-01-06      -0.999998   Wrong Value
6  2019-01-07       0.000000  Normal Value
7  2019-01-08       0.000000  Normal Value
8  2019-01-09  712327.147257   Wrong Value
9  2019-01-10      -0.999999   Wrong Value
</code></pre>

<p>Expected output:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'Date': ['2019-01-01','2019-01-02','2019-01-03','2019-01-04','2019-01-05','2019-01-06','2019-01-07','2019-01-08','2019-01-09', '2019-01-10'],
...                    'Value': [-0.011295, -0.013431, 580944.426061, 0.000000, 0.000000, -0.999998, 0.000000, 0.000000, 712327.147257, -0.999999],
...                    'String': ['Normal Value', 'Normal Value', 'Wrong Value', 'Normal Value', 'Normal Value', 'Wrong Value', 'Normal Value', 'Normal Value', 'Wrong Value', 'Wrong Value'],
...                    'Expected Value': [1, 1, 0, 0, 0, 0, 1, 1, 0, 0]})
&gt;&gt;&gt; df
         Date          Value        String  Expected Value
0  2019-01-01      -0.011295  Normal Value               1
1  2019-01-02      -0.013431  Normal Value               1
2  2019-01-03  580944.426061   Wrong Value               0
3  2019-01-04       0.000000  Normal Value               0
4  2019-01-05       0.000000  Normal Value               0
5  2019-01-06      -0.999998   Wrong Value               0
6  2019-01-07       0.000000  Normal Value               1
7  2019-01-08       0.000000  Normal Value               1
8  2019-01-09  712327.147257   Wrong Value               0
9  2019-01-10      -0.999999   Wrong Value               0
</code></pre>
","['python', 'pandas', 'dataframe']",58392473,"<p>basically what you want is to transform this list 
<code>[1,1,0,1,1,0,1,1,0,0,...]</code> (1 is normal and 0 is wrong values)
to:
<code>[1,1,0,0,0,0,1,1,0,0,...]</code></p>

<p>A simple for-loop could do the job:</p>

<pre><code>a = []
is_wrong = 0
for value in df['String'].values:
    if is_wrong == 0:
        if value == 'Normal Value':
            a.append(1)
        else:
            a.append(0)
            is_wrong = 1
    else:
        if value == 'Normal Value':
            a.append(0)
        else:
            a.append(0)
            is_wrong = 0
df['Expected Value'] = a
</code></pre>

<p>a (maybe) more elegant way could be:</p>

<pre><code>a = []
is_wrong = False # store the current state
for value in df['String'].map({'Normal Value':True,'Wrong Value':False}).values:
    a.append(value and not is_wrong) # check the current state and output value
    is_wrong = is_wrong if value else not is_wrong # change the state if needed
df['Expected Value'] = [int(x) for x in a]
</code></pre>

<p>in both cases:</p>

<pre><code>df['Expected Value'] = [1, 1, 0, 0, 0, 0, 1, 1, 0, 0]
</code></pre>
",How assign value rows two rows specific string column dataframe I dataframe time series one column strings Normal Value Wrong Value I would like find rows rows Wrong Value assign new column Rows Normal Value rows Wrong Value value Value column represents high peaks time series Sample dataframe gt gt gt df pd DataFrame Date Value String Normal Value Normal Value Wrong Value Normal Value Normal Value Wrong Value Normal Value Normal Value Wrong Value Wrong Value gt gt gt df Date Value String Normal Value Normal Value Wrong Value Normal Value Normal Value Wrong Value Normal Value Normal Value Wrong Value Wrong Value Expected output gt gt gt df pd DataFrame Date Value String Normal Value Normal Value Wrong Value Normal Value Normal Value Wrong Value Normal Value Normal Value Wrong Value Wrong Value Expected Value gt gt gt df Date Value String Expected Value Normal Value Normal Value,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
58476857,2019-10-20,2019,2,How to match &amp; replace multiple strings with regex in Python,"<p>I am trying to replace some text in Python with regex.</p>

<p>My text looks like this:</p>

<pre><code>WORKGROUP 1. John Doe ID123, Jane Smith ID456, Ohe Keedoke ID7890
Situation paragraph 1

WORKGROUP 2. John Smith ID321, Jane Doe ID654
Situation paragraph 2
</code></pre>

<p>What I am trying to do is put the names in double square brackets and remove the IDs so that it will end up looking like this.</p>

<pre><code>WORKGROUP 1. [[John Doe]], [[Jane Smith]], [[Ohe Keedoke]]
Situation paragraph 1

WORKGROUP 2. [[John Smith]], [[Jane Doe]]
Situation paragraph 2
</code></pre>

<p>So far I have this.</p>

<pre><code>re.sub(r""(WORKGROUP\s\d\.\s)"",r""\1[["")
re.sub(r""(WORKGROUP\s\d\..+?)(?:\s\b\w+\b),(?:\s)(.+\n)"",r""\1]], [[\2"")
re.sub(r""(WORKGROUP\s\d\..+?)(?:\s\b\w+\b)(\n)"",r""\1]]\2"")
</code></pre>

<p>This works for groups with two people (WORKGROUP 2) but leaves all the IDs except the first and last persons' if there are more than two. So WORKGROUP 1 ends up looking like this.</p>

<pre><code>WORKGROUP 1. [[John Doe]], [[Jane Smith ID456, Ohe Keedoke]]
Situation paragraph 1
</code></pre>

<p>Unfortunately, I can't do something like</p>

<pre><code>re.sub(r""((\s\b\w+\b),(\s))+"",r""\1]], [[\2"")
</code></pre>

<p>because it will match inside the situation paragraphs.</p>

<p>My question is: is it possible to do multiple match/replacements in a string segment without doing it universally?</p>
","['python', 'regex', 'python-3.x']",58477261,"<p>You can nest the substitutions and make the first substitution find lines that start with <code>WORKGROUP</code> first, and then let the second substitution find and replace the common-separated tokens inside:</p>

<pre><code>re.sub(
    r'^(WORKGROUP\s+\d+\.\s*)(.*)',
    lambda m: m.group(1) + re.sub(r'([^,\s][^,]*)\s+\S+(?=,|$)', r'[[\1]]', m.group(2)),
    text,
    flags=re.MULTILINE
)
</code></pre>

<p>so that given:</p>

<pre><code>text = '''WORKGROUP 1. John Doe ID123, Jane Smith ID456, Ohe Keedoke ID7890
Situation paragraph 1

WORKGROUP 2. John Smith ID321, Jane Doe ID654
Situation paragraph 2'''
</code></pre>

<p>the expression returns:</p>

<pre><code>WORKGROUP 1. [[John Doe]], [[Jane Smith]], [[Ohe Keedoke]]
Situation paragraph 1

WORKGROUP 2. [[John Smith]], [[Jane Doe]]
Situation paragraph 2
</code></pre>

<p>Demo: <a href=""https://repl.it/@blhsing/BoldElderlyQuerylanguage"" rel=""nofollow noreferrer"">https://repl.it/@blhsing/BoldElderlyQuerylanguage</a></p>
",How match amp replace multiple strings regex Python I trying replace text Python regex My text looks like WORKGROUP John Doe ID Jane Smith ID Ohe Keedoke ID Situation paragraph WORKGROUP John Smith ID Jane Doe ID Situation paragraph What I trying put names double square brackets remove IDs end looking like WORKGROUP John Doe Jane Smith Ohe Keedoke Situation paragraph WORKGROUP John Smith Jane Doe Situation paragraph So far I sub r WORKGROUP r sub r WORKGROUP b w b n r sub r WORKGROUP b w b n r This works groups two people WORKGROUP leaves IDs except first last persons two So WORKGROUP ends looking like WORKGROUP John Doe Jane Smith ID Ohe Keedoke Situation paragraph Unfortunately I something like sub r b w b r match inside situation paragraphs My question possible multiple match replacements string segment without universally,"startoftags, python, regex, python3x, endoftags",python arrays numpy endoftags,python regex python3x,python arrays numpy,0.33
58694383,2019-11-04,2019,3,Pandas to_datetime no error on wrong format,"<p>I read in a CSV file containing dates.  Some dates may be formatted wrong and I want to find those.  With the following approach I would <strong>expect the 2nd row to be <code>NaT</code></strong>.  But pandas seems to ignore the specified format no matter if I set <code>infer_datetime_format</code> or <code>exact</code>.</p>

<pre><code>import pandas as pd
from io import StringIO

DATA = StringIO(""""""date
2019 10 07
   2018 10
"""""")
df = pd.read_csv(DATA)

df['date'] = pd.to_datetime(df['date'], format=""%Y %m %d"", errors='coerce', exact=True)
</code></pre>

<p>results in</p>

<pre><code>        date
0 2019-10-07
1 2018-10-01
</code></pre>

<p>The <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html"" rel=""nofollow noreferrer"">pandas.to_datetime</a> documentation refers to <a href=""https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior"" rel=""nofollow noreferrer"">strftime() and strptime() Behavior</a> but when I test it with plain Python it works:</p>

<pre><code>datetime.datetime.strptime('  2018 10', '%Y %m %d')
</code></pre>

<p>I get the expected value error:</p>

<pre><code>ValueError: time data '  2018 10' does not match format '%Y %m %d'
</code></pre>

<p>What do I miss?</p>

<p>FYI: This question <a href=""https://stackoverflow.com/questions/34377123/pandas-to-datetime-not-working"">pandas to_datetime not working</a> seems to be related but is different and it seems to be fixed by now.  It is working with my pandas version 0.25.2.</p>
","['python', 'pandas', 'datetime']",58726769,"<p>This is a known bug, see <a href=""https://github.com/pandas-dev/pandas/issues/12649"" rel=""nofollow noreferrer"">github</a> for details.</p>

<p>Since we needed a solution I came up with the following workaround. Please note that in my question I used <code>read_csv</code> to keep the reproducible code snippet small and simple.  We actually use <code>read_fwf</code> and here is some sample data (time.txt):</p>

<pre><code>2019 10 07 + 14:45 15:00  # Foo
2019 10 07 + 18:00 18:30  # Bar
  2019 10 09 + 13:00 13:45  # Wrong indentation
</code></pre>

<p>I felt stating the row number is also a good idea so I added a little bit more voodoo:</p>

<pre><code>class FileSanitizer(io.TextIOBase):
    row = 0
    date_range = None

    def __init__(self, iterable, date_range):
        self.iterable = iterable
        self.date_range = date_range

    def readline(self):
        result = next(self.iterable)
        self.row += 1
        try:
            datetime.datetime.strptime(result[self.date_range[0]:self.date_range[1]], ""%Y %m %d"")
        except ValueError as excep:
            raise ValueError(f'row: {self.row} =&gt; {str(excep)}') from ValueError
        return result


filepath = 'time.txt'
colspecs = [[0, 10], [13, 18], [19, 25], [26, None]]
names = ['date', 'start', 'end', 'description']

with open(filepath, 'r') as file:
    df = pd.read_fwf(FileSanitizer(file, colspecs[0]),
                     colspecs=colspecs,
                     names=names,
                     )
</code></pre>

<p>The solution is based on this answer <a href=""https://stackoverflow.com/a/42584515/2606766"">How to skip blank lines with read_fwf in pandas?</a>. Please note this will <strong>not</strong> work with <code>read_csv</code>.</p>

<p>Now I get the following error as expected:</p>

<pre><code>ValueError: row: 3 =&gt; time data '  2019 10 ' does not match format '%Y %m %d'
</code></pre>

<p>If anyone has a more sophisticated answer I'm happy to learn.</p>
",Pandas datetime error wrong format I read CSV file containing dates Some dates may formatted wrong I want find With following approach I would expect nd row NaT But pandas seems ignore specified format matter I set infer datetime format exact import pandas pd io import StringIO DATA StringIO date df pd read csv DATA df date pd datetime df date format Y errors coerce exact True results date The pandas datetime documentation refers strftime strptime Behavior I test plain Python works datetime datetime strptime Y I get expected value error ValueError time data match format Y What I miss FYI This question pandas datetime working seems related different seems fixed It working pandas version,"startoftags, python, pandas, datetime, endoftags",python pandas datetime endoftags,python pandas datetime,python pandas datetime,1.0
58945075,2019-11-20,2019,2,Get the number of rows in groupby-ed dataframe,"<p>I am trying to get the number of rows(elements?) of groupby-ed dataframe. Image will make it easier to explain.
Here's my dataframe <code>winningPap_group</code>. It is groupby-ed with <code>Paper ID</code>
<a href=""https://i.stack.imgur.com/7m3ha.png"" rel=""nofollow noreferrer"">This is dataframe: <code>winningPap_group</code></a></p>

<p>Here what I need to do is get the number of Laureate ID in each paper ID group. For example the number of Laureate ID in Paper ID==1.147687e+07 is 3. I would like to know how should I code to make it happen.</p>
","['python', 'pandas', 'dataframe']",58945528,"<p>use </p>

<pre><code>winningPap_group.groupby('Paper ID')['Paper ID'].size()
</code></pre>

<p>or </p>

<pre><code>winningPap_group.groupby('Paper ID')['Paper ID'].transform('size')
</code></pre>
",Get number rows groupby ed dataframe I trying get number rows elements groupby ed dataframe Image make easier explain Here dataframe winningPap group It groupby ed Paper ID This dataframe winningPap group Here I need get number Laureate ID paper ID group For example number Laureate ID Paper ID e I would like know I code make happen,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
59091946,2019-11-28,2019,2,Using Certain Conditions to Find Certain Parts of a Dataframe,"<p>I have a dataframe that looks like this:</p>

<pre><code>&gt;&gt;&gt; data = {'Count':[15, 21, 1, 7, 6, 1, 25, 8, 56, 0, 5, 9, 0, 12, 12, 8, 7, 12, 0, 8]}
&gt;&gt;&gt; df = pd.DataFrame(data)
&gt;&gt;&gt; df
    Count
0      15
1      21
2       1
3       7
4       6
5       1
6      25
7       8
8      56
9       0
10      5
11      9
12      0
13     12
14     12
15      8
16      7
17     12
18      0
19      8
</code></pre>

<p>I need to add two columns to this df to detect ""floods"". ""Flood"" is defined as from the row where 'Count' goes above 10 and until 'Count' drops below 5.
So, in this case, I want this as a result:</p>

<pre><code>    Count   Flood   FloodNumber
0      15    True             1
1      21    True             1
2       1   False             0
3       7   False             0
4       6   False             0
5       1   False             0
6      25    True             2
7       8    True             2
8      56    True             2
9       0   False             0
10      5   False             0
11      9   False             0
12      0   False             0
13     12    True             3
14     12    True             3
15      8    True             3
16      7    True             3
17     12    True             3
18      0   False             0
19      8   False             0
</code></pre>

<p>I managed to add my 'Flood' column with a simple loop like this:</p>

<pre><code>df.loc[0, 'Flood'] = (df.loc[0, 'Count'] &gt; 10)
for index in range(1, len(df)):
    df.loc[index, 'Flood'] = ((df.loc[index, 'Count'] &gt; 10) | ((df.loc[index-1, 'Flood']) &amp; (df.loc[index, 'Count'] &gt; 5)))
</code></pre>

<p>, but this seems like an extremly slow and stupid way of doing this. Is there any ""proper"" way of doing it using pandas functions rather than loops?</p>
","['python', 'python-3.x', 'pandas']",59092477,"<p>To find <code>Flood</code> flags, we can play with masks and <code>ffill()</code>.</p>

<pre><code>df['Flood'] = ((df.Count &gt; 10).where(df.Count &gt; 10)
               .fillna((df.Count &gt; 5)
                       .where(df.Count &lt; 5))
               .ffill()
               .astype(bool))
</code></pre>

<p>To get the <code>FloodNumber</code>, let's ignore all rows which are <code>False</code> in the <code>Flood</code> column and groupby+cumsum</p>

<pre><code>s = df.Flood.where(df.Flood)
df.loc[:, 'FloodNumber'] = s.dropna().groupby((s != s.shift(1)).cumsum()).ngroup().add(1)
</code></pre>

<p>Outputs</p>

<pre><code>    Count  Flood  FloodNumber
0      15   True          1.0
1      21   True          1.0
2       1  False          NaN
3       7  False          NaN
4       6  False          NaN
5       1  False          NaN
6      25   True          2.0
7       8   True          2.0
8      56   True          2.0
9       0  False          NaN
10      5  False          NaN
11      9  False          NaN
12      0  False          NaN
13     12   True          3.0
14     12   True          3.0
15      8   True          3.0
16      7   True          3.0
17     12   True          3.0
18      0  False          NaN
19      8  False          NaN
</code></pre>
",Using Certain Conditions Find Certain Parts Dataframe I dataframe looks like gt gt gt data Count gt gt gt df pd DataFrame data gt gt gt df Count I need add two columns df detect floods Flood defined row Count goes Count drops So case I want result Count Flood FloodNumber True True False False False False True True True False False False False True True True True True False False I managed add Flood column simple loop like df loc Flood df loc Count gt index range len df df loc index Flood df loc index Count gt df loc index Flood amp df loc index Count gt seems like extremly slow stupid way Is proper way using pandas functions rather loops,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
59277973,2019-12-11,2019,3,legend colors are not matching seaborn,"<p>I am using sns.pointplot and since there is no <code>label</code> attribute I've decided to create my custom legend but my problem is that the colors do not match.</p>

<p>my dataframe looks like this:</p>

<pre><code>                  deploy    deployed_today_rent total_rent  cum_deploy  hourly percent  cum_percent
10min                       
2019-10-01 05:30:00 6           0   0   6   0.000000    0.000000
2019-10-01 05:40:00 0           0   0   6   0.000000    0.000000
2019-10-01 05:50:00 6           0   0   12  0.000000    0.000000
2019-10-01 06:00:00 13          0   0   25  0.000000    0.000000
2019-10-01 06:10:00 0           0   0   25  0.000000    0.000000
2019-10-01 06:20:00 0           1   1   25  0.040000    0.040000
2019-10-01 06:30:00 0           0   0   25  0.000000    0.040000
2019-10-01 06:40:00 0           1   1   25  0.040000    0.080000
2019-10-01 06:50:00 1           1   1   26  0.038462    0.118462

fig,(ax1)= plt.subplots(nrows=1)
fig.set_size_inches(22,17)

sns.pointplot(data=test, x=test.index, y=""total_rent"", ax=ax1,color=""blue"", label=""total"")
sns.pointplot(data=test, x=test.index, y=""deployed_today_rent"", ax=ax1, color=""green"", label=""deployed_rent"")
sns.pointplot(data=test, x=test.index, y=""cum_deploy"", ax=ax1, color=""#BEC647"", label=""cum_deploy"")

ax1.legend(labels=[""total"", ""deployed_rent"", ""cum_deploy""], fontsize=15)

plt.savefig(""test.png"", dpi=300, bbox_inches=""tight"");
</code></pre>

<p>It successfully creates a legend however colors in the legend does not match the lines.</p>

<p><a href=""https://i.stack.imgur.com/EkEH2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EkEH2.png"" alt=""enter image description here""></a></p>
","['python', 'matplotlib', 'seaborn']",59280487,"<p>From the <a href=""https://matplotlib.org/3.1.1/tutorials/intermediate/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists"" rel=""nofollow noreferrer"">legend guide</a>, you can do this by creating a âproxy artistâ for each line using <code>matplotlib.lines.Line2D</code> like so </p>

<pre class=""lang-py prettyprint-override""><code>from matplotlib.lines import Line2D

a = Line2D([], [], color=âblueâ, label=âtotalâ)
b = Line2D([], [], color=âgreenâ, label=âdeployed_rentâ)
c = Line2D([], [], color=â#BEC647â, label=âcum_deployâ)
plt.legend(handles=[a, b, c])
</code></pre>

<p>Which should produce a legend with three different color lines of default width with their respective labels.</p>
",legend colors matching seaborn I using sns pointplot since label attribute I decided create custom legend problem colors match dataframe looks like deploy deployed today rent total rent cum deploy hourly percent cum percent min fig ax plt subplots nrows fig set size inches sns pointplot data test x test index total rent ax ax color blue label total sns pointplot data test x test index deployed today rent ax ax color green label deployed rent sns pointplot data test x test index cum deploy ax ax color BEC label cum deploy ax legend labels total deployed rent cum deploy fontsize plt savefig test png dpi bbox inches tight It successfully creates legend however colors legend match lines,"startoftags, python, matplotlib, seaborn, endoftags",python matplotlib seaborn endoftags,python matplotlib seaborn,python matplotlib seaborn,1.0
59473736,2019-12-24,2019,3,Python bin sets of pairs of interleaving arrays,"<p>I have a set of pairs of numpy arrays. Each array in a pair is the same length, but arrays in different pairs have different lengths. An example of a pair of arrays from this set is:</p>

<pre><code>Time: [5,8,12,17,100,121,136,156,200]
Score: [3,4,5,-10,-90,-80,-70,-40,10]
</code></pre>

<p>Another pair is:</p>

<pre><code>Time: [6,7,9,15,199]
Score: [5,6,7,-11,-130]
</code></pre>

<p>I need to take an average (or perform binning) of all of these pairs based on the time. i.e. the time should be divided into intervals of 10 and the corresponding score(s) for each interval need to be averaged. </p>

<p>Thus, for the above 2 pairs, I want the following result:</p>

<pre><code>Time: [1-10,11-20,21-30,31-40,41-50,...,191-200]
Score: [(3+4+5+6+7)/5, (5-10-11)/2, ...]
</code></pre>

<p>How can I do this? Is there a simpler way to do this than bin everything individually and then take the average? How do you bin an array based on the bins of another array? i.e. for an individual pair of arrays, how can I bin the time array into intervals of 10 and then use this result to bin the corresponding score array in a consistent manner?</p>
","['python', 'arrays', 'numpy']",59473899,"<p>You can use <code>scipy.stats.binned_statistic</code>. This is a generalization of a histogram function. A histogram divides the space into bins, and returns the <strong>count</strong> of the number of points in each bin. This function allows the computation of the <strong>sum, mean, median, or other statistic</strong> of the values (or set of values) within each bin.</p>

<pre class=""lang-py prettyprint-override""><code>from scipy import stats
import numpy as np

T1 = [5,8,12,17,100,121,136,156,200]
S1 = [3,4,5,-10,-90,-80,-70,-40,10]

T2 = [6,7,9,15,199]
S2 = [5,6,7,-11,-130]

# Merging all Times and Scores in order
Time = T1 + T2
Score = S1 + S2

output = stats.binned_statistic(Time, Score, statistic='mean',range=(0,200), bins=20)

averages = output[0]

# For empty bins, it generates NaN, we can replace them with 0
print( np.nan_to_num(averages, 0) )

# Output of this code: 
# [  5.          -5.33333333   0.           0.           0.
#    0.           0.           0.           0.           0.
#  -90.           0.         -80.         -70.           0.
#  -40.           0.           0.           0.         -60.        ]
</code></pre>

<p>For more information follow <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binned_statistic.html"" rel=""nofollow noreferrer"">this link</a>.</p>
",Python bin sets pairs interleaving arrays I set pairs numpy arrays Each array pair length arrays different pairs different lengths An example pair arrays set Time Score Another pair Time Score I need take average perform binning pairs based time e time divided intervals corresponding score interval need averaged Thus pairs I want following result Time Score How I Is simpler way bin everything individually take average How bin array based bins another array e individual pair arrays I bin time array intervals use result bin corresponding score array consistent manner,"startoftags, python, arrays, numpy, endoftags",python pandas numpy endoftags,python arrays numpy,python pandas numpy,0.67
59601105,2020-01-05,2020,3,Difference of row values by matching values in another column in a Pandas DataFrame,"<p>I have a DataFrame generated from a call to Pandas.io.json.json_normalize(). Here is an 
example:</p>

<pre><code>dfIn = pd.DataFrame({'seed':[1324367672,1324367672,1324367673,1324367673,1324367674,1324367674], 'lanePolicy':[True,False,True,False,True,False,],
                   'stepsPerTrip':[40,37,93,72,23,70], 'density':[51,51,208,208,149,149]})
</code></pre>

<pre><code>  seed       lanePolicy stepsPerTrip density
0 1324367672 True       40           51
1 1324367672 False      37           51
2 1324367673 True       93           208
3 1324367673 False      72           208
4 1324367674 True       23           149
5 1324367674 False      70           149
</code></pre>

<p>Note that there are pairs of matching values in <code>dfIn['seed']</code> with one <code>True</code> and one <code>False</code> value in <code>dfIn['lanePolicy']</code>. It is also the case that if <code>dfIn['seed']</code> matches for two given rows, <code>dfIn['densitiy']</code> will match as well. I would like to compute a table similar to the following:</p>

<pre><code>dfDesired = pd.DataFrame({'seed':[1324367672,1324367673,1324367674], 
                   'stepsTrue':[40,93,23], 'stepsFalse':[37,72,70], 'stepsDiff':[3, 21, -47], 'density':[51,208,149]})

</code></pre>

<pre><code>  seed       stepsTrue stepsFalse stepsDiff density
0 1324367672 40        37         3         51
1 1324367673 93        72         21        208
2 1324367674 23        70         -47       149
</code></pre>

<p>In particular, I'm looking for the values in <code>dfDesired['stepsDiff']</code>, which are the differences between <code>dfIn['stepsPerTrip']</code> for the associated <code>False</code> and <code>True</code> values of <code>dfIn['lanePolicy']</code> for each pair of matching <code>dfIn['seed']</code>. Note also that <code>dfDesired</code> should have half the number of rows as <code>dfIn</code>.</p>

<p>I am able to calculate the values of that single column with:</p>

<pre><code>dfDiff = dfIn.loc[dfIn['lanePolicy'] == True]['stepsPerTrip'].reset_index()['stepsPerTrip'] - dfIn.loc[dfIn['lanePolicy'] == False]['stepsPerTrip'].reset_index()['stepsPerTrip']
</code></pre>

<pre><code>0     3
1    21
2   -47
Name: stepsPerTrip, dtype: int64
</code></pre>

<p>However, I would like to make a new DataFrame that preserves the other columns. I have also tried the following, but get an incorrect result:</p>

<pre><code>dfDesired = dfIn.groupby('seed').apply(lambda x:x.loc[x['lanePolicy']==True]['stepsPerTrip']-x.loc[x['lanePolicy']==False]['stepsPerTrip'])
</code></pre>

<pre><code>seed         
1324367672  0   NaN
            1   NaN
1324367673  2   NaN
            3   NaN
1324367674  4   NaN
            5   NaN
Name: stepsPerTrip, dtype: float64
</code></pre>

<p>Thank you, in advance.</p>
","['python', 'pandas', 'dataframe']",59601133,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot.html"" rel=""nofollow noreferrer""><code>DataFrame.pivot</code></a>, subtract columns by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.sub.html"" rel=""nofollow noreferrer""><code>Series.sub</code></a> and for <code>density</code> column add Series with <code>seed</code> without duplicated with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html"" rel=""nofollow noreferrer""><code>DataFrame.drop_duplicates</code></a>:</p>

<pre><code>df = dfIn.pivot('seed','lanePolicy','stepsPerTrip').add_prefix('steps')
df['stepsDiff'] = df['stepsTrue'].sub(df['stepsFalse'])
df['density'] = dfIn.drop_duplicates('seed').set_index('seed')['density']
df = df.reset_index().rename_axis(None, axis=1)
print (df)
         seed  stepsFalse  stepsTrue  stepsDiff  density
0  1324367672          37         40          3       51
1  1324367673          72         93         21      208
2  1324367674          70         23        -47      149
</code></pre>

<p>Another solution is with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html"" rel=""nofollow noreferrer""><code>DataFrame.pivot_table</code></a> and default aggregate function <code>mean</code> if duplicates in columns <code>seed</code>, 'density' and <code>lanePolicy</code>:</p>

<pre><code>df = (dfIn.pivot_table(index=['seed','density'], columns='lanePolicy',values='stepsPerTrip')
          .add_prefix('steps'))
df['stepsDiff'] = df['stepsTrue'].sub(df['stepsFalse'])
df = df.reset_index().rename_axis(None, axis=1)
print (df)
         seed  density  stepsFalse  stepsTrue  stepsDiff
0  1324367672       51          37         40          3
1  1324367673      208          72         93         21
2  1324367674      149          70         23        -47
</code></pre>
",Difference row values matching values another column Pandas DataFrame I DataFrame generated call Pandas io json json normalize Here example dfIn pd DataFrame seed lanePolicy True False True False True False stepsPerTrip density seed lanePolicy stepsPerTrip density True False True False True False Note pairs matching values dfIn seed one True one False value dfIn lanePolicy It also case dfIn seed matches two given rows dfIn densitiy match well I would like compute table similar following dfDesired pd DataFrame seed stepsTrue stepsFalse stepsDiff density seed stepsTrue stepsFalse stepsDiff density In particular I looking values dfDesired stepsDiff differences dfIn stepsPerTrip associated False True values dfIn lanePolicy pair matching dfIn seed Note also dfDesired half number rows dfIn I able calculate values single column dfDiff dfIn loc dfIn lanePolicy True stepsPerTrip reset index stepsPerTrip dfIn loc dfIn lanePolicy False stepsPerTrip reset index stepsPerTrip Name stepsPerTrip dtype int However I would like make,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
59702732,2020-01-12,2020,2,How to obtain &quot;time difference&quot; between 2 consecutive records?,"<p>Given that i have a dataset as below:</p>

<pre><code>import pandas as pd
import numpy as np

dt = {
    ""facility"":[""Ann Arbor"",""Ann Arbor"",""Detriot"",""Detriot"",""Detriot""],
    ""patient_ID"":[4388,4388,9086,9086,9086],
    ""year"":[2004,2007,2007,2008,2011],
    ""month"":[8,9,9,6,2],
    ""Nr_Small"":[0,0,5,12,10],
    ""Nr_Medium"":[3,1,1,4,3],
    ""Nr_Large"":[2,0,0,0,0]
}

dt = pd.DataFrame(dt)
dt.head()
</code></pre>

<p>For each group of users (consider it as <code>groupby patient_ID</code> ), I wish to <strong>obtain the the difference between</strong> <code>year</code> and <code>month</code> <strong>between each 2 consecutive rows.</strong> Here is my code:</p>

<pre><code>patients = dt['patient_ID'].unique()


for patient in patients:
    print(patient)
    patientDT = dt[ dt.patient_ID == patient] # Get group of records for each paitent
    patientDT['NumberOfVisits'] = np.shape(patientDT)[0] # Add number of records for each paitent as a new column     

    patientDT.sort_values(['year', 'month'], ascending=[True, True],inplace=True) # sort by year and month
    patientDT = addPeriodBetween2Visits(patientDT)

    print(patientDT)

    print(""------------------------------"")
</code></pre>

<p>and then the method which must obtain the difference is <code>addPeriodBetween2Visits</code> :</p>

<pre><code>def addPeriodBetween2Visits(patientDT):

    for i in range(0,np.shape(patientDT)[0]):

        if(i == 0):
            patientDT['PeriodBetween2Visits'] = 0
        else:
            lastVisit = patientDT.loc[i-1,'year']*12 + patientDT.loc[i-1,'month']
            recentVisit = patientDT.loc[i,'year']*12 + patientDT.loc[i,'month']
            patientDT.loc[i,'PeriodBetween2Visits'] = recentVisit - lastVisit


    return patientDT
</code></pre>

<p>Unfortunately, it is failing, but the error is not clear for me. Here is the error log in <strong>jupyter notebook</strong>:</p>

<pre><code>KeyError                                  Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
   2656             try:
-&gt; 2657                 return self._engine.get_loc(key)
   2658             except KeyError:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

KeyError: 0

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
&lt;ipython-input-4-eda12d36a355&gt; in &lt;module&gt;
      8 
      9     patientDT.sort_values(['year', 'month'], ascending=[True, True],inplace=True)
---&gt; 10     patientDT = addPeriodBetween2Visits(patientDT)
     11 
     12     print(patientDT)

&lt;ipython-input-2-c8b1e6851452&gt; in addPeriodBetween2Visits(patientDT)
      7         else:
      8             #print(patientDT.loc[i-1,'year'])
----&gt; 9             lastVisit = patientDT.loc[i-1,'year']*12 + patientDT.loc[i-1,'month']
     10             recentVisit = patientDT.loc[i,'year']*12 + patientDT.loc[i,'month']
     11             patientDT.loc[i,'PeriodBetween2Visits'] = recentVisit - lastVisit

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py in __getitem__(self, key)
   1492             except (KeyError, IndexError, AttributeError):
   1493                 pass
-&gt; 1494             return self._getitem_tuple(key)
   1495         else:
   1496             # we by definition only have the 0th axis

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py in _getitem_tuple(self, tup)
    866     def _getitem_tuple(self, tup):
    867         try:
--&gt; 868             return self._getitem_lowerdim(tup)
    869         except IndexingError:
    870             pass

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py in _getitem_lowerdim(self, tup)
    986         for i, key in enumerate(tup):
    987             if is_label_like(key) or isinstance(key, tuple):
--&gt; 988                 section = self._getitem_axis(key, axis=i)
    989 
    990                 # we have yielded a scalar ?

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py in _getitem_axis(self, key, axis)
   1911         # fall thru to straight lookup
   1912         self._validate_key(key, axis)
-&gt; 1913         return self._get_label(key, axis=axis)
   1914 
   1915 

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexing.py in _get_label(self, label, axis)
    139             raise IndexingError('no slices here, handle elsewhere')
    140 
--&gt; 141         return self.obj._xs(label, axis=axis)
    142 
    143     def _get_loc(self, key, axis=None):

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\generic.py in xs(self, key, axis, level, drop_level)
   3583                                                       drop_level=drop_level)
   3584         else:
-&gt; 3585             loc = self.index.get_loc(key)
   3586 
   3587             if isinstance(loc, np.ndarray):

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
   2657                 return self._engine.get_loc(key)
   2658             except KeyError:
-&gt; 2659                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2660         indexer = self.get_indexer([key], method=method, tolerance=tolerance)
   2661         if indexer.ndim &gt; 1 or indexer.size &gt; 1:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

KeyError: 0
</code></pre>

<p>Here is the favorit output:</p>

<p><a href=""https://i.stack.imgur.com/kbPGN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kbPGN.png"" alt=""enter image description here""></a></p>

<p><strong><em>I will be thankful, if you specifically tell me why my code is not working.</em></strong></p>
","['python', 'python-3.x', 'pandas', 'dataframe']",59702826,"<p>For counts use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>Series.map</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html"" rel=""nofollow noreferrer""><code>Series.value_counts</code></a>.</p>

<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.diff.html"" rel=""nofollow noreferrer""><code>DataFrameGroupBy.diff</code></a> with <code>Series</code>, so grouping by column <code>dt['patient_ID']</code>, then replace missing values and conver to integers:</p>

<pre><code>dt['NumberOfVisits'] = dt['patient_ID'].map(dt['patient_ID'].value_counts())

dt[""PeriodBetween2Visits""] = (dt[""year""].mul(12)
                                        .add(dt[""month""])
                                        .groupby(dt['patient_ID'])
                                        .diff()
                                        .fillna(0)
                                        .astype(int))


print (dt)
    facility  patient_ID  year  month  Nr_Small  Nr_Medium  Nr_Large  \
0  Ann Arbor        4388  2004      8         0          3         2   
1  Ann Arbor        4388  2007      9         0          1         0   
2    Detriot        9086  2007      9         5          1         0   
3    Detriot        9086  2008      6        12          4         0   
4    Detriot        9086  2011      2        10          3         0   

   NumberOfVisits  PeriodBetween2Visits  
0               2                     0  
1               2                    37  
2               3                     0  
3               3                     9  
4               3                    32  
</code></pre>
",How obtain quot time difference quot consecutive records Given dataset import pandas pd import numpy np dt facility Ann Arbor Ann Arbor Detriot Detriot Detriot patient ID year month Nr Small Nr Medium Nr Large dt pd DataFrame dt dt head For group users consider groupby patient ID I wish obtain difference year month consecutive rows Here code patients dt patient ID unique patient patients print patient patientDT dt dt patient ID patient Get group records paitent patientDT NumberOfVisits np shape patientDT Add number records paitent new column patientDT sort values year month ascending True True inplace True sort year month patientDT addPeriodBetween Visits patientDT print patientDT print method must obtain difference addPeriodBetween Visits def addPeriodBetween Visits patientDT range np shape patientDT patientDT PeriodBetween Visits else lastVisit patientDT loc year patientDT loc month recentVisit patientDT loc year patientDT loc month patientDT loc PeriodBetween Visits recentVisit lastVisit return patientDT Unfortunately failing,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
59762439,2020-01-16,2020,2,Plot pie chart with values,"<p>My df is below</p>

<pre><code>sales_qtr_month = sales_qtr.groupby(['Month']).agg('sum')
</code></pre>

<pre><code>    Sales2015   Sales2016
Month       
Q1  5.485800e+06    6.997953e+06
Q2  5.390862e+06    7.237361e+06
Q3  6.164094e+06    7.861546e+06
Q4  5.713634e+06    7.567868e+06
</code></pre>

<p>pie chart code</p>

<pre><code>sales_qtr_month.plot.pie(figsize=(15,15),subplots=True)
</code></pre>

<p><a href=""https://i.stack.imgur.com/5jxn0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5jxn0.png"" alt=""enter image description here""></a></p>

<ul>
<li>How to add the values of Q1,Q2,Q3,Q4 to chart?</li>
</ul>
","['python', 'pandas', 'matplotlib']",59763663,"<p>I am not sure how to generate your data, so I will just generate something similar to cover all the steps. </p>

<pre><code># Make some data
raw_data = {'Sales2016': [10, 20, 900, 100, 50],
            'Sales2015': [10, 20, 30, 100, 50],
        'Month': ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']}

# Generate a dataframe
df = pd.DataFrame(raw_data, columns = ['Sales2016','Sales2015', 'Month'])
</code></pre>

<p>After this, I will run groupby() which won't change the data at all. The purpose of this groupby() is to get the DataFrameGroupBy which is similar to the one given by OP.</p>

<pre><code>df_sum = df.groupby(['Month']).agg('sum')
</code></pre>

<p>Which constructs</p>

<pre><code>    Sales2016   Sales2015
Month       
Q1      10         10
Q2      20         20
Q3      900        30
Q4      100        100
Q5      50         50
</code></pre>

<p>From this point, I would flatten the DataFrameGroupBy and index it with month.</p>

<pre><code>df_flatten = df_sum.reset_index().set_index('Month')
</code></pre>

<p>The <a href=""https://matplotlib.org/gallery/pie_and_polar_charts/pie_and_donut_labels.html"" rel=""nofollow noreferrer"">link</a> provided by <a href=""https://stackoverflow.com/users/9900084/steven"">steven</a> holds a really good insight on how to label in matplotlib. The function pandas.DataFrame.plot.pie wraps matplotlib.pyplot.pie() so we can use some of the techniques from that link.</p>

<p>First we declare a function to covert percentage into whole number. This function is similar to the one given by this <a href=""https://matplotlib.org/gallery/pie_and_polar_charts/pie_and_donut_labels.html"" rel=""nofollow noreferrer"">link</a>. I have added round() to the function so that it won't given a false number (Ex. int(889.99) = 899 but we want 900) </p>

<pre><code># Covert percent and total to value
def func(pct, allvals):
    absolute = int(round(pct/100*np.sum(allvals)))
    return ""${:.1f}"".format(absolute)
</code></pre>

<p>Now we can plot the pie chart out with label, we can specify the autopct similar to the one from the <a href=""https://matplotlib.org/gallery/pie_and_polar_charts/pie_and_donut_labels.html"" rel=""nofollow noreferrer"">link</a></p>

<pre><code>f, axes = plt.subplots(1,2, figsize=(10,10))
for ax, col in zip(axes, df_flatten.columns):
    print(col)
    df_flatten[col].plot(kind='pie', autopct=lambda pct: func(pct, df_flatten[col].tolist()), labels=df_flatten.index,  ax=ax, title=col, fontsize=10)
    ax.legend(loc=3)
</code></pre>

<p>Your labels should now appear.</p>

<p><a href=""https://i.stack.imgur.com/gDFCo.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gDFCo.jpg"" alt=""result""></a></p>
",Plot pie chart values My df sales qtr month sales qtr groupby Month agg sum Sales Sales Month Q e e Q e e Q e e Q e e pie chart code sales qtr month plot pie figsize subplots True How add values Q Q Q Q chart,"startoftags, python, pandas, matplotlib, endoftags",python pandas matplotlib endoftags,python pandas matplotlib,python pandas matplotlib,1.0
59912093,2020-01-25,2020,2,"I have a dataframe, is it possible to get a matrix with rows as the name of the fruits and the values as frequency of that fruit?","<p>I have a data frame,  is it possible to get a matrix with rows as the name of the fruits and the values as the frequency of that fruit?</p>

<pre><code>       a       b       c
0  apple  orange  banana
1   kiwi  orange  orange
2   kiwi  banana   apple
</code></pre>

<hr>

<p>[Output:]</p>

<pre><code>--------------------------------
|       |  a   |   b  |   c    |
--------------------------------
|apple  |  1   |   0  |   1    |
|orange |  0   |   2  |   1    |
|kiwi   |  2   |   0  |   0    |
|banana |  0   |   1  |   1    | 
-------------------------------|
</code></pre>
","['python', 'pandas', 'dataframe']",59912151,"<p>We can also use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html"" rel=""nofollow noreferrer""><code>pd.crosstab</code></a> with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.melt.html#pandas.DataFrame.melt"" rel=""nofollow noreferrer""><code>DataFrame.melt</code></a></p>

<pre><code>new_df = df.melt()
pd.crosstab(new_df['value'],new_df['variable'])

#        a  b  c
#apple   1  0  1
#banana  0  1  1
#kiwi    2  0  0
#orange  0  2  1
</code></pre>
",I dataframe possible get matrix rows name fruits values frequency fruit I data frame possible get matrix rows name fruits values frequency fruit b c apple orange banana kiwi orange orange kiwi banana apple Output b c apple orange kiwi banana,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
60059300,2020-02-04,2020,2,How to fill the first date in the column?,"<p>I  have a <code>df</code>:</p>

<pre><code>dates   values
2020-01-01 00:15:00 38.61487
2020-01-01 00:30:00 36.905204
2020-01-01 00:45:00 35.136584
2020-01-01 01:00:00 33.60378
2020-01-01 01:15:00 32.306791999999994
2020-01-01 01:30:00 31.304574
</code></pre>

<p>I am creating a new column named <code>start</code> as follows:</p>

<pre><code>df = df.rename(columns={'dates': 'end'})
df['start']= df['end'].shift(1)
</code></pre>

<p>When I do this, I get the following:</p>

<pre><code>end values  start
2020-01-01 00:15:00 38.61487    NaT
2020-01-01 00:30:00 36.905204   2020-01-01 00:15:00
2020-01-01 00:45:00 35.136584   2020-01-01 00:30:00
2020-01-01 01:00:00 33.60378    2020-01-01 00:45:00
2020-01-01 01:15:00 32.306791999999994  2020-01-01 01:00:00
2020-01-01 01:30:00 31.304574   2020-01-01 01:15:00
</code></pre>

<p>I want to fill that <code>NaT</code> value with </p>

<pre><code>2020-01-01 00:00:00
</code></pre>

<p>How can this be done?</p>
","['python', 'python-3.x', 'pandas']",60059314,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.fillna.html"" rel=""nofollow noreferrer""><code>Series.fillna</code></a> with <code>datetime</code>s, e.g. by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html"" rel=""nofollow noreferrer""><code>Timestamp</code></a>:</p>

<pre><code>df['start']= df['end'].shift().fillna(pd.Timestamp('2020-01-01'))
</code></pre>

<p>Or if pandas 0.24+ with <code>fill_value</code> parameter:</p>

<pre><code>df['start']= df['end'].shift(fill_value=pd.Timestamp('2020-01-01'))
</code></pre>

<p>If all datetimes are regular, always difference <code>15 minutes</code> is possible subtracting by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.tseries.offsets.DateOffset.html"" rel=""nofollow noreferrer""><code>offsets.DateOffset</code></a>:</p>

<pre><code>df['start']= df['end'] - pd.offsets.DateOffset(minutes=15)
print (df)
                  end     values               start
0 2020-01-01 00:15:00  38.614870 2020-01-01 00:00:00
1 2020-01-01 00:30:00  36.905204 2020-01-01 00:15:00
2 2020-01-01 00:45:00  35.136584 2020-01-01 00:30:00
3 2020-01-01 01:00:00  33.603780 2020-01-01 00:45:00
4 2020-01-01 01:15:00  32.306792 2020-01-01 01:00:00
5 2020-01-01 01:30:00  31.304574 2020-01-01 01:15:00
</code></pre>
",How fill first date column I df dates values I creating new column named start follows df df rename columns dates end df start df end shift When I I get following end values start NaT I want fill NaT value How done,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
60122103,2020-02-07,2020,2,Split and transform columns into aggregated columns,"<p>The data I have is as described below:</p>
<h1>Input:</h1>
<pre><code>df = pd.DataFrame({&quot;col1&quot;: [&quot;A1&quot;, &quot;A2&quot;], &quot;col2&quot;: [&quot;B1&quot;, &quot;B2&quot;], &quot;2015-1&quot;: [231, 432], &quot;2015-2&quot;: [456, 324]})
print(df)
  col1 col2  2015-1  2015-2
0   A1   B1     231     456
1   A2   B2     432     324
</code></pre>
<p>The columns <code>2015-1</code> and <code>2015-2</code> corresponds to years and months. The data I want to transform is as below:</p>
<h1>Output:</h1>
<pre><code>print(df)
  col1 col2  year    month  values
0   A1   B1  2015        1     231
1   A1   B1  2015        2     456
2   A2   B2  2015        1     432
3   A2   B2  2015        2     324
</code></pre>
<p>I want to Transform the data into another dataframe without constructing a loop since the data I have contains lots of columns and rows, it takes a really long time. Is there any way to convert the input into output without a loop?</p>
","['python', 'pandas', 'dataframe']",60122190,"<p>We can do <code>melt</code> with <code>split</code> then <code>join</code> back </p>

<pre><code>s=df.melt(['col1','col2'])
s=s.join(s.variable.str.split('-',expand=True).rename(columns={0:'Year',1:'Month'}))
  col1 col2 variable  value  Year Month
0   A1   B1   2015-1    231  2015     1
1   A2   B2   2015-1    432  2015     1
2   A1   B1   2015-2    456  2015     2
3   A2   B2   2015-2    324  2015     2
</code></pre>
",Split transform columns aggregated columns The data I described Input df pd DataFrame quot col quot quot A quot quot A quot quot col quot quot B quot quot B quot quot quot quot quot print df col col A B A B The columns corresponds years months The data I want transform Output print df col col year month values A B A B A B A B I want Transform data another dataframe without constructing loop since data I contains lots columns rows takes really long time Is way convert input output without loop,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
60270081,2020-02-17,2020,3,ValueError: &quot;cannot reindex from a duplicate axis&quot; in groupby Pandas,"<p>My dataframe looks like this:</p>

<pre>
    SKU #    GRP    CATG   PRD
0   54995  9404000  4040  99999
1   54999  9404000  4040  99999
2   55037  9404000  4040  1556894
3   55148  9404000  4040  1556894
4   55254  9404000  4040  1556894
5   55291  9404000  4040  1556894
6   55294  9404000  4040  1556895
7   55445  9404000  4040  1556895
8   55807  9404001  4040  1556896
9   49021  9404002  4040  1556897
10  49035  9404002  4040  1556897
11  27538  9404000  4040  1556898
12  27539  9404000  4040  1556899
13  27540  9404000  4040  1556894
14  27542  9404000  4040  1556900
15  27543  9404000  4040  1556900
16  27544  9404003  4040  1556901
17  27546  9404004  4040  1556902
18  99111  9404005  4040  1556903
19  99112  9404006  4040  1556904
20  99113  9404007  4040  1556905
21  99116  9404008  4040  1556906
22  99119  9404009  4040  1556907
23  99122  94040010 4040  1556908
24  99125  94040011 4040  1556909
25  86007  94040012 4040  1556910
26  86010  94040013 4040  1556911 
</pre>

<p>And when I try to perform a group by operation on the above dataframe, I get the ""cannot reindex from a duplicate axis"" error.</p>

<pre><code>df.groupby(['GRP','CATG'],as_index=False)['PRD'].min()
</code></pre>

<p>I tried to find out the duplicate indices using:</p>

<pre><code>df[df.index.duplicated()]
</code></pre>

<p>But didn't return any thing. 
How can I go about resolving this issue?</p>
","['python', 'pandas', 'pandas-groupby']",60274227,"<p>This error is often thrown due to duplications in your column names (not necessarily values) </p>

<p>First, just check if there is any duplication in your column names using the code:
<code>df.columns.duplicated().any()</code></p>

<p>If it's true, then remove the duplicated columns</p>

<p><code>df.loc[:,~df.columns.duplicated()]</code></p>

<p>After you remove the duplicated columns, you should be able to run your <code>groupby</code> operation. </p>
",ValueError quot cannot reindex duplicate axis quot groupby Pandas My dataframe looks like SKU GRP CATG PRD And I try perform group operation dataframe I get cannot reindex duplicate axis error df groupby GRP CATG index False PRD min I tried find duplicate indices using df df index duplicated But return thing How I go resolving issue,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
60301466,2020-02-19,2020,2,Setting a surrounding block based on center index in a numpy array,"<p>I have a 2D numpy array of zeros representing some flat surface:</p>

<pre><code>field = np.zeros((10,10))

field
Out[165]: 
array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
</code></pre>

<p>Then I have an array of coordinates in the form <code>[row,column]</code> such as:</p>

<pre><code>In [166]:c = np.array([[1,2],[4,5],[7,3],[2,6]])

In [167]:c
Out[167]: 
array([[1, 2],
       [4, 5],
       [7, 3],
       [2, 6]])
</code></pre>

<p>What I would like to do is populate a block of the <em>field</em> array around the coordinates in <em>c</em>.</p>

<pre><code>Out[192]: 
array([[0., 1., 1., 1., 0., 0., 0., 0., 0., 0.],
       [0., 1., 1., 1., 0., 1., 1., 1., 0., 0.],
       [0., 1., 1., 1., 0., 1., 1., 1., 0., 0.],
       [0., 0., 0., 0., 1., 1., 1., 1., 0., 0.],
       [0., 0., 0., 0., 1., 1., 1., 0., 0., 0.],
       [0., 0., 0., 0., 1., 1., 1., 0., 0., 0.],
       [0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],
       [0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],
       [0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
</code></pre>

<p>My initial attempt using numpy vectorization was:</p>

<pre><code>In [168]:field[c[:,0]-1:c[:,0]+1,c[:,1]-1:c[:,1]+1] = 10
Traceback (most recent call last):

  File ""&lt;ipython-input-168-5433a2f4a5cf&gt;"", line 1, in &lt;module&gt;
    field[c[:,0]-1:c[:,0]+1,c[:,1]-1:c[:,1]+1] = 10

TypeError: only integer scalar arrays can be converted to a scalar index
</code></pre>

<p>I then tried first creating the <code>c[:,0]-1</code> and <code>c[:,1]+1</code> arrays before hand but got the same error which leads me to the conclusion that, that type of ranged indexing cannot be done in numpy.</p>

<p>I have also had a look at the <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.ix_.html"" rel=""nofollow noreferrer"">np.ix_()</a> but cannot set surrounding blocks of multiple coordinates without a for loop using this method.</p>

<p>I am able to achieve the desired output however using the for loop:</p>

<pre><code>for row,column in c:
        field[row-1:row+2,column-1:column+2] = 1
</code></pre>

<p>But would not like to use a for loop because both c and f will be large and multidimensional in my final application and I feel I can capitalize on the speed improvements made by numpy vectorization.</p>

<p>Also, I know in image processing this could be seen as a dilation or erosion problem but I already have the coordinates for an erosion/dilation kernel to be placed at and again, multiple dimensions and very large arrays.</p>
","['python', 'arrays', 'numpy']",60301928,"<p>Here's a simple way which only has a small amount of Python looping and a lot of vectorized work:</p>

<pre><code>x, y = c[:,0], c[:,1]
for i in -1,0,1:
    for j in -1,0,1:
        field[x+i,y+j] = 1
</code></pre>

<p>A more complicated way which may be faster:</p>

<pre><code>offsets = np.array([[-1,-1],[-1,0],[-1,1], [0,-1],[0,0],[0,1], [1,-1],[1,0],[1,1]])
fill = (offsets + c[:,None]).reshape(-1,2)
field[fill[:,0], fill[:,1]] = 1
</code></pre>
",Setting surrounding block based center index numpy array I D numpy array zeros representing flat surface field np zeros field Out array Then I array coordinates form row column In c np array In c Out array What I would like populate block field array around coordinates c Out array My initial attempt using numpy vectorization In field c c c c Traceback recent call last File lt ipython input f cf gt line lt module gt field c c c c TypeError integer scalar arrays converted scalar index I tried first creating c c arrays hand got error leads conclusion type ranged indexing cannot done numpy I also look np ix cannot set surrounding blocks multiple coordinates without loop using method I able achieve desired output however using loop row column c field row row column column But would like use loop c f large multidimensional final application I,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
60445075,2020-02-28,2020,6,Dataframe cell to be locked and used for a running balance calculation conditional of result on another cell on same row,"<p>Say I have the following dataframe:</p>

<pre><code>import pandas as pd
df = pd.DataFrame()

df['A'] = ('1/05/2019','2/05/2019','3/05/2019','4/05/2019','5/05/2019','6/05/2019','7/05/2019','8/05/2019','9/05/2019','10/05/2019','11/05/2019','12/05/2019','13/05/2019','14/05/2019','15/05/2019','16/05/2019','17/05/2019','18/05/2019','19/05/2019','20/05/2019')

df['B'] = ('SIT','SCLOSE', 'SHODL', 'SHODL', 'SHODL', 'SHODL', 'SHODL', 'SELL','SIT','SIT','BCLOSE', 'BHODL', 'BHODL', 'BHODL', 'BHODL', 'BHODL', 'BHODL','BUY','SIT','SIT')

df['C'] = (0.00,1.00,10.00, 5.00,6.00,-6.00, 6.00, 0.00,0.00,0.00,-8.00,33.00,-15.00,6.00,-1.00,5.00,10.00,0.00,0.00,0.00)

df.loc[19, 'D'] = 100.0000
</code></pre>

<p>As can be seen I am starting column D with 100 at the last row.</p>

<p>I am trying to code a calculation for column D so starting from the bottom row (row 19) when a BUY or SELL is shown on column B then the number on column D is locked (eg the 100) and used for a calculation based on col C for each SHODL or BHODL until the row after a BCLOSE or an SCLOSE is shown.</p>

<p>The locked number is used to calculate a running balance based on the percentages that are in column C. As you can see on row 16 column C has '10' representing 10%. As 10% of 100 = 10 the new runnning balance is 110. </p>

<p>Row 15 column C has 5% as such 5 is added to the running balance to result in 115. </p>

<p>The next row 14 column C has a -1% change as such 1% of 100 is = 1 and therefore the new running balance is 114 and so on.</p>

<p>The following are the results that should be returned in col D of the dataframe once the right code is run </p>

<pre><code>df['D'] = ('158.60','158.60', '157.30', '144.30', '137.80', '130.00', '137.80', '130.00','130.00','130.00','130.00', '138.00', '105.00', '120.00', '114.00', '115.00', '110.00','100.00','100.00','100.00')
</code></pre>

<p>This continues until after a SCLOSE or a BCLOSE is shown as a BCLOSE or SCLOSE row is the final row where the running balance is calculated.</p>

<p>As you can see this process is restarted when either a new BUY or SELL is shown.</p>
","['python', 'pandas', 'dataframe']",60499600,"<p>Next starting value depends on the last value of previous group, so I think it can't be vectorized. It requires some kind of iterative process. I came up with solution doing iteratively on groups of groupby. Reverse <code>df</code> and assign to <code>df1</code>. Working on each group of <code>df1</code> and assign the final list of groups to the original <code>df</code></p>

<pre><code>df1 = df[::-1]
s = df1.B.isin(['BCLOSE','SCLOSE']).shift(fill_value=False).cumsum()
grps = df1.groupby(s)
init_val= 100
l = []
for _, grp in grps:
    s = grp.C * 0.01 * init_val
    s.iloc[0] = init_val
    s = s.cumsum()
    init_val = s.iloc[-1]
    l.append(s)

df['D'] = pd.concat(l)

Out[50]:
             A       B     C      D
0    1/05/2019     SIT   0.0  158.6
1    2/05/2019  SCLOSE   1.0  158.6
2    3/05/2019   SHODL  10.0  157.3
3    4/05/2019   SHODL   5.0  144.3
4    5/05/2019   SHODL   6.0  137.8
5    6/05/2019   SHODL  -6.0  130.0
6    7/05/2019   SHODL   6.0  137.8
7    8/05/2019    SELL   0.0  130.0
8    9/05/2019     SIT   0.0  130.0
9   10/05/2019     SIT   0.0  130.0
10  11/05/2019  BCLOSE  -8.0  130.0
11  12/05/2019   BHODL  33.0  138.0
12  13/05/2019   BHODL -15.0  105.0
13  14/05/2019   BHODL   6.0  120.0
14  15/05/2019   BHODL  -1.0  114.0
15  16/05/2019   BHODL   5.0  115.0
16  17/05/2019   BHODL  10.0  110.0
17  18/05/2019     BUY   0.0  100.0
18  19/05/2019     SIT   0.0  100.0
19  20/05/2019     SIT   0.0  100.0        
</code></pre>
",Dataframe cell locked used running balance calculation conditional result another cell row Say I following dataframe import pandas pd df pd DataFrame df A df B SIT SCLOSE SHODL SHODL SHODL SHODL SHODL SELL SIT SIT BCLOSE BHODL BHODL BHODL BHODL BHODL BHODL BUY SIT SIT df C df loc D As seen I starting column D last row I trying code calculation column D starting bottom row row BUY SELL shown column B number column D locked eg used calculation based col C SHODL BHODL row BCLOSE SCLOSE shown The locked number used calculate running balance based percentages column C As see row column C representing As new runnning balance Row column C added running balance result The next row column C change therefore new running balance The following results returned col D dataframe right code run df D This continues SCLOSE BCLOSE shown BCLOSE SCLOSE row final row,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
60635218,2020-03-11,2020,2,Create a new dataframe with the lines that were not included on other dataframe,"<p>I need to create a new dataframe with the lines that were not included in the dataframe ''results'' but are on the dataframe ''df'' without creating a new filter.</p>

<p>I have no idea how to do it. Can you help me out? (sorry if noob question)</p>

<pre><code>import pandas as pd

#Creating dataframe
d = {'col1': [1, 2,3,4,5,6,7,8,9,10], 'col2': ['a','b','b','b','c','d','c','a','z','c']}
df = pd.DataFrame(data=d)

#Finding the lines that contain a certain letter
a = df[df['col2'].str.contains(""a"")]
b = df[df['col2'].str.contains(""b"")]
c = df[df['col2'].str.contains(""c"")]

#Merge the 3 data frames
frames = [a, b, c]
results = pd.concat(frames)
print(results)
</code></pre>
","['python', 'pandas', 'dataframe']",60635275,"<p>Better solution should be create mask by all 3 masks in one with <code>|</code> for OR and then for non match rows invert mask by <code>~</code>:</p>

<pre><code>m = df['col2'].str.contains(""a|b|c"")
results = df[m]
print(results)
   col1 col2
0     1    a
1     2    b
2     3    b
3     4    b
4     5    c
6     7    c
7     8    a
9    10    c

df1 = df[~m]
print (df1)
   col1 col2
5     6    d
8     9    z
</code></pre>

<p>Your solution should be changed by filter non matched index values:</p>

<pre><code>df1 = df[~df.index.isin(results.index)]
print (df1)
   col1 col2
5     6    d
8     9    z
</code></pre>
",Create new dataframe lines included dataframe I need create new dataframe lines included dataframe results dataframe df without creating new filter I idea Can help sorry noob question import pandas pd Creating dataframe col col b b b c c z c df pd DataFrame data Finding lines contain certain letter df df col str contains b df df col str contains b c df df col str contains c Merge data frames frames b c results pd concat frames print results,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
60701190,2020-03-16,2020,2,Spark - Merge / Union DataFrame with Different Schema (column names and sequence) to a DataFrame with Master common schema,"<p>I tried taking a schema as a common schema by df.schema() and load all the CSV files to it .But fails as to the assigned schema , the headers of other CSV files doesnot match</p>

<p>Any suggestions would be appreciated.  as in a function or spark script</p>
","['python', 'apache-spark', 'pyspark']",60702657,"<p>as I understand it. You want to Union / Merge files with different schemas ( though subset of one Master Schema) ..
I wrote this function UnionPro which I think just suits your requirement -</p>
<p><strong>EDIT</strong> - Added a Pyspark version</p>
<pre><code>def unionPro(DFList: List[DataFrame], spark: org.apache.spark.sql.SparkSession): DataFrame = {

    /**
     * This Function Accepts DataFrame with same or Different Schema/Column Order.With some or none common columns
     * Creates a Unioned DataFrame
     */

    import spark.implicits._

    val MasterColList: Array[String] = DFList.map(_.columns).reduce((x, y) =&gt; (x.union(y))).distinct

    def unionExpr(myCols: Seq[String], allCols: Seq[String]): Seq[org.apache.spark.sql.Column] = {
      allCols.toList.map(x =&gt; x match {
        case x if myCols.contains(x) =&gt; col(x)
        case _                       =&gt; lit(null).as(x)
      })
    }

    // Create EmptyDF , ignoring different Datatype in StructField and treating them same based on Name ignoring cases

    val masterSchema = StructType(DFList.map(_.schema.fields).reduce((x, y) =&gt; (x.union(y))).groupBy(_.name.toUpperCase).map(_._2.head).toArray)

    val masterEmptyDF = spark.createDataFrame(spark.sparkContext.emptyRDD[Row], masterSchema).select(MasterColList.head, MasterColList.tail: _*)

    DFList.map(df =&gt; df.select(unionExpr(df.columns, MasterColList): _*)).foldLeft(masterEmptyDF)((x, y) =&gt; x.union(y))

  }

</code></pre>
<p>Here is the sample test for it -</p>
<pre><code>
    val aDF = Seq((&quot;A&quot;, 1), (&quot;B&quot;, 2)).toDF(&quot;Name&quot;, &quot;ID&quot;)
    val bDF = Seq((&quot;C&quot;, 1), (&quot;D&quot;, 2)).toDF(&quot;Name&quot;, &quot;Sal&quot;)
    unionPro(List(aDF, bDF), spark).show

</code></pre>
<p>Which gives output as -</p>
<pre><code>+----+----+----+
|Name|  ID| Sal|
+----+----+----+
|   A|   1|null|
|   B|   2|null|
|   C|null|   1|
|   D|null|   2|
+----+----+----+
</code></pre>
<p>Here's Pyspark version of it -</p>
<pre><code>def unionPro(DFList: List[DataFrame], caseDiff: str = &quot;N&quot;) -&gt; DataFrame:
    &quot;&quot;&quot;
    :param DFList:
    :param caseDiff:
    :return:
    This Function Accepts DataFrame with same or Different Schema/Column Order.With some or none common columns
    Creates a Unioned DataFrame
    &quot;&quot;&quot;
    inputDFList = DFList if caseDiff == &quot;N&quot; else [df.select([F.col(x.lower) for x in df.columns]) for df in DFList]

    # &quot;This Preserves Order ( OrderedDict0-----------------------------------&quot;
    from collections import OrderedDict
    ## As columnNames ( String) are hashable
    masterColStrList = list(OrderedDict.fromkeys(reduce(lambda x, y: x + y, [df.columns for df in inputDFList])))

    # Create masterSchema ignoring different Datatype &amp; Nullable  in StructField and treating them same based on Name ignoring cases
    ignoreNullable = lambda x: StructField(x.name, x.dataType, True)

    import itertools

    
    # to get reliable results by groupby iterable must be sorted by grouping key
    # in sorted function key function( lambda) must be passed as named argument ( keyword argument)
    # but by Sorting now, I lost original order of columns. Hence I'll use masterColStrList while returning final DF
    masterSchema = StructType([list(y)[0] for x, y in itertools.groupby(
        sorted(reduce(lambda x, y: x + y, [[ignoreNullable(x) for x in df.schema.fields] for df in inputDFList]),
               key=lambda x: x.name),
        lambda x: x.name)])

    def unionExpr(myCols: List[str], allCols: List[str]) -&gt; List[Column]:
        return [F.col(x) if x in myCols else F.lit(None).alias(x) for x in allCols]

    # Create Empty Dataframe
    masterEmptyDF = spark.createDataFrame([], masterSchema)

    return reduce(lambda x, y: x.unionByName(y),
                  [df.select(unionExpr(df.columns, masterColStrList)) for df in inputDFList], masterEmptyDF).select(
        masterColStrList)

</code></pre>
",Spark Merge Union DataFrame Different Schema column names sequence DataFrame Master common schema I tried taking schema common schema df schema load CSV files But fails assigned schema headers CSV files doesnot match Any suggestions would appreciated function spark script,"startoftags, python, apachespark, pyspark, endoftags",python discord discordpy endoftags,python apachespark pyspark,python discord discordpy,0.33
60957046,2020-03-31,2020,2,Pandas get column contains character in all rows,"<p>I want to get list of dataframe columns that contains all rows with 2 spaces.</p>

<p><strong>Input:</strong></p>

<pre><code>import pandas as pd
import numpy as np
pd.options.display.max_columns = None
pd.options.display.max_rows = None
pd.options.display.expand_frame_repr = False

df = pd.DataFrame({'id': [101, 102, 103],
                   'full_name': ['John Brown', 'Bob Smith', 'Michael Smith'],
                   'comment_1': ['one two', 'qw er ty', 'one space'],
                   'comment_2': ['ab xfd xsxws', 'dsd sdd dwde', 'wdwd ofjpoej oihoe'],
                   'comment_3': ['ckdf cenfw cd', 'cewfwf wefep lwcpem', np.nan],
                   'birth_year': [1960, 1970, 1970]})

print(df)
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>    id      full_name  comment_1           comment_2            comment_3  birth_year
0  101     John Brown    one two        ab xfd xsxws        ckdf cenfw cd        1960
1  102      Bob Smith   qw er ty        dsd sdd dwde  cewfwf wefep lwcpem        1970
2  103  Michael Smith  one space  wdwd ofjpoej oihoe                  NaN        1970
</code></pre>

<p><strong>Expected Output:</strong></p>

<pre><code>['comment_2', 'comment_3']
</code></pre>
","['python', 'python-3.x', 'pandas', 'dataframe']",60957205,"<p>You can use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.count.html"" rel=""noreferrer"">series.str.count()</a> to count the appearances of a substring or pattern in a string, use <code>.all()</code> to check whether all items meet the criteria, and iterate over <code>df.columns</code> using only string columns with <code>select_dtypes('object')</code></p>

<pre><code>[i for i in df.select_dtypes('object').columns if (df[i].dropna().str.count(' ')==2).all()]    
['comment_2', 'comment_3']
</code></pre>
",Pandas get column contains character rows I want get list dataframe columns contains rows spaces Input import pandas pd import numpy np pd options display max columns None pd options display max rows None pd options display expand frame repr False df pd DataFrame id full name John Brown Bob Smith Michael Smith comment one two qw er ty one space comment ab xfd xsxws dsd sdd dwde wdwd ofjpoej oihoe comment ckdf cenfw cd cewfwf wefep lwcpem np nan birth year print df Output id full name comment comment comment birth year John Brown one two ab xfd xsxws ckdf cenfw cd Bob Smith qw er ty dsd sdd dwde cewfwf wefep lwcpem Michael Smith one space wdwd ofjpoej oihoe NaN Expected Output comment comment,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
61158963,2020-04-11,2020,3,how to sort the result by pandas.value_counts,"<p>I want to count the number of student who got 0-5 grade in one dataset</p>

<p>I use this function <code>final_grade_num =pd.value_counts(final_grade)</code> to get a result like </p>

<pre><code>4.0    487
3.0    432
2.0    376
5.0    334
1.0    232
0.0    139
</code></pre>

<p>however, I want to get a sorted list like </p>

<pre><code>0.0 139, 1.0 232...5.0 334
</code></pre>

<p>because I need to use this dataset to draw a plot</p>

<pre><code>plt.bar(range(0,6), final_grade_num)
</code></pre>

<p>Is there any method to change, I tried to use sorted method, but the result shows it depends on the number of students, not grade</p>
","['python', 'pandas', 'matplotlib']",61159083,"<p><code>value_counts()</code> has a <code>sort</code> argument that defaults to <code>True</code>. Just set it to <code>False</code> and it will be sorted by value instead.</p>

<pre><code>df['col'].value_counts(sort = False).plot.bar(title='My Title')
</code></pre>

<p>Or:</p>

<pre><code>df['col'].value_counts().sort_index().plot.bar()
</code></pre>
",sort result pandas value counts I want count number student got grade one dataset I use function final grade num pd value counts final grade get result like however I want get sorted list like I need use dataset draw plot plt bar range final grade num Is method change I tried use sorted method result shows depends number students grade,"startoftags, python, pandas, matplotlib, endoftags",python pandas dataframe endoftags,python pandas matplotlib,python pandas dataframe,0.67
61278504,2020-04-17,2020,2,Delete a row in pandas dataframe while using df.iterrows(),"<p>I need to delete the row if there is data missing in a certain column in the current row.</p>

<p>This is what I wrote:</p>

<pre><code>for c, r in data.iterrows():
if (r['A'] == """"):
    data = data.drop(r)
</code></pre>

<p>But I receive the error:</p>

<blockquote>
  <p>""unreadable key error""</p>
</blockquote>
","['python', 'pandas', 'dataframe']",61278704,"<p>You can do something like this:</p>

<pre><code>data = data.drop(data[data['A'] == ''].index)
</code></pre>

<p>OR</p>

<pre><code>data[data['A'] != """"]
</code></pre>
",Delete row pandas dataframe using df iterrows I need delete row data missing certain column current row This I wrote c r data iterrows r A data data drop r But I receive error unreadable key error,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
61302953,2020-04-19,2020,2,How to search a numpy array for a sub-vector?,"<p><br>
I have a numpy array (in Python 3) and I'd like to find a sub-vector in it.<br>
If I search for a full vector, this code works:</p>

<pre><code>import numpy as np

a = np.zeros([10,5])
a[0] = [5,6,4,8,5]
a[1] = [3,6,8,5,3]
a[2] = [3,2,1,5,3]
a[3] = [6,5,6,4,6]
a[4] = [3,4,7,6,3]
a[5] = [2,3,1,5,2]
a[6] = [1,1,3,2,1]
a[7] = [6,5,8,8,6]
a[8] = [5,4,9,7,5]
a[9] = [1,2,7,8,1]

print(a)
search = [2,3,1,5,2] # correctly returns 5
i = np.argwhere(np.all((a-np.array(search))==0, axis=1))
print(int(i))
</code></pre>

<p>Ok, but I'd like to find this sub-vector:</p>

<pre><code>search = [2,3,1,5]
</code></pre>

<p>How can I find it?</p>
","['python', 'arrays', 'numpy']",61305032,"<p>Simple numpy solution:  </p>

<p>Search the raveled array and stop in first occurrence (you will be able to modify this into any type of search you deem fit including if your <code>search</code> list spans over multiple rows AND also finding multiple occurrences of <code>search</code> in <code>a</code>).  </p>

<p>Following code assumes you are looking for first occurrence within a row of <code>a</code>:</p>

<pre><code>for i in range(a.size-len(search)):
    if np.array_equal(np.ravel(a)[i:i+len(search)], np.array(search)) and int(i/a.shape[1])==int((i+len(search)-1)/a.shape[1]):
        print(int(i/a.shape[1]))
        break
</code></pre>

<p>If speed matters and <code>a</code>/<code>search</code> is large, save a raveled version of <code>a</code> into <code>a_ravel = np.ravel(a)</code> and numpy array of <code>np.array(search)</code> and use that inside the for loop.</p>
",How search numpy array sub vector I numpy array Python I like find sub vector If I search full vector code works import numpy np np zeros print search correctly returns np argwhere np np array search axis print int Ok I like find sub vector search How I find,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
61354147,2020-04-21,2020,2,How to obtain xticks with alternating distances?,"<p>I have a bar plot where the x-axis has the 2-letter labels of countries. I have to choose a very small font, so the x-ticks do not overlap with each other. Is there any way to assign a distance between x-ticks and the x-axis, so I can choose a larger font without labels being overlapped?</p>

<p>Currently:</p>

<p><a href=""https://i.stack.imgur.com/gKYYU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gKYYU.png"" alt=""enter image description here""></a></p>

<p>Desired example: </p>

<pre><code>axis:  ----------------------
ticks: c1  c3  ...
         c2  c4   ...
</code></pre>
","['python', 'matplotlib', 'seaborn']",61354709,"<p>An approach is to create minor ticks at the alternating positions and give them a larger tick length. A <code>MultipleLocator</code> of 2 for the major ticks puts them every 2. Adding a <code>MultipleLocator</code> of 1 for the minor ticks fills in the gaps, as major ticks automatically suppress minor ticks in overlapping positions. The color of the ticks can be made lighter to obtain more contrast between the ticks and the labels.</p>

<p>The same approach would work when the plot would be generated via seaborn or pandas, as long as an explicit list of labels can be provided.</p>

<pre class=""lang-py prettyprint-override""><code>from matplotlib import pyplot as plt
from matplotlib import ticker
import numpy as np

letters = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')
N = 80
names = [letters[i // 26] + letters[i % 26] for i in range(N)]
values = np.random.binomial(100, 0.1, N)

cmap = plt.cm.get_cmap('rainbow')
colors = [cmap(i / N) for i in range(N)]
plt.bar(names, values, color=colors)
ax = plt.gca()
ax.xaxis.set_major_locator(ticker.MultipleLocator(2))
ax.xaxis.set_minor_locator(ticker.MultipleLocator(1))
ax.xaxis.set_minor_formatter(ticker.IndexFormatter(names))
ax.tick_params(axis='x', which='minor', length=15)
ax.tick_params(axis='x', which='both', color='lightgrey')
ax.autoscale(enable=True, axis='x', tight=True)

plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/G6lLT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G6lLT.png"" alt=""example plot""></a></p>

<p>PS: @MadPhysicist's idea from the comments to add newlines is even simpler. It looks a bit different:</p>

<pre class=""lang-py prettyprint-override""><code>from matplotlib import pyplot as plt
import numpy as np

letters = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')
N = 80
names = [('\n' if i % 2 == 1 else '') + letters[i // 26] + letters[i % 26] for i in range(N)]
values = np.random.binomial(100, 0.1, N)

cmap = plt.cm.get_cmap('rainbow')
colors = [cmap(i / N) for i in range(N)]
plt.bar(names, values, color=colors)
plt.gca().autoscale(enable=True, axis='x', tight=True)
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/JpZnj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JpZnj.png"" alt=""plot using newlines""></a></p>
",How obtain xticks alternating distances I bar plot x axis letter labels countries I choose small font x ticks overlap Is way assign distance x ticks x axis I choose larger font without labels overlapped Currently Desired example axis ticks c c c c,"startoftags, python, matplotlib, seaborn, endoftags",python pandas matplotlib endoftags,python matplotlib seaborn,python pandas matplotlib,0.67
61443261,2020-04-26,2020,4,What is the use of pd.plotting.register_matplotlib_converters() in Pandas,"<p>While learning from an online course on visualising data, I came across this line of code.</p>

<pre><code>import pandas as pd
pd.plotting.register_matplotlib_converters()
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
</code></pre>

<p>Can someone please tell me what is the use of </p>

<pre><code>pd.plotting.register_matplotlib_converters()
</code></pre>

<p>I referred to the official documentation, but a clear explanation is not given. <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.register_matplotlib_converters.html"" rel=""nofollow noreferrer"">Documentation</a></p>
","['python', 'pandas', 'matplotlib']",61443297,"<p>I found this in the <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.register_matplotlib_converters.html"" rel=""nofollow noreferrer"">documentation</a>:</p>

<p><em>This function modifies the global matplotlib.units.registry dictionary. Pandas adds custom converters for</em></p>

<ul>
<li><em>pd.Timestamp</em></li>
<li><em>pd.Period</em></li>
<li><p><em>np.datetime64</em></p>

<p>...</p></li>
</ul>

<p>So I guess it makes sure that <code>pandas</code> datatypes like <code>pd.Timestamp</code> can be used in <code>matplotlib</code> plots without having to cast them to another type. </p>
",What use pd plotting register matplotlib converters Pandas While learning online course visualising data I came across line code import pandas pd pd plotting register matplotlib converters import matplotlib pyplot plt matplotlib inline import seaborn sns Can someone please tell use pd plotting register matplotlib converters I referred official documentation clear explanation given Documentation,"startoftags, python, pandas, matplotlib, endoftags",python pandas matplotlib endoftags,python pandas matplotlib,python pandas matplotlib,1.0
61505749,2020-04-29,2020,12,"tensorflow:Can save best model only with val_acc available, skipping","<p>I have an issue with <code>tf.callbacks.ModelChekpoint</code>. As you can see in my log file, the warning comes always before the last iteration where the <code>val_acc</code> is calculated. Therefore, <code>Modelcheckpoint</code> never finds the <code>val_acc</code></p>
<pre><code>Epoch 1/30
1/8 [==&gt;...........................] - ETA: 19s - loss: 1.4174 - accuracy: 0.3000
2/8 [======&gt;.......................] - ETA: 8s - loss: 1.3363 - accuracy: 0.3500 
3/8 [==========&gt;...................] - ETA: 4s - loss: 1.3994 - accuracy: 0.2667
4/8 [==============&gt;...............] - ETA: 3s - loss: 1.3527 - accuracy: 0.3250
6/8 [=====================&gt;........] - ETA: 1s - loss: 1.3042 - accuracy: 0.3333
WARNING:tensorflow:Can save best model only with val_acc available, skipping.
8/8 [==============================] - 4s 482ms/step - loss: 1.2846 - accuracy: 0.3375 - val_loss: 1.3512 - val_accuracy: 0.5000

Epoch 2/30
1/8 [==&gt;...........................] - ETA: 0s - loss: 1.0098 - accuracy: 0.5000
3/8 [==========&gt;...................] - ETA: 0s - loss: 0.8916 - accuracy: 0.5333
5/8 [=================&gt;............] - ETA: 0s - loss: 0.9533 - accuracy: 0.5600
6/8 [=====================&gt;........] - ETA: 0s - loss: 0.9523 - accuracy: 0.5667
7/8 [=========================&gt;....] - ETA: 0s - loss: 0.9377 - accuracy: 0.5714
WARNING:tensorflow:Can save best model only with val_acc available, skipping.
8/8 [==============================] - 1s 98ms/step - loss: 0.9229 - accuracy: 0.5750 - val_loss: 1.2507 - val_accuracy: 0.5000
</code></pre>
<p>This is my code for training the CNN.</p>
<pre><code>    callbacks = [
        TensorBoard(log_dir=r'C:\Users\reda\Desktop\logs\{}'.format(Name),
                    histogram_freq=1),
        ModelCheckpoint(filepath=r&quot;C:\Users\reda\Desktop\checkpoints\{}&quot;.format(Name), monitor='val_acc',
                        verbose=2, save_best_only=True, mode='max')]
    history = model.fit_generator(
        train_data_gen,
        steps_per_epoch=total_train // batch_size,
        epochs=epochs,
        validation_data=val_data_gen,
        validation_steps=total_val // batch_size,
        callbacks=callbacks)```
</code></pre>
","['python', 'tensorflow', 'machine-learning', 'keras', 'deep-learning']",61510447,"<p>I know how frustrating these things can be sometimes..but tensorflow requires that you explicitly write out the name of metric you are wanting to calculate</p>

<p>You will need to actually say 'val_accuracy'</p>

<pre><code>metric = 'val_accuracy'
ModelCheckpoint(filepath=r""C:\Users\reda.elhail\Desktop\checkpoints\{}"".format(Name), monitor=metric,
                    verbose=2, save_best_only=True, mode='max')]
</code></pre>

<p>Hope this helps =)</p>
",tensorflow Can save best model val acc available skipping I issue tf callbacks ModelChekpoint As see log file warning comes always last iteration val acc calculated Therefore Modelcheckpoint never finds val acc Epoch gt ETA loss accuracy gt ETA loss accuracy gt ETA loss accuracy gt ETA loss accuracy gt ETA loss accuracy WARNING tensorflow Can save best model val acc available skipping ms step loss accuracy val loss val accuracy Epoch gt ETA loss accuracy gt ETA loss accuracy gt ETA loss accuracy gt ETA loss accuracy gt ETA loss accuracy WARNING tensorflow Can save best model val acc available skipping ms step loss accuracy val loss val accuracy This code training CNN callbacks TensorBoard log dir r C Users reda Desktop logs format Name histogram freq ModelCheckpoint filepath r quot C Users reda Desktop checkpoints quot format Name monitor val acc verbose save best True mode max history,"startoftags, python, tensorflow, machinelearning, keras, deeplearning, endoftags",python tensorflow keras endoftags,python tensorflow machinelearning keras deeplearning,python tensorflow keras,0.77
61519313,2020-04-30,2020,2,Simple moving average 2D array python,"<p>I am trying to compute a simple moving average for each line of a 2D array. The data in each row is a separate data set, so I can't just compute the SMA over the whole array, I need to do it seperately in each line. I have tried a for loop but it is taking the window as rows, rather than individual values. </p>

<p>The equation I am using to compute the SMA is: a1+a2+...an/n
This is the code I have so far:</p>

<pre><code>import numpy as np  


#make amplitude array
amplitude=[0,1,2,3, 5.5, 6,5,2,2, 4, 2,3,1,6.5,5,7,1,2,2,3,8,4,9,2,3,4,8,4,9,3]


#split array up into a line for each sample
traceno=5                  #number of traces in file
samplesno=6                #number of samples in each trace. This wont change.

amplitude_split=np.array(amplitude, dtype=np.int).reshape((traceno,samplesno))

#define window to average over:
window_size=3

#doesn't work for values that come before the window size. i.e. index 2 would not have enough values to divide by 3
#define limits:
lowerlimit=(window_size-1)
upperlimit=samplesno

i=window_size

for row in range(traceno):
  for n in range(samplesno):
    while lowerlimit&lt;i&lt;upperlimit:
      this_window=amplitude_split[(i-window_size):i] 

      window_average=sum(this_window)/window_size

      i+=1
      print(window_average)
</code></pre>

<p>My expected output for this data set is:</p>

<pre><code>[[1,    2,    3.33, 4.66]
 [3,    2.66, 2.66, 3.  ]
 [4,    6,    4.33, 3.33]
 [4.33, 5,    7,    5.  ]
 [5,    5.33, 7,    5.33]]
</code></pre>

<p>But I am getting:</p>

<pre><code>[2.         3.         3.         4.66666667 2.66666667 3.66666667]
[2.66666667 3.66666667 5.         5.         4.         2.33333333]
[2.         4.33333333 7.         5.         6.33333333 2.33333333]
</code></pre>
","['python', 'arrays', 'numpy']",61520402,"<p>You can use convolution to <code>[1, 1, ..., 1]</code> of <code>window_size</code> and then divide it to <code>window_size</code> to get average (no need for loop):  </p>

<pre><code>from scipy.signal import convolve2d

window_average = convolve2d(amplitude_split, np.ones((1, window_size)), 'valid') / window_size)
</code></pre>

<p>convolution to <code>ones</code> basically adds up elements in the window.</p>

<p>output:</p>

<pre><code>[[1.         2.         3.33333333 4.66666667]
 [3.         2.66666667 2.66666667 3.        ]
 [4.         6.         4.33333333 3.33333333]
 [4.33333333 5.         7.         5.        ]
 [5.         5.33333333 7.         5.33333333]]
</code></pre>
",Simple moving average D array python I trying compute simple moving average line D array The data row separate data set I compute SMA whole array I need seperately line I tried loop taking window rows rather individual values The equation I using compute SMA n This code I far import numpy np make amplitude array amplitude split array line sample traceno number traces file samplesno number samples trace This wont change amplitude split np array amplitude dtype np int reshape traceno samplesno define window average window size work values come window size e index would enough values divide define limits lowerlimit window size upperlimit samplesno window size row range traceno n range samplesno lowerlimit lt lt upperlimit window amplitude split window size window average sum window window size print window average My expected output data set But I getting,"startoftags, python, arrays, numpy, endoftags",python django djangomodels endoftags,python arrays numpy,python django djangomodels,0.33
61894015,2020-05-19,2020,2,Pandas extractall merge,"<p>Not sure if I should fix my regex pattern, or process more with pandas.</p>

<p>Here's a mock setup:</p>

<pre><code>import re
import pandas as pd

regex = r""(?P&lt;adv&gt;This)|(?P&lt;noun&gt;test)""
texts = [""This is a test"", ""Random stuff with no match""]
series = pd.Series(texts)
</code></pre>

<p>I want to find all matches for groups (<code>&lt;adv&gt;</code>, <code>&lt;noun&gt;</code> -- there are typically more than two). These groups are designed to be <strong>exclusive</strong> hence I would want to have only one row result with the captured string / NaN.  </p>

<p>Current output: multi-index rows, only for texts that have a match</p>

<pre><code>&gt;&gt;&gt; print(series.str.extractall(regex))
          adv  noun
  match            
0 0      This   NaN
  1       NaN  test
</code></pre>

<p>Expected output: one row per input text, and aggregated matchs per group</p>

<pre><code>          adv  noun
0        This  test
1         NaN   NaN
</code></pre>

<p>Any chance for a hand on this? Either fix the regex, or post-process with pandas.
Thanks!</p>
","['python', 'regex', 'pandas']",61894085,"<p>You can try;</p>

<pre><code>series.str.extractall(regex).groupby(level=0).first()

    adv  noun
0  This  test
</code></pre>
",Pandas extractall merge Not sure I fix regex pattern process pandas Here mock setup import import pandas pd regex r P lt adv gt This P lt noun gt test texts This test Random stuff match series pd Series texts I want find matches groups lt adv gt lt noun gt typically two These groups designed exclusive hence I would want one row result captured string NaN Current output multi index rows texts match gt gt gt print series str extractall regex adv noun match This NaN NaN test Expected output one row per input text aggregated matchs per group adv noun This test NaN NaN Any chance hand Either fix regex post process pandas Thanks,"startoftags, python, regex, pandas, endoftags",python pandas numpy endoftags,python regex pandas,python pandas numpy,0.67
61966893,2020-05-23,2020,2,Python: how do I filter data as long as a group contains any of a particular value,"<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({'VisitID':[1,1,1,1,2,2,2,3,3,4,4], 'Item':['A','B','C','D','A','D','B','B','C','D','C']})
</code></pre>

<p>I have a dataset like this:</p>

<pre><code>VisitID | Item |
1       | A    |
1       | B    |
1       | C    |
1       | D    |
2       | A    |
2       | D    |
2       | B    |
3       | B    |
3       | C    |
4       | D    |
4       | C    |
</code></pre>

<p>I want to return VisitID rows as long as that VisitID had a occurrence of item A OR B. How do I go about? Expected Result:</p>

<pre><code>VisitID | Item |
1       | A    |
1       | B    |
1       | C    |
1       | D    |
2       | A    |
2       | D    |
2       | B    |
3       | B    |
3       | C    |
</code></pre>

<p>In R, I can do this via </p>

<pre><code>library(dplyr)
df %&gt;% group_by(VisitID) %&gt;% filter(any(Item %in% c('A', 'B')))
</code></pre>

<p>How can I perform this in Python? 
Something like <code>df.groupby(['VisitID']).query(any(['A','B']))?</code></p>
","['python', 'python-3.x', 'pandas']",61966977,"<p>The syntax is similar, just use <code>groupby.filter</code>:</p>

<pre><code>df.groupby('VisitID').filter(lambda g: g.Item.isin(['A','B']).any())

   VisitID Item
0        1    A
1        1    B
2        1    C
3        1    D
4        2    A
5        2    D
6        2    B
7        3    B
8        3    C
</code></pre>
",Python I filter data long group contains particular value df pd DataFrame VisitID Item A B C D A D B B C D C I dataset like VisitID Item A B C D A D B B C D C I want return VisitID rows long VisitID occurrence item A OR B How I go Expected Result VisitID Item A B C D A D B B C In R I via library dplyr df gt group VisitID gt filter Item c A B How I perform Python Something like df groupby VisitID query A B,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
62377539,2020-06-14,2020,2,python: remove element from list of dict based on key-value condition,"<p>Im trying to remove a element from a dict only if the condition is met. </p>

<p>For example,</p>

<pre><code>dd = {'11': [{'xx': 259, 'yy': 1, 'channels': 55}, {'xx': 260, 'yy': 2, 'channels': 35}], '22': [{'xx': 259, 'yy': 1, 'channels': 40}, {'xx': 303, 'yy': 2, 'channels': 30}]}
</code></pre>

<p>Scenario:</p>

<pre><code>channels = 60
</code></pre>

<p>there are two elements with key <code>xx = 259</code> in the nested dict above, I wish to keep only one element which is closer to the <code>channels = 60</code> value.</p>

<p>expected output:</p>

<pre><code>dd = {'11': [{'xx': 259, 'yy': 1, 'channels': 55}, {'xx': 260, 'yy': 2, 'channels': 35}], '22': [{'xx': 303, 'yy': 2, 'channels': 30}]}
</code></pre>

<p>I tried so far:</p>

<pre><code>channels = 60
for key, values in dd.items():
    key = str(key)
    if key in dd:
        for elem in dd[key]:
            if elem['xx'] == 259:
                print(elem)
                # some logic here to check the closest value to channels and remove the furthest one
</code></pre>

<p>Which outputs:</p>

<pre><code>{'xx': 259, 'yy': 1, 'channels': 55}
{'xx': 259, 'yy': 1, 'channels': 40}
</code></pre>

<p>UPDATED APPROACH:</p>

<pre><code>    channels = 60
    xList = []
    for key, values in dd.items():
        key = str(key)
        if key in dd:
            for elem in dd[key]:
                if elem['xx'] == 259:
                    xList.append(elem['channels'])


    result =min(xList, key=lambda x: abs(x - 60))

    # find and remove elem from dict
    for key, values in dd.items():
        key = str(key)
        if key in dd:
            for elem in dd[key]:
                if elem['xx'] == 259:
                    if elem['channels'] == result:
                        pass
                    else:
                        print(""delete this elem: "", elem)
                        dd[key].remove(elem)
    print(dd)
</code></pre>

<p>OUTPUT:</p>

<pre><code>{'11': [{'xx': 259, 'yy': 1, 'channels': 55}, {'xx': 260, 'yy': 2, 'channels': 35}], '22': [{'xx': 303, 'yy': 2, 'channels': 30}]}
</code></pre>

<p>I managed to achieve the end goal, but I feel like this can certainly be improved. Any help?</p>
","['python', 'list', 'dictionary']",62379151,"<p>I think this does what you're looking for, without having to search for
matching items in the second loop (which remove() has to do), by keeping
the keys and indexes of all candidate elements, and then deleting all
but the one with the lowest delta value.</p>

<pre><code>channels = 60
xList = []
for key, values in dd.items():
    for index,elem in enumerate(values):
        if elem['xx'] == 259:
            xList.append((abs(elem['channels']-channels),key,index)) # capture delta, key, and index

# this drops the lowest difference entry and sorts the remaining (deletions) in reverse index order
xList=sorted([(key,index) for (delta,key,index) in sorted(xList)[1:]],reverse=True)

# the deletions must be done from highest index to lowest index
for key,index in xList:  # pull indexes from highest to lowest
    del dd[key][index]

print(dd)
</code></pre>

<p>(Edit:  earlier version of code above had a misplaced parenthesis)</p>

<p>This version removes all but the closest <em>within each key</em> instead of overall:</p>

<pre><code>channels = 60
for key, values in dd.items():
    xList = []
    for index,elem in enumerate(values):
        if elem['xx'] == 259:
            xList.append((abs(elem['channels']-channels),index)) # capture delta and index
    # this drops the lowest difference entry and sorts the remaining (deletions) in reverse index order
    xList=sorted([index for (delta,index) in sorted(xList)[1:]],reverse=True)
    for index in xList:  # pull indexes from highest to lowest
        del dd[key][index]
</code></pre>

<p>(I put processing within the loop and removed keys from the selection list)</p>
",python remove element list dict based key value condition Im trying remove element dict condition met For example dd xx yy channels xx yy channels xx yy channels xx yy channels Scenario channels two elements key xx nested dict I wish keep one element closer channels value expected output dd xx yy channels xx yy channels xx yy channels I tried far channels key values dd items key str key key dd elem dd key elem xx print elem logic check closest value channels remove furthest one Which outputs xx yy channels xx yy channels UPDATED APPROACH channels xList key values dd items key str key key dd elem dd key elem xx xList append elem channels result min xList key lambda x abs x find remove elem dict key values dd items key str key key dd elem dd key elem xx elem channels result pass else print delete,"startoftags, python, list, dictionary, endoftags",python python3x list endoftags,python list dictionary,python python3x list,0.67
62418465,2020-06-16,2020,5,Why does django&#39;s `apps.get_model()` return a `__fake__.MyModel` object,"<p>I am writing a custom Django migration script. As per the <a href=""https://docs.djangoproject.com/en/3.0/howto/writing-migrations/"" rel=""noreferrer"">django docs on custom migrations</a>, I should be able to use my model vis-a-vis <code>apps.get_model()</code>. However, when trying to do this I get the following error:</p>

<pre><code>AttributeError: type object 'MyModel' has no attribute 'objects'
</code></pre>

<p>I think this has to do with the apps registry not being ready, but I am not sure.</p>

<p>Sample code:</p>

<pre><code>def do_thing(apps, schema_editor):
    my_model = apps.get_model('app', 'MyModel')

    objects_ = my_model.objects.filter(
        some_field__isnull=True).prefetch_related(
        'some_field__some_other_field')  # exc raised here


class Migration(migrations.Migration):

    atomic = False

    dependencies = [
        ('app', '00xx_auto_xxx')
    ]

    operations = [
        migrations.RunPython(do_thing),
    ]
</code></pre>

<p>A simple print statement of <code>apps.get_model()</code>'s return value shows the following:
<code>&lt;class '__fake__.MyModel'&gt;</code>. I'm not sure what this is, and if it is a result of not being ready.</p>

<p>EDIT:</p>

<p>I couldn't find any resources to explain why I am getting a <code>__fake__</code> object so I decided to tinker with the code. I got it to work by preempting <code>apps</code> from args, as can be seen here:</p>

<pre><code>def do_thing(apps, schema_editor):
    from django.apps import apps

    my_model = apps.get_model('app', 'MyModel')

    objects_ = my_model.objects.filter(
        some_field__isnull=True).prefetch_related(
        'some_field__some_other_field')  # no more exc raised here
</code></pre>

<p>I am still confused and any help would be appreciated.</p>
","['python', 'django', 'django-models']",64193587,"<p>The fake objects are <em>historical models</em>. Here's the explanation from <a href=""https://docs.djangoproject.com/en/3.1/topics/migrations/#historical-models"" rel=""nofollow noreferrer"">Django docs</a>:</p>
<blockquote>
<p>When you run migrations, Django is working from historical versions of your models stored in the migration files.</p>
<p>[...]</p>
<p>Because itâs impossible to serialize arbitrary Python code, these historical models will not have any custom methods that you have defined. They will, however, have the same fields, relationships, managers (limited to those with <code>use_in_migrations = True</code>) and Meta options (also versioned, so they may be different from your current ones).</p>
</blockquote>
<p>In case <code>objects</code> is a custom manager, you can set <code>use_in_migrations = True</code> to make it available in migrations.</p>
",Why django apps get model return fake MyModel object I writing custom Django migration script As per django docs custom migrations I able use model vis vis apps get model However trying I get following error AttributeError type object MyModel attribute objects I think apps registry ready I sure Sample code def thing apps schema editor model apps get model app MyModel objects model objects filter field isnull True prefetch related field field exc raised class Migration migrations Migration atomic False dependencies app xx auto xxx operations migrations RunPython thing A simple print statement apps get model return value shows following lt class fake MyModel gt I sure result ready EDIT I find resources explain I getting fake object I decided tinker code I got work preempting apps args seen def thing apps schema editor django apps import apps model apps get model app MyModel objects model objects filter field,"startoftags, python, django, djangomodels, endoftags",python django djangomodels endoftags,python django djangomodels,python django djangomodels,1.0
62514812,2020-06-22,2020,3,How to conditionally select previous row&#39;s value in python?,"<p>I want to select the previous row's value only if it meets a certain condition
E.g.</p>
<pre><code>df:
Value  Marker  
10  0  
12  0  
50  1  
42  1  
52  0  
23  1
</code></pre>
<p>I want to select the previous row's value where <code>marker == 0</code>if the current value <code>marker == 1</code>.</p>
<p>Result:</p>
<pre><code>df:
Value  Marker  Prev_Value  
10  0  nan
12  0  nan
50  1  12
42  1  12
52  0  nan
23  1  52
</code></pre>
<p>I tried:
<code>df[prev_value] = np.where(df[marker] == 1, df[Value].shift(), np.nan)</code>
but that does not take conditional previous value like i want.</p>
","['python', 'pandas', 'numpy']",62515003,"<p>You could try this:</p>
<pre><code>df['Prev_Value']=np.where(dataframe['Marker'].diff()==1,dataframe['Value'].shift(1, axis = 0),np.nan)
</code></pre>
<p>Output:</p>
<pre><code>df

   Value  Marker  Prev_Value
0     10       0   NaN
1     12       0   NaN
2     50       1  12.0
3     42       1   NaN
4     52       0   NaN
5     23       1  52.0
</code></pre>
<p>If you want to get the previous non-1 marker value, if <code>marker==1</code>, you could try this:</p>
<pre><code>prevro=[]
for i in reversed(df.index):

    if df.iloc[i,1]==1:
        prevro_zero=df.iloc[0:i,0][df.iloc[0:i,1].eq(0)].tolist()
        if len(prevro_zero)&gt;0:
            prevro.append(prevro_zero[len(prevro_zero)-1])
        else:
            prevro.append(np.nan)
    else:
        prevro.append(np.nan)
        
df['Prev_Value']=list(reversed(prevro))

print(df)
</code></pre>
<p>Output:</p>
<pre><code>   Value  Marker  Prev_Value
0     10       0         NaN
1     12       0         NaN
2     50       1        12.0
3     42       1        12.0
4     52       0         NaN
5     23       1        52.0
</code></pre>
",How conditionally select previous row value python I want select previous row value meets certain condition E g df Value Marker I want select previous row value marker current value marker Result df Value Marker Prev Value nan nan nan I tried df prev value np df marker df Value shift np nan take conditional previous value like want,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
62544528,2020-06-23,2020,5,"Tensorflow DecodeJPEG: Expected image (JPEG, PNG, or GIF), got unknown format starting with &#39;\000\000\000\000\000\000\000\00&#39;","<p>I'm cycling through an image folder and this keeps happening.</p>
<blockquote>
<p>tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected image (JPEG, PNG, or GIF), got unknown format starting with '\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000'
[[{{node DecodeJpeg}}]]</p>
</blockquote>
<p>There are files in this folder that aren't images, but they should be filtered by my previous step. Anyone has an idea of what's going on?</p>
<pre><code>test_files_ds = tf.data.Dataset.list_files(myFolder + '/*.jpg') 

AUTOTUNE = tf.data.experimental.AUTOTUNE


def process_unlabeled_img(file_path):
    img = tf.io.read_file(file_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.convert_image_dtype(img, tf.float32)
    img = tf.image.resize(images=img, size=(224, 224))
    return file_path, img
</code></pre>
","['python', 'tensorflow', 'keras']",62544801,"<p>It's hard to know exactly what is going on without having the file at hand, but  what is probably happening here is that you have files in your dataset that have either a <code>.jpg</code>, <code>.jpeg</code>, <code>.png</code> or <code>.gif</code> extension but that are not actually JPEG, PNG of GIF images. Thus, TensorFlow isn't able to properly load them.</p>
<p>One way to overcome this problem would be to check your files that are supposedly images and get rid of the ones that aren't actual JPEG, PNG or GIF images.</p>
<p>Checking if a file is a <em>valid</em> JPEG, PNG or GIF image is definitely more complicated than it seems, but checking for the file signature / magic number (that is, the first few bytes of your file) is a good start and should most of the time solve your problems.</p>
<p>So, practically, you could do so in many different ways, one of which being checking for each picture individually if it is <em>valid</em> or not, with some function of this sort:</p>
<pre><code>def is_image(filename, verbose=False):

    data = open(filename,'rb').read(10)

    # check if file is JPG or JPEG
    if data[:3] == b'\xff\xd8\xff':
        if verbose == True:
             print(filename+&quot; is: JPG/JPEG.&quot;)
        return True

    # check if file is PNG
    if data[:8] == b'\x89\x50\x4e\x47\x0d\x0a\x1a\x0a':
        if verbose == True:
             print(filename+&quot; is: PNG.&quot;)
        return True

    # check if file is GIF
    if data[:6] in [b'\x47\x49\x46\x38\x37\x61', b'\x47\x49\x46\x38\x39\x61']:
        if verbose == True:
             print(filename+&quot; is: GIF.&quot;)
        return True

    return False
</code></pre>
<p>You would then be able to <strong>get rid</strong> of your <em>non valid</em> images by doing something like this (this would <strong>delete</strong> your <em>non valid</em> images):</p>
<pre><code>import os

# go through all files in desired folder
for filename in os.listdir(folder):
     # check if file is actually an image file
     if is_image(filename, verbose=False) == False:
          # if the file is not valid, remove it
          os.remove(os. path. join(folder, filename))
</code></pre>
<p>Now, as I said, this would probably solve your problem but please note that the function <code>is_image</code> will <strong>not</strong> be able to tell for sure if a file can or cannot be read as a JPG, JPEG, PNG or GIF image. It is only a quick and dirty solution that will get the vast majority of errors alike away, but not all.</p>
",Tensorflow DecodeJPEG Expected image JPEG PNG GIF got unknown format starting I cycling image folder keeps happening tensorflow python framework errors impl Expected image JPEG PNG GIF got unknown format starting node DecodeJpeg There files folder images filtered previous step Anyone idea going test files ds tf data Dataset list files myFolder jpg AUTOTUNE tf data experimental AUTOTUNE def process unlabeled img file path img tf io read file file path img tf image decode jpeg img channels img tf image convert image dtype img tf float img tf image resize images img size return file path img,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
62593392,2020-06-26,2020,2,New data frame with the % of the debt paid in the month of the payment,"<p>I have two dataframes <code>df1</code> and <code>df2</code>.
One with clients <code>debt</code>, the other with client <code>payments</code> with <code>dates</code>.</p>
<p>I want to create a new data frame with the % of the debt paid in the month of the payment until <code>01-2017</code>.</p>
<pre><code>import pandas as pd

d1 = {'client number': ['2', '2','3','6','7','7','8','8','8','8','8','8','8','8'],
     'month': [1, 2, 3,1,10,12,3,5,8,1,2,4,5,8],
    'year':[2013,2013,2013,2019,2013,2013,2013,2013,2013,2014,2014,2015,2016,2017],
    'payment' :[100,100,200,10000,200,100,300,500,200,100,200,200,500,50]}

df1 = pd.DataFrame(data=d1).set_index('client number')
df1
d2 = {'client number': ['2','3','6','7','8'],
     'debt': [200, 600,10000,300,3000]}

df2 = pd.DataFrame(data=d2)


x=[1,2,3,4,5,6,7,8,9,10]
y=[2013,2014,2015,2016,2017]

for x in month and y in year
if df1['month']=x and df1['year']=year :
    df2[month&amp;year] = df1['payment']/df2['debt']
</code></pre>
<p>the result needs to be something like this for all the clients</p>
<p><a href=""https://i.stack.imgur.com/OCFLA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OCFLA.png"" alt="""" /></a></p>
<p>what am I missing?</p>
<p>thank you for your time and help</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",62594559,"<p>First set the <code>index</code> of both the dataframes <code>df1</code> and <code>df2</code> to <code>client number</code>, then use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.map.html"" rel=""nofollow noreferrer""><code>Index.map</code></a> to map the client numbers in <code>df1</code> to their corresponding <code>debt's</code> from <code>df2</code>, then use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.div.html"" rel=""nofollow noreferrer""><code>Series.div</code></a> to divide the <code>payments</code> of each client by their respective <code>debt's</code>, thus obtaining the fraction of debt which is paid, then create a new column <code>date</code> in <code>df1</code> from <code>month</code> and <code>year</code> columns finally use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html"" rel=""nofollow noreferrer""><code>DataFrame.join</code></a> along with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html"" rel=""nofollow noreferrer""><code>DataFrame.pivot_table</code></a>:</p>
<pre><code>df1 = df1.set_index('client number')
df2 = df2.set_index('client number')

df1['pct'] = df1['payment'].div(df1.index.map(df2['debt'])).round(2)
df1['date'] = df1['year'].astype(str) + '-' + df1['month'].astype(str).str.zfill(2)

df3 = (
    df2.join(
        df1.pivot_table(index=df1.index, columns='date', values='pct', aggfunc='sum').fillna(0))
    .reset_index()
)
</code></pre>
<p>Result:</p>
<pre><code># print(df3)

  client number   debt  2013-01  2013-02  2013-03  2013-05  2013-08  ...  2013-12  2014-01  2014-02  2015-04  2016-05  2017-08  2019-01
0             2    200      0.5      0.5     0.00     0.00     0.00  ...     0.00     0.00     0.00     0.00     0.00     0.00      0.0
1             3    600      0.0      0.0     0.33     0.00     0.00  ...     0.00     0.00     0.00     0.00     0.00     0.00      0.0
2             6  10000      0.0      0.0     0.00     0.00     0.00  ...     0.00     0.00     0.00     0.00     0.00     0.00      1.0
3             7    300      0.0      0.0     0.00     0.00     0.00  ...     0.33     0.00     0.00     0.00     0.00     0.00      0.0
4             8   3000      0.0      0.0     0.10     0.17     0.07  ...     0.00     0.03     0.07     0.07     0.17     0.02      0.0
</code></pre>
",New data frame debt paid month payment I two dataframes df df One clients debt client payments dates I want create new data frame debt paid month payment import pandas pd client number month year payment df pd DataFrame data set index client number df client number debt df pd DataFrame data x x month year df month x df year year df month amp year df payment df debt result needs something like clients I missing thank time help,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas numpy endoftags,python python3x pandas dataframe,python pandas numpy,0.58
62709681,2020-07-03,2020,2,Sum of every two columns and leave one column in pandas dataframe,"<p>My task is like this:</p>
<pre><code>df=pd.DataFrame([(1,2,3,4,5,6),(1,2,3,4,5,6),(1,2,3,4,5,6)],columns=['a','b','c','d','e','f'])
Out:
    a b c d e f
0   1 2 3 4 5 6
1   1 2 3 4 5 6 
2   1 2 3 4 5 6
</code></pre>
<p>I want to do is the output dataframe looks like this:</p>
<pre><code>Out
        s1 b s2  d  s3  f
    0   3  2  7  4  11  6
    1   3  2  7  4  11  6
    2   3  2  7  4  11  6
</code></pre>
<p>That is to say, sum the column (a,b),(c,d),(e,f) separately and keep each last column and rename the result columns names as (s1,s2,s3). Could anyone help solve this problem in Pandas? Thank you so much.</p>
","['python', 'pandas', 'dataframe']",62709889,"<p>You can seelct columns by posistions by <code>iloc</code>, sum each <code>2</code> values and last rename columns by <code>f-string</code>s</p>
<pre><code>i = 2
for x in range(0, len(df.columns), i):
    df.iloc[:, x] = df.iloc[:, x:x+i].sum(axis=1)
    df = df.rename(columns={df.columns[x]:f's{x // i + 1}'})
print (df)
   s1  b  s2  d  s3  f
0   3  2   7  4  11  6
1   3  2   7  4  11  6
2   3  2   7  4  11  6
</code></pre>
",Sum every two columns leave one column pandas dataframe My task like df pd DataFrame columns b c e f Out b c e f I want output dataframe looks like Out b f That say sum column b c e f separately keep last column rename result columns names Could anyone help solve problem Pandas Thank much,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
62869058,2020-07-13,2020,3,Can split pandas dataframe based on row values?,"<p>I have a pandas dataframe that effectively contains several different datasets. Between each dataset is a row full of NaN. Can I split the dataframe on the NaN row to make two dataframes? Thanks in advance.</p>
","['python', 'pandas', 'numpy', 'dataframe']",62869210,"<p>You can use this to split into many data frames based on all NaN rows:</p>
<pre><code>#index of all NaN rows (+ beginning and end of df)
idx = [0] + df.index[df.isnull().all(1)].tolist() + [df.shape[0]]
#list of data frames split at all NaN indices
list_of_dfs = [df.iloc[idx[n]:idx[n+1]] for n in range(len(idx)-1)]
</code></pre>
<p>And if you want to exclude the NaN rows from split data frames:</p>
<pre><code>idx = [-1] + df.index[df.isnull().all(1)].tolist() + [df.shape[0]]
list_of_dfs = [df.iloc[idx[n]+1:idx[n+1]] for n in range(len(idx)-1)]
</code></pre>
<p>Example:</p>
<p>df:</p>
<pre><code>     0    1
0  1.0  1.0
1  NaN  1.0
2  1.0  NaN
3  NaN  NaN
4  NaN  NaN
5  1.0  1.0
6  1.0  1.0
7  NaN  1.0
8  1.0  NaN
9  1.0  NaN
</code></pre>
<p>list_of_dfs:</p>
<pre><code>[     0    1
0  1.0  1.0
1  NaN  1.0
2  1.0  NaN, 

Empty DataFrame
Columns: [0, 1]
Index: [],   

     0    1
5  1.0  1.0
6  1.0  1.0
7  NaN  1.0
8  1.0  NaN
9  1.0  NaN]
</code></pre>
",Can split pandas dataframe based row values I pandas dataframe effectively contains several different datasets Between dataset row full NaN Can I split dataframe NaN row make two dataframes Thanks advance,"startoftags, python, pandas, numpy, dataframe, endoftags",python pandas dataframe endoftags,python pandas numpy dataframe,python pandas dataframe,0.87
62902731,2020-07-14,2020,3,Sort the columns by unique values,"<p>I have this data-frame:</p>
<pre><code>  AAA  X_980  X_100  X_990  X_1100  X_2200  X_Y_100  X_Y_2200  X_Y_990  X_Y_1100  X_Y_980  X_10_100  X_10_980  X_10_990  X_10_1100  X_10_2200  X_A  X_A_B
  100      6      6      6       3       4        1         7        5         1        9         9         2         7          3          7    3      8
  980      2      9      5       5       9        3         6        2         1        3         1         8         2          9          4    8      4
  990      8      8      7       7       9        3         5        7         3        1         5         5         6          6          1    3      4
 1100      6      5      4       7       4        6         2        1         6        2         3         5         3          9          7    5      2
 2200      7      4      3       2       4        5         9        1         9        4         6         5         8          7          7    7      9
</code></pre>
<p>As you can see, there are 5 unique values in column <code>AAA</code>, and 3 groups of columns: <code>X_</code>, <code>X_Y_</code>, and <code>X_10_</code>, followed by a suffix with each of the unique values. I want to change the order of the columns so each group of columns is sorted by the unique values (ascending).</p>
<p>Expected result:</p>
<pre><code>  AAA  X_100  X_980  X_990  X_1100  X_2200  X_Y_100  X_Y_980  X_Y_990  X_Y_1100  X_Y_2200  X_10_100  X_10_980  X_10_990  X_10_1100  X_10_2200  X_A  X_A_B
  100      6      6      6       3       4        1        9        5         1         7         9         2         7          3          7    3      8
  980      9      2      5       5       9        3        3        2         1         6         1         8         2          9          4    8      4
  990      8      8      7       7       9        3        1        7         3         5         5         5         6          6          1    3      4
 1100      5      6      4       7       4        6        2        1         6         2         3         5         3          9          7    5      2
 2200      4      7      3       2       4        5        4        1         9         9         6         5         8          7          7    7      9
</code></pre>
","['python', 'pandas', 'numpy']",62902847,"<p><strong>Approach #1</strong></p>
<p>With simple columns manipulation -</p>
<pre><code>c = df.columns.values.copy()
c1 = df1.columns
c[np.isin(c,c1)] = c1
df_out = df.loc[:,c]
</code></pre>
<p>Sample output -</p>
<pre><code>In [174]: df_out
Out[174]: 
    AAA  X_100  X_980  X_990  X_1100  X_2200  X_Y_100  X_Y_980  X_Y_990  X_Y_1100  X_Y_2200  X_10_100  X_10_980  X_10_990  X_10_1100  X_10_2200  X_A  X_A_B
0   100      6      6      6       3       4        1        9        5         1         7         9         2         7          3          7    3      8
1   980      9      2      5       5       9        3        3        2         1         6         1         8         2          9          4    8      4
2   990      8      8      7       7       9        3        1        7         3         5         5         5         6          6          1    3      4
3  1100      5      6      4       7       4        6        2        1         6         2         3         5         3          9          7    5      2
4  2200      4      7      3       2       4        5        4        1         9         9         6         5         8          7          7    7      9
</code></pre>
<p><strong>Approach #2</strong> : Pushes the new data up-front</p>
<pre><code>In [117]: df1 = df[[i+str(j) for i in ['X_', 'X_Y_', 'X_10_'] for j in df.AAA]]

In [118]: c,c1 = df.columns,df1.columns

In [119]: pd.concat(( df1, df[c[~np.isin(c,c1)]]),axis=1)
Out[119]: 
   X_100  X_980  X_990  X_1100  X_2200  X_Y_100  X_Y_980  X_Y_990  X_Y_1100  X_Y_2200  X_10_100  X_10_980  X_10_990  X_10_1100  X_10_2200   AAA  X_A  X_A_B
0      6      6      6       3       4        1        9        5         1         7         9         2         7          3          7   100    3      8
1      9      2      5       5       9        3        3        2         1         6         1         8         2          9          4   980    8      4
2      8      8      7       7       9        3        1        7         3         5         5         5         6          6          1   990    3      4
3      5      6      4       7       4        6        2        1         6         2         3         5         3          9          7  1100    5      2
4      4      7      3       2       4        5        4        1         9         9         6         5         8          7          7  2200    7      9
</code></pre>
",Sort columns unique values I data frame AAA X X X X X X Y X Y X Y X Y X Y X X X X X X A X A B As see unique values column AAA groups columns X X Y X followed suffix unique values I want change order columns group columns sorted unique values ascending Expected result AAA X X X X X X Y X Y X Y X Y X Y X X X X X X A X A B,"startoftags, python, pandas, numpy, endoftags",python python3x list endoftags,python pandas numpy,python python3x list,0.33
62977087,2020-07-19,2020,3,Using values in a pandas dataframe as column names for another,"<p>This is my first dataframe.</p>
<pre><code>Date        0   1   2   3   
2003-01-31  CA  KY  ID  CO
2003-02-28  CA  KY  HI  CO 
2003-03-31  CA  KY  CO  HI 
</code></pre>
<p>This is my second dataframe.</p>
<pre><code>Date        CA  KY  ID  CO  HI                                            
2003-01-31   5   3   4   5   1 
2003-02-28   2   7   8   4   5  
2003-03-31   6   3   9   3   5 
</code></pre>
<p>How do I get this dataframe to print as output?</p>
<pre><code>Date         0   1   2   3                                                
2003-01-31   5   3   4   5
2003-02-28   2   7   5   4
2003-03-31   6   3   3   5
</code></pre>
<p>I am wondering if there is a way to use the whole dataframe as an index to another instead of having to loop through all the dates/columns.</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",62977502,"<p>You can use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.lookup.html"" rel=""nofollow noreferrer""><code>df.lookup</code></a> with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer""><code>df.apply</code></a> here.</p>
<pre><code># <b>If `Date` is not index.</b>
# df1.set_index('Date')
#              0   1   2   3
# Date
# 2003-01-31  CA  KY  ID  CO
# 2003-02-28  CA  KY  HI  CO
# 2003-03-31  CA  KY  CO  HI

# df2.set_index('Date')
#             CA  KY  ID  CO  HI
# Date
# 2003-01-31   5   3   4   5   1
# 2003-02-28   2   7   8   4   5
# 2003-03-31   6   3   9   3   5

def f(x):
    return <b>df2.lookup(x.index, x)</b>

df1.apply(f)
# df1.apply(lambda x: <b>df2.lookup(x.index, x)</b>

            0  1  2  3
Date
2003-01-31  5  3  4  5
2003-02-28  2  7  5  4
2003-03-31  6  3  3  5</code></pre>
",Using values pandas dataframe column names another This first dataframe Date CA KY ID CO CA KY HI CO CA KY CO HI This second dataframe Date CA KY ID CO HI How I get dataframe print output Date I wondering way use whole dataframe index another instead loop dates columns,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
63011748,2020-07-21,2020,6,Contour (iso-z) or threshold lines in seaborn heatmap,"<p>Is there a way to automatically add contour (iso-z) lines to a heatmap with concrete x and y values?</p>
<p>Please consider the official seaborn flights dataset:</p>
<pre><code>import seaborn as sns
flights = sns.load_dataset(&quot;flights&quot;)
flights = flights.pivot(&quot;month&quot;, &quot;year&quot;, &quot;passengers&quot;)
sns.heatmap(flights, annot=True, fmt='d')
</code></pre>
<p>I imagine the step-like lines to look something like shown below (lhs), indicating thresholds (here 200 and 400). They do not need to be interpolated or smoothed in any way, although that would do as well, if easier to realize.</p>
<p>If the horizontal lines complicate the solution further, they too could be omitted (rhs).</p>
<p><a href=""https://i.stack.imgur.com/D65Sj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/D65Sj.png"" alt=""Seaborn flights example with step-like iso-z lines"" /></a></p>
<p>So far, I have tried to add hlines and vlines manually, to overlay a kdeplot etc. without the desired result. Could somebody hint me into the right direction?</p>
","['python', 'matplotlib', 'seaborn']",63013691,"<p>You can use a<a href=""https://matplotlib.org/3.1.1/api/collections_api.html#matplotlib.collections.LineCollection"" rel=""nofollow noreferrer""><code>LineCollection</code></a>:</p>
<pre><code>import seaborn as sns
import numpy as np
from matplotlib.collections import LineCollection

flights = sns.load_dataset(&quot;flights&quot;)
flights = flights.pivot(&quot;month&quot;, &quot;year&quot;, &quot;passengers&quot;)
ax = sns.heatmap(flights, annot=True, fmt='d')

def add_iso_line(ax, value, color):
    v = flights.gt(value).diff(axis=1).fillna(False).to_numpy()
    h = flights.gt(value).diff(axis=0).fillna(False).to_numpy()
    
    try:
        l = np.argwhere(v.T)    
        vlines = np.array(list(zip(l, np.stack((l[:,0], l[:,1]+1)).T)))
        
        l = np.argwhere(h.T)    
        hlines = np.array(list(zip(l, np.stack((l[:,0]+1, l[:,1])).T)))
        
        lines = np.vstack((vlines, hlines))
        ax.add_collection(LineCollection(lines, lw=3, colors=color ))
    except:
        pass
    
add_iso_line(ax, 200, 'b')
add_iso_line(ax, 400, 'y')
</code></pre>
<p><a href=""https://i.stack.imgur.com/9ddqI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9ddqI.png"" alt=""enter image description here"" /></a></p>
",Contour iso z threshold lines seaborn heatmap Is way automatically add contour iso z lines heatmap concrete x values Please consider official seaborn flights dataset import seaborn sns flights sns load dataset quot flights quot flights flights pivot quot month quot quot year quot quot passengers quot sns heatmap flights annot True fmt I imagine step like lines look something like shown lhs indicating thresholds They need interpolated smoothed way although would well easier realize If horizontal lines complicate solution could omitted rhs So far I tried add hlines vlines manually overlay kdeplot etc without desired result Could somebody hint right direction,"startoftags, python, matplotlib, seaborn, endoftags",python matplotlib seaborn endoftags,python matplotlib seaborn,python matplotlib seaborn,1.0
63118141,2020-07-27,2020,2,Keras Image Classification for Different Input Sizes,"<p>I want to use Tensorflow and Keras to train a dataset composed of images with very different sizes in order to classify them. But some of them are horizontal (1400x100) and some of them are vertical (100x1000).
As far as I understand, Keras accepts same size input images.
I'm not sure if it's wise to convert all of them to a classical resolution like 150x150 or 180x180 since they are horizontal and vertical.</p>
<p>How can I solve this problem?</p>
","['python', 'tensorflow', 'keras']",63118955,"<p>A few methods have been developed to process images with multiple sizes, including images with unequal horizontal and vertical dimensions. For example, <a href=""https://arxiv.org/abs/1406.4729"" rel=""nofollow noreferrer"">spatial pyramid pooling</a> or <a href=""https://arxiv.org/abs/1803.09218"" rel=""nofollow noreferrer"">scale recurrent neural networks</a>. You could also set network <a href=""https://github.com/keras-team/keras/issues/1920"" rel=""nofollow noreferrer"">dimensions to be variable</a>, then use a pooling operation (e.g. global average pooling) to get fixed size dimensions before fully connected or other layers than need a fixed size.</p>
<p>The simplest approach is cropping or padding images (e.g. with zeros) so that they are all the same size.</p>
",Keras Image Classification Different Input Sizes I want use Tensorflow Keras train dataset composed images different sizes order classify But horizontal x vertical x As far I understand Keras accepts size input images I sure wise convert classical resolution like x x since horizontal vertical How I solve problem,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
63344365,2020-08-10,2020,3,"Pandas dataframe grouping by two columns, count and sum","<p>I have the following df, which I would like to group by 'Name' so there is an 'A' and 'B' count column and a 'total sales' sum column:</p>
<p>eg turn this:</p>
<pre><code>data = {'A or B' : ['A','A','B','B','A','B'],
        'Name' : ['Ben','Ben','Ben','Sam','Sam','Sam'],
        'Sales ($)' : [10,5,2,5,6,7]
       }

df=pd.DataFrame(data, columns = ['A or B','Name','Sales ($)'])
</code></pre>
<p>so it looks like this:</p>
<pre><code>grouped_data = {'A' : [2,1],
        'B' : [1,2],
        'Name' : ['Ben','Sam'],
        'Total Sales ($)' : [17,18]
       }

df=pd.DataFrame(grouped_data, columns = ['A','B','Name','Total Sales ($)'])
</code></pre>
","['python', 'pandas', 'dataframe']",63344763,"<p>You can try with <code>pd.get_dummies</code>, <code>join</code> and <code>groupby</code>+<code>sum</code>:</p>
<pre><code>pd.get_dummies(df['A or B'])\
  .join(df.drop('A or B',1))\
  .groupby('Name',as_index=False).sum()
</code></pre>
<p>Output:</p>
<pre><code>  Name  A  B  Sales ($)
0  Ben  2  1         17
1  Sam  1  2         18
</code></pre>
<hr />
<p><strong>Details:</strong></p>
<p>First, use <code>get_dummies</code> to get categorical variable into dummy/indicator variables:</p>
<pre><code>pd.get_dummies(df['A or B'])
#   A  B
#0  1  0
#1  1  0
#2  0  1
#3  0  1
#4  1  0
#5  0  1
</code></pre>
<p>Then use join, to concat the dummies with original df with <code>'A or B'</code> column dropped:</p>
<pre><code>pd.get_dummies(df['A or B']).join(df.drop('A or B',1))
#   A  B Name  Sales ($)
#0  1  0  Ben         10
#1  1  0  Ben          5
#2  0  1  Ben          2
#3  0  1  Sam          5
#4  1  0  Sam          6
#5  0  1  Sam          7
</code></pre>
<p>And finally, do the <code>groupby</code>+<code>sum</code> based on name:</p>
<pre><code>pd.get_dummies(df['A or B']).join(df.drop('A or B',1)).groupby('Name',as_index=False).sum()
#  Name  A  B  Sales ($)
#0  Ben  2  1         17
#1  Sam  1  2         18
</code></pre>
",Pandas dataframe grouping two columns count sum I following df I would like group Name A B count column total sales sum column eg turn data A B A A B B A B Name Ben Ben Ben Sam Sam Sam Sales df pd DataFrame data columns A B Name Sales looks like grouped data A B Name Ben Sam Total Sales df pd DataFrame grouped data columns A B Name Total Sales,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
63352450,2020-08-11,2020,2,Convert dataframe Pandas using pre-defined columns,"<p>I have a Pandas DataFrame with multiple rows and columns as follows:</p>
<pre><code>      a    b   c   d
0     2    1   3   1
1     2    2   1   1      
2     2    3   6   1    
3     2    4   4   1    
</code></pre>
<p>I want to convert the above DataFrame to matrix based on <code>column b</code> as <code>rows</code> and <code>column c</code> as <code>column</code>. Using <code>crosstab</code>, I will get:</p>
<pre><code>c   1   3   4   6
b
1   0   1   0   0
2   1   0   0   0
3   0   0   0   1
4   0   0   1   0
</code></pre>
<p>What I want is the column should show from <code>1-6</code> including <code>2</code> and <code>5</code> as below.</p>
<pre><code>c   1   2   3   4   5   6
b
1   0   0   1   0   0   0
2   1   0   0   0   0   0
3   0   0   0   0   0   1
4   0   0   0   1   0   0
</code></pre>
<p>How to obtain the above form using Pandas?</p>
","['python', 'pandas', 'numpy']",63352468,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>DataFrame.reindex</code></a> with minimal and maximal columns:</p>
<pre><code>df1 = df.reindex(np.arange(df.columns.min(), df.columns.max() + 1), axis=1, fill_value=0)
print (df1)
   1  2  3  4  5  6
c                  
1  0  0  1  0  0  0
2  1  0  0  0  0  0
3  0  0  0  0  0  1
4  0  0  0  1  0  0
</code></pre>
",Convert dataframe Pandas using pre defined columns I Pandas DataFrame multiple rows columns follows b c I want convert DataFrame matrix based column b rows column c column Using crosstab I get c b What I want column show including c b How obtain form using Pandas,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
63452575,2020-08-17,2020,2,Partially summing pandas columns,"<p>I am interested in doing partial sums for the following dataframe:</p>
<pre><code>    ID  Name    A   B
1   111 foo     248 123
2   222 bar     331 94
3   111 foo     266 102
4   111 foo     217 163
5   222 bar     194 102
6   222 bar     188 89
</code></pre>
<p>I could use <code>groupby</code> with either <code>sum</code> or <code>agg</code>, such as:</p>
<p><code>df = df.groupby([&quot;ID&quot;, &quot;Name&quot;]).agg(sum).reset_index()</code></p>
<p>which yields:</p>
<pre><code>    ID  Name    A   B
1   111 foo     731 388
2   222 bar     713 285
</code></pre>
<p>However, I would like to combine indices only until column A passes some pre-specified value, and then begin a second grouping.  When that one passes the pre-specified value, begin a third grouping, and so on.  For example, if the threshold is set at 500, the code would yield:</p>
<pre><code>    ID  Name    A   B
1   111 foo     514 225
2   222 bar     525 196
3   111 foo     217 163
4   222 bar     188 89
</code></pre>
<p>Rows 1 and 3 in the original df were grouped.  Rows 2 and 4 were grouped.  Row 5 does not group with rows 1 and 3 because the threshold of 500 has been passed.  Row 6 was similarly left ungrouped.</p>
<p>Order of rows does not matter.  Which rows are combined with which other rows does not matter.  I just need the ability to group column values with a threshold.  I'm stumped, especially when trying to figure out a Pythonic solution as opposed to iterating through the dataframe row by row and explicitly evaluating each row.  Any feedback would be much appreciated.</p>
","['python', 'pandas', 'dataframe']",63453400,"<p>You can do this with a custom function to pass to the apply function.
First use cumsum to identify the group ends, create an extra column with the new group id, then perform another groupby on the new intermediate dataframe.</p>
<p>I've made threshold a parameter in the funtion.</p>
<pre><code>def grouper(x,threshold=500):
    A = (x['A'].cumsum().values/threshold).astype(int)
    loc = (np.diff(A)!=0).nonzero()[0]+1
    A[loc] = A[loc]-1 
    x['C'] = A
    
    return x.groupby(['C'])['A','B'].sum().reset_index(drop=True)
    
    

df.groupby([&quot;ID&quot;, &quot;Name&quot;]).apply(grouper,threshold=500)
</code></pre>
",Partially summing pandas columns I interested partial sums following dataframe ID Name A B foo bar foo foo bar bar I could use groupby either sum agg df df groupby quot ID quot quot Name quot agg sum reset index yields ID Name A B foo bar However I would like combine indices column A passes pre specified value begin second grouping When one passes pre specified value begin third grouping For example threshold set code would yield ID Name A B foo bar foo bar Rows original df grouped Rows grouped Row group rows threshold passed Row similarly left ungrouped Order rows matter Which rows combined rows matter I need ability group column values threshold I stumped especially trying figure Pythonic solution opposed iterating dataframe row row explicitly evaluating row Any feedback would much appreciated,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
63462109,2020-08-18,2020,2,Need help understanding this range function,"<p>I'm a new programmer and I don't understand:</p>
<pre><code>np.array([range(i, i+3) for i in [2,4,6]])
</code></pre>
<p>I know that in the range function there is a start, stop and step, but it's written quite differently here.</p>
<p>The output is a 3*3 array</p>
<pre><code>234
456
678
</code></pre>
<p>I don't understand how this array came about. What does the I+3 mean? Are the 2,4,6 x values? What exactly if the for loop doing in this case? What is it iterating over?</p>
<p>Thanks for the help.</p>
","['python', 'python-3.x', 'numpy']",63462227,"<p>If I break down that line of code a little bit I can show you what happens.</p>
<p>The following line is a list comprehension, it's saying return <code>I</code> where <code>I</code> is set to each number in the list [2,4,6]. This returns a list that looks like <code>[2,4,6]</code>. So that's essentially useless because you're not manipulating your original list.</p>
<pre><code>[I for I in [2,4,6]]
</code></pre>
<p>when you put this inside <code>np.array()</code> it just creates an array from your list.</p>
<p>Now with the following code, your going through each number in the range 2-5 not including 5 (5 is I+3)</p>
<pre><code>for i in range(2,5):
     print(i)
2
3
4
</code></pre>
<p>You could also do</p>
<pre><code>for i in range(2,5):
     print(np.array([range(2,4)]))
 
[[2 3]]
[[2 3]]
[[2 3]]
</code></pre>
<p>Which is a loop that executes 3 times, and prints an array which contains a range from 2-4 in it.</p>
<p>When you combine everything in</p>
<pre><code>[range(I, I+3) for I in [2,4,6]]
</code></pre>
<p>You're saying for each number in the list <code>[2,4,6]</code> return a range from that number up to that number + 3 and put that in a list, which returns</p>
<pre><code>[range(2, 5), range(4, 7), range(6, 9)]
</code></pre>
<p>Then you convert that to an array to get</p>
<pre><code>array([[2, 3, 4],
       [4, 5, 6],
       [6, 7, 8]])
</code></pre>
",Need help understanding range function I new programmer I understand np array range I know range function start stop step written quite differently The output array I understand array came What I mean Are x values What exactly loop case What iterating Thanks help,"startoftags, python, python3x, numpy, endoftags",python python3x pandas endoftags,python python3x numpy,python python3x pandas,0.67
63754311,2020-09-05,2020,2,UnidentifiedImageError: cannot identify image file,"<p>Hello I am training a model with TensorFlow and Keras, and the dataset was downloaded from <a href=""https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765"" rel=""nofollow noreferrer"">https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765</a></p>
<p>This is a zip folder that I split in the following directories:</p>
<pre><code>.
âââ test
âÂ Â  âââ Cat
âÂ Â  âââ Dog
âââ train
    âââ Cat
    âââ Dog
</code></pre>
<p>Test.cat and test.dog have each folder 1000 jpg photos, and train.cat and traing.dog have each folder 11500 jpg photos.</p>
<p>The load is doing with this code:</p>
<pre><code>batch_size = 16

# Data augmentation and preprocess
train_datagen = ImageDataGenerator(rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.20) # set validation split

# Train dataset
train_generator = train_datagen.flow_from_directory(
    'PetImages/train',
    target_size=(244, 244),
    batch_size=batch_size,
    class_mode='binary',
    subset='training') # set as training data

# Validation dataset
validation_generator = train_datagen.flow_from_directory(
    'PetImages/train',
    target_size=(244, 244),
    batch_size=batch_size,
    class_mode='binary',
    subset='validation') # set as validation data

test_datagen = ImageDataGenerator(rescale=1./255)
# Test dataset
test_datagen = test_datagen.flow_from_directory(
    'PetImages/test')
</code></pre>
<p>THe model is training with the following code:</p>
<pre><code>history = model.fit(train_generator,
                    validation_data=validation_generator,
                    epochs=5)
</code></pre>
<p>And i get the following input:</p>
<pre><code>Epoch 1/5
1150/1150 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9906
</code></pre>
<p>But when the epoch is in this point I get the following error:</p>
<blockquote>
<p>UnidentifiedImageError: cannot identify image file &lt;_io.BytesIO object
at 0x7f9e185347d0&gt;</p>
</blockquote>
<p>How can I solve this, in order to finish the training?</p>
<p>Thanks</p>
","['python', 'tensorflow', 'keras']",63754719,"<p>Try this function to check if the image are all in correct format.</p>
<pre><code>import os
from PIL import Image
folder_path = 'data\img'
extensions = []
for fldr in os.listdir(folder_path):
    sub_folder_path = os.path.join(folder_path, fldr)
    for filee in os.listdir(sub_folder_path):
        file_path = os.path.join(sub_folder_path, filee)
        print('** Path: {}  **'.format(file_path), end=&quot;\r&quot;, flush=True)
        im = Image.open(file_path)
        rgb_im = im.convert('RGB')
        if filee.split('.')[1] not in extensions:
            extensions.append(filee.split('.')[1])
    
</code></pre>
",cannot identify image file Hello I training model TensorFlow Keras dataset downloaded https www microsoft com en us download confirmation aspx id This zip folder I split following directories test Cat Dog train Cat Dog Test cat test dog folder jpg photos train cat traing dog folder jpg photos The load code batch size Data augmentation preprocess train datagen ImageDataGenerator rescale shear range zoom range horizontal flip True validation split set validation split Train dataset train generator train datagen flow directory PetImages train target size batch size batch size class mode binary subset training set training data Validation dataset validation generator train datagen flow directory PetImages train target size batch size batch size class mode binary subset validation set validation data test datagen ImageDataGenerator rescale Test dataset test datagen test datagen flow directory PetImages test THe model training following code history model fit train generator validation data validation generator epochs,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
63859064,2020-09-12,2020,2,Take columns to row based on date column,"<p>I've a dataframe which look like:</p>
<p>df1:</p>
<pre><code>+-------------+-------------------+------------+
| date        | status            | counts     |
+-------------+-------------------+------------+
| 2020-03-02  |  death            |  0         |
| nan.        |  positive         |  5         |
| nan.        |  recovery         |  0         |
| nan.        |  positive cum     |  5         |
| nan.        |  recovery cum     |  0         |
| 2020-03-03  |  death            |  0         |
| nan.        |  positive         |  10        |
| nan.        |  recovery         |  0         |
| nan.        |  positive cum     |  15        |
| nan.        |  recovery cum     |  0         |
+-------------+-------------------+------------+
</code></pre>
<p>I want to pivot the dataframe tobe like this in order to visualize the table:</p>
<pre><code>+-------------+---------+------------+----------+---------------+---------------+
| date        | death   | positive   | recovery |  positive cum | recovery cum. |
+-------------+---------+------------+----------+---------------+---------------+
| 2020-03-02  |  0      |  5         | 0        | 5             | 0             |
| 2020-03-03  |  0      |  10        | 0        | 15            | 0             |
+-------------+---------+------------+----------+---------------+---------------+
</code></pre>
<p>I've tried:</p>
<pre class=""lang-py prettyprint-override""><code>pd.pivot_table(df, index=['date'], columns=['status'], values=['counts'], aggfunc='sum')
</code></pre>
<p>But the results only take the row that had non-nan date. Please advice</p>
","['python', 'pandas', 'dataframe']",63859181,"<p>First <code>ffill</code> the <code>NaN</code> values in the <code>date</code> column then use <code>pivot_table</code> with <code>aggfunc=first</code> to reshape the dataframe:</p>
<pre><code>pvt = df.assign(date=df['date'].ffill())\
        .pivot_table(index='date', columns='status', values='counts', aggfunc='first')
</code></pre>
<p>Or, if there are no repeating values in <code>status</code> corresponding to a particular <code>date</code> you can instead use <code>pivot</code>:</p>
<pre><code>pvt = df.assign(date=df['date'].ffill()).pivot('date', 'status', 'counts')
</code></pre>
<hr />
<pre><code>status      death  positive  positive cum  recovery  recovery cum
date                                                             
2020-03-02      0         5             5         0             0
2020-03-03      0        10            15         0             0
</code></pre>
",Take columns row based date column I dataframe look like df date status counts death nan positive nan recovery nan positive cum nan recovery cum death nan positive nan recovery nan positive cum nan recovery cum I want pivot dataframe tobe like order visualize table date death positive recovery positive cum recovery cum I tried pd pivot table df index date columns status values counts aggfunc sum But results take row non nan date Please advice,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
63936121,2020-09-17,2020,2,Grouping data in pandas by rows,"<p>I have data with this structure:</p>
<pre><code>id   month   val   
1    0        4
2    0        4
3    0        5
1    1        3
2    1        7
3    1        9
1    2        12
2    2        1
3    2        5
1    3        10
2    3        4
3    3        7
...
</code></pre>
<p>I want to get mean val for each id, grouped by two months. Expected result:</p>
<pre><code>id   two_months    val   
1       0          3.5
2       0          5.5
3       0          7
1       1          11
2       1          2.5
3       1          6
</code></pre>
<p>What's the simplest way to do it using Pandas?</p>
","['python', 'pandas', 'dataframe']",63936162,"<p>If months are consecutive integers starting by <code>0</code> use integer division by <code>2</code>:</p>
<pre><code>df = df.groupby(['id',df['month'] // 2])['val'].mean().sort_index(level=[1,0]).reset_index()
print (df)
   id  month   val
0   1      0   3.5
1   2      0   5.5
2   3      0   7.0
3   1      1  11.0
4   2      1   2.5
5   3      1   6.0
</code></pre>
<p>Possible solution with convert to datetimes:</p>
<pre><code>df.index = pd.to_datetime(df['month'].add(1), format='%m')
df = df.groupby(['id', pd.Grouper(freq='2MS')])['val'].mean().sort_index(level=[1,0]).reset_index()
print (df)
   id      month   val
0   1 1900-01-01   3.5
1   2 1900-01-01   5.5
2   3 1900-01-01   7.0
3   1 1900-03-01  11.0
4   2 1900-03-01   2.5
5   3 1900-03-01   6.0
</code></pre>
",Grouping data pandas rows I data structure id month val I want get mean val id grouped two months Expected result id two months val What simplest way using Pandas,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
64150122,2020-10-01,2020,3,Python: How to sort a list_of_lists based on another list_of_lists?,"<p>Below is the use-case I am trying to solve:</p>
<p>I have 2 lists of lists: (<code>l</code> and <code>d</code>)</p>
<pre><code>In [1197]: l
Out[1197]: 
[['Cancer A', 'Ecog 9', 'Fill 6'],
 ['Cancer B', 'Ecog 1', 'Fill 1'],
 ['Cancer A', 'Ecog 0', 'Fill 0']]

In [1198]: d
Out[1198]: [[100], [200], [500]]
</code></pre>
<p>It's a 2-part problem here:</p>
<ol>
<li>Sort <code>l</code> based on the priority of values. eg: <code>Cancer</code>, <code>Ecog</code> and <code>Fill</code> (in this case <code>key=(0,1,2)</code>). It could be anything like <code>Ecog</code>, <code>Cancer</code>, <code>Fill</code> so, key=(1,0,2).</li>
<li>Sort <code>d</code> in the same order in which <code>l</code> has been sorted int above step.</li>
</ol>
<p>Step #1 I'm able to achieve, like below:</p>
<pre><code>In [1199]: import operator
In [1200]: sorted_l = sorted(l, key=operator.itemgetter(0,1,2))

In [1201]: sorted_l
Out[1200]: 
[['Cancer A', 'Ecog 0', 'Fill 0'],
 ['Cancer A', 'Ecog 9', 'Fill 6'],
 ['Cancer B', 'Ecog 1', 'Fill 1']]
</code></pre>
<p>Now, I want to sort values of <code>d</code> in the same order as the <code>sorted_l</code>.</p>
<p>Expected output:</p>
<pre><code>In [1201]: d
Out[1201]: [[500], [100], [200]]
</code></pre>
<p>What is the best way to do this?</p>
","['python', 'python-3.x', 'list']",64154329,"<p>Below is the solution with help from @juanpa.arrivillaga :</p>
<pre><code>In [1272]: import operator
In [1273]: key = operator.itemgetter(0, 1, 2)

# Here param key, lets you sort `l` with your own function.
In [1275]: sorted_l,sorted_d = zip(*sorted(zip(l, d), key=lambda x: key(x[0])))

In [1276]: sorted_l
Out[1276]: 
(['Cancer A', 'Ecog 0', 'Fill 0'],
 ['Cancer A', 'Ecog 9', 'Fill 6'],
 ['Cancer B', 'Ecog 1', 'Fill 1'])

In [1277]: sorted_d
Out[1277]: ([500], [100], [200])
</code></pre>
",Python How sort list lists based another list lists Below use case I trying solve I lists lists l In l Out Cancer A Ecog Fill Cancer B Ecog Fill Cancer A Ecog Fill In Out It part problem Sort l based priority values eg Cancer Ecog Fill case key It could anything like Ecog Cancer Fill key Sort order l sorted int step Step I able achieve like In import operator In sorted l sorted l key operator itemgetter In sorted l Out Cancer A Ecog Fill Cancer A Ecog Fill Cancer B Ecog Fill Now I want sort values order sorted l Expected output In Out What best way,"startoftags, python, python3x, list, endoftags",python python3x list endoftags,python python3x list,python python3x list,1.0
64328033,2020-10-13,2020,4,How can I transform a DataFrame so that the headers become column values?,"<p>I have Pandas DataFrame in this form:</p>
<p><a href=""https://i.stack.imgur.com/Frudq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Frudq.png"" alt=""Data Format 1"" /></a></p>
<p>How can I transform this into a new DataFrame with this form:</p>
<p><a href=""https://i.stack.imgur.com/gJKZ8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gJKZ8.png"" alt=""Data Format 2"" /></a></p>
<p>I am beginning to use Seaborn and Plotly for plotting, and it seems like they prefer data to be formatted in the second way.</p>
","['python', 'pandas', 'dataframe']",64328097,"<p>Lets try <code>set_index()</code>, <code>unstack()</code>, <code>renamecolumns</code></p>
<pre><code>`df.set_index('Date').unstack().reset_index().rename(columns={'level_0':'Name',0:'Score'})`
</code></pre>
<p><strong>How it works</strong></p>
<pre><code>df.set_index('Date')#Sets Date as index
df.set_index('Date').unstack()#Flips, melts the dataframe
d=df.set_index('Date').unstack().reset_index()# resets the datframe and allocates columns, those in index become level_suffix and attained values become 0
d.rename(columns={'level_0':'Name',0:'Score'})#renames columns
</code></pre>
",How I transform DataFrame headers become column values I Pandas DataFrame form How I transform new DataFrame form I beginning use Seaborn Plotly plotting seems like prefer data formatted second way,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
64382010,2020-10-16,2020,5,How to create a seaborn heatmap by hour/day from timestamp with multiple data points per hour,"<p>I have a data frame with a <code>date</code> column which is a <code>timestamp</code>. There are multiple data points per hour of a day eg <code>2014-1-1 13:10, 2014-1-1 13:20</code> etc. I want to group the data points from the same hour of a specific day and then create a heatmap using seaborn and plot a different column.</p>
<p>I have tried to use <code>groupby</code> but I'm not sure how to specify i want the <code>hour</code> and <code>day</code></p>
<pre><code>date             data
2014-1-1 13:10  50
2014-1-1 13:20  51
2014-1-1 13:30  51
2014-1-1 13:40  56
2014-1-1 13:50  67
2014-1-1 14:00  43
2014-1-1 14:10  78
2014-1-1 14:20  45
2014-1-1 14:30  58 
</code></pre>
<p>I want to combine the data by its mean value</p>
","['python', 'matplotlib', 'seaborn']",64382139,"<p>You can use <code>dt.strftime('%H')</code> to get the hours, and <code>dt.strftime('%Y-%m-%D')</code> or <code>dt.normalize()</code> for the days</p>
<pre><code>sns.heatmap(df.groupby([df.date.dt.normalize(), df.date.dt.strftime('%H:00')])
   ['data'].mean()
   .rename_axis(index=['day','hour'])
   .unstack(level=0)
)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.stack.imgur.com/z5QZn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z5QZn.png"" alt=""enter image description here"" /></a></p>
<hr />
<p><strong>Update</strong>: for the weeks, we can use a similar approach</p>
<pre><code>s = (df.groupby([df.date.dt.isocalendar().week,
                 df.date.dt.strftime('%Y-%m-%d'), 
                 df.date.dt.strftime('%H:00')])
       ['data'].mean()
       .rename_axis(index=['week','day','hour'])
    )

fig, axes = plt.subplots(2,2, figsize=(10,10))
for w, ax in zip(s.index.unique('week'), axes.ravel()):
    sns.heatmap(s.loc[w].unstack(level='day'), ax=ax)
    ax.set_title(f'Week {w}')
</code></pre>
<p>Output:</p>
<p><a href=""https://i.stack.imgur.com/71wXp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/71wXp.png"" alt=""enter image description here"" /></a></p>
",How create seaborn heatmap hour day timestamp multiple data points per hour I data frame date column timestamp There multiple data points per hour day eg etc I want group data points hour specific day create heatmap using seaborn plot different column I tried use groupby I sure specify want hour day date data I want combine data mean value,"startoftags, python, matplotlib, seaborn, endoftags",python arrays numpy endoftags,python matplotlib seaborn,python arrays numpy,0.33
64447670,2020-10-20,2020,2,How to get dictionary value from key in a list?,"<p>I am trying to create a new list from a list of dictionary items.
Below is an example of 1 dictionary item.</p>
<pre><code>{'id': 'bitcoin',
  'symbol': 'btc',
  'name': 'Bitcoin',
  'current_price': 11907.43,
  'market_cap': 220817187069,
  'market_cap_rank': 1}
</code></pre>
<p>I want the list to just be of the id item. So what I am trying to achieve is a list with items {'bitcoin', 'etc', 'etc}</p>
","['python', 'pandas', 'numpy']",64447756,"<p>You can use list comprehension:</p>
<pre><code>my_list = [{'id': 'bitcoin', 'symbol': 'btc', ...}, ...]
[d['id'] for d in my_list]
</code></pre>
<p>Which translates to : for each dictionary in my_list, extract the 'id' key.</p>
",How get dictionary value key list I trying create new list list dictionary items Below example dictionary item id bitcoin symbol btc name Bitcoin current price market cap market cap rank I want list id item So I trying achieve list items bitcoin etc etc,"startoftags, python, pandas, numpy, endoftags",python python3x list endoftags,python pandas numpy,python python3x list,0.33
64536734,2020-10-26,2020,2,Convert period_range to list of string,"<p>I have this</p>
<pre><code>&gt;&gt;&gt; start
'1905'
&gt;&gt;&gt; end
'2003'
&gt;&gt;&gt; p = pd.period_range(pd.to_datetime(start, format=&quot;%y%m&quot;), pd.to_datetime(end, format=&quot;%y%m&quot;), freq='M')
&gt;&gt;&gt; p
PeriodIndex(['2019-05', '2019-06', '2019-07', '2019-08', '2019-09', '2019-10',
             '2019-11', '2019-12', '2020-01', '2020-02', '2020-03'],
            dtype='period[M]', freq='M')
</code></pre>
<p>I want to convert p to list of string which will contain period as</p>
<pre><code>&gt;&gt;&gt; p
['1905', '1906', '1907', '1908', '1909', '1910', '1911', '1912', '2001', '2002', '2003']
</code></pre>
<p>Format of string matters, it should be in same format as shown in eg('yymm')
Can someone help me in achieving this cleanly?</p>
","['python', 'pandas', 'datetime']",64536808,"<p>You can use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.PeriodIndex.strftime.html"" rel=""nofollow noreferrer""><code>PeriodIndex.strftime</code></a> with <code>'%y%m'</code> to indicate &quot;YYMM&quot; format:</p>
<pre><code>p.strftime('%y%m')
# Index(['1905', '1906', '1907', '1908', '1909', '1910', '1911', '1912', '2001',
#        '2002', '2003'],
#       dtype='object')
</code></pre>
",Convert period range list string I gt gt gt start gt gt gt end gt gt gt p pd period range pd datetime start format quot quot pd datetime end format quot quot freq M gt gt gt p PeriodIndex dtype period M freq M I want convert p list string contain period gt gt gt p Format string matters format shown eg yymm Can someone help achieving cleanly,"startoftags, python, pandas, datetime, endoftags",python python3x pandas endoftags,python pandas datetime,python python3x pandas,0.67
64773001,2020-11-10,2020,3,Split two columns in a pandas dataframe into two and name them,"<p>I have this pandas dataframe</p>
<pre><code>         x           y       Values
0       A B         C D       4.7
1       A B         C D       10.9
2       A B         C D       1.8
3       A B         C D       6.5
4       A B         C D       3.4
</code></pre>
<p>I would like to split the x and y columns and get an output with these given names on the columns.</p>
<pre><code>     x    f    y    g   Values
0    A    B    C    D    4.7
1    A    B    C    D    10.9
2    A    B    C    D    1.8
3    A    B    C    D    6.5
4    A    B    C    D    3.4
</code></pre>
<p>Is there a straight forward way to do this in python?</p>
","['python', 'pandas', 'dataframe']",64773223,"<pre><code>df[['x', 'f']] = df.x.str.split(&quot; &quot;, expand=True)
df[['y', 'g']] = df.y.str.split(&quot; &quot;, expand=True)
df[['x','f','y','g', 'Values']]
</code></pre>
<p>You can make it scalable as well, defining a dict in which the keys are the columns, and the values a list with the desired new column names:</p>
<pre><code># Define the target columns to split, and their new column names
cols={
    'x': ['x','f'],
    'y': ['y','g']
}
# Apply the function to each target-column
for k in cols:
    df[cols[k]] = df[k].str.split(&quot; &quot;, expand=True)

# Reorder the dataframe as you wish
new_columns = sum(cols.values(),[])
old_columns = set(df.columns) - set(new_columns)
df[new_columns + list(old_columns)]
</code></pre>
",Split two columns pandas dataframe two name I pandas dataframe x Values A B C D A B C D A B C D A B C D A B C D I would like split x columns get output given names columns x f g Values A B C D A B C D A B C D A B C D A B C D Is straight forward way python,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
64878086,2020-11-17,2020,2,pandas: create n new columns to contain last n values (rolling),"<p>consider this sample data:</p>
<pre><code>import pandas as pd
import numpy as np
thisDict = {0: '20141216',
 1: '20150115',
 2: '20150212',
 3: '20150316',
 4: '20150415',
 5: '20150514',
 6: '20150615',
 7: '20150716',
 8: '20150814',
 9: '20150915'}
pd.DataFrame.from_dict(thisDict, orient = 'index')
</code></pre>
<p>I want basically to turn this into a new dataframe with n columns, where for each row we have the last n values.
Consider the data above, for n = 3, it would be:</p>
<pre><code>    pd.DataFrame.from_dict({0: [np.nan,np.nan ,np.nan] ,
     1: ['20141216',np.nan,np.nan],
     2: ['20141216','20150115', np.nan],
     3: ['20150316','20150115','20150212'],
...
    },orient = 'index')
</code></pre>
","['python', 'pandas', 'numpy']",64878268,"<p>For one, you can do a concat:</p>
<pre><code>n=3
pd.concat([df.shift(i) for i in range(n)], axis=1)
</code></pre>
<p>Output:</p>
<pre><code>          0         0         0
0  20141216       NaN       NaN
1  20150115  20141216       NaN
2  20150212  20150115  20141216
3  20150316  20150212  20150115
4  20150415  20150316  20150212
5  20150514  20150415  20150316
6  20150615  20150514  20150415
7  20150716  20150615  20150514
8  20150814  20150716  20150615
9  20150915  20150814  20150716
</code></pre>
",pandas create n new columns contain last n values rolling consider sample data import pandas pd import numpy np thisDict pd DataFrame dict thisDict orient index I want basically turn new dataframe n columns row last n values Consider data n would pd DataFrame dict np nan np nan np nan np nan np nan np nan orient index,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
64978530,2020-11-24,2020,2,How to do an outer product of 3 vectors to create a 3d matrix in numpy? (and same for nd),"<p>If i want to do an outer product of 2 vectors to create a 2d matrix, each element a product of the two respective elements in the original vectors:</p>
<pre><code>b = np.arange(5).reshape((1, 5))
a = np.arange(5).reshape((5, 1))
a * b
array([[ 0,  0,  0,  0,  0],
       [ 0,  1,  2,  3,  4],
       [ 0,  2,  4,  6,  8],
       [ 0,  3,  6,  9, 12],
       [ 0,  4,  8, 12, 16]])
</code></pre>
<p>I want the same for 3 (or for n) vectors.</p>
<p>An equivalent non numpy answer:</p>
<pre><code>a = np.arange(5)
b = np.arange(5)
c = np.arange(5)
res = np.zeros((a.shape[0], b.shape[0], c.shape[0]))

for ia in range(len(a)):
    for ib in range(len(b)):
        for ic in range(len(c)):
            res[ia, ib, ic] = a[ia] * b[ib] * c[ic]
print(res)
</code></pre>
<p>out:</p>
<blockquote>
<pre><code>[[[ 0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.]
  [ 0.  0.  0.  0.  0.]]

 [[ 0.  0.  0.  0.  0.]
  [ 0.  1.  2.  3.  4.]
  [ 0.  2.  4.  6.  8.]
  [ 0.  3.  6.  9. 12.]
  [ 0.  4.  8. 12. 16.]]

 [[ 0.  0.  0.  0.  0.]
  [ 0.  2.  4.  6.  8.]
  [ 0.  4.  8. 12. 16.]
  [ 0.  6. 12. 18. 24.]
  [ 0.  8. 16. 24. 32.]]

 [[ 0.  0.  0.  0.  0.]
  [ 0.  3.  6.  9. 12.]
  [ 0.  6. 12. 18. 24.]
  [ 0.  9. 18. 27. 36.]
  [ 0. 12. 24. 36. 48.]]

 [[ 0.  0.  0.  0.  0.]
  [ 0.  4.  8. 12. 16.]
  [ 0.  8. 16. 24. 32.]
  [ 0. 12. 24. 36. 48.]
  [ 0. 16. 32. 48. 64.]]]
</code></pre>
</blockquote>
<p>How to do this with numpy [no for loops]?</p>
<hr />
<p>Also, how to do this for a general function, not necessarily <code>*</code>?</p>
","['python', 'arrays', 'numpy']",64978645,"<p>NumPy provides you with <a href=""https://numpy.org/doc/stable/reference/generated/numpy.outer.html"" rel=""nofollow noreferrer""><code>np.outer()</code></a> for computing the outer product.
This is a less powerful version of more versatile approaches:</p>
<ul>
<li><a href=""https://numpy.org/doc/stable/reference/generated/numpy.ufunc.outer.html#numpy.ufunc.outer"" rel=""nofollow noreferrer""><code>ufunc.outer()</code></a></li>
<li><a href=""https://numpy.org/doc/stable/reference/generated/numpy.tensordot.html#numpy.tensordot"" rel=""nofollow noreferrer""><code>np.tensordot()</code></a></li>
<li><a href=""https://numpy.org/doc/stable/reference/generated/numpy.einsum.html#numpy.einsum"" rel=""nofollow noreferrer""><code>np.einsum()</code></a></li>
</ul>
<p><code>np.einsum()</code> is the only one capable of handling more than two input arrays:</p>
<pre><code>import numpy as np


def prod(items, start=1):
    for item in items:
        start = start * item
    return start


a = np.arange(5)
b = np.arange(5)
c = np.arange(5)


r0 = np.zeros((a.shape[0], b.shape[0], c.shape[0]))
for ia in range(len(a)):
    for ib in range(len(b)):
        for ic in range(len(c)):
            r0[ia, ib, ic] = a[ia] * b[ib] * c[ic]

r1 = prod([a[:, None, None], b[None, :, None], c[None, None, :]])
# same as: r1 = a[:, None, None] * b[None, :, None] * c[None, None, :]
# same as: r1 = a.reshape(-1, 1, 1) * b.reshape(1, -1, 1) * c.reshape(1, 1, -1)
print(np.all(r0 == r2))
# True

r2 = np.einsum('i,j,k-&gt;ijk', a, b, c)
print(np.all(r0 == r2))
# True

# as per @hpaulj suggestion
r3 = prod(np.ix_(a, b, c))
print(np.all(r0 == r3))
# True
</code></pre>
<p>Of course, the <a href=""https://numpy.org/doc/stable/user/theory.broadcasting.html#array-broadcasting-in-numpy"" rel=""nofollow noreferrer"">broadcasting</a> approach (which is the same that you used with the <code>array.reshape()</code> version of your code, except that it uses a slightly different syntax for providing the correct shape), can be automatized by explicitly building the slicing (or equivalently the <code>array.reshape()</code> parameters).</p>
",How outer product vectors create matrix numpy nd If want outer product vectors create matrix element product two respective elements original vectors b np arange reshape np arange reshape b array I want n vectors An equivalent non numpy answer np arange b np arange c np arange res np zeros shape b shape c shape ia range len ib range len b ic range len c res ia ib ic ia b ib c ic print res How numpy loops Also general function necessarily,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
65033492,2020-11-27,2020,2,Apply multiple functions to GroupBy object in a specific order,"<p>I have a dataframe <code>df</code> with at column <code>date</code> which consists of date.</p>
<p>If I want to calculate the maximum difference between the dates within each group, is that doable (without having to re-group and without using <code>.apply</code>)? If I do</p>
<pre class=""lang-py prettyprint-override""><code>
df

id | date |
---+-------
1  | 2020-01-20
1  | 2020-01-25
2  | 2020-02-03
2  | 2020-02-04

max_diff_for_each_id = df.groupby(&quot;id&quot;).diff(1).max()
max_diff_for_each_id 

id
--
1  5
</code></pre>
<p>that of course give the maximum difference between all groups, where I want</p>
<pre class=""lang-py prettyprint-override""><code>id 
--
1  5
2  1
</code></pre>
<p>I know I can just re-group <code>max_diff_for_each_id</code> but I think that</p>
<pre class=""lang-py prettyprint-override""><code>max_diff_for_each_id = df.groupby(&quot;id&quot;).diff(1).groupby(&quot;id&quot;).max()
</code></pre>
<p>is not really &quot;pretty&quot; and say you have multiple functions to apply, there is a ton of overhead by having to re-group all the time</p>
","['python', 'pandas', 'pandas-groupby']",65033505,"<blockquote>
<p>is that doable (without having to re-group and without using .apply)</p>
</blockquote>
<p>I think generally not, if only 2 values per groups or some another patterns of data there should be alternatives.</p>
<pre><code>#if always 2 values per id in order
df1 = df.groupby(&quot;id&quot;)['date'].agg(['min','max'])
max_diff_for_each_id = df1['max'].sub(df1['min']).dt.days
</code></pre>
<p>Or:</p>
<pre><code>#if always 2 values per id 
df2 = df.groupby(&quot;id&quot;)['date'].agg(['first','last'])

max_diff_for_each_id = df2['last'].sub(df2['first']).dt.days
</code></pre>
<p>One idea with convert <code>id</code> to index, but <code>max(level=0)</code> is only hidden <code>.groupby(level=0).max()</code>, so this should be trick solution (in my opinion)</p>
<pre><code>max_diff_for_each_id = df.set_index('id').groupby(&quot;id&quot;)['date'].diff().max(level=0).dt.days
</code></pre>
<hr />
<p>There is possible multiple <code>groupby</code> like:</p>
<pre><code>max_diff_for_each_id = df.groupby(&quot;id&quot;)['date'].diff(1).groupby(df[&quot;id&quot;]).max().dt.days
</code></pre>
<p>Or create custom functions like:</p>
<pre><code>max_diff_for_each_id = df.groupby(&quot;id&quot;)['date'].apply(lambda x: x.diff().max()).dt.days

max_diff_for_each_id = df.groupby(&quot;id&quot;)['date'].agg(lambda x: x.diff().max()).dt.days
</code></pre>
<hr />
<pre><code>print (max_diff_for_each_id)
id
1    5
2    1
dtype: int64
</code></pre>
",Apply multiple functions GroupBy object specific order I dataframe df column date consists date If I want calculate maximum difference dates within group doable without group without using apply If I df id date max diff id df groupby quot id quot diff max max diff id id course give maximum difference groups I want id I know I group max diff id I think max diff id df groupby quot id quot diff groupby quot id quot max really quot pretty quot say multiple functions apply ton overhead group time,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
65403705,2020-12-22,2020,2,Python: Finding the most frequent occurrences of combinations of any length in a list of lists,"<p>How to find most occurring combinations in a list of lists. Combinations length can be any.</p>
<p>So, sample data:</p>
<pre><code>l = [['action','mystery','horror','thriller'],
 ['drama','romance'],
 ['comedy','drama','romance'],
 ['scifi','mystery','horror','thriller'],
 ['horror','mystery','thriller']]
</code></pre>
<p>Expected output:</p>
<pre><code>'mystery','horror','thriller' - 3 times
'drama','romance' - 2 times
</code></pre>
<p>With the help of <a href=""https://stackoverflow.com/questions/31495507/finding-the-most-frequent-occurrences-of-pairs-in-a-list-of-lists""><code>this post</code></a>, I was able to find out most occurring pairs(combination of 2), but how to extend it find combinations of any length.</p>
<p><strong>EDIT:</strong> As per @CrazyChucky's comment:</p>
<p>Sample input:</p>
<pre><code>l = [['action','mystery','horror','thriller'],
     ['drama','romance'],
     ['comedy','drama','romance'],
     ['scifi','mystery','horror','thriller'],
     ['horror','mystery','thriller'],
     ['mystery','horror']]
</code></pre>
<p>Expected output:</p>
<pre><code>'mystery','horror' - 4 times
'mystery','horror','thriller' - 3 times
'drama','romance' - 2 times
</code></pre>
","['python', 'python-3.x', 'list']",65403795,"<p>You can adapt the code from that question to iterate over all the possible combinations of each possible size from each sublist:</p>
<pre><code>from collections import Counter
from itertools import combinations

l = [['action','mystery','horror','thriller'],
 ['drama','romance'],
 ['comedy','drama','romance'],
 ['scifi','mystery','horror','thriller'],
 ['horror','mystery','thriller']]
d  = Counter()
for sub in l:
    if len(sub) &lt; 2:
        continue
    sub.sort()
    for sz in range(2, len(sub)+1):
        for comb in combinations(sub, sz):
            d[comb] += 1

print(d.most_common())
</code></pre>
<p>Output:</p>
<pre><code>[
 (('horror', 'mystery'), 3),
 (('horror', 'thriller'), 3),
 (('mystery', 'thriller'), 3),
 (('horror', 'mystery', 'thriller'), 3),
 (('drama', 'romance'), 2),
 (('action', 'horror'), 1),
 (('action', 'mystery'), 1),
 (('action', 'thriller'), 1),
 (('action', 'horror', 'mystery'), 1),
 (('action', 'horror', 'thriller'), 1),
 (('action', 'mystery', 'thriller'), 1),
 (('action', 'horror', 'mystery', 'thriller'), 1),
 (('comedy', 'drama'), 1),
 (('comedy', 'romance'), 1),
 (('comedy', 'drama', 'romance'), 1),
 (('horror', 'scifi'), 1),
 (('mystery', 'scifi'), 1),
 (('scifi', 'thriller'), 1),
 (('horror', 'mystery', 'scifi'), 1),
 (('horror', 'scifi', 'thriller'), 1),
 (('mystery', 'scifi', 'thriller'), 1),
 (('horror', 'mystery', 'scifi', 'thriller'), 1)
]
</code></pre>
<p>To get just the genres which have the highest count you can iterate over the counter:</p>
<pre><code>most_frequent = [g for g, cnt in d.items() if cnt == d.most_common(1)[0][1]]
</code></pre>
",Python Finding frequent occurrences combinations length list lists How find occurring combinations list lists Combinations length So sample data l action mystery horror thriller drama romance comedy drama romance scifi mystery horror thriller horror mystery thriller Expected output mystery horror thriller times drama romance times With help post I able find occurring pairs combination extend find combinations length EDIT As per CrazyChucky comment Sample input l action mystery horror thriller drama romance comedy drama romance scifi mystery horror thriller horror mystery thriller mystery horror Expected output mystery horror times mystery horror thriller times drama romance times,"startoftags, python, python3x, list, endoftags",python python3x list endoftags,python python3x list,python python3x list,1.0
65658779,2021-01-10,2021,2,Going through string columns and sort cell values in Pandas,"<p>Suppose we have the following dataframe:</p>
<pre><code>d = {'col1':['cat; banana','kiwi; orange; apple','melon'],
    'col2':['a; d; c','p; u; c','m; a'],
    'col3':[4,1,4]}
df= pd.DataFrame(d)
</code></pre>
<p>for all the string columns I want to sort the values alphabetically, I know how to do this column by column, namely:</p>
<pre><code>df['col1'] = df['col1'].map(lambda x: '; '.join(sorted(x.split('; '))))
</code></pre>
<p>and similarly for <code>col2</code> I wonder how one can does this for the whole dataframe? I tried to select the string objects and do the map method, but it didn't work. Namely:</p>
<pre><code>df.select_dtypes(include='object').map(lambda x: '; '.join(sorted(x.split('; '))))
</code></pre>
<p><strong>Update:</strong> So an inefficient way of doing this would be:</p>
<pre><code>v = df.select_dtypes(include='object').applymap(lambda x: '; '.join(sorted(x.split('; '))))
w = df.select_dtypes(exclude='object')
pd.concat([v, w], axis=1)
</code></pre>
<p>But I am sure there are better ways.</p>
","['python', 'python-3.x', 'pandas']",65659308,"<p>You can use this trick (unpacking a dataframe and using <code>pd.DataFrame.assign</code>):</p>
<pre><code>df.assign(**df.select_dtypes(include='object').applymap(lambda x: '; '.join(sorted(x.split('; ')))))
</code></pre>
<p>Output:</p>
<pre><code>                  col1     col2  col3
0          banana; cat  a; c; d     4
1  apple; kiwi; orange  c; p; u     1
2                melon     a; m     4
</code></pre>
",Going string columns sort cell values Pandas Suppose following dataframe col cat banana kiwi orange apple melon col c p u c col df pd DataFrame string columns I want sort values alphabetically I know column column namely df col df col map lambda x join sorted x split similarly col I wonder one whole dataframe I tried select string objects map method work Namely df select dtypes include object map lambda x join sorted x split Update So inefficient way would v df select dtypes include object applymap lambda x join sorted x split w df select dtypes exclude object pd concat v w axis But I sure better ways,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
65761375,2021-01-17,2021,2,Display another column where the conditions are determined in other columns,"<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>address</th>
<th>latitude</th>
<th>longitude</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tokyo</td>
<td>124.4423</td>
<td>95.223</td>
</tr>
<tr>
<td>Budapest</td>
<td>156.2442</td>
<td>78.112</td>
</tr>
<tr>
<td>Perth</td>
<td>124.9234</td>
<td>20.490</td>
</tr>
</tbody>
</table>
</div>
<p>I have an example of a data frame as seen above (please don't mind the accuracy); address is string, latitude and longitude are floats. I would like to to display the address where the condition meets both latitude and longitude.</p>
<p>Example:</p>
<pre><code>Latitude = 156.2442
Longitude = 78.112
</code></pre>
<p>Therefore I want to display/print the corresponding column address, which would be &quot;Budapest&quot;.</p>
","['python', 'pandas', 'dataframe']",65764236,"<p>One approach you can take is:</p>
<pre><code>lat = 156.2442 # insert your latitude
long = 78.112 # insert your longtitude
print(df[(df['latitude']==lat) &amp; (df['longitude'] == long)]['address'])
</code></pre>
<p>which prints:</p>
<pre><code>1    Budapest
Name: address, dtype: object
</code></pre>
",Display another column conditions determined columns address latitude longitude Tokyo Budapest Perth I example data frame seen please mind accuracy address string latitude longitude floats I would like display address condition meets latitude longitude Example Latitude Longitude Therefore I want display print corresponding column address would quot Budapest quot,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
65855938,2021-01-23,2021,2,How to fill in missing values in Pandas dataframe according to pattern in column?,"<p>Suppose I have a dataframe with a column as follows:</p>
<pre><code>Column
10
NaN
20
NaN
30
</code></pre>
<p>I want each row to be filled in with increments of 5 so that the final output would appear like:</p>
<pre><code>Column
10
15
20
25
30
</code></pre>
<p>I've tried using <code>np.arange</code> and <code>.reindex()</code> but haven't had much luck. I'm looking for an iterative approach instead of simply manually filling in. Can anyone please help with this?</p>
","['python', 'pandas', 'dataframe']",65855949,"<p>Try with <code>interpolate</code></p>
<pre><code>df['Column']=df.Column.interpolate()
Out[86]: 
0    10.0
1    15.0
2    20.0
3    25.0
4    30.0
Name: Column, dtype: float64
</code></pre>
",How fill missing values Pandas dataframe according pattern column Suppose I dataframe column follows Column NaN NaN I want row filled increments final output would appear like Column I tried using np arange reindex much luck I looking iterative approach instead simply manually filling Can anyone please help,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
65903287,2021-01-26,2021,3,pandas 1.2.1 to_csv performance with datetime as the index and setting date_format,"<p>I'm observing a massive performance regression after upgrade to Python 3.8 and pandas 1.2.1.</p>
<p>The following simple code takes nearly 8 minutes to complete:</p>
<pre class=""lang-py prettyprint-override""><code>import sys
import pandas as pd
import csv
import datetime

pd.show_versions()

start = datetime.datetime(2020,1,1,0,0,0)
print(str(start))
data = { 'timestamp': [], 'i': []}
for i in range(0, 2000000):
        data['timestamp'].append(str(start))
        data['i'].append(i)

df = pd.DataFrame(data)
df['timestamp'] = pd.to_datetime(df['timestamp'])
df.set_index('timestamp', inplace=True)
print(df.tail(10), flush=True)

df.to_csv('/DATA1/TEMP/df.csv', sep=',', date_format='%Y-%m-%d %H:%M:%S', quoting=csv.QUOTE_ALL)
print(&quot;DONE&quot;, flush=True)
</code></pre>
<p>The output of this script in my virtual environment is:</p>
<pre><code>/home/user/venv-test/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.
  warnings.warn(

INSTALLED VERSIONS
------------------
commit           : 9d598a5e1eee26df95b3910e3f2934890d062caa
python           : 3.8.7.final.0
python-bits      : 64
OS               : Linux
OS-release       : 3.10.0-1062.el7.x86_64
Version          : #1 SMP Wed Aug 7 18:08:02 UTC 2019
machine          : x86_64
processor        : x86_64
byteorder        : little
LC_ALL           : None
LANG             : en_US.UTF-8
LOCALE           : en_US.UTF-8

pandas           : 1.2.1
numpy            : 1.19.5
pytz             : 2020.5
dateutil         : 2.8.1
pip              : 21.0
setuptools       : 49.2.1
Cython           : 0.29.21
pytest           : None
hypothesis       : None
sphinx           : None
blosc            : None
feather          : None
xlsxwriter       : None
lxml.etree       : None
html5lib         : None
pymysql          : None
psycopg2         : 2.8.6 (dt dec pq3 ext lo64)
jinja2           : None
IPython          : None
pandas_datareader: None
bs4              : None
bottleneck       : None
fsspec           : None
fastparquet      : None
gcsfs            : None
matplotlib       : None
numexpr          : None
odfpy            : None
openpyxl         : None
pandas_gbq       : None
pyarrow          : None
pyxlsb           : None
s3fs             : None
scipy            : 1.6.0
sqlalchemy       : None
tables           : None
tabulate         : None
xarray           : None
xlrd             : None
xlwt             : None
numba            : None
2020-01-01 00:00:00
                  
timestamp          
2020-01-01  1999990
2020-01-01  1999991
2020-01-01  1999992
2020-01-01  1999993
2020-01-01  1999994
2020-01-01  1999995
2020-01-01  1999996
2020-01-01  1999997
2020-01-01  1999998
2020-01-01  1999999
DONE

real    7m49.337s
user    7m39.805s
sys     0m14.665s

</code></pre>
<p>On my production environment this is run on a dataframe with about 20 more columns and the time of the <code>to_csv()</code> call went from a few minutes to more than 40 hours.</p>
<p>Am I missing something obvious here? Or is there maybe some known bug in this specific version of pandas?</p>
<p>I checked my system is not IO bound by dumping the file to several different partitions on different hard drives. Additionally the process stays at 100% CPU usage while dumping the data frame to CSV what would be inconsistent with an IO bound system.</p>
","['python', 'pandas', 'dataframe']",65904289,"<ul>
<li><strong>I can't tell you why, but I can tell you what.</strong></li>
</ul>
<h2>The issue is caused by using <code>date_format</code> and when the <code>datetime</code> is the index.</h2>
<ul>
<li>As noted by <a href=""https://stackoverflow.com/users/3944322/stef"">Stef</a>: <a href=""https://github.com/pandas-dev/pandas/issues/37484"" rel=""nofollow noreferrer"">BUG (Performance): Performance of to_csv varies significantly depending on when/how index is set #37484</a></li>
<li><code>date_format='%Y-%m-%d %H:%M:%S'</code> is making the process slow, because it does not seem to be vectorized.
<ul>
<li>This parameter seems to cause the process to write the row to a the file, and then change the format in the file, or it changes the date format, for the row, and then writes it.</li>
<li>I can see that this is happening, because I stop the process, and the file is partially written.</li>
</ul>
</li>
<li>In this case, the time for all the data is <code>00:00:00</code>, which pandas doesn't display, however if there is a time other than <code>00:00:00</code> in the column, then all time components are displayed.
<ul>
<li>If you write to a file without <code>date_format</code>, when all the time components are <code>00:00:00</code>, the format of the timestamp will be <code>'%Y-%m-%d'</code>.</li>
</ul>
</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>start = datetime(2020,1,1,0,0,0)
df.to_csv('df.csv', sep=',', quoting=csv.QUOTE_ALL)

# resulting csv
&quot;timestamp&quot;,&quot;i&quot;
&quot;2020-01-01&quot;,&quot;0&quot;
&quot;2020-01-01&quot;,&quot;1&quot;
&quot;2020-01-01&quot;,&quot;2&quot;
</code></pre>
<ul>
<li>If you write to a file without <code>date_format</code>, when all the time components are <code>00:00:01</code>, the format of the timestamp will be <code>'%Y-%m-%d %H:%M:%S'</code>.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>start = datetime(2020,1,1,0,0,1)
df.to_csv('df.csv', sep=',', quoting=csv.QUOTE_ALL)

# resulting csv
&quot;timestamp&quot;,&quot;i&quot;
&quot;2020-01-01 00:00:01&quot;,&quot;0&quot;
&quot;2020-01-01 00:00:01&quot;,&quot;1&quot;
&quot;2020-01-01 00:00:01&quot;,&quot;2&quot;
</code></pre>
<hr />
<h2>Resolve the issue by setting the format before <code>.to_csv()</code>, or reset the <code>datetime</code> index.</h2>
<ul>
<li>If you need to set the format of a datetime column, do it before writing to the csv.</li>
<li>Use <code>df.index.strftime('%Y-%m-%d %H:%M:%S')</code> or <code>df[some column].dt.strftime('%Y-%m-%d %H:%M:%S')</code>.</li>
<li><code>'%Y-%m-%d %H:%M:%S'</code> is already the default format for a <code>datetime</code> <code>dtype</code>, so it's not necessary to reformat a <code>datetime</code> formatted column to a <code>string</code>.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>df.index = df.index.strftime('%Y-%m-%d %H:%M:%S')
df.to_csv('df.csv', sep=',', quoting=csv.QUOTE_ALL)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>df.reset_index(inplace=True)
df.to_csv('df.csv', sep=',', date_format='%Y-%m-%d %H:%M:%S', index=False, quoting=csv.QUOTE_ALL)
</code></pre>
<hr />
<h2>Test Data</h2>
<ul>
<li>The code to write the test dataframe can be consolidated</li>
<li><code>datetime(2020,1,1,0,0,0)</code> is already a <code>datetime</code> <code>dtype</code>, so there is no reason to do <code>df['timestamp'] = pd.to_datetime(df['timestamp'])</code></li>
</ul>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from datetime import datetime

cols = 2000000
df = pd.DataFrame({'i': range(cols)}, index=[datetime(2020,1,1,0,0,1)] * cols)

df.to_csv('df.csv', sep=',', quoting=csv.QUOTE_ALL)
</code></pre>
",pandas csv performance datetime index setting date format I observing massive performance regression upgrade Python pandas The following simple code takes nearly minutes complete import sys import pandas pd import csv import datetime pd show versions start datetime datetime print str start data timestamp range data timestamp append str start data append df pd DataFrame data df timestamp pd datetime df timestamp df set index timestamp inplace True print df tail flush True df csv DATA TEMP df csv sep date format Y H M S quoting csv QUOTE ALL print quot DONE quot flush True The output script virtual environment home user venv test lib python site packages setuptools distutils patch py UserWarning Distutils imported Setuptools This usage discouraged may exhibit undesirable behaviors errors Please use Setuptools objects directly least import Setuptools first warnings warn INSTALLED VERSIONS commit e eee df b e f caa python final python bits,"startoftags, python, pandas, dataframe, endoftags",python pandas matplotlib endoftags,python pandas dataframe,python pandas matplotlib,0.67
66041108,2021-02-04,2021,2,Sorting a pandas dataframe based on number of values of a categorical column,"<p>The sample dataset looks like this</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">col1</th>
<th style=""text-align: center;"">col2</th>
<th style=""text-align: right;"">col3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">A</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: right;"">as</td>
</tr>
<tr>
<td style=""text-align: left;"">A</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: right;"">sd</td>
</tr>
<tr>
<td style=""text-align: left;"">B</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: right;"">df</td>
</tr>
<tr>
<td style=""text-align: left;"">C</td>
<td style=""text-align: center;"">5</td>
<td style=""text-align: right;"">fg</td>
</tr>
<tr>
<td style=""text-align: left;"">D</td>
<td style=""text-align: center;"">6</td>
<td style=""text-align: right;"">gh</td>
</tr>
<tr>
<td style=""text-align: left;"">A</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: right;"">hj</td>
</tr>
<tr>
<td style=""text-align: left;"">B</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: right;"">jk</td>
</tr>
<tr>
<td style=""text-align: left;"">B</td>
<td style=""text-align: center;"">4</td>
<td style=""text-align: right;"">kt</td>
</tr>
<tr>
<td style=""text-align: left;"">A</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: right;"">re</td>
</tr>
<tr>
<td style=""text-align: left;"">C</td>
<td style=""text-align: center;"">5</td>
<td style=""text-align: right;"">we</td>
</tr>
<tr>
<td style=""text-align: left;"">D</td>
<td style=""text-align: center;"">6</td>
<td style=""text-align: right;"">qw</td>
</tr>
<tr>
<td style=""text-align: left;"">D</td>
<td style=""text-align: center;"">7</td>
<td style=""text-align: right;"">aa</td>
</tr>
</tbody>
</table>
</div>
<p>I want to sort the column col1 based on the number of occurences each item has, e.g. A has 4 occurences, B and D have 3 and C has 2 occurences. The dataframe should be sorted like A,A,A,A,B,B,B,D,D,D,C,C so that</p>
<p>Is there a way to achieve the same? Can I use sort_values to get desired result?</p>
","['python', 'pandas', 'dataframe']",66041140,"<p>Create helper column by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html"" rel=""noreferrer""><code>Series.map</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html"" rel=""noreferrer""><code>Series.value_counts</code></a> and use it for sorting with <code>col1</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html"" rel=""noreferrer""><code>DataFrame.sort_values</code></a>:</p>
<pre><code>df['new'] = df['col1'].map(df['col1'].value_counts())
#alternative
#df['new'] = df.groupby('col1')['col1'].transform('count')

df1 = df.sort_values(['new','col1'], ascending=[False, True]).drop('new', axis=1)
</code></pre>
<p>One line solution:</p>
<pre><code>df1 = (df.assign(new =df['col1'].map(df['col1'].value_counts()))
         .sort_values(['new','col1'], ascending=[False, True])
         .drop('new', axis=1))

print (df1)
   col1  col2 col3
0     A     1   as
1     A     2   sd
5     A     1   hj
8     A     1   re
2     B     3   df
6     B     3   jk
7     B     4   kt
4     D     6   gh
10    D     6   qw
11    D     7   aa
3     C     5   fg
9     C     5   we
</code></pre>
",Sorting pandas dataframe based number values categorical column The sample dataset looks like col col col A A sd B df C fg D gh A hj B jk B kt A C D qw D aa I want sort column col based number occurences item e g A occurences B D C occurences The dataframe sorted like A A A A B B B D D D C C Is way achieve Can I use sort values get desired result,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
66361621,2021-02-25,2021,5,Is there a way to get the number of occurrences of the last value in a groupby?,"<p>Here is a sample dataframe -</p>
<pre><code>df = pd.DataFrame({'ID': ['a1', 'a1', 'a1', 'a1', 'b2', 'b2', 'b2'],
                   'Price': [15, 12, 10, 10, 36, 34, 36]})

  ID  Price 
0 a1  15
1 a1  12
2 a1  10
3 a1  10
4 b2  36
5 b2  34
6 b2  36
</code></pre>
<p>Here is the expected output -</p>
<pre><code>df.groupby('ID').agg({'Price': ['last', 'last_count']})
ID  Price_last Price_last_count
a1  10         2
b2  36         2
</code></pre>
<p>I need to be able to perform the 'last_count' operation in the agg.</p>
","['python', 'pandas', 'pandas-groupby']",66361754,"<pre><code>df.groupby('ID')['Price'].agg(lastvalue = 'last', 
                              count = lambda x: sum(x==x.iloc[-1]) )


    lastvalue  count
ID                  
a1         10      2
b2         36      2
</code></pre>
<p>Edit to get OP exact format (by Scott Boston):</p>
<pre><code>df.groupby('ID', as_index=False)\
  .agg(Price_last= ('Price' , 'last'), 
       Price_last_count=('Price' , lambda x: sum(x==x.iloc[-1])))
</code></pre>
<p>Output:</p>
<pre><code>   ID  Price_last  Price_last_count
0  a1          10                 2
1  b2          36                 2
</code></pre>
",Is way get number occurrences last value groupby Here sample dataframe df pd DataFrame ID b b b Price ID Price b b b Here expected output df groupby ID agg Price last last count ID Price last Price last count b I need able perform last count operation agg,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas numpy endoftags,python pandas pandasgroupby,python pandas numpy,0.67
66480871,2021-03-04,2021,2,compare two numpy array,"<p>I am trying to compare two 1d numpy arrays for mismatch as follows.
I have this working with partial success.</p>
<pre><code>import numpy as np
a= np.array([0,1,2,3,4,13])
b= np.array([0,1,2,3,4,10,11,12])
mis = max( np.sum(~np.isin(b,a)), np.sum(~np.isin(a,b)))
print(mis)
</code></pre>
<p>output: 3</p>
<p>expected output : 4 ( 13,10,11,12 are mismatches)</p>
","['python', 'arrays', 'numpy']",66480992,"<p>Why are you taking the max of the two sums instead of adding the sums? You are only grabbing the missing entries from a single array (the larger one) by doing that, when you clearly want both.</p>
<pre><code>mis = np.sum(~np.isin(b,a)) + np.sum(~np.isin(a,b))
</code></pre>
",compare two numpy array I trying compare two numpy arrays mismatch follows I working partial success import numpy np np array b np array mis max np sum np isin b np sum np isin b print mis output expected output mismatches,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
66771989,2021-03-23,2021,2,dataframe diff val from previous row if other columns value match,"<p>this is my dataframe analytics:
glnumber,nom,Year, YerarMonth,nom,amount</p>
<pre><code>4020 Honoraires de consultation,,2018,201809,234294.31000
4020 Honoraires de consultation,,2018,201810,166337.95000
4020 Honoraires de consultation,,2018,201811,250590.67000
4020 Honoraires de consultation,,2018,201812,92206.82000
4020 Honoraires de consultation,,2019,201901,196868.71000
4020 Honoraires de consultation,,2019,201902,148145.20000
4020 Honoraires de consultation,,2019,201903,110973.24000
4020 Honoraires de consultation,,2019,201904,184858.18000
4020 Honoraires de consultation,,2019,201905,119166.87000
4020 Honoraires de consultation,,2019,201906,10428.10000
4020 Honoraires de consultation,,2019,201907,19927.05000
4020 Honoraires de consultation,,2019,201908,-22677.79000
4020 Honoraires de consultation,,2019,201909,-8560.00000
4020 Honoraires de consultation,,2020,202004,-26.25000
4020 Honoraires de consultation,,2020,202007,-0.02000
4020 Honoraires de consultation,,2021,202101,-105.00000
4020 Honoraires de consultation,,2021,202103,104.99000
4020 Honoraires de consultation,Aclient1,2020,202007,9000.00000
4020 Honoraires de consultation,Aclient1,2020,202008,14040.00000
4020 Honoraires de consultation,Aclient1,2020,202010,31185.00000
4020 Honoraires de consultation,Aclient1,2020,202011,14310.00000
4020 Honoraires de consultation,Aclient1,2020,202012,11160.00000
4020 Honoraires de consultation,Aclient1,2021,202101,14490.00000
4020 Honoraires de consultation,Aclient1,2021,202102,14670.00000
4020 Honoraires de consultation,Aclient2,2020,202003,21045.00000
4020 Honoraires de consultation,Aclient2,2020,202004,13340.00000
4020 Honoraires de consultation,Aclient2C,2020,202006,15640.00000
4020 Honoraires de consultation,Aclient2,2020,202008,54165.00000
4020 Honoraires de consultation,Aclient2,2020,202010,51750.00000
4020 Honoraires de consultation,Aclient2,2020,202011,23000.00000
4020 Honoraires de consultation,Aclient2,2020,202012,19550.00000
4020 Honoraires de consultation,Aclient2,2021,202101,21850.00000
4020 Honoraires de consultation,Aclient2,2021,202102,23000.00000
4020 Honoraires de consultation,Aclient3,2020,202001,937.50000
4020 Honoraires de consultation,Aclient2,2020,202003,437.50000
</code></pre>
<p>I want to have difference of amount with same gl, same client with previous month</p>
<p>I tried this but does not work</p>
<pre><code># check frequency by month by gl
analytics = q1.groupby(['glnumber','nom','Year','YearMonth'])[['amount']].sum().reset_index()
# order
        
#add previous sales to the next row
if analytics['glnumber'] == analytics['glnumber'].shift(1) and analytics['nom'] == analytics['nom'].shift(1):
            analytics['prev_$'] = 0
else:
            analytics['prev_$'] = analytics['amount'].shift(1)
    
#drop the null values and calculate the difference
analytics = analytics.dropna()
analytics['diff'] = (analytics['amount'] - analytics['prev_$'])
analytics = analytics.drop(['prev_$'],
      axis='columns')
    
analytics['Perc_diff'] = np.where(analytics['amount']==0,0,analytics['diff']/analytics['amount'])
</code></pre>
<p>my if condition is not working due to this error:
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</p>
","['python', 'pandas', 'dataframe']",66772410,"<p>You need to check for NaN first and then compare. You can do it as follows in a single np.where condition.</p>
<pre><code>import pandas as pd
import numpy as np
from io import StringIO 
c = ['glnumber','nom','Year', 'YearMonth','nom_amount']
df = pd.read_csv(StringIO(d), sep = ',', header=None, names = c)
df = df.sort_values(by=['glnumber','nom','YearMonth'])
print (df.iloc[:,1:])
df['diff'] = np.where((((df.glnumber.isnull()) | (df.glnumber.shift(1).isnull()) | (df.glnumber == df.glnumber.shift(1))) &amp; 
                       ((df.nom.isnull()) | (df.nom.shift(1).isnull()) | (df.nom == df.nom.shift(1))) &amp; 
                       (df.YearMonth.diff() == 1)), df.nom_amount.diff(), 0)
print (df.iloc[:,1:])
</code></pre>
<p>I am checking if <code>glnumber</code> is null or <code>glnumber.shift(1)</code> is null. If they are not, then I am doing a comparison of both values to ensure they are same.</p>
<p>Similarly, for <code>df.nom</code>, checking if <code>df.nom</code> is null or <code>df.nom.shift(1)</code> is null. If not, compare both and see if they are same.</p>
<p>Then checking if the difference between the months is <code>1</code> as you want previous month only. If you want to exclude this and consider the previous line to be the previous month, thats OK too.</p>
<p>If it meets the condition, then find the difference between the <code>nom_amount</code> between the two lines. If the condition is not met, then set <code>np.NaN</code> as the value. Alternate, you can set the else to 0.</p>
<p>The output of this will be:</p>
<pre><code>          nom  Year  YearMonth  nom_amount       diff
17   Aclient1  2020     202007     9000.00       0.00
18   Aclient1  2020     202008    14040.00    5040.00
19   Aclient1  2020     202010    31185.00       0.00
20   Aclient1  2020     202011    14310.00  -16875.00
21   Aclient1  2020     202012    11160.00   -3150.00
22   Aclient1  2021     202101    14490.00       0.00
23   Aclient1  2021     202102    14670.00     180.00
24   Aclient2  2020     202003    21045.00       0.00
34   Aclient2  2020     202003      437.50       0.00
25   Aclient2  2020     202004    13340.00   12902.50
27   Aclient2  2020     202008    54165.00       0.00
28   Aclient2  2020     202010    51750.00       0.00
29   Aclient2  2020     202011    23000.00  -28750.00
30   Aclient2  2020     202012    19550.00   -3450.00
31   Aclient2  2021     202101    21850.00       0.00
32   Aclient2  2021     202102    23000.00    1150.00
26  Aclient2C  2020     202006    15640.00       0.00
33   Aclient3  2020     202001      937.50       0.00
0         NaN  2018     201809   234294.31       0.00
1         NaN  2018     201810   166337.95  -67956.36
2         NaN  2018     201811   250590.67   84252.72
3         NaN  2018     201812    92206.82 -158383.85
4         NaN  2019     201901   196868.71       0.00
5         NaN  2019     201902   148145.20  -48723.51
6         NaN  2019     201903   110973.24  -37171.96
7         NaN  2019     201904   184858.18   73884.94
8         NaN  2019     201905   119166.87  -65691.31
9         NaN  2019     201906    10428.10 -108738.77
10        NaN  2019     201907    19927.05    9498.95
11        NaN  2019     201908   -22677.79  -42604.84
12        NaN  2019     201909    -8560.00   14117.79
13        NaN  2020     202004      -26.25       0.00
14        NaN  2020     202007       -0.02       0.00
15        NaN  2021     202101     -105.00       0.00
16        NaN  2021     202103      104.99       0.00
</code></pre>
<p>Note that if glnumber and nom are <code>NaN</code> for the second group, then this may result in a small problem. Alternate, you can groupby and do the same.</p>
<p>Groupby will ensure that the <code>glnumber</code> and <code>nom</code> are same for comparison.</p>
",dataframe diff val previous row columns value match dataframe analytics glnumber nom Year YerarMonth nom amount Honoraires de consultation Honoraires de consultation Honoraires de consultation Honoraires de consultation Honoraires de consultation Honoraires de consultation Honoraires de consultation Honoraires de consultation Honoraires de consultation Honoraires de consultation Honoraires de consultation Honoraires de consultation Honoraires de consultation Honoraires de consultation Honoraires de consultation Honoraires de consultation Honoraires de consultation Honoraires de consultation Aclient Honoraires de consultation Aclient Honoraires de consultation Aclient Honoraires de consultation Aclient Honoraires de consultation Aclient Honoraires de consultation Aclient Honoraires de consultation Aclient Honoraires de consultation Aclient Honoraires de consultation Aclient Honoraires de consultation Aclient C Honoraires de consultation Aclient Honoraires de consultation Aclient Honoraires de consultation Aclient Honoraires de consultation Aclient Honoraires de consultation Aclient Honoraires de consultation Aclient Honoraires de consultation Aclient Honoraires de consultation Aclient I want difference amount gl client previous month I tried,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
66810295,2021-03-26,2021,2,Slowdown a pygame function without slowing the whole game,"<p>I'm trying to make my first game in pygame.</p>
<p>I want the game to have this animation in the background</p>
<p>I'm using python 3.9.2</p>
<pre><code>import pygame, math, random, sys
from pygame.locals import *

screen = pygame.display.set_mode((400, 400))
fps = 60

pygame.init()
mainClock = pygame.time.Clock()

# Colors 
black = (0, 0, 0)
white = (255, 255, 255)

# Function for close the game ------------------------------------------------------------------------------------------------------------

def close_game():
    pygame.quit()
    sys.exit()

# Functions for drawing ------------------------------------------------------------------------------------------------------------------

def background():
    screen.fill(black)

def canvas():
    margin = pygame.draw.rect(screen, white, (50, 50, 300, 300), 1)

def bar_animation():
    bar_width = 15
    for b in range(0, 20):
        bar_b_height = random.randint(10, 100)
        bar_b = pygame.draw.rect(screen, white, (50 + (bar_width * b), 350 - bar_b_height,    bar_width, bar_b_height), 0)


# Main game loop --------------------------------------------------------------------------------------------------------------------------

def main_loop():
    running = True
    while running:

        background()
        bar_animation()
        canvas()

        for event in pygame.event.get():
            if event.type == QUIT:
                close_game()

            pass

        pygame.display.update()
        mainClock.tick(fps)


# Run game --

main_loop()
</code></pre>
<p>On that code, I recreated what I want for the background of my game.
I want to have the bar_animation() function running slower without making the whole game or other functions run slower...</p>
<p>That code produce something like this:
<a href=""https://i.stack.imgur.com/pdfm9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pdfm9.png"" alt=""."" /></a></p>
<p>i recommend you to execute the code to understand what im doing</p>
","['python', 'python-3.x', 'pygame']",66810814,"<p>You can slow down your function by updating it once per several ticks:</p>
<pre><code>import pygame
import random

pygame.init()
screen = pygame.display.set_mode((400, 400))
mainClock = pygame.time.Clock()

running = True
bar_b_heights = []
while running:
    screen.fill((0, 0, 0))
    pygame.draw.rect(screen, (255, 255, 255), (50, 50, 300, 300), 1)

    if pygame.time.get_ticks() % 10 == 0 or not bar_b_heights:
        bar_b_heights = [random.randint(10, 100) for b in range(0, 20)]
    for i, bar_b_height in enumerate(bar_b_heights):
        pygame.draw.rect(screen, (255, 255, 255), (50 + (15 * i), 350 - bar_b_height, 15, bar_b_height), 0)

    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            running = False

    pygame.display.update()
    mainClock.tick(60)

pygame.display.quit()
pygame.quit()
</code></pre>
<p>Output:</p>
<p><a href=""https://i.stack.imgur.com/Aa1v3.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Aa1v3.gif"" alt=""enter image description here"" /></a></p>
",Slowdown pygame function without slowing whole game I trying make first game pygame I want game animation background I using python import pygame math random sys pygame locals import screen pygame display set mode fps pygame init mainClock pygame time Clock Colors black white Function close game def close game pygame quit sys exit Functions drawing def background screen fill black def canvas margin pygame draw rect screen white def bar animation bar width b range bar b height random randint bar b pygame draw rect screen white bar width b bar b height bar width bar b height Main game loop def main loop running True running background bar animation canvas event pygame event get event type QUIT close game pass pygame display update mainClock tick fps Run game main loop On code I recreated I want background game I want bar animation function running slower without making whole,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
66924508,2021-04-02,2021,3,How to count number of unique groups missing information in a groupby?,"<p>I have a dataframe that looks like this:</p>
<pre><code>data = [
    [&quot;AB&quot;, np.nan, 134, &quot;ID1&quot;],
    [&quot;AB&quot;, np.nan, 252, &quot;ID1&quot;],
    [&quot;BC&quot;, np.nan, 56, &quot;ID2&quot;],
    [&quot;CD&quot;, np.nan, 159, &quot;ID1&quot;],
    [&quot;DE&quot;, 32, np.nan, &quot;ID3&quot;],
]

df = pd.DataFrame(data, columns=[&quot;method&quot;, &quot;var_1&quot;, &quot;var_2&quot;, &quot;ID&quot;])
df
</code></pre>
<p>I am trying to get a count of unique IDs grouped by method which have missing values for <code>var_1</code> and <code>var_2</code> and am unable to find a way to do this.</p>
<p>I have been able to get counts of missing data using <code>count()</code> and <code>size()</code> and subtracting one from the other, but unfortunately I really need counts of unique IDs. It seems so simple I feel as though I must be missing something obvious!</p>
<p>My desired output is:</p>
<p>Where we are grouping by method and counting the number of unique IDs missing information for the other columns.</p>
<pre><code>method  var_1  var_2
  AB      1      0
  BC      1      0
  CD      1      0
  DE      0      1
</code></pre>
","['python', 'pandas', 'pandas-groupby']",66924576,"<p>To count unique IDs, check where it's null then <code>max</code> within [ID, method], to indicate any missing value within that [ID, method]. Then sum over the method to get the Number of unique IDS missing something.</p>
<pre><code>(df[['var_1', 'var_2']].isnull()
    .groupby([df['ID'], df['method']]).max()
    .sum(level='method')
</code></pre>
<hr />
<pre><code>        var_1  var_2
method              
AB          1      0
CD          1      0
BC          1      0
DE          0      1
</code></pre>
",How count number unique groups missing information groupby I dataframe looks like data quot AB quot np nan quot ID quot quot AB quot np nan quot ID quot quot BC quot np nan quot ID quot quot CD quot np nan quot ID quot quot DE quot np nan quot ID quot df pd DataFrame data columns quot method quot quot var quot quot var quot quot ID quot df I trying get count unique IDs grouped method missing values var var unable find way I able get counts missing data using count size subtracting one unfortunately I really need counts unique IDs It seems simple I feel though I must missing something obvious My desired output Where grouping method counting number unique IDs missing information columns method var var AB BC CD DE,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
66954962,2021-04-05,2021,2,which is the most efficient way to remove DataFrame rows based on a condition in pandas?,"<p>I have a non-optimal solution to a problem and I'm searching for a better one.</p>
<p>My data looks like this:</p>
<pre><code>df = pd.DataFrame(columns=['id', 'score', 'duration', 'user'],
                  data=[[1, 800, 60, 'abc'], [1, 900, 60, 'zxc'], [2, 800, 250, 'abc'], [2, 5000, 250, 'bvc'],
                        [3, 6000, 250, 'zxc'], [3, 8000, 250, 'klp'], [4, 1400, 500,'kod'],
                        [4, 8000, 500, 'bvc']])```
</code></pre>
<p>As you can see instances are pairs of identical ids with the same duration and different scores. My goal is to remove all id pairs that have a duration of less than 120 or where at least one user has a score less than 1500.</p>
<p>So far my solution is like this:</p>
<pre><code># remove instances with duration &gt; 120 (duration is the same for every instance of the same id)
df= df[df['duration'] &gt; 120]

# groupby id and get the min value of score
test= df.groupby('id')['score'].min().reset_index()

# then I can get a list of the id's where at least one user has a score below 1500 and remove both instances with the same id

for x in list(test[test['score'] &lt; 1500]['id']):
    df.drop(df.loc[df['id']==x].index, inplace=True)

</code></pre>
<p>However, the last bit is not very efficient and quite slow. I have around 700k instances in df and was wondering what is the most efficient way to remove all instances with id equal to the ones found in list(test[test['score'] &lt; 1500]['id']). Also a note, for simplicity i used an integer for id in this example but my id's are objects that have this kind of format 4240c195g794530fj4e10z53.</p>
<p>However, you're welcome to show me a better initial approach to this problem. Thanks!</p>
","['python', 'pandas', 'dataframe']",66955299,"<p>You can first create the condition , then groupby on the boolean column based on the id column and then transform with <code>all</code> to retain groups that satisfies the condition for all the rows in the group.</p>
<pre><code>#retain duration greater than or equal to (ge) 120 and id that has score ge 1500
cond = df['duration'].ge(120) &amp; df['score'].ge(1500)
out = df[cond.groupby(df['id']).transform('all')]
</code></pre>
<p>Or chaining them up in 1 line:</p>
<pre><code>out = df[(df['duration'].ge(120) &amp; df['score'].ge(1500))
                    .groupby(df['id']).transform('all')]
</code></pre>
<hr />
<pre><code>   id  score  duration user
4   3   6000       250  zxc
5   3   8000       250  klp
</code></pre>
",efficient way remove DataFrame rows based condition pandas I non optimal solution problem I searching better one My data looks like df pd DataFrame columns id score duration user data abc zxc abc bvc zxc klp kod bvc As see instances pairs identical ids duration different scores My goal remove id pairs duration less least one user score less So far solution like remove instances duration gt duration every instance id df df df duration gt groupby id get min value score test df groupby id score min reset index I get list id least one user score remove instances id x list test test score lt id df drop df loc df id x index inplace True However last bit efficient quite slow I around k instances df wondering efficient way remove instances id equal ones found list test test score lt id Also note simplicity used integer id,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
67328076,2021-04-30,2021,4,How to merge dictionary having same keys,"<p>I have a data structure like this:</p>
<pre><code>    [ {'SNAPSHOT': {'SnapshotVersion': '304'}},

      {'SNAPSHOT': {'SnapshotCreationDate': '2015-06-21 17:33:41'}},


      {'CafeData': {'CafeVersion': '2807'}}, 

      {'CafeData': {'IsSoftwareOnly': '1'}}, 

      {'CafeData'{'IsPassportTCPIP': '1'}} 
</code></pre>
<p>]</p>
<p>The output should like this:</p>
<pre><code> [ {'SNAPSHOT': {'SnapshotVersion': '304','SnapshotCreationDate': '2015-06-21 17:33:41'}},

   {'CafeData': {'CafeVersion': '2807','IsSoftwareOnly': '1','IsPassportTCPIP': '1'}} 
 
]
</code></pre>
","['python', 'list', 'dictionary']",67328147,"<p>Using <a href=""https://docs.python.org/3/library/collections.html#collections.defaultdict"" rel=""noreferrer"">https://docs.python.org/3/library/collections.html#collections.defaultdict</a> which creates a dict within the defaultdict anytime a new key is encountered.</p>
<pre><code>import collections as co

dd = co.defaultdict(dict)

l = [ {'SNAPSHOT': {'SnapshotVersion': '304'}},
      {'SNAPSHOT': {'SnapshotCreationDate': '2015-06-21 17:33:41'}},
      {'CafeData': {'CafeVersion': '2807'}}, 
      {'CafeData': {'IsSoftwareOnly': '1'}}, 
      {'CafeData': {'IsPassportTCPIP': '1'}} ]

for i in l: 
    for k,v in i.items(): 
        dd[k].update(v) 
</code></pre>
<p>Result:</p>
<pre><code>In [8]: dd
Out[8]: 
defaultdict(dict,
            {'SNAPSHOT': {'SnapshotVersion': '304',
              'SnapshotCreationDate': '2015-06-21 17:33:41'},
             'CafeData': {'CafeVersion': '2807',
              'IsSoftwareOnly': '1',
              'IsPassportTCPIP': '1'}})
</code></pre>
",How merge dictionary keys I data structure like SNAPSHOT SnapshotVersion SNAPSHOT CafeData CafeVersion CafeData IsSoftwareOnly CafeData IsPassportTCPIP The output like SNAPSHOT SnapshotVersion CafeData CafeVersion IsSoftwareOnly IsPassportTCPIP,"startoftags, python, list, dictionary, endoftags",python python3x list endoftags,python list dictionary,python python3x list,0.67
67420832,2021-05-06,2021,2,Pandas convert data from two tables into third table. Cross Referencing and converting unique rows to columns,"<p>I have the following tables:</p>
<p><strong>Table A</strong></p>
<pre><code>listData = {'id':[1,2,3],'date':['06-05-2021','07-05-2021','17-05-2021']}
pd.DataFrame(listData,columns=['id','date'])
</code></pre>
<img src=""https://i.stack.imgur.com/jeLaK.png"" width=""200"" />
<p><strong>Table B</strong></p>
<pre><code>detailData = {'code':['D123','F268','A291','D123','F268','A291'],'id':['1','1','1','2','2','2'],'stock':[5,5,2,10,11,8]}
pd.DataFrame(detailData,columns=['code','id','stock'])
</code></pre>
<img src=""https://i.stack.imgur.com/O1E5B.png"" width=""200"" />
<p><strong>OUTPUT TABLE</strong></p>
<pre><code>output = {'code':['D123','F268','A291'],'06-05-2021':[5,5,2],'07-05-2021':[10,11,8]}
pd.DataFrame(output,columns=['code','06-05-2021','07-05-2021'])
</code></pre>
<img src=""https://i.stack.imgur.com/cnrKh.png"" width=""400""/>
<p>Note: The code provided is hard coded code for the output. I need to generate the output table from Table A and Table B</p>
<p>Here is brief explanation of how the output table is generated if it is not self explanatory.</p>
<ol>
<li>The id column needs to be cross reference from Table A to Table B and the dates should be put instead in Table B</li>
<li>Then all the unique dates in Table B should be made into columns and the corresponding stock values need to be shifted to then newly created date columns.</li>
</ol>
<p>I am not sure where to start to do this. I am new to pandas and have only ever used it for simple data manipulation. If anyone can suggest me where to get started, it will be of great help.</p>
","['python', 'python-3.x', 'pandas']",67421733,"<p>Try:</p>
<pre><code>tableA['id'] = tableA['id'].astype(str)
tableB.merge(tableA, on='id').pivot('code', 'date', 'stock')
</code></pre>
<p>Output:</p>
<pre><code>date  06-05-2021  07-05-2021
code                        
A291           2           8
D123           5          10
F268           5          11
</code></pre>
<p>Details:</p>
<ul>
<li>First, <code>merge</code> on id, this is like doing a SQL join.  First, the
dtypes much match, hence using <code>astype</code> to str.</li>
<li>Next, reshape the dataframe using <code>pivot</code> to get code by date.</li>
</ul>
",Pandas convert data two tables third table Cross Referencing converting unique rows columns I following tables Table A listData id date pd DataFrame listData columns id date Table B detailData code D F A D F A id stock pd DataFrame detailData columns code id stock OUTPUT TABLE output code D F A pd DataFrame output columns code Note The code provided hard coded code output I need generate output table Table A Table B Here brief explanation output table generated self explanatory The id column needs cross reference Table A Table B dates put instead Table B Then unique dates Table B made columns corresponding stock values need shifted newly created date columns I sure start I new pandas ever used simple data manipulation If anyone suggest get started great help,"startoftags, python, python3x, pandas, endoftags",python pandas numpy endoftags,python python3x pandas,python pandas numpy,0.67
67561501,2021-05-16,2021,2,splitting list in dataframe columns to separate columns,"<p>my data frame looks like as follows</p>
<pre><code>    col1     col2     col3
0  [1, a]  [1, a1]  [1, a2]
1  [2, b]  [2, b1]  [2, b2]
2  [3, c]  [3, c1]  [3, c2]
</code></pre>
<p>I need to make it look like:</p>
<pre><code>   col1     col2     col3  col4
0  a         a1      a2    1
1  b         b1      b2    2
2  c         c1      c2    3
</code></pre>
<hr />
<p>My code</p>
<pre><code>import pandas as pd

d = {'col1':[[1,'a'],[2,'b'],[3,'c']],
     'col2':[[1,'a1'],[2,'b1'],[3,'c1']],
     'col3':[[1,'a2'],[2,'b2'],[3,'c2']]}

df = pd.DataFrame.from_dict(d)
</code></pre>
<p>so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success</p>
","['python', 'pandas', 'dataframe']",67561636,"<p>Here is a way using <code>applymap</code> and <code>map</code>:</p>
<pre><code>df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))
</code></pre>
",splitting list dataframe columns separate columns data frame looks like follows col col col b b b c c c I need make look like col col col col b b b c c c My code import pandas pd col b c col b c col b c df pd DataFrame dict far I tried using apply pd Series iterating loop reassign values success,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
67613400,2021-05-20,2021,2,Counting comma separated string in dataframe in a new column,"<p>I have the following df:</p>
<pre><code>df = pd.DataFrame({'Name': ['John', 'Sara', 'Paul', 'Guest'], 'Interaction': ['share,like,share,like,like,like', 'love,like,share,like,love,like', 'share,like,share,like,like,like,share,like,share,like,like,hug','share,like,care,like,like,like']})

Name    Interaction
0   John    share,like,share,like,like,like
1   Sara    love,like,share,like,love,like
2   Paul    share,like,share,like,like,like,share,like,sha...
3   Guest   share,like,care,like,like,like
</code></pre>
<p>I would like to create a third column calculating the number of single interactions as <code>int</code></p>
<p>What I did:</p>
<pre><code>df['likes'] = df[df['Interaction'] == 'like'].groupby('Name')['Interaction'].transform(lambda x: x[x.str.contains('like')].count())
</code></pre>
<p>I did the same line for share, care .. etc
But it does not work!</p>
<pre><code>Name    Interaction                                           likes     shares
0   John    share,like,share,like,like,like                     NaN     NaN
1   Sara    love,like,share,like,love,like                      NaN     NaN
2   Paul    share,like,share,like,like,like,share,like,sha...   NaN     NaN
3   Guest   share,like,care,like,like,like                      NaN     NaN
</code></pre>
<p>How can I count each interaction as <code>int</code> and then found the total per row in a final column?</p>
<p>Thanks</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",67613468,"<p>First you need to <code>str.split</code> the column on the comma, expand the result to create a dataframe, <code>stack</code> to get a series and use <code>str.get_dummies</code> that will create a column for each different word and add 1 for the corresponding value in the series. Finally <code>sum</code> on level=0 to go back to original shape. <code>join</code> the result to the original dataframe</p>
<pre><code>df = df.join( df['Interaction'].str.split(',', expand=True)
                .stack()
                .str.get_dummies()
                .sum(level=0)
            )
print(df)
    Name                                        Interaction  care  hug  like  \
0   John                    share,like,share,like,like,like     0    0     4   
1   Sara                     love,like,share,like,love,like     0    0     3   
2   Paul  share,like,share,like,like,like,share,like,sha...     0    1     7   
3  Guest                     share,like,care,like,like,like     1    0     4   

   love  share  
0     0      2  
1     2      1  
2     0      4  
3     0      1  
</code></pre>
",Counting comma separated string dataframe new column I following df df pd DataFrame Name John Sara Paul Guest Interaction share like share like like like love like share like love like share like share like like like share like share like like hug share like care like like like Name Interaction John share like share like like like Sara love like share like love like Paul share like share like like like share like sha Guest share like care like like like I would like create third column calculating number single interactions int What I df likes df df Interaction like groupby Name Interaction transform lambda x x x str contains like count I line share care etc But work Name Interaction likes shares John share like share like like like NaN NaN Sara love like share like love like NaN NaN Paul share like share like like like share,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
67670063,2021-05-24,2021,3,Sum column values for each row,"<p>I would sum each row of this dataframe based on a column. <br></p>
<p>Input Dataframe:</p>
<pre><code>    a   b
0   red 5
1   red 8
2   red 2
3   red 4
4   red 1
</code></pre>
<p>I used this code to sum values <code>sumcol = df['b'].sum()</code> but this sum the entire column I would a value for each row -1 in a new column.</p>
<p>Expected Dataframe:</p>
<pre><code>    a   b   c
0   red 5   15
1   red 8   7
2   red 2   5
3   red 4   1
4   red 1   0
</code></pre>
","['python', 'pandas', 'dataframe']",67670259,"<p>Use <code>apply lambda</code> with <code>axis=1</code>. <br>
<code>x.name</code> will give you the current index for each row. Simple make slices for each row using <code>x.name+1:</code> which means get all below rows.</p>
<pre><code>df[&quot;c&quot;] = df.apply(lambda x: df.loc[x.name+1:,&quot;b&quot;].sum(),axis=1)

print(df)

         a  b   c
    0   red 5   15
    1   red 8   7
    2   red 2   5
    3   red 4   1
    4   red 1   0

</code></pre>
<p>And if you wants to get summation/number_of_values which seems like a case of <code>mean()</code> then try:</p>
<pre><code>df[&quot;c&quot;] = df.apply(lambda x: df.loc[x.name+1:,&quot;b&quot;].mean(),axis=1)
print(df)

         a  b    c
    0   red 5   3.750000
    1   red 8   2.333333
    2   red 2   2.500000
    3   red 4   1.000000
    4   red 1   NaN

</code></pre>
",Sum column values row I would sum row dataframe based column Input Dataframe b red red red red red I used code sum values sumcol df b sum sum entire column I would value row new column Expected Dataframe b c red red red red red,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
67714397,2021-05-27,2021,4,Drop Duplicates on dataframe on value inside dictionary in column,"<p>I am having problems to use the proper pandas function to drop rows in dataframe of duplicate value of a key inside a dict in one of its column <em>lugar</em>.
<a href=""https://i.stack.imgur.com/02qFA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/02qFA.png"" alt=""enter image description here"" /></a></p>
<p>This is the data of the dataframe:</p>
<pre><code>{'calculado': {73683: '2021-05-27 00:43:46',
  83767: '2021-05-27 00:43:46',
  103395: '2021-05-27 00:43:46',
  105314: '2021-05-27 00:43:46',
  116555: '2021-05-27 00:43:46',
  120764: '2021-05-27 00:43:46',
  120892: '2021-05-27 00:43:46',
  122760: '2021-05-27 00:43:46',
  124269: '2021-05-27 00:43:46',
  125707: '2021-05-27 00:43:46'},
 'geom': {73683: '17.649999999999995,-93.65',
  83767: '15.55,-93.25',
  103395: '11.45,-98.45',
  105314: '11.049999999999997,-98.55',
  116555: '8.75,-78.45',
  120764: '7.849999999999997,-89.54999999999998',
  120892: '7.849999999999997,-76.75',
  122760: '7.449999999999998,-81.95',
  124269: '7.149999999999999,-75.04999999999998',
  125707: '6.849999999999998,-75.25'},
 'lat': {73683: 17.649999999999995,
  83767: 15.55,
  103395: 11.45,
  105314: 11.049999999999997,
  116555: 8.75,
  120764: 7.849999999999997,
  120892: 7.849999999999997,
  122760: 7.449999999999998,
  124269: 7.149999999999999,
  125707: 6.849999999999998},
 'lon': {73683: -93.65,
  83767: -93.25,
  103395: -98.45,
  105314: -98.55,
  116555: -78.45,
  120764: -89.54999999999998,
  120892: -76.75,
  122760: -81.95,
  124269: -75.04999999999998,
  125707: -75.25},
 'lugar': {73683: {'distancia': 12.55,
   'mensaje': '13 kms. de Huimanguillo, Tabasco, MÃ©xico',
   'nombre': 'Huimanguillo, Tabasco, MÃ©xico',
   'pais': 'mx'},
  83767: {'distancia': 16.74,
   'mensaje': '17 kms. de Pijijiapan, Chiapas, 30540, MÃ©xico',
   'nombre': 'Pijijiapan, Chiapas, 30540, MÃ©xico',
   'pais': 'mx'},
  103395: 'Mar abierto',
  105314: 'Mar abierto',
  116555: {'distancia': 6.7,
   'mensaje': '7 kms. de RÃ­o Congo Arriba, Distrito Santa Fe, DariÃ©n, PanamÃ¡',
   'nombre': 'RÃ­o Congo Arriba, Distrito Santa Fe, DariÃ©n, PanamÃ¡',
   'pais': 'pa'},
  120764: 'Mar abierto',
  120892: {'distancia': 5.83,
   'mensaje': '6 kms. de Veraguas, PanamÃ¡',
   'nombre': 'Veraguas, PanamÃ¡',
   'pais': 'co'},
  122760: {'distancia': 100.26,
   'mensaje': '100 kms. de Veraguas, PanamÃ¡',
   'nombre': 'Veraguas, PanamÃ¡',
   'pais': 'pa'},
  124269: {'distancia': 12.09,
   'mensaje': '12 kms. de AnorÃ­, Nordeste, Antioquia, RegiÃ³n Andina, 052857, Colombia',
   'nombre': 'AnorÃ­, Nordeste, Antioquia, RegiÃ³n Andina, 052857, Colombia',
   'pais': 'co'},
  125707: {'distancia': 4.03,
   'mensaje': '4 kms. de Guadalupe, Norte, Antioquia, RegiÃ³n Andina, Colombia',
   'nombre': 'Guadalupe, Norte, Antioquia, RegiÃ³n Andina, Colombia',
   'pais': 'co'}},
 'valor': {73683: 198,
  83767: 198,
  103395: 197,
  105314: 198,
  116555: 198,
  120764: 198,
  120892: 198,
  122760: 198,
  124269: 196,
  125707: 198},
 'variable': {73683: 'T',
  83767: 'T',
  103395: 'T',
  105314: 'T',
  116555: 'T',
  120764: 'T',
  120892: 'T',
  122760: 'T',
  124269: 'T',
  125707: 'T'}}
</code></pre>
<p>As you can see, the <strong>lugar</strong> column has a dictionary and one of the keys is <strong>nombre</strong> in this case the value: <em>Veraguas, PanamÃ¡</em> is duplicated, I will like to drop duplicates rows of dataframe and keep one row only per name for the rows that has the dict and key in <strong>lugar</strong> column.</p>
<p>One approach I have tried is to create a new column with the value of the key and then run drop_duplicates but I am unable to get the value inside the column. but I am able to get it for the first row like this</p>
<p><code>df_asc['lugar'].iloc[0]['nombre']</code> -&gt; <em>Huimanguillo, Tabasco, MÃ©xico</em></p>
<p>Is there a way to do this without looping the df doing it manually?  I am really new to Python and Pandas.</p>
<p>EDITED: Expected result I converted to csv to be able to delete in spreadsheet as I am unable to do it with pandas...
<a href=""https://i.stack.imgur.com/rNwlp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rNwlp.png"" alt=""enter image description here"" /></a></p>
","['python', 'pandas', 'dataframe']",67714648,"<p>An option via <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html#pandas-dataframe-loc"" rel=""nofollow noreferrer""><code>loc</code></a> + <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.duplicated.html#pandas-series-duplicated"" rel=""nofollow noreferrer""><code>duplicated</code></a> + <a href=""https://pandas.pydata.org/docs/reference/api/pandas.Series.isna.html#pandas-series-isna"" rel=""nofollow noreferrer""><code>isna</code></a>:</p>
<pre><code>s = df['lugar'].str['nombre']
df.loc[~s.duplicated() | s.isna()]
</code></pre>
<pre><code>                  calculado  ... variable
73683   2021-05-27 00:43:46  ...        T
83767   2021-05-27 00:43:46  ...        T
103395  2021-05-27 00:43:46  ...        T
105314  2021-05-27 00:43:46  ...        T
116555  2021-05-27 00:43:46  ...        T
120764  2021-05-27 00:43:46  ...        T
120892  2021-05-27 00:43:46  ...        T
124269  2021-05-27 00:43:46  ...        T
125707  2021-05-27 00:43:46  ...        T

[9 rows x 7 columns]
</code></pre>
",Drop Duplicates dataframe value inside dictionary column I problems use proper pandas function drop rows dataframe duplicate value key inside dict one column lugar This data dataframe calculado geom lat lon lugar distancia mensaje kms de Huimanguillo Tabasco M xico nombre Huimanguillo Tabasco M xico pais mx distancia mensaje kms de Pijijiapan Chiapas M xico nombre Pijijiapan Chiapas M xico pais mx Mar abierto Mar abierto distancia mensaje kms de R Congo Arriba Distrito Santa Fe Dari n Panam nombre R Congo Arriba Distrito Santa Fe Dari n Panam pais pa Mar abierto distancia mensaje kms de Veraguas Panam nombre Veraguas Panam pais co distancia mensaje kms de Veraguas Panam nombre Veraguas Panam pais pa distancia mensaje kms de Anor Nordeste Antioquia Regi n Andina Colombia nombre Anor Nordeste Antioquia Regi n Andina Colombia pais co distancia mensaje kms de Guadalupe Norte Antioquia Regi n Andina Colombia nombre Guadalupe Norte,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
67817426,2021-06-03,2021,2,New line as a separator in python,"<p>I have following code to get lines with string i need</p>
<pre><code>import pyscp
f = open(&quot;base.txt&quot;, &quot;r&quot;)
searchlines = f.readlines()
f.close()
for i, line in enumerate(searchlines):
    if &quot;ÑÐ¼ÐµÑÑÑ&quot; in line: 
        for l in searchlines[i: i+3]: print(l)
</code></pre>
<p>For example, i am searching for &quot;ÑÐ¼ÐµÑÑÑ&quot;
It gives me following</p>
<pre><code>SCP-1056-RU-V - ÐÐ¸Ð·Ð½Ñ Ð¸ ÑÐ¼ÐµÑÑÑ

SCP-1056-RU - Ð¢Ð²Ð¾ÑÑÐµÑÐºÐ¸Ðµ Ð¼ÐµÑÐ°Ð½Ð¸Ñ

SCP-2061 - Ð¦ÐµÐ»Ð°Ñ Ð¼ÐµÑÑÐ½Ð°Ñ ÑÐµÐ¼ÑÑ Ð¿Ð¾Ð´Ð°Ð²Ð¸Ð»Ð°ÑÑ Ð½Ð°ÑÐ¼ÐµÑÑÑ Ð¾Ð´Ð½Ð¸Ð¼ ÐºÐ°Ð»ÑÐºÑÐ»ÑÑÐ¾ÑÐ¾Ð¼

SCP-2062 - ÐÑÐ¾Ð½ÐµÐºÐµÑ

SCP-2668 - ÐÐ´ÑÑÐ¸Ðµ Ð½Ð° ÑÐ¼ÐµÑÑÑ Ð¿ÑÐ¸Ð²ÐµÑÑÑÐ²ÑÑÑ ÑÐµÐ±Ñ!

SCP-2669 - ÐÐµÐ±ÑÐµÑÐ» 1

SCP-510 - ÐÑÐ³ÐºÐ°Ñ ÑÐ¼ÐµÑÑÑ

SCP-5115 - ÐÐµ Ð¾Ñ Ð¼Ð¸ÑÐ° ÑÐµÐ³Ð¾

SÐ¡P-3984 - ÐÐ¾ÑÑÐºÐ°Ð¹ Ð² ÑÐ¼ÐµÑÑÑ Ð¿Ð°Ð»Ð¾ÑÐºÐ¾Ð¹

SÐ¡P-4220 - Ð¢ÐµÐ¼Ð½Ð°Ñ ÑÑÐ¾ÑÐ¾Ð½Ð° ÐÑÐ½Ñ
</code></pre>
<p>I want to make list like this
[scp-xxx--ejdj, scp-xxxx-jddj...]
So i need to use new line as a separator, how can i do that?
linesplite doesn't work here.</p>
","['python', 'python-3.x', 'list']",67817565,"<p>You can use <code>str.split()</code> and firs value is your <code>SCP-xxx</code> variable:</p>
<pre class=""lang-py prettyprint-override""><code>f = open(&quot;your_file.txt&quot;, &quot;r&quot;)
searchlines = [line.strip() for line in f.readlines() if line.strip()] # &lt;-- skip empty lines
f.close()

out = []
for i, line in enumerate(searchlines):
    if &quot;ÑÐ¼ÐµÑÑÑ&quot; in line:
        for l in searchlines[i : i + 3]:
            out.append(l.split(maxsplit=1)[0])  # &lt;-- add to out the first value

print(out)
</code></pre>
<p>Prints:</p>
<pre class=""lang-none prettyprint-override""><code>['SCP-1056-RU-V', 'SCP-1056-RU', 'SCP-2061', 'SCP-2061', 'SCP-2062', 'SCP-2668', 'SCP-2668', 'SCP-2669', 'SCP-510', 'SCP-510', 'SCP-5115', 'SÐ¡P-3984', 'SÐ¡P-3984', 'SÐ¡P-4220']
</code></pre>
",New line separator python I following code get lines string need import pyscp f open quot base txt quot quot r quot searchlines f readlines f close line enumerate searchlines quot quot line l searchlines print l For example searching quot quot It gives following SCP RU V SCP RU SCP SCP SCP SCP SCP SCP S P S P I want make list like scp xxx ejdj scp xxxx jddj So need use new line separator linesplite work,"startoftags, python, python3x, list, endoftags",python python3x list endoftags,python python3x list,python python3x list,1.0
67836590,2021-06-04,2021,2,Pandas: New column from conditions and from another data frame,"<p>I have two data frames, df1 and df2.</p>
<pre><code>df1                df2
A   B              C   D   E
ad  df             ad  se  1
ad  se             xc  je  2
xc  je             ad  df  3
...                ...
</code></pre>
<p>I need to create a new column in <strong>df1</strong> with the values contained in column <strong>E</strong> from <strong>df2</strong>. For this, I need to find the matching values between columns <strong>A</strong> and <strong>C</strong>, and from  <strong>B</strong> and <strong>D</strong> from <strong>df1</strong> and <strong>df2</strong> respectively.</p>
<pre><code>where A == C &amp; B == D 
      XX = E
</code></pre>
<p>The result should be like this:</p>
<pre><code>df1                
A   B   XX         
ad  df  3          
ad  se  1          
xc  je  2         
...                
</code></pre>
<p>This might be straightforward but I'm quite new to pandas and haven't really found a way to accomplish this.</p>
<p>Any advice is very welcome!</p>
","['python', 'pandas', 'dataframe']",67836698,"<p>try this:</p>
<pre><code>pd.merge(df1, df2.rename(columns={'C':'A', 'D':'B'}), on=['A', 'B'], how='left')
</code></pre>
",Pandas New column conditions another data frame I two data frames df df df df A B C D E ad df ad se ad se xc je xc je ad df I need create new column df values contained column E df For I need find matching values columns A C B D df df respectively A C amp B D XX E The result like df A B XX ad df ad se xc je This might straightforward I quite new pandas really found way accomplish Any advice welcome,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
48049635,2018-01-01,2018,3,Transform pandas dataframe to another layout,"<p>I have a dataframe that looks like this:</p>

<pre><code>  column1  column2  column3
0       A    0.020     0.76
1       B    0.045     1.30
2       C    0.230     0.32
3       D    0.130     0.67
</code></pre>

<p>I would like to modify this dataframe structure to make it look like this:</p>

<pre><code>column1  newCol 
A        column2    0.020
         column3    0.760
B        column2    0.045
         column3    1.300
C        column2    0.230
         column3    0.320
D        column2    0.130
         column3    0.670
Name: value, dtype: float64
</code></pre>

<p>Where <code>column1</code>, <code>column2</code>, <code>column3</code>, <code>newCol</code> are the names for the columns
A, B, C, D are unique values for rows</p>

<p>My problem is that I don't know how to convert <code>column1</code> and <code>column2</code> from columns to rows in the new dataframe.</p>
","['python', 'pandas', 'dataframe']",48049702,"<p>Use <code>melt</code> + <code>set_index</code> + <code>sort_index</code> - </p>

<pre><code>df.melt('column1', var_name='newCol')\
  .set_index(['column1', 'newCol'])\
  .sort_index().value

column1  newCol 
A        column2    0.020
         column3    0.760
B        column2    0.045
         column3    1.300
C        column2    0.230
         column3    0.320
D        column2    0.130
         column3    0.670
Name: value, dtype: float64
</code></pre>

<p>Works with <code>v0.20</code> and above. For older versions, use <code>pd.melt</code> instead.</p>
",Transform pandas dataframe another layout I dataframe looks like column column column A B C D I would like modify dataframe structure make look like column newCol A column column B column column C column column D column column Name value dtype float Where column column column newCol names columns A B C D unique values rows My problem I know convert column column columns rows new dataframe,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
48050695,2018-01-01,2018,2,how to sort JSON float data before printing out,"<p>I'm trying to get data in JSON file, but before I print out, I want to sort the <code>money</code>. This is my code</p>

<pre><code>with open(""cash.json"", ""r"", encoding=""utf-8"") as f:
    data = json.load(f)
    for c in data['NotAdmin']:
        a_new_list = sorted(c[""money""])
        print(a_new_list)
</code></pre>

<p>This is what is inside my <code>cash.json</code> file</p>

<pre><code>{
    ""NotAdmin"": [
        {
            ""money"": 200.0,
            ""name"": ""Sam"",
            ""password"": ""1234""
        }, {
            ""money"": 150.0,
            ""name"": ""Jasmin"",
            ""password"": ""1573""
        }, {
            ""money"": 100.0,
            ""name"": ""Ali"",
            ""password"": ""1856""
        }
    ],
    ""admin"": [
        {
            ""name"": ""Adam"",
            ""password"": ""6767""
        }, {
            ""name"": ""Eva"",
            ""password"": ""2222""
        }
    ]
}
</code></pre>

<p>Im keep getting this error <code>TypeError: 'float' object is not iterable</code></p>
","['python', 'python-3.x', 'dictionary']",48050749,"<p>The reason that you are getting the <code>TypeError</code> is that you were iterating through the <code>dictionaries</code> in the <code>NotAdmin</code> <code>list</code> and trying to apply to the <code>sorted</code> functions to the <code>money</code> attributes. This will obviously fail as you are trying to ""sort"" a <code>float</code>(?).</p>

<p>So, the way the <code>sorted</code> (or <code>sort</code> function which modifies the <code>iterable</code> in place) works is to take an <code>iterable</code> and apply an ""evaluation"" function to each element, in order to sort them. This function can either be passed in under the <code>key</code> parameter, or if not passed in, Python will automatically decide how to sort the elements.</p>

<p>To apply this to our code, we want to take the <code>data[""NotAdmin""]</code> list and apply the <code>.sort</code> method with a <code>key</code> to select the <code>money</code> attribute of each of the dictionaries. We can do this with an (anonymous) <code>lambda</code> function. </p>

<p>So, here is the full code:</p>

<pre><code>with open(""cash.json"", ""r"", encoding=""utf-8"") as f:
    data = json.load(f)
    data[""NotAdmin""].sort(key = lambda d: d[""money""])
    print(data)
</code></pre>

<p>output:</p>

<pre><code>{'admin': [{'password': '6767', 'name': 'Adam'}, {'password': '2222', 'name': 'Eva'}], 'NotAdmin': [{'password': '1856', 'money': 100.0, 'name': 'Ali'}, {'password': '1573', 'money': 150.0, 'name': 'Jasmin'}, {'password': '1234', 'money': 200.0, 'name': 'Sam'}]}
</code></pre>

<hr>

<p>And also, if you want to have the elements in <em>descending</em> order, then we can take a look at <a href=""https://docs.python.org/3/library/stdtypes.html#list.sort"" rel=""nofollow noreferrer"">the documentation</a> and see that we can set the <code>reverse</code> flag to <code>True</code>.</p>

<p>Which would just change the line to:</p>

<pre><code>...
    ...
    data[""NotAdmin""].sort(key = lambda d: d[""money""], reverse = True)
    ...
</code></pre>
",sort JSON float data printing I trying get data JSON file I print I want sort money This code open cash json r encoding utf f data json load f c data NotAdmin new list sorted c money print new list This inside cash json file NotAdmin money name Sam password money name Jasmin password money name Ali password admin name Adam password name Eva password Im keep getting error TypeError float object iterable,"startoftags, python, python3x, dictionary, endoftags",python python3x pandas endoftags,python python3x dictionary,python python3x pandas,0.67
48100396,2018-01-04,2018,2,How to rotate Pandas Dataframe MultiIndex Rows into MultiIndex Columns?,"<p>I was wondering if someone can help me with this problem.</p>

<p>If I have a simple dataframe:</p>

<pre><code>  one  two three  four
0   A    1     a    1
1   A    2     b    2
2   B    1     c    3
3   B    2     d    4
4   C    1     e    5
5   C    2     f    6
</code></pre>

<p>I can easily create a multi-index on the rows by issuing:</p>

<pre><code>a.set_index(['one', 'two'])

         three   four
one two      
A    1     a       1
     2     b       2
B    1     c       3
     2     d       4
C    1     e       5
     2     f       6
</code></pre>

<p>Is there a similarly easy way to create a multi-index on the columns?</p>

<p>Here's the code to get up to this point.</p>

<pre><code>import numpy as np
import pandas as pd
df=[['A','1','a','1'],['A','2','b','2'],['B','1','c','3'],
    ['B','2','d','4'],['C','1','e','5'],['C','2','f','6']]
df=pd.DataFrame(df)
df.columns=['one','two','three','four']
df.set_index(['one','two'])
</code></pre>

<p>I'd like to end up with:</p>

<pre><code>       A            B           C
two    three four   three four  three four
1       a      1     c      3     e    5
2       b      2     d      4     f    6
</code></pre>

<p>Thanks.</p>
","['python', 'python-3.x', 'pandas']",48100492,"<p>This can be done using <code>unstack</code> + <code>swaplevel</code> + <code>sort_index</code> - </p>

<pre><code>df

        three four
one two           
A   1       a    1
    2       b    2
B   1       c    3
    2       d    4
C   1       e    5
    2       f    6
</code></pre>

<p></p>

<pre><code>df = df.unstack(0)
df.columns = df.columns.swaplevel()

df.sort_index(axis=1)

one    A          B          C      
    four three four three four three
two                                 
1      1     a    3     c    5     e
2      2     b    4     d    6     f
</code></pre>
",How rotate Pandas Dataframe MultiIndex Rows MultiIndex Columns I wondering someone help problem If I simple dataframe one two three four A A b B c B C e C f I easily create multi index rows issuing set index one two three four one two A b B c C e f Is similarly easy way create multi index columns Here code get point import numpy np import pandas pd df A A b B c B C e C f df pd DataFrame df df columns one two three four df set index one two I like end A B C two three four three four three four c e b f Thanks,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
48122276,2018-01-05,2018,2,Changing values in a dataframe based on condition,"<p>Here is the dataframe which I have:</p>

<pre><code>import pandas as pd
list = ['RK','G', 'CUSTOM', 'CUSTOM', 'KL', 'kj']
df=pd.DataFrame(list,columns=['A'])
</code></pre>

<p>What I would like to do here is to find out all the entries in column <code>A</code> which are equal to <code>CUSTOM</code> and replace it with <code>CUSTOM1</code> AND <code>CUSTOM2</code>.</p>

<p>The output should be:</p>

<pre><code>       A
0      RK
1      G
2    CUSTOM1
3    CUSTOM2
4     KL
5     kj
</code></pre>

<p>Had there been 3 <code>CUSTOM</code> instead of 2, the output should be <code>CUSTOM1</code> , <code>CUSTOM2</code> and <code>CUSTOM3</code>.</p>

<p>Thanks.</p>
","['python', 'pandas', 'dataframe']",48122395,"<p>Using <code>cumsum</code> + <code>np.where</code> - </p>

<pre><code>m = df.A.eq('CUSTOM')
df.A = np.where(m, df.A + m.cumsum().astype(str), df.A)

df
         A
0       RK
1        G
2  CUSTOM1
3  CUSTOM2
4       KL
5       kj
</code></pre>

<hr>

<p>A similar solution using <code>pd.Series.where</code>/<code>mask</code> - </p>

<pre><code>df.A = df.A.where(~m, df.A + m.cumsum().astype(str))
</code></pre>

<p>Or,</p>

<pre><code>df.A = df.A.mask(m, df.A + m.cumsum().astype(str))
</code></pre>

<p></p>

<pre><code>df
         A
0       RK
1        G
2  CUSTOM1
3  CUSTOM2
4       KL
5       kj
</code></pre>
",Changing values dataframe based condition Here dataframe I import pandas pd list RK G CUSTOM CUSTOM KL kj df pd DataFrame list columns A What I would like find entries column A equal CUSTOM replace CUSTOM AND CUSTOM The output A RK G CUSTOM CUSTOM KL kj Had CUSTOM instead output CUSTOM CUSTOM CUSTOM Thanks,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
48144828,2018-01-08,2018,4,How to replace certain values in a pandas column with the mean column value of similar rows?,"<h1>The Problem</h1>

<p>I currently have a pandas dataframe with property information from <a href=""https://www.kaggle.com/new-york-city/nyc-property-sales"" rel=""nofollow noreferrer"">this</a> kaggle dataset. The following is an example dataframe from that set:</p>

<pre><code>| neighborhood  | borough | block | year built | ... |
------------------------------------------------------
| Annadale      | 5       | 5425  | 2015       | ... |
| Woodside      | 4       | 2327  | 1966       | ... |
| Alphabet City | 1       | 396   | 1985       | ... |
| Alphabet City | 1       | 405   | 1996       | ... |
| Alphabet City | 1       | 396   | 1986       | ... |
| Alphabet City | 1       | 396   | 1992       | ... |
| Alphabet City | 1       | 396   | 0          | ... |
| Alphabet City | 1       | 396   | 1990       | ... |
| Alphabet City | 1       | 396   | 1984       | ... |
| Alphabet City | 1       | 396   | 0          | ... |
</code></pre>

<p>What I want to do is take every row where the value in the ""year built"" column equals zero, and replace the ""year built"" value in those rows with the median of the ""year built"" values in the rows with the same neighborhood, borough, and block. There are cases where there are multiple rows within a {neighborhood, borough, block} set that have a zero in the ""year built"" column. This is shown in the example dataframe above.</p>

<p>To illustrate the problem I put these two rows in the example dataframe. </p>

<pre><code>| neighborhood  | borough | block | year built | ... |
------------------------------------------------------
| Alphabet City | 1       | 396   | 0          | ... |
| Alphabet City | 1       | 396   | 0          | ... |
</code></pre>

<p>To solve the problem I want to use the mean of the ""year built"" values from all the other rows that had the same neighborhood, borough, and block to fill the ""year built"" value in rows that had a zero in the ""year built"" column. For the example rows the neighborhood is Alphabet City, the borough is 1, and the block is 396 so I would use the following matching rows from the example dataframe to calculate the mean:</p>

<pre><code>| neighborhood  | borough | block | year built | ... |
------------------------------------------------------
| Alphabet City | 1       | 396   | 1985       | ... |
| Alphabet City | 1       | 396   | 1986       | ... |
| Alphabet City | 1       | 396   | 1992       | ... |
| Alphabet City | 1       | 396   | 1990       | ... |
| Alphabet City | 1       | 396   | 1984       | ... |
</code></pre>

<p>I would take the mean of the ""year built"" column from those rows (which is 1987.4) and replace the zeros with the mean. The rows that originally had zeros would end up looking like this:</p>

<pre><code>| neighborhood  | borough | block | year built | ... |
------------------------------------------------------
| Alphabet City | 1       | 396   | 1987.4     | ... |
| Alphabet City | 1       | 396   | 1987.4     | ... |
</code></pre>

<h1>The code I have so far</h1>

<p>All I've managed to do so far is chop out rows with zeros in the ""year built"" column and find the mean year of every {neighborhood, borough, block} set. The original dataframe is stored in raw_data and it looks like the example dataframe at the very top of this post. The code looks like this:</p>

<pre><code># create a copy of the data
temp_data = raw_data.copy()

# remove all rows with zero in the ""year built"" column
mean_year_by_location = temp_data[temp_data[""YEAR BUILT""] &gt; 0]

# group the rows into {neighborhood, borough, block} sets and take the mean of the ""year built"" column in those sets
mean_year_by_location = mean_year_by_location.groupby([""NEIGHBORHOOD"",""BOROUGH"",""BLOCK""], as_index = False)[""YEAR BUILT""].mean()
</code></pre>

<p>and the output looks like this:</p>

<pre><code>| neighborhood  | borough | block | year built | 
------------------------------------------------
| ....          | ...     | ...   | ...        |
| Alphabet City | 1       | 390   | 1985.342   | 
| Alphabet City | 1       | 391   | 1986.76    | 
| Alphabet City | 1       | 392   | 1992.8473  | 
| Alphabet City | 1       | 393   | 1990.096   | 
| Alphabet City | 1       | 394   | 1984.45    | 
</code></pre>

<p>So how can I take those average ""year built"" values from the mean_year_by_location dataframe and replace the zeros in the original raw_data dataframe?</p>

<p>I apologize for the long post. I just wanted to be really clear.</p>
","['python', 'pandas', 'dataframe']",48144871,"<p>Use <code>set_index</code> + <code>replace</code>, and then <code>fillna</code> on <code>mean</code>.</p>

<pre><code>v = df.set_index(
    ['neighborhood', 'borough', 'block']
)['year built'].replace(0, np.nan)   

df = v.fillna(v.mean(level=[0, 1, 2])).reset_index()
df

    neighborhood  borough  block  year built
0       Annadale        5   5425      2015.0
1       Woodside        4   2327      1966.0
2  Alphabet City        1    396      1985.0
3  Alphabet City        1    405      1996.0
4  Alphabet City        1    396      1986.0
5  Alphabet City        1    396      1992.0
6  Alphabet City        1    396      1987.4
7  Alphabet City        1    396      1990.0
8  Alphabet City        1    396      1984.0
9  Alphabet City        1    396      1987.4
</code></pre>

<hr>

<p><strong>Details</strong></p>

<p>First, set the index, and replace 0s with NaNs so that the forthcoming <code>mean</code> calculation is not affected by these values - </p>

<pre><code>v = df.set_index(
    ['neighborhood', 'borough', 'block']
)['year built'].replace(0, np.nan)   

v 

neighborhood   borough  block
Annadale       5        5425     2015.0
Woodside       4        2327     1966.0
Alphabet City  1        396      1985.0
                        405      1996.0
                        396      1986.0
                        396      1992.0
                        396         NaN
                        396      1990.0
                        396      1984.0
                        396         NaN
Name: year built, dtype: float64
</code></pre>

<p>Next, calculate the <code>mean</code> -</p>

<pre><code>m = v.mean(level=[0, 1, 2])
m

neighborhood   borough  block
Annadale       5        5425     2015.0
Woodside       4        2327     1966.0
Alphabet City  1        396      1987.4
                        405      1996.0
Name: year built, dtype: float64
</code></pre>

<p>This serves as a mapping, which we'll pass to <code>fillna</code>. <code>fillna</code> accordingly replaces the NaNs introduced earlier, and replaces them with the corresponding mean values mapped by the index. Once that's done, just reset the index to get our original structure back.</p>

<pre><code>v.fillna(m).reset_index()

    neighborhood  borough  block  year built
0       Annadale        5   5425      2015.0
1       Woodside        4   2327      1966.0
2  Alphabet City        1    396      1985.0
3  Alphabet City        1    405      1996.0
4  Alphabet City        1    396      1986.0
5  Alphabet City        1    396      1992.0
6  Alphabet City        1    396      1987.4
7  Alphabet City        1    396      1990.0
8  Alphabet City        1    396      1984.0
9  Alphabet City        1    396      1987.4
</code></pre>
",How replace certain values pandas column mean column value similar rows The Problem I currently pandas dataframe property information kaggle dataset The following example dataframe set neighborhood borough block year built Annadale Woodside Alphabet City Alphabet City Alphabet City Alphabet City Alphabet City Alphabet City Alphabet City Alphabet City What I want take every row value year built column equals zero replace year built value rows median year built values rows neighborhood borough block There cases multiple rows within neighborhood borough block set zero year built column This shown example dataframe To illustrate problem I put two rows example dataframe neighborhood borough block year built Alphabet City Alphabet City To solve problem I want use mean year built values rows neighborhood borough block fill year built value rows zero year built column For example rows neighborhood Alphabet City borough block I would use following matching rows example dataframe calculate mean,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
48179257,2018-01-10,2018,3,Keras one hot embedding before LSTM,"<p>Suppose I have a training dataset as several sequences with padded length = 40 and a dictionary length of 80, e.g., <code>example = [0, 0, 0, 3, 4, 9, 22, ...]</code> and I want to feed that into a LSTM layer. What I want to do is to apply one hot encoder to the sequences, e.g., <code>example_after_one_hot.shape = (40, 80)</code>. Is there a keras layer that is able to do this? I have tried <code>Embedding</code>, however, it seems that is not an one-hot encoding.</p>

<p>Edit: another way is to use Embedding layer. Given the dictionary only contains 80 different keys, how should I set the output of Embedding layer?</p>
","['python', 'tensorflow', 'keras']",48180781,"<p>I think you're looking for a pre-processing task, not something that is strictly part of your network.</p>

<p>Keras has a one-hot text pre-processing function that may be able to help you.  Take a look at <a href=""https://keras.io/preprocessing/text/"" rel=""nofollow noreferrer"">Keras text preprocessing</a>.  If this doesn't fit your needs, it's fairly easy to pre-process it yourself with numpy.  You can do something like...</p>

<pre><code>X = numpy.zeros(shape=(len(sentences), 40, 80), dtype='float32')
for i, sent in enumerate(sentences):
    for j, word in enumerate(sent):
        X[i, j, word] = 1.0
</code></pre>

<p>This will give you a one-hot encoding for a 2D-array of ""sentences"", where each word in the array is an integer less than 80.  Of course the data doesn't have to be sentences, it can be any type of data.</p>

<p>Note that Embedding layers are for for <strong>learning</strong> a distributed representation of the data not for putting data in a one-hot format.</p>
",Keras one hot embedding LSTM Suppose I training dataset several sequences padded length dictionary length e g example I want feed LSTM layer What I want apply one hot encoder sequences e g example one hot shape Is keras layer able I tried Embedding however seems one hot encoding Edit another way use Embedding layer Given dictionary contains different keys I set output Embedding layer,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
48330137,2018-01-18,2018,9,Adding regularizer to an existing layer of a trained model without resetting weights?,"<p>Let's say I'm transfer learning via Inception. I add a few layers and train it for a while.</p>

<p>Here is what my model topology looks like:</p>

<pre><code>base_model = InceptionV3(weights='imagenet', include_top=False)
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu', name = 'Dense_1')(x)
predictions = Dense(12, activation='softmax', name = 'Predictions')(x)
model = Model(input=base_model.input, output=predictions)
</code></pre>

<p>I train this model for a while, save it and load it again for retraining; this time I want to add l2-regularizer to the <code>Dense_1</code> without resetting the weights? Is this possible?</p>

<pre><code>path = .\model.hdf5
from keras.models import load_model
model = load_model(path)
</code></pre>

<p>The docs show only show the that regularizer can be added as parameter when you initialize a layer:</p>

<pre><code>from keras import regularizers
model.add(Dense(64, input_dim=64,
                kernel_regularizer=regularizers.l2(0.01),
                activity_regularizer=regularizers.l1(0.01)))
</code></pre>

<p>This is essentially creating a new layer, so my layer's weights would be resetted.</p>

<p>EDIT:</p>

<p>So I'm playing around with the code the past couple of days, and something strange is happening with my loss when I load the model (after training a bit with the new regularizer).</p>

<p>So the first time I run this code (first time with new regularizer):</p>

<pre><code>from keras.models import load_model
base_model = load_model(path)
x = base_model.get_layer('dense_1').output
predictions = base_model.get_layer('dense_2')(x)
model = Model(inputs = base_model.input, output = predictions)
model.get_layer('dense_1').kernel_regularizer = regularizers.l2(0.02) 

model.compile(optimizer=SGD(lr= .0001, momentum=0.90),
              loss='categorical_crossentropy',
              metrics = ['accuracy'])
</code></pre>

<p>My training output seems to be normal:</p>

<pre><code>Epoch 43/50
 - 2918s - loss: 0.3834 - acc: 0.8861 - val_loss: 0.4253 - val_acc: 0.8723
Epoch 44/50
Epoch 00044: saving model to E:\Keras Models\testing_3\2018-01-18_44.hdf5
 - 2692s - loss: 0.3781 - acc: 0.8869 - val_loss: 0.4217 - val_acc: 0.8729
Epoch 45/50
 - 2690s - loss: 0.3724 - acc: 0.8884 - val_loss: 0.4169 - val_acc: 0.8748
Epoch 46/50
Epoch 00046: saving model to E:\Keras Models\testing_3\2018-01-18_46.hdf5
 - 2684s - loss: 0.3688 - acc: 0.8896 - val_loss: 0.4137 - val_acc: 0.8748
Epoch 47/50
 - 2665s - loss: 0.3626 - acc: 0.8908 - val_loss: 0.4097 - val_acc: 0.8763
Epoch 48/50
Epoch 00048: saving model to E:\Keras Models\testing_3\2018-01-18_48.hdf5
 - 2681s - loss: 0.3586 - acc: 0.8924 - val_loss: 0.4069 - val_acc: 0.8767
Epoch 49/50
 - 2679s - loss: 0.3549 - acc: 0.8930 - val_loss: 0.4031 - val_acc: 0.8776
Epoch 50/50
Epoch 00050: saving model to E:\Keras Models\testing_3\2018-01-18_50.hdf5
 - 2680s - loss: 0.3493 - acc: 0.8950 - val_loss: 0.4004 - val_acc: 0.8787
</code></pre>

<p>However, if I try to load the model after this mini-training session(I will load the model from epoch 00050, so new regularizer value should be already implemented, I get a really high loss value)</p>

<p>Code:</p>

<pre><code>path = r'E:\Keras Models\testing_3\2018-01-18_50.hdf5' #50th epoch model

from keras.models import load_model
model = load_model(path)
model.compile(optimizer=SGD(lr= .0001, momentum=0.90),
              loss='categorical_crossentropy',
              metrics = ['accuracy'])
</code></pre>

<p>return:</p>

<pre><code>Epoch 51/65
 - 3130s - loss: 14.0017 - acc: 0.8953 - val_loss: 13.9529 - val_acc: 0.8800
Epoch 52/65
Epoch 00052: saving model to E:\Keras Models\testing_3\2018-01-20_52.hdf5
 - 2813s - loss: 13.8017 - acc: 0.8969 - val_loss: 13.7553 - val_acc: 0.8812
Epoch 53/65
 - 2759s - loss: 13.6070 - acc: 0.8977 - val_loss: 13.5609 - val_acc: 0.8824
Epoch 54/65
Epoch 00054: saving model to E:\Keras Models\testing_3\2018-01-20_54.hdf5
 - 2748s - loss: 13.4115 - acc: 0.8992 - val_loss: 13.3697 - val_acc: 0.8824
Epoch 55/65
 - 2745s - loss: 13.2217 - acc: 0.9006 - val_loss: 13.1807 - val_acc: 0.8840
Epoch 56/65
Epoch 00056: saving model to E:\Keras Models\testing_3\2018-01-20_56.hdf5
 - 2752s - loss: 13.0335 - acc: 0.9014 - val_loss: 12.9951 - val_acc: 0.8840
Epoch 57/65
 - 2756s - loss: 12.8490 - acc: 0.9023 - val_loss: 12.8118 - val_acc: 0.8849
Epoch 58/65
Epoch 00058: saving model to E:\Keras Models\testing_3\2018-01-20_58.hdf5
 - 2749s - loss: 12.6671 - acc: 0.9032 - val_loss: 12.6308 - val_acc: 0.8849
Epoch 59/65
 - 2738s - loss: 12.4871 - acc: 0.9039 - val_loss: 12.4537 - val_acc: 0.8855
Epoch 60/65
Epoch 00060: saving model to E:\Keras Models\testing_3\2018-01-20_60.hdf5
 - 2765s - loss: 12.3086 - acc: 0.9059 - val_loss: 12.2778 - val_acc: 0.8868
Epoch 61/65
 - 2767s - loss: 12.1353 - acc: 0.9065 - val_loss: 12.1055 - val_acc: 0.8867
Epoch 62/65
Epoch 00062: saving model to E:\Keras Models\testing_3\2018-01-20_62.hdf5
 - 2757s - loss: 11.9637 - acc: 0.9061 - val_loss: 11.9351 - val_acc: 0.8883
</code></pre>

<p>Notice the really high <code>loss</code> values. Is this normal? I understand the l2 regularizer  would bring the loss up (if there large weights), but wouldn't that be reflected in the first mini-training session (where I first implemented the regularizer?). The accuracy seems to stay consistent though. </p>

<p>Thank you. </p>
","['python', 'tensorflow', 'keras']",53524430,"<p>The solution from <a href=""https://stackoverflow.com/users/5974433/marcin-mo%C5%BCejko"">Marcin</a> hasn't worked for me. As <a href=""https://stackoverflow.com/users/7442271/apatsekin"">apatsekin</a> mentioned, if you print <code>layer.losses</code> after adding the regularizers as <a href=""https://stackoverflow.com/users/5974433/marcin-mo%C5%BCejko"">Marcin</a> proposed, you will get an empty list. </p>

<p>I found a workaround that I do not like at all, but I am posting here so someone more capable can find a way to do this in an easier way. </p>

<p>I believe it works for most <code>keras.application</code> networks. I copied the <code>.py</code> file of a specific architecture from keras-application in Github (for example, <a href=""https://github.com/keras-team/keras-applications/blob/master/keras_applications/inception_resnet_v2.py"" rel=""nofollow noreferrer"">InceptionResNetV2</a>) to a local file <code>regularizedNetwork.py</code> in my machine. I had to edit it to fix some relative imports such as:</p>

<pre class=""lang-py prettyprint-override""><code>#old version
from . import imagenet_utils
from .imagenet_utils import decode_predictions
from .imagenet_utils import _obtain_input_shape

backend = None
layers = None
models = None
keras_utils = None
</code></pre>

<p>to:</p>

<pre class=""lang-py prettyprint-override""><code>#new version
from keras import backend
from keras import layers
from keras import models
from keras import utils as keras_utils

from keras.applications import imagenet_utils
from keras.applications.imagenet_utils import decode_predictions
from keras.applications.imagenet_utils import _obtain_input_shape
</code></pre>

<p>Once the relative paths and import issues were solved, I added the regularizers in each desired layer, just as you would do when defining a new untrained network. Usually, after defining the architecture, the models from <code>keras.application</code> load the pre-trained weights.</p>

<p>Now, in your main code/notebook, just import the new <code>regularizedNetwork.py</code> and call the main method to instantiate the network. </p>

<pre class=""lang-py prettyprint-override""><code>#main code
from regularizedNetwork import InceptionResNetV2
</code></pre>

<p>The regularizers should be all set and you can fine-tune the regularized model normally. </p>

<p>I am certain there is a less gimmicky way of doing this, so, please, if someone finds it, write a new answer and/or comment in this answer.</p>

<p>Just for the record, I also tried instantiating the model from <code>keras.application</code>, getting the its architecture with <code>regModel = model.get_config()</code>, adding the regularizers as <a href=""https://stackoverflow.com/users/5974433/marcin-mo%C5%BCejko"">Marcin</a> suggested and then loading the weights with <code>regModel.set_weights(model.get_weights())</code>, but it still didn't work.</p>

<p>Edit: spelling errors.</p>
",Adding regularizer existing layer trained model without resetting weights Let say I transfer learning via Inception I add layers train Here model topology looks like base model InceptionV weights imagenet include top False x base model output x D x x Dense activation relu name Dense x predictions Dense activation softmax name Predictions x model Model input base model input output predictions I train model save load retraining time I want add l regularizer Dense without resetting weights Is possible path model hdf keras models import load model model load model path The docs show show regularizer added parameter initialize layer keras import regularizers model add Dense input dim kernel regularizer regularizers l activity regularizer regularizers l This essentially creating new layer layer weights would resetted EDIT So I playing around code past couple days something strange happening loss I load model training bit new regularizer So first time I,"startoftags, python, tensorflow, keras, endoftags",python django djangorestframework endoftags,python tensorflow keras,python django djangorestframework,0.33
48351575,2018-01-20,2018,2,Loop over dataframe with two or multiple index,"<p>I have a pandas dataframe that looks like this:</p>

<pre><code>Location    Test#       Type        Parm1   Weight
M36         Test1       A           1.39    233
            Test2       B           1.44    281
            Test3       B           1.40    239
            Test4       A           1.49    438
            Test5       C           0.99    112
            Test6       C           1.74    200
            Test7       A           1.17    100
            Test8       A           2.40    7.8
M37         Test1       B           2.91    232
            Test2       A           20.2    0
            Test3       C           4.88    958
            Test4       A           9.46    0
</code></pre>

<p>I want to calculate weighted average for each location and add it as an extra column to the dataframe which should looks like this:</p>

<pre><code>Location    Test#       Type        Parm1   Weight  Weighted Ave.
M36         Test1       A           1.39    233     1.434
            Test2       B           1.44    281
            Test3       B           1.40    239
            Test4       A           1.49    438
            Test5       C           0.99    112
            Test6       C           1.74    200
            Test7       A           1.17    100
            Test8       A           2.40    7.8
M37         Test1       B           2.91    232     4.495
            Test2       A           20.2    0
            Test3       C           4.88    958
            Test4       A           9.46    0       
</code></pre>
","['python', 'pandas', 'dataframe']",48351774,"<p>There are a lot of ways to do this, using <code>groupby</code>. This should be one of the more performant wats of doing it.</p>

<pre><code>df.set_index('Location', inplace=True)                # set the index

df['Weighted_Sum'] = (df.Parm1 * df.Weight)           # calculated weighted sum
v = df[['Weighted_Sum', 'Weight']].sum(level=0)       # groupby + sum

df['Weighted Ave'] = v['Weighted_Sum'] / v['Weight']  # calculate the mean
del df['Weighted_Sum']                                # drop the surrogate column
</code></pre>

<p></p>

<pre><code>df

          Test# Type  Parm1  Weight  Weighted Ave
Location                                         
M36       Test1    A   1.39   233.0      1.434275
M36       Test2    B   1.44   281.0      1.434275
M36       Test3    B   1.40   239.0      1.434275
M36       Test4    A   1.49   438.0      1.434275
M36       Test5    C   0.99   112.0      1.434275
M36       Test6    C   1.74   200.0      1.434275
M36       Test7    A   1.17   100.0      1.434275
M36       Test8    A   2.40     7.8      1.434275
M37       Test1    B   2.91   232.0      4.495933
M37       Test2    A  20.20     0.0      4.495933
M37       Test3    C   4.88   958.0      4.495933
M37       Test4    A   9.46     0.0      4.495933
</code></pre>

<p>To get the <code>Weighted Ave</code> column in your format, use <code>mask</code> - </p>

<pre><code>df['Weighted Ave'] = df['Weighted Ave'].mask(df['Weighted Ave'].duplicated(), '')
</code></pre>
",Loop dataframe two multiple index I pandas dataframe looks like Location Test Type Parm Weight M Test A Test B Test B Test A Test C Test C Test A Test A M Test B Test A Test C Test A I want calculate weighted average location add extra column dataframe looks like Location Test Type Parm Weight Weighted Ave M Test A Test B Test B Test A Test C Test C Test A Test A M Test B Test A Test C Test A,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
48509147,2018-01-29,2018,3,No module named &#39;bs4&#39; Error,"<p>I planned to do web scraping, but I seem to be stuck on very first step.</p>

<pre><code>import urllib.request
from bs4 import beautifulSoup

wiki = ""https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India""

page = urllib.urlopen(wiki)

soup = BeautifulSoup(page)

print(soup.prettify())
</code></pre>

<p>I wrote these line just to test, but it shows an error that </p>

<pre><code>Traceback (most recent call last):
  File ""C:/python programs/Web Scraping/wiki.py"", line 3, in &lt;module&gt;
    from bs4 import beautifulSoup
ModuleNotFoundError: No module named 'bs4'


Process finished with exit code 1
</code></pre>

<p>Things which i tried to clear it.</p>

<p>1) pip install beautifulsoup4 (tried this with easy_install as well)</p>

<p>2) check the python path in environment variable. i have included both C:\python and C:\python\Scripts in paths.</p>

<p>3) Tried downloading Beautiful Soup from crummy.com and then install from `python setup.py install command.</p>

<p>I spent almost whole day clearing this out, tried almost every solution and its literally frustrating now. but if anyone still wants to mark it as duplicate you are free to do so.</p>

<p>is there anything i have missed ?</p>
","['python', 'web-scraping', 'beautifulsoup']",48509327,"<p>I don't really see a clear way to answer your question with the information you provided, but it seems that you are not installing the bs4 package properly...</p>

<p>--Easy Way to Fix--</p>

<p>Install PyCharm:
<a href=""https://www.jetbrains.com/pycharm/download/#section=windows"" rel=""noreferrer"">https://www.jetbrains.com/pycharm/download/#section=windows</a></p>

<p>Once installed, configure your interpreter to use latest version of Python (install from here):
<a href=""https://www.python.org/downloads/"" rel=""noreferrer"">https://www.python.org/downloads/</a></p>

<p>Once you have your file open, click ""File > Settings > Project Interpreter > Show All (in the drop down) > Add(+) > Add Local"" and select your installed version Python. Then click the add button (+) and search for bs4 from the list of packages and install it, and you should get it working. PyCharm does everything for you, so there is really no room for error when installing packages.</p>
",No module named bs Error I planned web scraping I seem stuck first step import urllib request bs import beautifulSoup wiki https en wikipedia org wiki List state union territory capitals India page urllib urlopen wiki soup BeautifulSoup page print soup prettify I wrote line test shows error Traceback recent call last File C python programs Web Scraping wiki py line lt module gt bs import beautifulSoup ModuleNotFoundError No module named bs Process finished exit code Things tried clear pip install beautifulsoup tried easy install well check python path environment variable included C python C python Scripts paths Tried downloading Beautiful Soup crummy com install python setup py install command I spent almost whole day clearing tried almost every solution literally frustrating anyone still wants mark duplicate free anything missed,"startoftags, python, webscraping, beautifulsoup, endoftags",python webscraping beautifulsoup endoftags,python webscraping beautifulsoup,python webscraping beautifulsoup,1.0
48518881,2018-01-30,2018,2,Python Pandas/Matplot - Annotating values above and below a line,"<p>I have a dataframe of values which I am using to plot a scatter/line graph with confidence intervals:</p>

<p>The dataframe (<code>sqlDF2</code>) is like this:</p>

<pre><code>Statu   Total   count   Success   Pred   Upper95    Lower95      Upper99    Lower99
Org                             
A        391    391       38    0.35064  0.398903   0.302377    0.423034    0.278245
B        360    360       30    0.343464 0.393519   0.293408    0.418546    0.268381
C        271    271       29    0.319606 0.37626    0.262951    0.404587    0.234624
D        247    247       22    0.312089 0.371053   0.253125    0.400535    0.223643
...
</code></pre>

<p>The code that I plot the graph is:</p>

<pre><code>y = sqlDf2['Success'].values
x = sqlDf2['Total'].values

up95 = (sqlDf2['Upper95'].values)*100
low95 = (sqlDf2['Lower95'].values)*100
up99 = (sqlDf2['Upper99'].values)*100
low99 = (sqlDf2['Lower99'].values)*100
middleLine = (sqlDf2['Pred'].values)*100

plt.figure(figsize=(15,8))
plt.ylim(0, 100)
plt.margins(x=0)

plt.scatter(x,y,marker='o',c='white',edgecolors = 'black', alpha=.5)
plt.plot(x,up95, 'red', linestyle=':', dashes=(1, 5), linewidth=1)
plt.plot(x,low95, 'red', linestyle=':', dashes=(1, 5), linewidth=1)
plt.plot(x,up99, 'red', linestyle=':', dashes=(1, 5), linewidth=1)
plt.plot(x,low99, 'red', linestyle=':', dashes=(1, 5), linewidth=1)
plt.plot(x,middleLine, 'red', linestyle='-', dashes=(1, 2), linewidth=1)

plt.show() 
</code></pre>

<p>The graph looks like this:</p>

<p><a href=""https://i.stack.imgur.com/pSGlQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pSGlQ.jpg"" alt=""enter image description here""></a></p>

<p>What I want to do is annotate the values that fall ABOVE and BELOW the 99% confidence intervals with the value of 'Org'. Is there a easy way to work out those values which fall above and below two lines in Python?</p>

<p>Thank you</p>
","['python', 'pandas', 'matplotlib']",48519677,"<p>In your DataFrame you have the y-values of the data-points and the y-values of the lines in a single line. Therefore, you could use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html"" rel=""nofollow noreferrer"">np.where</a> for this purpose.</p>

<pre><code>C = np.where(condition, A, B)
</code></pre>

<p><code>A</code> is set if the condition is <code>True</code> and <code>B</code> if the condition is <code>False</code>. If you want to check against the <code>Upper99</code> and <code>Lower99</code> lines you could achieve this as follows:</p>

<pre><code>sqlDF2['Outside'] = np.where((sqlDf2['Success'] &gt; sqlDf2['Upper99']*100) | (sqlDf2['Success']&lt;sqlDf['Lower99']*100), True, False)
</code></pre>

<p>This will result in a new column containing <code>True</code> if the data-point lies outside of the given boundaries and <code>False</code> if it is inside of the boundaries.</p>
",Python Pandas Matplot Annotating values line I dataframe values I using plot scatter line graph confidence intervals The dataframe sqlDF like Statu Total count Success Pred Upper Lower Upper Lower Org A B C D The code I plot graph sqlDf Success values x sqlDf Total values sqlDf Upper values low sqlDf Lower values sqlDf Upper values low sqlDf Lower values middleLine sqlDf Pred values plt figure figsize plt ylim plt margins x plt scatter x marker c white edgecolors black alpha plt plot x red linestyle dashes linewidth plt plot x low red linestyle dashes linewidth plt plot x red linestyle dashes linewidth plt plot x low red linestyle dashes linewidth plt plot x middleLine red linestyle dashes linewidth plt show The graph looks like What I want annotate values fall ABOVE BELOW confidence intervals value Org Is easy way work values fall two lines Python Thank,"startoftags, python, pandas, matplotlib, endoftags",python pandas matplotlib endoftags,python pandas matplotlib,python pandas matplotlib,1.0
48710933,2018-02-09,2018,2,python: print numbers like product of two natural numbers,"<p>Sorry noob here.
I would like to have a function which, for a given natural number, will printout all possible pairs of 2 natural numbers whose product makes the input.</p>

<p>I think it could be like this, but still can't figured out how to write it properly: </p>

<p>(the last output line is missing in my code output)</p>

<pre><code>def print_products(n):
    for i in range(1, n):
        if n % i == 0:
            print(n, ""="", i, ""*"", int((n/i)))
</code></pre>

<p>And output should be like this:</p>

<pre><code>print(print_products(36))

36 = 1 * 36
36 = 2 * 18
36 = 3 * 12
36 = 4 * 9
36 = 6 * 6
36 = 9 * 4
36 = 12 * 3
36 = 18 * 2
36 = 36 * 1
</code></pre>

<p>Thanks for the help and possible explanation</p>
","['python', 'python-3.x', 'python-2.7']",48711020,"<p>The <code>range()</code> function does not include the end (stop) value so <code>i</code> is never actually <code>n</code>. This is easy to correct with a <code>+1</code>. You should also use integer division (<code>//</code>) as it is neater than passing a <code>float</code> into <code>int()</code>.</p>

<p>Making your function:</p>

<pre><code>def print_products(n):
    for i in range(1, n+1):
        if n % i == 0:
            print(n, ""="", i, ""*"", n//i)
</code></pre>

<p>and it now works:</p>

<pre><code>36 = 1 * 36
36 = 2 * 18
36 = 3 * 12
36 = 4 * 9
36 = 6 * 6
36 = 9 * 4
36 = 12 * 3
36 = 18 * 2
36 = 36 * 1
</code></pre>

<hr>

<p>We can see from the <a href=""https://docs.python.org/3/library/stdtypes.html#range"" rel=""nofollow noreferrer"">documentation</a> that <code>range()</code> does not include the <code>stop</code> parameter:</p>

<blockquote>
  <p>For a positive step, the contents of a range <code>r</code> are determined by the formula <code>r[i] = start + step*i</code> where <code>i &gt;= 0</code> and <code>r[i] &lt; stop</code>.</p>
</blockquote>

<p>The proper terminology is to say that <code>i</code> <em>does not <strong>attain</strong></em> the upper bound (<code>stop</code>).</p>
",python print numbers like product two natural numbers Sorry noob I would like function given natural number printout possible pairs natural numbers whose product makes input I think could like still figured write properly last output line missing code output def print products n range n n print n int n And output like print print products Thanks help possible explanation,"startoftags, python, python3x, python27, endoftags",python python3x list endoftags,python python3x python27,python python3x list,0.67
48713510,2018-02-09,2018,3,Discord.py silence command,"<p>I have been asking lots of questions lately about discord.py and this is one of them.</p>
<p>Sometimes there are those times when some people spam your discord server but kicking or banning them seems too harsh. I had the idea for a <code>silence</code> command which would delete every new message on a channel for a given amount of time.</p>
<p>My code so far is:</p>
<pre><code>@BSL.command(pass_context = True)
async def silence(ctx, lenghth = None):
        if ctx.message.author.server_permissions.administrator or ctx.message.author.id == ownerID:
             global silentMode
             global silentChannel
             silentChannel = ctx.message.channel
             silentMode = True
             lenghth = int(lenghth)
             if lenghth != '':
                  await asyncio.sleep(lenghth)
                  silentMode = False
             else:
                  await asyncio.sleep(10)
                  silentMode = False
        else:
             await BSL.send_message(ctx.message.channel, 'Sorry, you do not have the permissions to do that @{}!'.format(ctx.message.author))
</code></pre>
<p>The code in my <code>on_message</code> section is:</p>
<pre><code>if silentMode == True:
        await BSL.delete_message(message)
        if message.content.startswith('bsl;'):
                await BSL.process_commands(message)
</code></pre>
<p>All the variables used are pre-defined at the top of the bot.</p>
<p>My problem is that the bot deletes all new messages in all channels which it has access to. I tried putting <code>if silentChannel == ctx.message.channel</code> in the <code>on_message</code> section but this made the command stop working completely.</p>
<p>Any suggestions as to why this is happening are much appreciated.</p>
","['python', 'discord', 'discord.py']",48714472,"<p>Something like </p>

<pre><code>silent_channels = set()

@BSL.event
async def on_message(message):
    if message.channel in silent_channels:
        if not message.author.server_permissions.administrator and  message.author.id != ownerID:
            await BSL.delete_message(message)
            return
    await BSL.process_commands(message)

@BSL.command(pass_context=True)
async def silent(ctx, length=0): # Corrected spelling of length
    if ctx.message.author.server_permissions.administrator or ctx.message.author.id == ownerID:
        silent_channels.add(ctx.message.channel)
        await BSL.say('Going silent.')
        if length:
            length = int(length)
            await asyncio.sleep(length)
            if ctx.message.channel not in silent_channels: # Woken manually
                return
            silent_channels.discard(ctx.message.channel)
            await BSL.say('Waking up.')

@BSL.command(pass_context=True)
async def wake(ctx):
    silent_channels.discard(ctx.message.channel)
</code></pre>

<p>Should work (I haven't tested it, testing bots is a pain).  Searching through sets is fast, so doing it for every message shouldn't be a real burden on your resources. </p>
",Discord py silence command I asking lots questions lately discord py one Sometimes times people spam discord server kicking banning seems harsh I idea silence command would delete every new message channel given amount time My code far BSL command pass context True async def silence ctx lenghth None ctx message author server permissions administrator ctx message author id ownerID global silentMode global silentChannel silentChannel ctx message channel silentMode True lenghth int lenghth lenghth await asyncio sleep lenghth silentMode False else await asyncio sleep silentMode False else await BSL send message ctx message channel Sorry permissions format ctx message author The code message section silentMode True await BSL delete message message message content startswith bsl await BSL process commands message All variables used pre defined top bot My problem bot deletes new messages channels access I tried putting silentChannel ctx message channel message section made command stop working completely Any,"startoftags, python, discord, discordpy, endoftags",python discord discordpy endoftags,python discord discordpy,python discord discordpy,1.0
49186350,2018-03-09,2018,4,Assigning values to a column based on condition using np.where,"<p>Though the question seems very similar but I am stuck a this . Below is the sample of data. I want to add number of productView and Order where productView &lt; Order. </p>

<pre><code>           productView  order
userId      
A                4.5    5.0
B               1.5     2.5
C               4.0     2.0
D                2.0    3.0
</code></pre>

<p>I tried following code but its throwing error</p>

<pre><code>order_Segemnt.productView=np.where(order_Segment[order_Segment.productView&lt; order_Segment.order]['productView'],order_Segment.productView+order_Segment.order, order_Segment.productView)
</code></pre>

<p>Error which I am getting is this  </p>

<pre><code>Error- ValueError: operands could not be broadcast together with shapes (408,) (7464,) (7464,) 
</code></pre>

<p>How to achieve my objective??</p>
","['python', 'pandas', 'numpy']",49186379,"<p><em>Syntax</em> - </p>

<pre><code>np.where(condition, value1, value2)
</code></pre>

<p></p>

<p><em>Solution</em> -</p>

<pre><code>np.where(
   df.productView &lt; df.order, df.productView + df.order, df.productView
)
array([9.5, 4. , 4. , 5. ])
</code></pre>

<p>As an efficient alternative, you can use <code>loc</code>:</p>

<pre><code>m = df.productView &lt; df.order
df.loc[m, 'productView'] = df.loc[m, ['productView', 'order']].sum(1)
</code></pre>
",Assigning values column based condition using np Though question seems similar I stuck Below sample data I want add number productView Order productView lt Order productView order userId A B C D I tried following code throwing error order Segemnt productView np order Segment order Segment productView lt order Segment order productView order Segment productView order Segment order order Segment productView Error I getting Error ValueError operands could broadcast together shapes How achieve objective,"startoftags, python, pandas, numpy, endoftags",python python3x pandas endoftags,python pandas numpy,python python3x pandas,0.67
49203408,2018-03-09,2018,2,Matplotlib / Seaborn Countplot with different Categories in one Plot,"<p>I have two series with different lengths and amount of variables and want to plot how often each variable (Name) occurs per series.
I want a grey countplot for series 1 and a red countplot for series 2, and I want them to be shown on top of each other.
However, since series 2 is missing 'Nancy' it is also cutting series 1 count of 'Nancy'. 
How do i get a full overlay of the two series inkluding a bar for Nancy?</p>

<pre><code>import matplotlib.pyplot as plt
import seaborn as sns

ser1 = pd.Series( ['tom','tom','bob','bob','nancy'])
ser2 = pd.Series( ['tom','bob'])

fig = plt.figure()
sns.countplot(x=ser1, color='grey')
sns.countplot(x=ser2, color='red')
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/PuMOD.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PuMOD.jpg"" alt=""enter image description here""></a></p>

<p>Edit:
Changing to the following will cause problems again. How do I make Matplotlib recognize that the two series have the same categorical values that are being counted?</p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

ser1 = pd.Series( ['tom','tom','bob','bob','nancy','zulu'])
ser2 = pd.Series( ['tom','nancy'])

ser1 = ser1.astype('category')
ser2 = ser2.astype('category')

fig = plt.figure()
ax = sns.countplot(x=ser2, color='red', zorder=2)
sns.countplot(x=ser1, color='grey')

plt.show()
</code></pre>
","['python', 'matplotlib', 'seaborn']",49203922,"<p>You may store the setting for the first plot and restore them after having plotted the second plot.</p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

ser1 = pd.Series( ['tom','tom','bob','bob','nancy','zulu'])
ser2 = pd.Series( ['tom','bob'])

fig = plt.figure()
ax = sns.countplot(x=ser1, color='grey')
ticks = ax.get_xticks()
ticklabels = ax.get_xticklabels()
lim = ax.get_xlim()

sns.countplot(x=ser2, color='red')
ax.set_xlim(lim)
ax.set_xticks(ticks)
ax.set_xticklabels(ticklabels)
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/IVMbO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IVMbO.png"" alt=""enter image description here""></a></p>

<p>The other option could be to plot the second plot first but set the zorder to a higher value, such that those bars appear in front of the later plot.</p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

ser1 = pd.Series( ['tom','tom','bob','bob','nancy','zulu'])
ser2 = pd.Series( ['tom','bob'])

fig = plt.figure()
ax = sns.countplot(x=ser2, color='red', zorder=2)
sns.countplot(x=ser1, color='grey')

plt.show()
</code></pre>

<p>In the more general case you need to use the <code>order</code> arument. </p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

ser1 = pd.Series( ['tom','tom','bob','bob','nancy', 'nancy' ,'zulu'])
ser2 = pd.Series( ['tom','nancy'])
order = ser1.append(ser2).unique()

fig = plt.figure()
ax = sns.countplot(x=ser2, color='red', order=order, zorder=2)
sns.countplot(x=ser1, color='grey', order=order)

plt.show()
</code></pre>

<hr>

<p>In case you would rather use matplotlib's categoricals to create the plot, this would look as follows:</p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

ser1 = pd.Series( ['tom','tom','bob','bob','nancy', 'nancy' ,'zulu'])
ser2 = pd.Series( ['tom','nancy'])

u1, counts1 = np.unique(ser1.values, return_counts=True)
u2, counts2 = np.unique(ser2.values, return_counts=True)

fig, ax = plt.subplots()
ax.bar(u1,counts1, color='grey')
ax.bar(u2,counts2, color='red')

plt.show()
</code></pre>
",Matplotlib Seaborn Countplot different Categories one Plot I two series different lengths amount variables want plot often variable Name occurs per series I want grey countplot series red countplot series I want shown top However since series missing Nancy also cutting series count Nancy How get full overlay two series inkluding bar Nancy import matplotlib pyplot plt import seaborn sns ser pd Series tom tom bob bob nancy ser pd Series tom bob fig plt figure sns countplot x ser color grey sns countplot x ser color red plt show Edit Changing following cause problems How I make Matplotlib recognize two series categorical values counted import pandas pd import matplotlib pyplot plt import seaborn sns ser pd Series tom tom bob bob nancy zulu ser pd Series tom nancy ser ser astype category ser ser astype category fig plt figure ax sns countplot x ser color red zorder sns countplot,"startoftags, python, matplotlib, seaborn, endoftags",python pandas numpy endoftags,python matplotlib seaborn,python pandas numpy,0.33
49328235,2018-03-16,2018,7,Int too large to convert to C long while doing .astype(int),"<p>I'm running a python script, where I have to convert a string column from a pandas <code>df</code> to <code>int</code>, using the <code>astype(int)</code> method. However, I get this following error:</p>

<pre><code>""Python int too large to convert to C long""
</code></pre>

<p>My strings are all numbers in string format, up to 15 characters. Is there a way to convert this column to int type without this popping this error?</p>
","['python', 'pandas', 'numpy']",49329034,"<p>You need to use .astype('int64') </p>

<pre><code>import pandas as pd
df = pd.DataFrame({'test': ['999999999999999','111111111111111']})
df['int'] = df['test'].astype('int64')
</code></pre>
",Int large convert C long astype int I running python script I convert string column pandas df int using astype int method However I get following error Python int large convert C long My strings numbers string format characters Is way convert column int type without popping error,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
49468589,2018-03-24,2018,2,How do i program a button in python (without Tkinter)?,"<p>I've recently started learning Python3 and I was trying to make a game with Python3 by importing <code>pygame</code>. I tried to make a menu and I am having some struggles with it. I just tried to make it seem like its a button by letting the rectangle change color when you hover over it but its not working. I already tried some things but its not working. Anyhow here is the full code: <a href=""https://hastebin.com/qahocacuxe.py"" rel=""nofollow noreferrer"">hastebin link.</a></p>

<p>This is the part where I tried to make a button:</p>

<pre><code>def game_intro():
    intro = True

    gameDisplay.fill(white)
    largeText = pygame.font.Font('freesansbold.ttf', 90)
    TextSurf, TextRect = text_objects(""Run Abush Run!"", largeText)
    TextRect.center = ((display_width / 2), (display_height / 2))
    gameDisplay.blit(TextSurf, TextRect)

    mouse = pygame.mouse.get_pos()

    if 150+100 &gt; mouse[0] &gt; 150 and 430+50 &gt; mouse[1] &gt; 430:
        pygame.draw.rect(gameDisplay, bright_green, (150,430,100,50))
    else:
        pygame.draw.rect(gameDisplay, green, (150, 430, 100, 50))

    smallText = pygame.font.Font('freesansbold.ttf' ,20)
    textSurf, textRect = text_objects(""START!"", smallText)
    textRect.center = ( (150+(100/2)), (450+(430/2)) )
    gameDisplay.blit(textSurf, textRect)

    pygame.draw.rect(gameDisplay, red, (550, 430, 100, 50))

    pygame.display.update()
    clock.tick(15)

    while intro:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                quit()
</code></pre>
","['python', 'python-3.x', 'pygame']",49468904,"<p>The problem here is that you're only doing your mouseover test once, at the start of the function. If the mouse later moves into your rectangle, it won't matter, because you never do the test again.</p>

<p>What you want to do is move it into the event loop. One tricky bit in PyGame event loops is which code you want to run once per event (the inner <code>for event inâ¦</code> loop), and which you only want to run once per batch (the outer <code>while intro</code> loop). Here, I'm going to assume you want to do this once per event. So:</p>

<pre><code>def game_intro():
    intro = True

    # ... other setup stuff 

    while intro:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                quit()

            mouse = pygame.mouse.get_pos()

            if 150+100 &gt; mouse[0] &gt; 150 and 430+50 &gt; mouse[1] &gt; 430:
                pygame.draw.rect(gameDisplay, bright_green, (150,430,100,50))
            else:
                pygame.draw.rect(gameDisplay, green, (150, 430, 100, 50))
</code></pre>

<p>It looks like some of the other stuff you do only once also belongs inside the loop, so your game may still have some problems. But this should get you past the hurdle you're stuck on, and show you how to get started on those other problems.</p>
",How program button python without Tkinter I recently started learning Python I trying make game Python importing pygame I tried make menu I struggles I tried make seem like button letting rectangle change color hover working I already tried things working Anyhow full code hastebin link This part I tried make button def game intro intro True gameDisplay fill white largeText pygame font Font freesansbold ttf TextSurf TextRect text objects Run Abush Run largeText TextRect center display width display height gameDisplay blit TextSurf TextRect mouse pygame mouse get pos gt mouse gt gt mouse gt pygame draw rect gameDisplay bright green else pygame draw rect gameDisplay green smallText pygame font Font freesansbold ttf textSurf textRect text objects START smallText textRect center gameDisplay blit textSurf textRect pygame draw rect gameDisplay red pygame display update clock tick intro event pygame event get event type pygame QUIT quit,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
49535966,2018-03-28,2018,2,What is the pythonic way of collapsing values into a set for multiple columns per each group in pandas dataframes?,"<p>Given a dataframe, collapsing values into a set per group for a column is straightforward:</p>

<pre><code>df.groupby('A')['B'].apply(set)
</code></pre>

<p>But how do you do it in a pythonic way if you want to do it on multiple columns and the result to be in a dataframe?</p>

<p>For example for the following dataframe:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'user_id': [1, 2, 3, 4, 1, 2, 3], 
               'class_type': ['Krav Maga', 'Yoga', 'Ju-jitsu', 'Krav Maga', 'Ju-jitsu','Krav Maga', 'Karate'], 
               'instructor': ['Bob', 'Alice','Bob', 'Alice','Alice', 'Alice','Bob']})
</code></pre>

<p>The result wanted is the data frame below produced in a pythonic way:</p>

<pre><code>|user_id|class_type             |instructor     |
|-------|-----------------------|---------------|
|  1    | {Krav Maga, Ju-jitsu} | {Bob, Alice}  |
|  2    | {Krav Maga, Yoga}     | {Alice}       | 
|  3    | {Karate, Ju-jitsu}    | {Bob}         | 
|  4    | {Krav Maga}           | {Alice}       | 
</code></pre>

<p>This is a dummy example. The question spurred from: ""what if I have a table with 30 columns and I want to achieve this in a pythonic way?""</p>

<p>Currently I have a solution but I don't think is the best way to do it:</p>

<pre><code>df[['grouped_B', 'grouped_C']] = df.groupby('A')[['B','C']].transform(set)
deduped_and_collapsed_df = df.groupby('A')[['A','grouped_B', 'grouped_C']].head(1)
</code></pre>

<p>Thank you in advance!</p>
","['python', 'pandas', 'pandas-groupby']",49536025,"<pre><code>In [11]: df.groupby('user_id', as_index=False).agg(lambda col: set(col.values.tolist()))
Out[11]:
   user_id             class_type    instructor
0        1  {Krav Maga, Ju-jitsu}  {Alice, Bob}
1        2      {Yoga, Krav Maga}       {Alice}
2        3     {Ju-jitsu, Karate}         {Bob}
3        4            {Krav Maga}       {Alice}
</code></pre>

<p>or shorter version from @jezrael:</p>

<pre><code>In [12]: df.groupby('user_id').agg(lambda x: set(x))
Out[12]:
                    class_type    instructor
user_id
1        {Krav Maga, Ju-jitsu}  {Alice, Bob}
2            {Yoga, Krav Maga}       {Alice}
3           {Ju-jitsu, Karate}         {Bob}
4                  {Krav Maga}       {Alice}
</code></pre>
",What pythonic way collapsing values set multiple columns per group pandas dataframes Given dataframe collapsing values set per group column straightforward df groupby A B apply set But pythonic way want multiple columns result dataframe For example following dataframe import pandas pd df pd DataFrame user id class type Krav Maga Yoga Ju jitsu Krav Maga Ju jitsu Krav Maga Karate instructor Bob Alice Bob Alice Alice Alice Bob The result wanted data frame produced pythonic way user id class type instructor Krav Maga Ju jitsu Bob Alice Krav Maga Yoga Alice Karate Ju jitsu Bob Krav Maga Alice This dummy example The question spurred I table columns I want achieve pythonic way Currently I solution I think best way df grouped B grouped C df groupby A B C transform set deduped collapsed df df groupby A A grouped B grouped C head Thank advance,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
49681392,2018-04-05,2018,9,Python Pandas - How to write in a specific column in an Excel Sheet,"<p>I am having trouble updating an Excel Sheet using pandas by writing new values in it. I already have an existing frame df1 that reads the values from MySheet1.xlsx. so this needs to either be a new dataframe or somehow to copy and overwrite the existing one.</p>

<p>The spreadsheet is in this format:</p>

<p><a href=""https://i.stack.imgur.com/cmljr.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/cmljr.jpg"" alt=""enter image description here""></a></p>

<p>I have a python list: values_list = [12.34, 17.56, 12.45]. My goal is to insert the list values under Col_C header vertically. It is currently overwriting the entire dataframe horizontally, without preserving the current values.</p>

<pre><code>df2 = pd.DataFrame({'Col_C': values_list})
writer = pd.ExcelWriter('excelfile.xlsx', engine='xlsxwriter')
df2.to_excel(writer, sheet_name='MySheet1')
workbook  = writer.book
worksheet = writer.sheets['MySheet1']
</code></pre>

<p>How to get this end result? Thank you!</p>

<p><a href=""https://i.stack.imgur.com/NNqSb.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NNqSb.jpg"" alt=""enter image description here""></a></p>
","['python', 'excel', 'pandas']",49682266,"<p>Below I've provided a fully reproducible example of how you can go about modifying an existing .xlsx workbook using pandas and the openpyxl module <a href=""https://openpyxl.readthedocs.io/en/stable/"" rel=""noreferrer"">(link to Openpyxl Docs)</a>.</p>

<p>First, for demonstration purposes, I create a workbook called test.xlsx:</p>

<pre><code>from openpyxl import load_workbook
import pandas as pd
writer = pd.ExcelWriter('test.xlsx', engine='openpyxl') 
wb  = writer.book
df = pd.DataFrame({'Col_A': [1,2,3,4],
                  'Col_B': [5,6,7,8],
                  'Col_C': [0,0,0,0],
                  'Col_D': [13,14,15,16]})

df.to_excel(writer, index=False)
wb.save('test.xlsx')
</code></pre>

<p>This is the Expected output at this point:</p>

<p><a href=""https://i.stack.imgur.com/r6fo5.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/r6fo5.jpg"" alt=""Expected Output after first section of code""></a></p>

<p>In this second part, we load the existing workbook ('test.xlsx') and modify the third column with different data.</p>

<pre><code>from openpyxl import load_workbook
import pandas as pd
df_new = pd.DataFrame({'Col_C': [9, 10, 11, 12]})
wb = load_workbook('test.xlsx')

ws = wb['Sheet1']

for index, row in df_new.iterrows():
    cell = 'C%d'  % (index + 2)
    ws[cell] = row[0]

wb.save('test.xlsx')
</code></pre>

<p>This is the Expected output at the end:</p>

<p><a href=""https://i.stack.imgur.com/RIMPr.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RIMPr.jpg"" alt=""enter image description here""></a></p>
",Python Pandas How write specific column Excel Sheet I trouble updating Excel Sheet using pandas writing new values I already existing frame df reads values MySheet xlsx needs either new dataframe somehow copy overwrite existing one The spreadsheet format I python list values list My goal insert list values Col C header vertically It currently overwriting entire dataframe horizontally without preserving current values df pd DataFrame Col C values list writer pd ExcelWriter excelfile xlsx engine xlsxwriter df excel writer sheet name MySheet workbook writer book worksheet writer sheets MySheet How get end result Thank,"startoftags, python, excel, pandas, endoftags",python python3x pandas endoftags,python excel pandas,python python3x pandas,0.67
49963491,2018-04-22,2018,3,fillna by selected rows in pandas DataFrame,"<p>I have next pandas DataFrame:</p>

<pre><code>a b c 
1 1 5.0
1 1 None
1 1 4.0
1 2 1.0
1 2 1.0
1 2 4.0 
2 1 3.0
2 1 2.0
2 1 None
2 2 3.0
2 2 4.0
</code></pre>

<p>I want to replace <code>None</code>, but not by the column mean. I want to select all rows, where the values in <code>a</code> and <code>b</code> are similar and if <code>c</code> has a <code>None</code>-values in selected rows, replace them only with the <code>c</code>-mean of selected rows. Something like (this code doesn't work):</p>

<pre><code>df[df[('a'==1) &amp; ('b'==1)]]['c'].fillna(df[df[('a'==1) &amp; ('b'==1)]]['c'].mean())
</code></pre>

<p>which should get me the output:</p>

<pre><code>a b c 
1 1 5.0
1 1 4.5
1 1 4.0
1 2 1.0
1 2 1.0
1 2 4.0 
2 1 3.0
2 1 2.0
2 1 None
2 2 3.0
2 2 4.0
</code></pre>
","['python', 'pandas', 'dataframe']",49963520,"<p>You need filter values of <code>c</code> by conditions and assign back column <code>c</code>:</p>

<pre><code>mask = (df['a']==1) &amp; (df['b']==1)
mean = df.loc[mask, 'c'].mean()
df.loc[mask, 'c'] = df.loc[mask, 'c'].fillna(mean)
</code></pre>

<p>Or use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.mask.html"" rel=""noreferrer""><code>mask</code></a> for replace by conditions:</p>

<pre><code>df['c'] = df['c'].mask(mask, df['c'].fillna(mean))
#similar
#df['c'] = np.where(mask, df['c'].fillna(mean), df['c'])

print (df)
    a  b    c
0   1  1  5.0
1   1  1  4.5
2   1  1  4.0
3   1  2  1.0
4   1  2  1.0
5   1  2  4.0
6   2  1  3.0
7   2  1  2.0
8   2  1  NaN
9   2  2  3.0
10  2  2  4.0
</code></pre>
",fillna selected rows pandas DataFrame I next pandas DataFrame b c None None I want replace None column mean I want select rows values b similar c None values selected rows replace c mean selected rows Something like code work df df amp b c fillna df df amp b c mean get output b c None,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
49965394,2018-04-22,2018,2,keyerror after removing nans in pandas,"<p>I am reading a file with <code>pd.read_csv</code> and removing all the values that are <code>-1</code>. Here's the code</p>

<pre><code>import pandas as pd
import numpy as np

columns = ['A', 'B', 'C', 'D']
catalog = pd.read_csv('data.txt', sep='\s+', names=columns, skiprows=1)

a = cataog['A']
b = cataog['B']
c = cataog['C']
d = cataog['D']

print len(b) # answer is 700

# remove rows that are -1 in column b
idx = np.where(b != -1)[0]
a = a[idx]
b = b[idx]
c = c[idx]
d = d[idx]

print len(b) # answer is 612
</code></pre>

<p>So I am assuming that I have successfully managed to remove all the rows where the value in column b is -1.</p>

<p>In order to test this, I am doing the following naive way:</p>

<pre><code>for i in range(len(b)):
    print i, a[i], b[i]
</code></pre>

<p>It prints out the values until it reaches a row which was supposedly filtered out. But now it gives a <code>KeyError</code>. </p>
","['python', 'pandas', 'numpy']",49965414,"<p>You can filtering by <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""nofollow noreferrer""><code>boolean indexing</code></a>:</p>

<pre><code>catalog = catalog[catalog['B'] != -1]

a = cataog['A']
b = cataog['B']
c = cataog['C']
d = cataog['D']
</code></pre>

<p>It is expected you get <code>KeyError</code>, because index values not match, because filtering.</p>

<p>One possible solution is convert <code>Series</code> to <code>list</code>s:</p>

<pre><code>for i in range(len(b)):
    print i, list(a)[i], list(b)[i]
</code></pre>

<p><strong>Sample</strong>:</p>

<pre><code>catalog = pd.DataFrame({'A':list('abcdef'),
                   'B':[-1,5,4,5,-1,4],
                   'C':[7,8,9,4,2,3],
                   'D':[1,3,5,7,1,0]})

print (catalog)
   A  B  C  D
0  a -1  7  1
1  b  5  8  3
2  c  4  9  5
3  d  5  4  7
4  e -1  2  1

#filtered DataFrame have no index 0, 4
catalog = catalog[catalog['B'] != -1]
print (catalog)
   A  B  C  D
1  b  5  8  3
2  c  4  9  5
3  d  5  4  7
5  f  4  3  0
</code></pre>

<hr>

<pre><code>a = catalog['A']
b = catalog['B']
c = catalog['C']
d = catalog['D']

print (b)
1    5
2    4
3    5
5    4
Name: B, dtype: int64

#a[i] in first loop want match index value 0 (a[0]) what does not exist, so KeyError,
#same problem for b[0]
for i in range(len(b)):
    print (i, a[i], b[i])
</code></pre>

<blockquote>
  <p>KeyError: 0</p>
</blockquote>

<hr>

<pre><code>#convert Series to list, so list(a)[0] return first value of list - there is no Series index
for i in range(len(b)):
    print (i, list(a)[i], list(b)[i])
0 b 5
1 c 4
2 d 5
3 f 4
</code></pre>

<p>Another solution should be create default index <code>0,1,...</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>reset_index</code></a> with <code>drop=True</code>:</p>

<pre><code>catalog = catalog[catalog['B'] != -1].reset_index(drop=True)    
print (catalog)
   A  B  C  D
0  b  5  8  3
1  c  4  9  5
2  d  5  4  7
3  f  4  3  0

a = catalog['A']
b = catalog['B']
c = catalog['C']
d = catalog['D']

#default index values match a[0] and a[b]
for i in range(len(b)):
    print (i, a[i], b[i])

0 b 5
1 c 4
2 d 5
3 f 4  
</code></pre>
",keyerror removing nans pandas I reading file pd read csv removing values Here code import pandas pd import numpy np columns A B C D catalog pd read csv data txt sep names columns skiprows cataog A b cataog B c cataog C cataog D print len b answer remove rows column b idx np b idx b b idx c c idx idx print len b answer So I assuming I successfully managed remove rows value column b In order test I following naive way range len b print b It prints values reaches row supposedly filtered But gives KeyError,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
50096891,2018-04-30,2018,3,Activity regularizer in TensorFlow,"<p>In Keras, for dense layers, we can use the parameter activity_regularizer.
In Tensorflow, there is no similar parameter.</p>

<p>Keras : </p>

<pre><code>from keras import regularizers
encoding_dim = 32
input_img = Input(shape=(784,))
# add a Dense layer with a L1 activity regularizer
encoded = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(10e-5))(input_img)
decoded = Dense(784, activation='sigmoid')(encoded)
autoencoder = Model(input_img, decoded)
</code></pre>

<p>How to make an activity_regularizer in tensorflow?</p>
","['python', 'tensorflow', 'keras']",50097680,"<p>The Keras documentation is not too precise, but from what I've read the activity regularization is simply a L1 or L2 term for the output of a specific layer added to the corresponding loss function of the model.</p>

<p>So let's say you have some loss, for example the MSE for some labels:</p>

<pre><code>loss = tf.metrics.mean_squared_error(labels, model_output)
</code></pre>

<p>To add L1 activity regularization to a certain layer you would simply add the L1 regularization term for the output of that layer to your loss with some regularization strength (I'll take <code>10e-5</code> like given in your question):</p>

<pre><code>loss += 10e-5*tf.nn.l1_loss(layer_output)
</code></pre>

<p>Where <code>layer_output</code> is the output of the layer you want to regulate.</p>

<p>If you did the same with the layer's weights instead of its output you would have what the Keras <a href=""https://keras.io/regularizers/"" rel=""noreferrer"">documentation</a> calls kernel regularization. If you do the same for the bias vector of that layer you get Keras's bias regularization.</p>
",Activity regularizer TensorFlow In Keras dense layers use parameter activity regularizer In Tensorflow similar parameter Keras keras import regularizers encoding dim input img Input shape add Dense layer L activity regularizer encoded Dense encoding dim activation relu activity regularizer regularizers l e input img decoded Dense activation sigmoid encoded autoencoder Model input img decoded How make activity regularizer tensorflow,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
50103918,2018-04-30,2018,3,How to compare two pandas dataframes and remove duplicates on one file without appending data from other file,"<p>I am trying to compare two csv files using pandas dataframes.  One is a master sheet that is going to have data appended to it daily (test_master.csv).  The second is a daily report (test_daily.csv) that contains the data I want to append to the test_master.csv.</p>

<p>I am creating two pandas dataframes from these files:</p>

<pre><code>import pandas as pd

dfmaster = pd.read_csv(test_master.csv)
dfdaily = pd.read_csv(test_daily.csv)
</code></pre>

<p>I want the daily list to get compared to the master list to see if there are any duplicate rows on the daily list that are already in the master list.  If so, I want them to remove the duplicates from dfdaily.  I then want to write this non-duplicate data to dfmaster.</p>

<p>The duplicate data will always be an entire row.  My plan was to iterate through the sheets row by row to make the comparison, then.</p>

<p>I realize I could append my daily data to the dfmaster dataframe and use drop_duplicates to remove the duplicates.  I cannot figure out how to remove the duplicates in the dfdaily dataframe, though.  And I need to be able to write the dfdaily data back to test_daily.csv (or another new file) without the duplicate data.</p>

<p>Here is an example of what the dataframes could look like.</p>

<p>test_master.csv</p>

<pre><code>  column 1   |  column 2   |  column 3   |
+-------------+-------------+-------------+
| 1           | 2           | 3           |
| 4           | 5           | 6           |
| 7           | 8           | 9           |
| duplicate 1 | duplicate 1 | duplicate 1 |
| duplicate 2 | duplicate 2 | duplicate 2
</code></pre>

<p>test_daily.csv</p>

<pre><code>+-------------+-------------+-------------+
|  column 1   |  column 2   |  column 3   |
+-------------+-------------+-------------+
| duplicate 1 | duplicate 1 | duplicate 1 |
| duplicate 2 | duplicate 2 | duplicate 2 |
| 10          | 11          | 12          |
| 13          | 14          | 15          |
+-------------+-------------+-------------+
</code></pre>

<p>Desired output is: </p>

<p>test_master.csv</p>

<pre><code>+-------------+-------------+-------------+
|  column 1   |  column 2   |  column 3   |
+-------------+-------------+-------------+
| 1           | 2           | 3           |
| 4           | 5           | 6           |
| 7           | 8           | 9           |
| duplicate 1 | duplicate 1 | duplicate 1 |
| duplicate 2 | duplicate 2 | duplicate 2 |
| 10          | 11          | 12          |
| 13          | 14          | 15          |
+-------------+-------------+-------------+
</code></pre>

<p>test_daily.csv</p>

<pre><code>+----------+----------+----------+
| column 1 | column 2 | column 3 |
+----------+----------+----------+
|       10 |       11 |       12 |
|       13 |       14 |       15 |
+----------+----------+----------+
</code></pre>

<p>Any help would be greatly appreciated! </p>

<p>EDIT</p>

<p>I incorrectly thought solutions from the <a href=""https://stackoverflow.com/questions/18180763/set-difference-for-pandas"">set difference</a> question solved my problem.  I ran into certain cases where those solutions did not work.  I believe it had something to do with index numbers labels as mentioned in a comment by Troy D below.  Troy D's solution is the solution that I am now using.</p>
","['python', 'pandas', 'csv']",50104566,"<p>Try this:</p>

<p>I crate 2 indexes, and then set rows 2-4 to be duplicates:</p>

<pre><code>import numpy as np

test_master = pd.DataFrame(np.random.rand(3, 3), columns=['A', 'B', 'C'])
test_daily = pd.DataFrame(np.random.rand(5, 3), columns=['A', 'B', 'C'])
test_daily.iloc[1:4] = test_master[:3].values

print(test_master)
print(test_daily)
</code></pre>

<p>output:</p>

<pre><code>      A         B         C
0  0.009322  0.330057  0.082956
1  0.197500  0.010593  0.356774
2  0.147410  0.697779  0.421207
      A         B         C
0  0.643062  0.335643  0.215443
1  0.009322  0.330057  0.082956
2  0.197500  0.010593  0.356774
3  0.147410  0.697779  0.421207
4  0.973867  0.873358  0.502973
</code></pre>

<p>Then, add a multiindex level to identify which data is from which dataframe:</p>

<pre><code>test_master['master'] = 'master'
test_master.set_index('master', append=True, inplace=True)
test_daily['daily'] = 'daily'
test_daily.set_index('daily', append=True, inplace=True)
</code></pre>

<p>Now merge as you suggested and drop duplicates:</p>

<pre><code>merged = test_master.append(test_daily)
merged = merged.drop_duplicates().sort_index()
print(merged)
</code></pre>

<p>output:</p>

<pre><code>             A         B         C
  master                              
0 daily   0.643062  0.335643  0.215443
  master  0.009322  0.330057  0.082956
1 master  0.197500  0.010593  0.356774
2 master  0.147410  0.697779  0.421207
4 daily   0.973867  0.873358  0.502973
</code></pre>

<p>There you see the combined dataframe with the origin of the data labeled in the index.  Now just slice for the daily data:</p>

<pre><code>idx = pd.IndexSlice
print(merged.loc[idx[:, 'daily'], :])
</code></pre>

<p>output:</p>

<pre><code>             A         B         C
  master                              
0 daily   0.643062  0.335643  0.215443
4 daily   0.973867  0.873358  0.502973
</code></pre>
",How compare two pandas dataframes remove duplicates one file without appending data file I trying compare two csv files using pandas dataframes One master sheet going data appended daily test master csv The second daily report test daily csv contains data I want append test master csv I creating two pandas dataframes files import pandas pd dfmaster pd read csv test master csv dfdaily pd read csv test daily csv I want daily list get compared master list see duplicate rows daily list already master list If I want remove duplicates dfdaily I want write non duplicate data dfmaster The duplicate data always entire row My plan iterate sheets row row make comparison I realize I could append daily data dfmaster dataframe use drop duplicates remove duplicates I cannot figure remove duplicates dfdaily dataframe though And I need able write dfdaily data back test daily csv another new file without,"startoftags, python, pandas, csv, endoftags",python pandas dataframe endoftags,python pandas csv,python pandas dataframe,0.67
50211016,2018-05-07,2018,2,Populating values from another dataframe whilst comparing datetimes,"<p>I have two dataframes of the below nature:</p>

<pre><code>df1
Index    Date          NextEntry_Date
0        2018-01-25
1        2018-02-25
2        2018-03-25
3        2018-04-25
4        2018-05-25
5        2018-06-25
6        2018-07-25
7        2018-08-25
8        2018-09-25
9        2018-10-25


df2
Index    Date 
2000    2018-07-20
2500    2019-07-20
2800    2020-07-20
</code></pre>

<p>I want to populate the NextEntry_Date column such that the df1 looks like below</p>

<pre><code>     df1
Index    Date          NextEntry_Date
0        2018-01-25    2018-07-20
1        2018-02-25    2018-07-20
2        2018-03-25    2018-07-20
3        2018-04-25    2018-07-20
4        2018-05-25    2018-07-20
5        2018-06-25    2018-07-20
6        2018-07-25    **2019-07-20**
7        2018-08-25    2019-07-20
8        2018-09-25    2019-07-20
9        2018-10-25    2019-07-20
</code></pre>

<p>Can you please advise...
Is there a way other than writing loops ?</p>
","['python', 'pandas', 'datetime']",50211306,"<p>This is one way using <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html"" rel=""nofollow noreferrer""><code>pandas.cut</code></a>:</p>

<pre><code># convert columns to datetime, if not already done
df1['Date'] = pd.to_datetime(df1['Date'])
df2['Date'] = pd.to_datetime(df2['Date'])

# get bins as list, adding one to the top for the earlier dates
bins = [pd.to_datetime('07-20-2017')] + df2['Date'].tolist()

# use pandas.cut to map bins with labels
df1['NextEntry_Date'] = pd.cut(df1['Date'], bins, labels=df2['Date'])

print(df1)

   Index       Date NextEntry_Date
0      0 2018-01-25     2018-07-20
1      1 2018-02-25     2018-07-20
2      2 2018-03-25     2018-07-20
3      3 2018-04-25     2018-07-20
4      4 2018-05-25     2018-07-20
5      5 2018-06-25     2018-07-20
6      6 2018-07-25     2019-07-20
7      7 2018-08-25     2019-07-20
8      8 2018-09-25     2019-07-20
9      9 2018-10-25     2019-07-20
</code></pre>
",Populating values another dataframe whilst comparing datetimes I two dataframes nature df Index Date NextEntry Date df Index Date I want populate NextEntry Date column df looks like df Index Date NextEntry Date Can please advise Is way writing loops,"startoftags, python, pandas, datetime, endoftags",python pandas dataframe endoftags,python pandas datetime,python pandas dataframe,0.67
50290390,2018-05-11,2018,3,List Serializer with dynamic fields in Django Rest Framework,"<p>I'm trying to add fields dynamically to a serializer of Django Rest Framework, by overwriting the <code>__init__</code> method. The approach is similar to the one described here: <a href=""http://masnun.com/2015/10/21/django-rest-framework-dynamic-fields-in-serializers.html"" rel=""nofollow noreferrer"">http://masnun.com/2015/10/21/django-rest-framework-dynamic-fields-in-serializers.html</a></p>

<p>The reason I'm doing this is, because I want to change the field type dynamically to a one, determined by the property <code>type_</code> of the instance to be serialized. This works pretty well, if I serialize one instance at a time:</p>

<pre><code>from rest_framework import serializers
from rest_framework.fields import empty


class VariableDetails:
    def __init__(self, name, type_, value):
        self.name = name
        self.type_ = type_
        self.value = value


class VariableDetailSerializer(serializers.Serializer):
    TYPE_FIELD_MAP = {
        'string': serializers.CharField,
        'integer': serializers.IntegerField,
        'float': serializers.FloatField,
    }

    name = serializers.CharField()
    type_ = serializers.CharField()

    def __init__(self, instance=None, data=empty, **kwargs):
        # this is where the magic happens
        super().__init__(instance, data, **kwargs)
        if instance is not None:
            field_type = self.TYPE_FIELD_MAP[instance.type_]
            self.fields['value'] = field_type()

string_details = VariableDetails('character value', 'string', 'hello world')
integer_details = VariableDetails('integer value', 'integer', 123)

print(VariableDetailSerializer(string_details).data)
# {'name': 'character value', 'type_': 'string', 'value': 'hello world'}

print(VariableDetailSerializer(integer_details).data)
# {'name': 'integer value', 'type_': 'integer', 'value': 123}
</code></pre>

<p>If I want to serialize multiple instances of <code>VariableDetails</code> that are related to a parent instance (calling it <code>Parent</code> for the example), the value field is missing:</p>

<pre><code>class Parent:
    def __init__(self, variable_details):
        self.variable_details = variable_details


class ParentSerializer(serializers.Serializer):
    variable_details = VariableDetailSerializer(many=True)


parent = Parent(variable_details=[string_details, integer_details])
print(ParentSerializer(parent).data)
# {
#     'variable_details': [
#         {
#             'name': 'character_value',
#             'type_': 'string'
#             # value is missing
#         },
#         {
#             'name': 'integer_value',
#             'type_': 'integer'
#         },
#     ]
# }
</code></pre>

<p>Apparently <code>VariableDetailSerializer.__init__</code> is only called during the creation of the <code>ParentSerializer</code> and once when a new instance of the <code>ParentSerializer</code> is initialized. In both cases <code>instance</code> is <code>None</code>. Hence, it is not called for each of the <code>VariableDetails</code>.</p>

<p>Does anybody know how to add fields dynamically to a <code>Serializer</code> that is also serializing list instances?</p>

<p>To facilitate running the code, I created a gist: <a href=""https://gist.github.com/larsrinn/861f8d50bf5bb0626d73321b546d8cb3"" rel=""nofollow noreferrer"">https://gist.github.com/larsrinn/861f8d50bf5bb0626d73321b546d8cb3</a> 
The code should be copy&amp;pastable into a python repl, if Django Rest Framework is installed. However, you don't need to create a Django Project</p>
","['python', 'django', 'django-rest-framework']",50299581,"<p>Because <code>VariableDetailSerializer</code> is nested in <code>ParentSerializer</code> it behaves a like a field itself. In this scenario you can override the <code>to_representation()</code> method to do the dynamic type switching.</p>

<pre><code>def to_representation(self, instance):
    field_type = self.TYPE_FIELD_MAP[instance.type_]
    self.fields['value'] = field_type()
    try:
        del self._readable_fields  # Clear the cache
    except AttributeError:
        pass
    return super().to_representation(instance)
</code></pre>

<p>The main catch here is that the serializer caches the fields because it does not expect them to change between instances. Therefore it is necessary to clear the cache with the <code>del self._readable_fields</code>.</p>

<p>When I add the above to <code>VariableDetailSerializer</code> and run your example, I get:</p>

<pre><code>{
    'variable_details': [
        OrderedDict([('name', u'character value'), 
                     ('type_', u'string'), 
                     ('value', u'hello world')]), 
        OrderedDict([('name', u'integer value'), 
                     ('type_', u'integer'), 
                     ('value', 123)])
    ]
}
</code></pre>
",List Serializer dynamic fields Django Rest Framework I trying add fields dynamically serializer Django Rest Framework overwriting init method The approach similar one described http masnun com django rest framework dynamic fields serializers html The reason I I want change field type dynamically one determined property type instance serialized This works pretty well I serialize one instance time rest framework import serializers rest framework fields import empty class VariableDetails def init self name type value self name name self type type self value value class serializers Serializer TYPE FIELD MAP string serializers CharField integer serializers IntegerField float serializers FloatField name serializers CharField type serializers CharField def init self instance None data empty kwargs magic happens super init instance data kwargs instance None field type self TYPE FIELD MAP instance type self fields value field type string details VariableDetails character value string hello world integer details VariableDetails integer value integer print,"startoftags, python, django, djangorestframework, endoftags",python tensorflow keras endoftags,python django djangorestframework,python tensorflow keras,0.33
50381076,2018-05-16,2018,2,Groupby and divide count of grouped elements in pyspark data frame,"<p>I have a data frame in <code>pyspark</code> like below. I want to do <code>groupby</code> and count of <code>category</code> column in <code>data frame</code></p>

<pre><code>df.show()
+--------+----+
|category| val|
+--------+----+
|    cat1|  13|
|    cat2|  12|
|    cat2|  14|
|    cat3|  23|
|    cat1|  20|
|    cat1|  10|
|    cat2|  30|
|    cat3|  11|
|    cat1|   7|
|    cat1|   8|
+--------+----+


res = df.groupBy('category').count()

res.show()

+--------+-----+
|category|count|
+--------+-----+
|    cat2|    3|
|    cat3|    2|
|    cat1|    5|
+--------+-----+
</code></pre>

<p>I am getting my desired result. Now I want to calculate the <code>average</code> of category. <code>data frame</code> has records for 3 days. I want to calculate average of count for these 3 days.</p>

<p>The result I want is below. I basically want to do <code>count/no.of.days</code> </p>

<pre><code>+--------+-----+
|category|count|
+--------+-----+
|    cat2|    1|
|    cat3|    1|
|    cat1|    2|
+--------+-----+
</code></pre>

<p>How can I do that?</p>
","['python', 'apache-spark', 'pyspark']",50381139,"<p>I believe what you want is </p>

<pre><code>from pyspark.sql import functions as F

df.groupby('category').agg((F.count('val') / 3).alias('average'))
</code></pre>
",Groupby divide count grouped elements pyspark data frame I data frame pyspark like I want groupby count category column data frame df show category val cat cat cat cat cat cat cat cat cat cat res df groupBy category count res show category count cat cat cat I getting desired result Now I want calculate average category data frame records days I want calculate average count days The result I want I basically want count days category count cat cat cat How I,"startoftags, python, apachespark, pyspark, endoftags",python pandas numpy endoftags,python apachespark pyspark,python pandas numpy,0.33
50383109,2018-05-17,2018,3,How to find the numbers in a list that come in numerical order in Python3?,"<p>I have a list, like this:</p>

<pre><code>list_1 = [1,2,2,3,1,2]
</code></pre>

<p>I want to create another nested lists like this:</p>

<pre><code>[[1,2,2,1,2],[2,2,3,2]]
</code></pre>

<p>The first nested list is only composed of 1 and 2 because, according to the numerical order, 2 comes after 1 and 1 is must be included. 3 is not in the first nested list because after 1 comes 2 and not 3.</p>

<p>In the second nested list, there is 2 and 3 because 3 comes after 2 and 2 needs to be included. 1 is not there because one doesn't come after 2.</p>

<p>How can I achieve this in Python 3?</p>
","['python', 'python-3.x', 'list']",50383181,"<pre><code>list_1 = [1,2,2,3,1,2]

m = max(list_1)
print([[i for i in list_1 if i in [j,j+1] ] for j in range(1,m)])

#[[1, 2, 2, 1, 2], [2, 2, 3, 2]]




#list_1 = [1,2,2,3,1,2,4,2,3,6,5,4]
#-&gt;[[1, 2, 2, 1, 2, 2], [2, 2, 3, 2, 2, 3], [3, 4, 3, 4], [4, 5, 4], [6, 5]]
</code></pre>
",How find numbers list come numerical order Python I list like list I want create another nested lists like The first nested list composed according numerical order comes must included first nested list comes In second nested list comes needs included one come How I achieve Python,"startoftags, python, python3x, list, endoftags",python python3x list endoftags,python python3x list,python python3x list,1.0
50418645,2018-05-18,2018,3,Unique Items in a pandas dataframe with a list,"<p>I'm trying to remove all columns from a pandas dataframe where there are fewer than 10 unique items in the column. However, some of my data is lists and I get the error <code>unhashable type: 'list'</code>. Makes sense, since pandas compares with a hashmap.</p>

<p>My current code is </p>

<pre><code>for i in df.columns:
    if len(df[i].unique()) &lt; 10:
        df.drop(i, 1)
</code></pre>

<p>which works fine up until I get to a list object. For my purposes, a list1 and list2 aren't unique. <code>[1, 2]</code> and <code>[2, 1]</code> are not unique, even though <code>[1, 2] == [2, 1]</code> is False.</p>

<p>How should I go about performing this operation? It doesn't make sense to separate the lists, and I can't explicitly type out the columns since I have 1400 of them.</p>

<p>Many thanks in advance!</p>
","['python', 'pandas', 'dataframe']",50419169,"<p><code>list</code> objects are not hashable because they are mutable, but <code>tuple</code>, on the other hand, are immutable. You can <code>transform</code> list values to <code>tuple</code> and make use of this property.</p>

<p>Suppose you have</p>

<pre><code>df = pd.DataFrame({""A"": [1,2,3,4], 
                   ""B"": [""a"", ""b"", ""c"", ""d""],
                   ""C"": [[1,2,3], [2], [2,3,1], [4]] })

    A   B   C
0   1   a   [1, 2, 3]
1   2   b   [2]
2   3   c   [2, 3, 1]
3   4   d   [4]
</code></pre>

<p>Thus, you can do something like</p>

<pre><code>df.C.apply(sorted).transform(tuple).unique()
</code></pre>

<p>which returns </p>

<pre><code>array([(1, 2, 3), (2,), (4,)], dtype=object)
</code></pre>

<p>Thus, your code could be something like below, making use of <code>collections.Hashable</code> to check whether the content of the column is indeed hashable or not</p>

<pre><code>import collections

for i in df.columns:
    if isinstance(df[i].iloc[0], collections.Hashable):
        if len(df[i].unique()) &lt; 10: 
            df = df.drop(i, 1)
    else:
        if len(df[i].apply(sorted).transform(tuple).unique()) &lt; 10: 
            df = df.drop(i, 1)
</code></pre>

<hr>

<p>Notice that this would also apply to other unhashable types, such as <code>dict</code>s</p>

<pre><code>&gt;&gt;&gt; df[""D""] = [{""a"":2}, {}, {""k"":3}, {""k"":3}]})
&gt;&gt;&gt; print(df.D.apply(sorted).transform(tuple).unique())
[('a',) () ('k',)]
</code></pre>
",Unique Items pandas dataframe list I trying remove columns pandas dataframe fewer unique items column However data lists I get error unhashable type list Makes sense since pandas compares hashmap My current code df columns len df unique lt df drop works fine I get list object For purposes list list unique unique even though False How I go performing operation It make sense separate lists I explicitly type columns since I Many thanks advance,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
50432927,2018-05-20,2018,2,"TypeError: must be str, not int couldn&#39;t be solved","<p>The original time data is like thisï¼</p>

<pre><code>df['time'][0:4]

   2015-07-08
        05-11
        05-12
   2008-07-26
</code></pre>

<p>I want all these data contains year value.
And I applied this:</p>

<p>con_time = []</p>

<pre><code>i=0
for i in df['time']:
    if len(df['time'])==5:
        time = '2018'+'-'+df['time']
        con_time.append(time)
        i +=1
    else:
        con_time.append(df['time']) 
        i +=1
</code></pre>

<p>Error occurred:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-78-b7d87c72f412&gt; in &lt;module&gt;()
      8     else:
      9         con_time.append(df['time'])
---&gt; 10         i +=1

TypeError: must be str, not int
</code></pre>

<p>This error is so strange....
Actually I want to create a new list, converting it to a np.array and concat it into the df.
Do I have a better way to achieve the goal?</p>
","['python', 'pandas', 'numpy', 'dataframe']",50433059,"<p>Since you have asked about an alternative approach. instead of an explicit loop in python and filling a list, one should rather use DataFrame methods directly. In your case this would be</p>

<pre><code>df['time'].apply(lambda x: x if len(x) != 5 else '2018-'+x)
</code></pre>

<p>This might run faster for some datasets</p>

<p><strong>EDIT</strong>
I actually ran a timing benchmark using a random toy dataset with ~50% of complete and incomplete dates. In short, it seems that for a small dataset the simple for-loop solution is faster for a large dataset both methods show similar performance:</p>

<pre><code># 1M examples
import random
import numpy as np
y = pd.Series(np.random.randint(0,2,1000000))
s = {0:'2015-07-08',  1:'05-11'}
y = y.map(s)
%%timeit -n100
_ = y.apply(lambda x: x if len(x) != 5 else '2018-'+x)
&gt;&gt;&gt; 275 ms Â± 6.42 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each)
%%timeit -n100
con_time = []
for i in y:
    if len(i)==5:
        time = '2018-'+i
        con_time.append(time)
    else:
        con_time.append(i) 
con_time_a = np.array(con_time)
&gt;&gt;&gt; 289 ms Â± 5.23 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each)

# 1K examples
import random
import numpy as np
y = pd.Series(np.random.randint(0,2,1000))
s = {0:'2015-07-08',  1:'05-11'}
y = y.map(s)
%%timeit -n100
_ = y.apply(lambda x: x if len(x) != 5 else '2018-'+x)
&gt;&gt;&gt; 431 Âµs Â± 70.3 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
%%timeit -n100
con_time = []
for i in y:
    if len(i)==5:
        time = '2018-'+i
        con_time.append(time)
    else:
        con_time.append(i) 
con_time_a = np.array(con_time)
&gt;&gt;&gt; 289 Âµs Â± 40.4 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
</code></pre>
",TypeError must str int solved The original time data like df time I want data contains year value And I applied con time df time len df time time df time con time append time else con time append df time Error occurred TypeError Traceback recent call last lt ipython input b c f gt lt module gt else con time append df time gt TypeError must str int This error strange Actually I want create new list converting np array concat df Do I better way achieve goal,"startoftags, python, pandas, numpy, dataframe, endoftags",python pandas dataframe endoftags,python pandas numpy dataframe,python pandas dataframe,0.87
50535182,2018-05-25,2018,2,tkinter.Text widget exceeds grid manager constraints,"<p><strong>TL;DR:</strong></p>

<p>Text widget expands further than a specified column, row combo of a <code>grid()</code> call. Only resolution comes from changing the <code>.width</code> and <code>.height</code> attributes to something small and forcing the sticky flag of the call to expand and fill using <code>'nsew'</code>.</p>

<p><strong>Background:</strong></p>

<p>The Text widget has sparked some confusion for me, take for example the following:</p>

<pre><code>root = Tk()
root.geometry('400x400')
root.grid_columnconfigure(0, weight = 1)
root.grid_columnconfigure(1, weight = 1)
root.grid_rowconfigure(0, weight = 1)
root.grid_rowconfigure(1, weight = 1)
b = Button(root)
b.grid(column = 0, row = 0, sticky = 'nsew')
t = Text(root)
t.grid(column = 1, row = 1, sticky = 'nsew')
</code></pre>

<p>Given this, I should have a 400x400 Window like:</p>

<pre><code>+-------+-------+
|       |       |
|   b   |       |
|       |       |
+-------+-------+
|       |       |
|       |   t   |
|       |       |
+-------+-------+
</code></pre>

<p>But what ends up happening is that the Text widget goes well beyond the lower right column, row:</p>

<pre><code>+---+-----------+
| b |           |
+---+           |
|   |     t     |
|   |           |
|   |           |
|   |           |
+---+-----------+
</code></pre>

<p>The only way to resolve this is to alter the Text widget like so:</p>

<p><code>t = Text(root, height = 1, width = 1)</code></p>

<p>Then the sticky call expands the widget to only fit within the column, row of the grid. This however, seems like an extra step I shouldn't have to take even though the Text widget defaults to a <code>.width</code> and <code>.height</code> like all the other widgets.</p>

<p><strong>Question:</strong>
Why does the Text widget expand beyond the specified column, row of a grid?</p>
","['python', 'python-3.x', 'tkinter']",50535419,"<blockquote>
  <p>Question: Why does the Text widget expand beyond the specified column, row of a grid?</p>
</blockquote>

<p>It doesn't; it's the other way around: the rows and columns expand to fit the text widget.  This isn't unique to the text widget. Tkinter will try to arrange things so that any widget will be displayed in its requested size if possible. In the case of a text widget, the default size is 80 characters by 24 characters, so your row and column expand to try to fit that widget.</p>

<p>This can be fairly easily seen if you add something in the third column and third row.  You'll see that the second row is very tall and the second column is very wide. ie: the row and column expands to fit the text widget, rather than the text widget going over the edge of the row and column.</p>

<p>Add the following to your example code:</p>

<pre><code>bottom_edge = Frame(root, background=""red"", height=4)
right_edge = Frame(root, background=""red"", width=4)
bottom_edge.grid(row=2, column=0, columnspan=3, sticky=""ew"")
right_edge.grid(row=0, column=2, rowspan=3, sticky=""ns"")
</code></pre>

<p>You'll have to resize the window since you made the window too small to hold everything. Once you do, you'll see a red border on the right and below, showing that it is the row and column that is expanding, rather than the text widget expanding beyond the row and column. The text widget is within the bounds of the second row, second column.</p>

<p><a href=""https://i.stack.imgur.com/4y3mJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4y3mJ.png"" alt=""enter image description here""></a></p>
",tkinter Text widget exceeds grid manager constraints TL DR Text widget expands specified column row combo grid call Only resolution comes changing width height attributes something small forcing sticky flag call expand fill using nsew Background The Text widget sparked confusion take example following root Tk root geometry x root grid columnconfigure weight root grid columnconfigure weight root grid rowconfigure weight root grid rowconfigure weight b Button root b grid column row sticky nsew Text root grid column row sticky nsew Given I x Window like b But ends happening Text widget goes well beyond lower right column row b The way resolve alter Text widget like Text root height width Then sticky call expands widget fit within column row grid This however seems like extra step I take even though Text widget defaults width height like widgets Question Why Text widget expand beyond specified column row grid,"startoftags, python, python3x, tkinter, endoftags",python python3x list endoftags,python python3x tkinter,python python3x list,0.67
50552756,2018-05-27,2018,3,"From pandas dataframe, how to find the number of duplicate comments for each user?","<p>I have dataframe with list of usernames and their comments, see format below.</p>

<p>What would be the quickest and most efficient approach to find repetitive duplicate comments (spam) for each user? </p>

<p>Dataframe format: </p>

<pre><code>Author  | Comment
casy    Nice picture! 
linda   I like this 
casy    Nice picture! 
tom     I disagree 
bob     Follow me 
bob     Follow me 
bob     Follow me 
bob     Follow me 
casy    Nice picture! 
casy    Wow! 
linda   Interesting post 
linda   Check my profile
bob     Dissapointing
casy    Wow! 
</code></pre>

<p>I want to get the result in the following format, so the resulting table would be: </p>

<pre><code>Author  | Number of dup. comments (descending)  | Comment   
bob     4   Follow me 
casy    3   Nice picture
casy    2   Wow! 
bob     1   Dissapointing 
linda   1   I like this 
linda   1   Check my profile
linda   1   Interesting post 
tom     1   I disagree
</code></pre>
","['python', 'pandas', 'dataframe']",50552776,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>groupby</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.size.html"" rel=""nofollow noreferrer""><code>size</code></a> first, then <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html"" rel=""nofollow noreferrer""><code>sort_values</code></a>, create columns by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>reset_index</code></a> and last if necessary change order of columns by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reindex.html"" rel=""nofollow noreferrer""><code>reindex</code></a>:</p>

<pre><code>df = (df.groupby(['Author', 'Comment'], sort=False).size()
       .sort_values(ascending=False)
       .reset_index(name='Number')
       .reindex(columns=['Author','Number','Comment']))
print (df)
  Author  Number           Comment
0    bob       4         Follow me
1   casy       3     Nice picture!
2   casy       2              Wow!
3    bob       1     Dissapointing
4  linda       1  Check my profile
5  linda       1  Interesting post
6    tom       1        I disagree
7  linda       1       I like this
</code></pre>
",From pandas dataframe find number duplicate comments user I dataframe list usernames comments see format What would quickest efficient approach find repetitive duplicate comments spam user Dataframe format Author Comment casy Nice picture linda I like casy Nice picture tom I disagree bob Follow bob Follow bob Follow bob Follow casy Nice picture casy Wow linda Interesting post linda Check profile bob Dissapointing casy Wow I want get result following format resulting table would Author Number dup comments descending Comment bob Follow casy Nice picture casy Wow bob Dissapointing linda I like linda Check profile linda Interesting post tom I disagree,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
50594318,2018-05-29,2018,4,numpy array indicator operation,"<p>I want to modify an empty bitmap by given indicators (x and y axis).
For every coordinate given by the indicators the value should be raised by one.</p>

<p>So far so good everything seems to work. But if I have some similar indicators in my array of indicators it will only raise the value once.</p>

<pre><code>&gt;&gt;&gt; img
array([[0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0]])

&gt;&gt;&gt; inds
array([[0, 0],
       [3, 4],
       [3, 4]])
</code></pre>

<p>Operation:</p>

<pre><code>&gt;&gt;&gt; img[inds[:,1], inds[:,0]] += 1
</code></pre>

<p>Result:</p>

<pre><code>&gt;&gt;&gt; img
    array([[1, 0, 0, 0, 0],
           [0, 0, 0, 0, 0],
           [0, 0, 0, 0, 0],
           [0, 0, 0, 0, 0],
           [0, 0, 0, 1, 0]])
</code></pre>

<p>Expected result:</p>

<pre><code>&gt;&gt;&gt; img
    array([[1, 0, 0, 0, 0],
           [0, 0, 0, 0, 0],
           [0, 0, 0, 0, 0],
           [0, 0, 0, 0, 0],
           [0, 0, 0, 2, 0]])
</code></pre>

<p>Does someone have an idea how to solve this? Preferably a fast approach without the use of loops.</p>
","['python', 'arrays', 'numpy']",50594865,"<p>Two remarks on the other two answers:</p>

<p>1) @jpp's can be improved by using <code>np.unique</code> with the <code>axis</code> and <code>return_counts</code> keywords.</p>

<p>2) If we translate to flat indexing we can use <code>np.bincount</code> which often (but not always, see last test case in benchmarks) is faster than <code>np.add.at</code>.</p>

<p>Thanks @miradulo for initial version of benchmarks.</p>

<pre><code>import numpy as np

def jpp(img, inds):
    counts = (inds[:, None] == inds).all(axis=2).sum(axis=1)
    img[inds[:,1], inds[:,0]] += counts

def jpp_pp(img, inds):
    unq, cnts = np.unique(inds, axis=0, return_counts=True)
    img[unq[:,1], unq[:,0]] += cnts

def miradulo(img, inds):
    np.add.at(img, tuple(inds[:, [1, 0]].T), 1)

def pp(img, inds):
    imgf = img.ravel()
    indsf = np.ravel_multi_index(inds.T[::-1], img.shape[::-1])
    imgf += np.bincount(indsf, None, img.size)

inds = np.random.randint(0, 5, (3, 2))
big_inds = np.random.randint(0, 5, (10000, 2))
sml_inds = np.random.randint(0, 1000, (5, 2))
from timeit import timeit


for f in jpp, jpp_pp, miradulo, pp:
    print(f.__name__)
    for i, n, a in [(inds, 1000, 5), (big_inds, 10, 5), (sml_inds, 10, 1000)]:
        img = np.zeros((a, a), int)
        print(timeit(""f(img, i)"", globals=dict(img=img, i=i, f=f), number=n) * 1000 / n, 'ms')
</code></pre>

<p>Output:</p>

<pre><code>jpp
0.011815106990979984 ms
2623.5026352020213 ms
0.04642329877242446 ms
jpp_pp
0.041291153989732265 ms
5.418520100647584 ms
0.05826510023325682 ms
miradulo
0.007099648006260395 ms
0.7788308983435854 ms
0.009103797492571175 ms
pp
0.0035401539935264736 ms
0.06540440081153065 ms
3.486583800986409 ms
</code></pre>
",numpy array indicator operation I want modify empty bitmap given indicators x axis For every coordinate given indicators value raised one So far good everything seems work But I similar indicators array indicators raise value gt gt gt img array gt gt gt inds array Operation gt gt gt img inds inds Result gt gt gt img array Expected result gt gt gt img array Does someone idea solve Preferably fast approach without use loops,"startoftags, python, arrays, numpy, endoftags",python python3x numpy endoftags,python arrays numpy,python python3x numpy,0.67
50604328,2018-05-30,2018,3,Adding two unequally sized numpy arrays (length n and m) to produce a n * m array without using for loops,"<p>I'm trying to vectorise a couple of simple calculations. I have two arrays:</p>

<pre><code>A = np.array([1,2,3,4])
B = np.array([1,2])
</code></pre>

<p>I want to compute an array C, which is calculated as follows:</p>

<pre><code>C = []
for i in A:
  for j in B:
     C.append(i+j)

C = np.array(C)
</code></pre>

<p>such that</p>

<pre><code>C = array([2, 3, 3, 4, 4, 5, 5, 6])
</code></pre>

<p>In reality the arrays are quite large and these for loops take quite a while. Is there a way to vectorise these calculations?</p>
","['python', 'arrays', 'numpy']",50604383,"<p>Use broadcasting:</p>

<pre><code>In [4]: (A[:, None] + B).reshape(-1)
Out[4]: array([2, 3, 3, 4, 4, 5, 5, 6])
</code></pre>

<p>Or <code>outer</code>:</p>

<pre><code>In [7]: np.add.outer(A, B).reshape(-1)
Out[7]: array([2, 3, 3, 4, 4, 5, 5, 6])
</code></pre>
",Adding two unequally sized numpy arrays length n produce n array without using loops I trying vectorise couple simple calculations I two arrays A np array B np array I want compute array C calculated follows C A j B C append j C np array C C array In reality arrays quite large loops take quite Is way vectorise calculations,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
50652209,2018-06-01,2018,3,conv2d in custom Keras loss function,"<p>I am trying to implement a custom loss function in Keras with TF backend based on the Laplacian of two images.</p>

<pre><code>def blur_loss(y_true, y_pred):
    #weighting of blur loss
    alpha = 1
    mae = losses.mean_absolute_error(y_true, y_pred)
    lapKernel = K.constant([0, 1, 0, 1, -4, 1, 0, 1, 0],shape = [3, 3])

    trueLap = K.conv2d(y_true, lapKernel)
    predLap = K.conv2d(y_pred, lapKernel)
    trueBlur = K.var(trueLap)
    predBlur = K.var(predLap)
    blurLoss = alpha * K.abs(trueBlur - predBlur)
    loss = (1-alpha) * mae + alpha * blurLoss
    return loss
</code></pre>

<p>When I try to compile the model I get this error</p>

<pre><code>Traceback (most recent call last):
  File ""kitti_train.py"", line 65, in &lt;module&gt;
    model.compile(loss='mean_absolute_error', optimizer='adam', metrics=[blur_loss])
  File ""/home/ubuntu/.virtualenvs/dl4cv/lib/python3.5/site-packages/keras/engine/training.py"", line 924, in compile
    handle_metrics(output_metrics)
  File ""/home/ubuntu/.virtualenvs/dl4cv/lib/python3.5/site-packages/keras/engine/training.py"", line 921, in handle_metrics
    mask=masks[i])
  File ""/home/ubuntu/.virtualenvs/dl4cv/lib/python3.5/site-packages/keras/engine/training.py"", line 450, in weighted
    score_array = fn(y_true, y_pred)
  File ""/home/ubuntu/prednet/blur_loss.py"", line 14, in blur_loss
    trueLap = K.conv2d(y_true, lapKernel)
  File ""/home/ubuntu/.virtualenvs/dl4cv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 3164, in conv2d
    data_format='NHWC')
  File ""/home/ubuntu/.virtualenvs/dl4cv/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py"", line 655, in convolution
    num_spatial_dims, strides, dilation_rate)
  File ""/home/ubuntu/.virtualenvs/dl4cv/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py"", line 483, in _get_strides_and_dilation_rate
    (len(dilation_rate), num_spatial_dims))
ValueError: len(dilation_rate)=2 but should be 0
</code></pre>

<p>After reading other questions, my understanding is that this problem stems from the compilation using placeholder tensors for y_true and y_pred. I've tried checking if the inputs are placeholders and replacing them with zero tensors, but this gives me other errors.</p>

<p>How do I use a convolution (the image processing function, not a layer) in my loss function without getting these errors?</p>
","['python', 'tensorflow', 'keras']",50686150,"<p>The problem here was a misunderstanding of the conv2d function which is not simply a 2-dimensional convolution. It is a batched 2-d convolution of multiple channels. So while you might expect a *2d function to accept 2-dimensional tensors, the input should actually 4 dimensions (batch_size, height, width, channels) and the filter should also be 4 dimensions (filter_height, filter_width, input_channels, output_channels). Details can be found in the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"" rel=""nofollow noreferrer"">TF docs</a></p>
",conv custom Keras loss function I trying implement custom loss function Keras TF backend based Laplacian two images def blur loss true pred weighting blur loss alpha mae losses mean absolute error true pred lapKernel K constant shape trueLap K conv true lapKernel predLap K conv pred lapKernel trueBlur K var trueLap predBlur K var predLap blurLoss alpha K abs trueBlur predBlur loss alpha mae alpha blurLoss return loss When I try compile model I get error Traceback recent call last File kitti train py line lt module gt model compile loss mean absolute error optimizer adam metrics blur loss File home ubuntu virtualenvs dl cv lib python site packages keras engine training py line compile handle metrics output metrics File home ubuntu virtualenvs dl cv lib python site packages keras engine training py line handle metrics mask masks File home ubuntu virtualenvs dl cv lib python site packages keras,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
50778190,2018-06-09,2018,3,Pandas dataframe creation returning none,"<p>I want to add a column of 1s in the beginning of a pandas dataframe which is created from an external data file 'ex1data1.txt'. I wrote the following code. The problem is the <code>print(data)</code> command, in the end, is returning None. What is wrong with this code? I want <code>data</code> to be a pandas dataframe. The <code>raw_data</code> and <code>X0_</code> are fine, I have printed them. </p>

<pre><code>import numpy as np
import pandas as pd
raw_data = pd.read_csv('ex1data1.txt', header= None, names= ['x1','y'])
X0_ = np.ones(len(raw_data))
idx = 0
data = raw_data.insert(loc=idx, column='x0', value=X0_)
print(data)
</code></pre>
","['python', 'python-3.x', 'pandas', 'dataframe']",50778272,"<p>Another solution might look like this:</p>

<pre><code>import numpy as np
import pandas as pd
raw_data = pd.read_csv('ex1data1.txt', header= None, names= ['x1','y'])

raw_data.insert(loc=0, column='x0', value=1.0)

print(raw_data)
</code></pre>
",Pandas dataframe creation returning none I want add column beginning pandas dataframe created external data file ex data txt I wrote following code The problem print data command end returning None What wrong code I want data pandas dataframe The raw data X fine I printed import numpy np import pandas pd raw data pd read csv ex data txt header None names x X np ones len raw data idx data raw data insert loc idx column x value X print data,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas matplotlib endoftags,python python3x pandas dataframe,python pandas matplotlib,0.58
50789880,2018-06-11,2018,4,How to get the column name in pandas based on row values?,"<p>I have a dataframe, df:</p>

<pre><code>        id_0         id_1          id_2
0         1            0             1
1         1            0             0
2         0            1             0
3         1            1             0
4         0            0             1
5         0            0             0
</code></pre>

<p>I want to get the column name for each row if there is 1. How to do that? Thank you.</p>

<p>Result:</p>

<pre><code>           result
0         id_0, id_2
1         id_0
2         id_1
3         id_0, id_1
4         id_2
5         NaN
</code></pre>
","['python', 'pandas', 'dataframe']",50789941,"<p>Using <code>dot</code></p>

<pre><code>df.dot(df.columns+',').str[:-1]
Out[168]: 
0    id_0,id_2
1         id_0
2         id_1
3    id_0,id_1
4         id_2
5             
dtype: object
</code></pre>
",How get column name pandas based row values I dataframe df id id id I want get column name row How Thank Result result id id id id id id id NaN,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
50864196,2018-06-14,2018,3,Populate new columns when list values match substring of column values in Pyspark dataframe,"<p>I have a data frame in <code>Pyspark</code> like below</p>

<pre><code>df.show()

+---+----------------------+
| id|                   con|
+---+----------------------+
|  3|           mac,mac pro|
|  1|        iphone5,iphone|
|  1| android,android phone|
|  1|    windows,windows pc|
|  1| spy camera,spy camera|
|  2|               camera,|
|  3|             cctv,cctv|
|  2|   apple iphone,iphone|
|  3|           ,spy camera|
+---+----------------------+
</code></pre>

<p>I want to create new columns based on certain <code>lists</code>. Lists are below</p>

<pre><code>phone_list = ['iphone', 'android', 'nokia']
pc_list = ['windows', 'mac']
</code></pre>

<p><code>Condition:</code></p>

<pre><code>if a element in a list matches a string/substring in a column then flag the column to the value of that particular list
</code></pre>

<p>Basically what I want is in <code>phone_list</code> I have element <code>iphone</code> so that should match <code>id</code> <code>1</code> where <code>con</code> is <code>iphone5, iphone</code> and flag as <code>phones</code> and so on.</p>

<p><code>Expected result</code></p>

<pre><code>+---+----------------------+------+----+
| id|                   con|   cat| abc|
+---+----------------------+------+----+
|  3|           mac,mac pro|  null|  pc|
|  1|        iphone5,iphone|phones|null|
|  1| android,android phone|phones|null|
|  1|    windows,windows pc|  null|  pc|
|  1| spy camera,spy camera|  null|null|
|  2|               camera,|  null|null|
|  3|             cctv,cctv|  null|null|
|  2|   apple iphone,iphone|phones|null|
|  3|           ,spy camera|  null|null|
+---+----------------------+------+----+
</code></pre>

<p>I have done like below.</p>

<pre><code>df1 = df.withColumn('cat', F.when(df.con.isin(phone_list), 'phones')).withColumn('abc', F.when(df.con.isin(pc_list), 'pc'))
</code></pre>

<p><code>output</code></p>

<pre><code>df1.show()

+---+----------------------+----+----+
| id|                   con| cat| abc|
+---+----------------------+----+----+
|  3|           mac,mac pro|null|null|
|  1|        iphone5,iphone|null|null|
|  1| android,android phone|null|null|
|  1|    windows,windows pc|null|null|
|  1| spy camera,spy camera|null|null|
|  2|               camera,|null|null|
|  3|             cctv,cctv|null|null|
|  2|   apple iphone,iphone|null|null|
|  3|           ,spy camera|null|null|
+---+----------------------+----+----+
</code></pre>

<p>How can I do this type of comparison in the correct way?</p>
","['python', 'apache-spark', 'pyspark']",50864479,"<p>The best way would be to avoid using <code>udf</code> and use <a href=""http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.Column.rlike"" rel=""nofollow noreferrer""><code>pyspark.sql.Column.rlike()</code></a>. This will return <code>True</code> if the column matches the regular expression contained within the argument.</p>

<p>In this case, you can use <code>""|"".join(list_of_terms)</code> to create a regex pattern that will match any word in the list. (<code>""|""</code> is the <code>OR</code> operator)</p>

<pre><code>from pyspark.sql.functions import col, when

df.select(
    ""*"", 
    when(col(""con"").rlike(""|"".join(phone_list)), ""phones"").alias(""cat""), 
    when(col(""con"").rlike(""|"".join(pc_list)), ""pc"").alias(""abc"")
).show(truncate=False)
#+---+---------------------+------+----+
#|id |con                  |cat   |abc |
#+---+---------------------+------+----+
#|3  |mac,mac pro          |null  |pc  |
#|1  |iphone5,iphone       |phones|null|
#|1  |android,android phone|phones|null|
#|1  |windows,windows pc   |null  |pc  |
#|1  |spy camera,spy camera|null  |null|
#|2  |camera,              |null  |null|
#|3  |cctv,cctv            |null  |null|
#|2  |apple iphone,iphone  |phones|null|
#|3  |,spy camera          |null  |null|
#+---+---------------------+------+----+
</code></pre>

<p>We're also using the fact that <a href=""http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.when"" rel=""nofollow noreferrer""><code>pyspark.sql.functions.when()</code></a> will return <code>null</code> if no <code>otherwise()</code> condition is specified.</p>
",Populate new columns list values match substring column values Pyspark dataframe I data frame Pyspark like df show id con mac mac pro iphone iphone android android phone windows windows pc spy camera spy camera camera cctv cctv apple iphone iphone spy camera I want create new columns based certain lists Lists phone list iphone android nokia pc list windows mac Condition element list matches string substring column flag column value particular list Basically I want phone list I element iphone match id con iphone iphone flag phones Expected result id con cat abc mac mac pro null pc iphone iphone phones null android android phone phones null windows windows pc null pc spy camera spy camera null null camera null null cctv cctv null null apple iphone iphone phones null spy camera null null I done like df df withColumn cat F df con isin phone list phones withColumn,"startoftags, python, apachespark, pyspark, endoftags",python pandas numpy endoftags,python apachespark pyspark,python pandas numpy,0.33
50934089,2018-06-19,2018,4,&quot;Drop random rows&quot; from pandas dataframe,"<p>In a pandas dataframe, how can I drop a <strong>random subset</strong> of rows that <strong>obey a condition</strong>?</p>

<p>In other words, if I have a Pandas dataframe with a <code>Label</code> column, I'd like to drop 50% (or some other percentage) of rows where <code>Label == 1</code>, but keep all of the rest:</p>

<pre><code>Label A     -&gt;    Label A
0     1           0     1
0     2           0     2
0     3           0     3
1     10          1     11
1     11          1     12
1     12
1     13
</code></pre>

<p>I'd love to know the simplest and most pythonic/panda-ish way of doing this!</p>

<hr>

<p>Edit: <a href=""https://stackoverflow.com/questions/28556942/pandas-remove-rows-at-random-without-shuffling-dataset"" title=""This question"">This question</a> provides part of an answer, but it only talks about dropping rows by index, disregarding the row values. I'd still like to know how to drop only from rows that are labeled a certain way.</p>
","['python', 'pandas', 'dataframe']",50934196,"<p>Use the <code>frac</code> argument</p>

<pre><code>df.sample(frac=.5)
</code></pre>

<p>If you define the amount you want to drop in a variable <code>n</code></p>

<pre><code>n = .5
df.sample(frac=1 - n)
</code></pre>

<p>To include the condition, use <code>drop</code></p>

<pre><code>df.drop(df.query('Label == 1').sample(frac=.5).index)

   Label   A
0      0   1
1      0   2
2      0   3
4      1  11
6      1  13
</code></pre>
",quot Drop random rows quot pandas dataframe In pandas dataframe I drop random subset rows obey condition In words I Pandas dataframe Label column I like drop percentage rows Label keep rest Label A gt Label A I love know simplest pythonic panda ish way Edit This question provides part answer talks dropping rows index disregarding row values I still like know drop rows labeled certain way,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
51121583,2018-07-01,2018,2,Double look-ahead assertion in python,"<p>I know that look ahead assertions matches based on the condition. But suddenly I was struck by this double look-ahead assertion.</p>

<pre><code>&gt;&gt;&gt; a = compile(r'a(?=b)(?=c)')
&gt;&gt;&gt; b = a.findall('abc')
&gt;&gt;&gt; b
[]
</code></pre>

<p>Then what it matches. Thanks in advance!</p>
","['python', 'regex', 'python-3.x']",51121632,"<p>You are matching <code>a</code> and <a href=""https://www.rexegg.com/regex-disambiguation.html#lookarounds"" rel=""nofollow noreferrer"">assert</a> that after a, there should be a <code>b</code>. That assertions succeeds.</p>

<p>But after that, you assert that what comes after the <code>a</code> should be a <code>c</code>. That assertion fails, so there will be no match.</p>

<p>This for example <a href=""https://regex101.com/r/ehokah/1"" rel=""nofollow noreferrer""><code>a(?=b)</code></a> will succeed and matches <code>a</code>.</p>

<p>This <a href=""https://regex101.com/r/ehokah/2"" rel=""nofollow noreferrer""><code>a(?=c)</code></a> will not succeed because there is no c after a.</p>

<p>To assert that there is <code>bc</code> after a you might use <a href=""https://regex101.com/r/ehokah/3"" rel=""nofollow noreferrer""><code>a(?=bc)</code></a> or an assertion inside an assertion <a href=""https://regex101.com/r/ehokah/4"" rel=""nofollow noreferrer""><code>a(?=b(?=c))</code></a> as <a href=""https://stackoverflow.com/users/7832176/keyur-potdar"">@Keyur Potdar</a> points out.</p>
",Double look ahead assertion python I know look ahead assertions matches based condition But suddenly I struck double look ahead assertion gt gt gt compile r b c gt gt gt b findall abc gt gt gt b Then matches Thanks advance,"startoftags, python, regex, python3x, endoftags",python python3x list endoftags,python regex python3x,python python3x list,0.67
51495782,2018-07-24,2018,2,Return all rows after groupby pandas (i.e. not a reduced number of rows that is the unique values for the group key),"<p>The following code from the tutorials yields the following results:</p>

<p>Code:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',
                    'foo', 'bar', 'foo', 'foo'],
                    'B' : ['one', 'one', 'two', 'three',
                    'two', 'two', 'one', 'three'],
                    'C' : np.random.randn(8),
                    'D' : np.random.randn(8)})

print(df)

grouped = df.groupby('A').mean()
print(grouped)
</code></pre>

<p>Result:</p>

<pre><code>     A      B         C         D
0  foo    one -0.787410 -0.857863
1  bar    one  0.140572  1.330183
2  foo    two -0.770166  2.123528
3  bar  three -0.965523  0.771663
4  foo    two  0.215037 -0.597935
5  bar    two -1.023839 -0.248445
6  foo    one -1.377515  2.041921
7  foo  three -0.314333  1.379423
            C         D
A                      
bar -0.616263  0.617800
foo -0.606877  0.817815
</code></pre>

<p>However I would like to see all the rows as in the following:</p>

<pre><code>0   foo one   -0.606877   0.817815
1   bar one   -0.616263   0.617800
2   foo two   -0.606877   0.817815
3   bar three -0.616263   0.617800
4   foo two   -0.606877   0.817815
5   bar two   -0.616263   0.617800
6   foo one   -0.606877   0.817815
7   foo three -0.606877   0.817815
</code></pre>

<p>I am open to use any other library as well. I just need to do this fast and efficiently using python3</p>

<p>Thanks in advance</p>
","['python', 'pandas', 'pandas-groupby']",51495900,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.transform.html"" rel=""nofollow noreferrer""><code>GroupBy.transform</code></a> with specifying columns:</p>

<pre><code>cols = ['C','D']
df[cols] = df.groupby('A')[cols].transform('mean')
print(df)
     A      B         C         D
0  foo    one  0.444616 -0.232363
1  bar    one  0.173897 -0.603437
2  foo    two  0.444616 -0.232363
3  bar  three  0.173897 -0.603437
4  foo    two  0.444616 -0.232363
5  bar    two  0.173897 -0.603437
6  foo    one  0.444616 -0.232363
7  foo  three  0.444616 -0.232363
</code></pre>
",Return rows groupby pandas e reduced number rows unique values group key The following code tutorials yields following results Code import pandas pd import numpy np df pd DataFrame A foo bar foo bar foo bar foo foo B one one two three two two one three C np random randn D np random randn print df grouped df groupby A mean print grouped Result A B C D foo one bar one foo two bar three foo two bar two foo one foo three C D A bar foo However I would like see rows following foo one bar one foo two bar three foo two bar two foo one foo three I open use library well I need fast efficiently using python Thanks advance,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas numpy endoftags,python pandas pandasgroupby,python pandas numpy,0.67
51644434,2018-08-02,2018,2,Way to select pandas series with required data_type and apply function in place,"<p>I have a dataframe with many <code>float64</code>, <code>int8</code> and <code>object</code> datatype columns/series. I want to apply a set of functions based on the datatype, but in place. I am unable to do that. I can separate out the columns based on <code>data_type</code> and concat them back based on a index. But I was wondering if there is a way to do it without separation. </p>

<pre><code>df1 = pd.DataFrame(np.random.randn(3, 3))
df2 = pd.DataFrame({'A': ['A0', 'A1', 'A2'],
                        'B': ['B0', 'B1', 'B2']},
                        index=[0, 1, 2] )
df=pd.concat ([df1,df2])
df.dtypes
# 0    float64
# 1    float64
# 2    float64
# A     object
# B     object
# dtype: object
</code></pre>

<p><code>df.select_dtypes(include = [""float64""]).apply(lambda x: x*x).dropna()</code></p>

<p>Gets me a new data frame. </p>

<p><code>df.select_dtypes(include = [""float64""]) = df.select_dtypes(include = [""float64""]).apply(lambda x: x*x)</code></p>

<p><code>SyntaxError: can't assign to function call</code></p>

<p>Ridiculous attempt to try and do it place. I realized I'm asking assignment to map 'lhs' to 'rhs' automatically, when there are different series on both sides. </p>

<p>Is there a way to do this operation in place.</p>
","['python', 'python-3.x', 'pandas']",51644459,"<p>I think should be 'float64' rather than 'int64'</p>

<pre><code>df.loc[:,df.select_dtypes(include = [""float64""]).columns] = df.select_dtypes(include = [""float64""]).apply(lambda x: x*x)
df
Out[117]: 
          0         1         2    A    B
0  0.232743  0.107359  1.512470  NaN  NaN
1  0.831272  1.935141  0.010660  NaN  NaN
2  0.017718  0.078454  0.056315  NaN  NaN
0       NaN       NaN       NaN   A0   B0
1       NaN       NaN       NaN   A1   B1
2       NaN       NaN       NaN   A2   B2
</code></pre>

<p>More info <code>update</code> </p>

<pre><code>df.update(df.select_dtypes(include = [""float64""]).apply(lambda x: x*x))
df
Out[139]: 
          0         1         2    A    B
0  0.074513  0.679018  0.070407  NaN  NaN
1  0.748732  0.004991  0.591979  NaN  NaN
2  0.006658  1.934269  0.106463  NaN  NaN
0       NaN       NaN       NaN   A0   B0
1       NaN       NaN       NaN   A1   B1
2       NaN       NaN       NaN   A2   B2
</code></pre>
",Way select pandas series required data type apply function place I dataframe many float int object datatype columns series I want apply set functions based datatype place I unable I separate columns based data type concat back based index But I wondering way without separation df pd DataFrame np random randn df pd DataFrame A A A A B B B B index df pd concat df df df dtypes float float float A object B object dtype object df select dtypes include float apply lambda x x x dropna Gets new data frame df select dtypes include float df select dtypes include float apply lambda x x x SyntaxError assign function call Ridiculous attempt try place I realized I asking assignment map lhs rhs automatically different series sides Is way operation place,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
51802421,2018-08-11,2018,2,Regex to find last two places in a group of numbers,"<p>I have numbers with the following form:</p>

<pre><code>02R-01240-250A
02R-01242-250
15-09110-200X
15-09110-212
</code></pre>

<p>I can capture the middle 5 digits (always 5) with:</p>

<pre><code>([^-]+[0-9]{4})
</code></pre>

<p>but I really want to grab the last two digits of this 5 number set and make a new column in my Pandas dataframe. I also need be able to grab the middle for a new column as well.</p>
","['python', 'regex', 'python-3.x']",51802616,"<p>Try these patterns:</p>

<pre><code>-[0-9]{2}[0-9]([0-9]{2})- #last two digits
-[0-9]{2}([0-9])[0-9]{2}- #middle digit
</code></pre>

<p>Based on your need to extract two things, you could use two capture groups with <code>re.search()</code> to get the last two digits and middle digit at once. This requires that there are always five numbers, but you say that there always are.</p>

<pre><code>s = re.search(""-[0-9]{2}([0-9])([0-9]{2})-"", your_string)
oneColumn = s.group(1) #middle digit
anotherColumn = s.group(2) #last two digits
</code></pre>

<p>The parentheses in the pattern enclose the two different capture groups.</p>
",Regex find last two places group numbers I numbers following form R A R X I capture middle digits always I really want grab last two digits number set make new column Pandas dataframe I also need able grab middle new column well,"startoftags, python, regex, python3x, endoftags",python arrays numpy endoftags,python regex python3x,python arrays numpy,0.33
51920827,2018-08-19,2018,3,How to take the max value of a series when using pandas Grouper?,"<p>I have the following dataframe:</p>

<pre><code>U_ID     Group   Location  Hours  People  Date
149      17      USA       2      2       2014-11-03
149      17      USA       2      1       2014-11-07
149      21      USA       3      2       2014-12-21
149      18      UK        1.5    1       2014-11-14
149      19      Spain     2      4       2014-11-21
</code></pre>

<p>Which I can rollup the number of hours with the following code snippet:</p>

<pre><code>def process_hours(hr_df):
    hr_df['Date'] = pd.to_datetime(hr_df['Date'])
    hr_df['Hours'] = pd.to_numeric(hr_df['Hours'])
    hr_df = (vol_df.groupby(['U_ID', 'Group', 'Location', 'People', pd.Grouper(key='Date', freq='MS')])['Hours'].sum().reset_index(level=[0, 1, 2, 3]))
</code></pre>

<p>However this splits when the number of people differ:</p>

<pre><code>           U_ID Group Location People  Hours
Date                                        
2014-11-01  149    17      USA      1    2.0
2014-11-01  149    17      USA      2    2.0
2014-11-01  149    18       UK      1    1.5
2014-11-01  149    19    Spain      4    2.0
2014-12-01  149    21      USA      2    3.0
</code></pre>

<p>How do I take the max number of people when the grouping happens to result in this:</p>

<pre><code>           U_ID Group Location People  Hours
Date                                        
2014-11-01  149    17      USA      2    4.0
2014-11-01  149    18       UK      1    1.5
2014-11-01  149    19    Spain      4    2.0
2014-12-01  149    21      USA      2    3.0
</code></pre>
","['python', 'pandas', 'dataframe']",51920852,"<p>Remove ""People"" from the grouper and use <code>agg</code> to specify <code>groupby</code> to additionally take the <code>max</code> of people. </p>

<pre><code>(hr_df.groupby(['U_ID', 'Group', 'Location', pd.Grouper(key='Date', freq='MS')])
      .agg({'Hours' : 'sum', 'People' : 'max'})
      .reset_index()  # Don't hardcode levels here.
      .set_index('Date'))

            U_ID  Group Location  Hours  People
Date                                           
2014-11-01   149     17      USA    4.0       2
2014-11-01   149     18       UK    1.5       1
2014-11-01   149     19    Spain    2.0       4
2014-12-01   149     21      USA    3.0       2
</code></pre>

<p>The reason I recommend not hardcoding levels here is for better maintainability. Using <code>reset_index(level=[0, 1, 2])</code> is more performant than <code>reset_index</code> + <code>set_index</code>. However, for example, if you decide to add another column to the grouper, you'll need to modify the reset index call... which is fine and dandy. If you want more easily maintainable code, consider not hardcoding them.</p>
",How take max value series using pandas Grouper I following dataframe U ID Group Location Hours People Date USA USA USA UK Spain Which I rollup number hours following code snippet def process hours hr df hr df Date pd datetime hr df Date hr df Hours pd numeric hr df Hours hr df vol df groupby U ID Group Location People pd Grouper key Date freq MS Hours sum reset index level However splits number people differ U ID Group Location People Hours Date USA USA UK Spain USA How I take max number people grouping happens result U ID Group Location People Hours Date USA UK Spain USA,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
52129791,2018-09-01,2018,3,How can I have different types in Pandas Series if Pandas Series uses numpy?,"<p>As far as I know numpys <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.dtype.html#numpy.dtype"" rel=""nofollow noreferrer"">ndarrays</a> element must be of same type and pandas series uses <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.html"" rel=""nofollow noreferrer"">ndarray</a> to hold values. But it seems like I am able to append an integer to a Series that holds string. </p>

<p>Sample code I have..</p>

<pre><code>import pandas as pd

sr = pd.Series(['foo'])
sr = sr.append(pd.Series([1], index=[1]))
print(type(sr.values))
print(sr.values.dtype)
print(type(sr.iloc[0]))
print(type(sr.iloc[1]))
</code></pre>

<p>and the output:</p>

<pre><code>&lt;class 'numpy.ndarray'&gt;
object
&lt;class 'str'&gt;
&lt;class 'int'&gt;
</code></pre>

<p>If the ndarrays type is object, how come int is returned for the item at index loc 1?</p>
","['python', 'pandas', 'numpy']",52129923,"<p>An <code>object</code> dtype series consists of pointers to arbitrary Python objects. Think of <code>object</code> dtype in the same way as you might a Python list. For example, the Python list <code>['foo', 1]</code> does not store values in a contiguous memory block.</p>

<p>In the same way you can't attach a specific <em>data type</em> to <code>list</code>, even if all elements are of the same type, a Pandas <code>object</code> series contains pointers to any number of types.</p>

<p>In general, Pandas dtype changes to <em>accommodate</em> values. So adding a float value to an integer series will turn the whole series to <code>float</code>. Adding a string to a numeric series will force the series to <code>object</code>. You can even force a numeric series to have <code>object</code> dtype, though this is not recommended:</p>

<pre><code>s = pd.Series(list(range(100000)), dtype=object)
</code></pre>

<p>The main benefit of Pandas, i.e. vectorised computations, is lost as soon as you start using <code>object</code> series. These should be <em>avoided</em> where possible. You can, for example, use <code>pd.Categorical</code> to factorise categories if applicable.</p>

<p>Here's a trivial example demonstrating the performance drop:</p>

<pre><code>t = pd.Series(list(range(100000)))

%timeit s*10  # 7.31 ms
%timeit t*10  # 366 Âµs
</code></pre>

<p>Related: <a href=""https://stackoverflow.com/questions/21018654/strings-in-a-dataframe-but-dtype-is-object"">Strings in a DataFrame, but dtype is object</a></p>
",How I different types Pandas Series Pandas Series uses numpy As far I know numpys ndarrays element must type pandas series uses ndarray hold values But seems like I able append integer Series holds string Sample code I import pandas pd sr pd Series foo sr sr append pd Series index print type sr values print sr values dtype print type sr iloc print type sr iloc output lt class numpy ndarray gt object lt class str gt lt class int gt If ndarrays type object come int returned item index loc,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
52354832,2018-09-16,2018,2,How to select a column element according to the size of another column elements?,"<p>I have a DataFrame named <code>a</code>. I want to get the top most used time apps.</p>

<pre><code>import pandas as pd 
a=pd.DataFrame({'user':[1,1,1,2,2,2,2],'app':['k','p','s','k','p','s','t'],'time':[5,10,15,10,5,3,1]})
</code></pre>

<p><code>Input:</code></p>

<pre><code>      user   app   time
0        1      k     5
1        1      p    10
2        1      s    15
3        2      k    10
4        2      p     5
5        2      s     3
6        2      t     1
</code></pre>

<p>For example, I want to get the top two most used <code>apps</code> according the column <code>time</code>. I expect the output as follows.</p>

<p><code>Expected:</code></p>

<pre><code>      user top1_app top2_app
0        1      s     p
1        2      k     p
</code></pre>

<p>As you see, <code>user 1</code> has the longest time to use the <code>app</code> called <code>s</code>, and has the second longest time to use the <code>app</code> called <code>p</code>.</p>

<p>Hopefully for help and thanks!</p>
","['python', 'pandas', 'dataframe']",52355480,"<p>You can rank the time column and then reshape</p>

<pre><code>a['time1'] = a.groupby('user').time.rank(method = 'dense', ascending = False).map({1.0 : 'top1_app', 2.0 : 'top2_app'})

a = a.dropna().pivot('user', 'time1', 'app')
a.columns.name = None
a.reset_index(inplace = True)


    user    top1_app    top2_app
0   1       s           p
1   2       k           p
</code></pre>
",How select column element according size another column elements I DataFrame named I want get top used time apps import pandas pd pd DataFrame user app k p k p time Input user app time k p k p For example I want get top two used apps according column time I expect output follows Expected user top app top app p k p As see user longest time use app called second longest time use app called p Hopefully help thanks,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
52423688,2018-09-20,2018,2,Fastest way to assign value to pandas cell,"<p>I have a function called <code>calculate_distance</code>, which takes 4 Pandas cells as an input and returns a new value that I want to assign it to a specific Pandas cell.
The 4 input values change dynamically as seen in the code below.</p>

<pre><code>df['distance'] = ''
for i in range(1, df.shape[0]):
   df.at[i, 'distance'] = calculate_distance(df['latitude'].iloc[i-1], df['longitude'].iloc[i-1], df['latitude'].iloc[i], df['longitude'].iloc[i])
</code></pre>

<p>Is there a faster way to do it than this ""newbie"" for loop?</p>
","['python', 'python-3.x', 'pandas']",52423803,"<p>You could use</p>

<pre><code>df['distance'] = df.apply(your_calculate_distance_def, axis=1)
</code></pre>

<p>Its faster than loop. I dont know what your definition does. But apply will help you boost speed.</p>

<p>You may refer to Pandas apply documentation here for more help - <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer"">pandas.DataFrame.apply</a></p>
",Fastest way assign value pandas cell I function called calculate distance takes Pandas cells input returns new value I want assign specific Pandas cell The input values change dynamically seen code df distance range df shape df distance calculate distance df latitude iloc df longitude iloc df latitude iloc df longitude iloc Is faster way newbie loop,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
52573275,2018-09-29,2018,5,Get ALL items in a python list?,"<p>it's very easy to get the number of items in a list, <code>len(list)</code>, but say I had a matrix like:
<code>[[1,2,3],[1,2,3]]</code>
Is there a pythonic way to return 6? Or do I have to iterate.</p>
","['python', 'python-3.x', 'list']",52573291,"<p>You can use <a href=""https://docs.python.org/3.7/library/itertools.html#itertools.chain"" rel=""nofollow noreferrer""><code>chain</code></a></p>

<pre><code>from itertools import chain
l = [[1,2,3],[1,2,3]]
len(list(chain(*l))) # give you 6
</code></pre>

<p>the expression <code>list(chain(*l))</code> give you flat list: <code>[1, 2, 3, 1, 2, 3]</code></p>
",Get ALL items python list easy get number items list len list say I matrix like Is pythonic way return Or I iterate,"startoftags, python, python3x, list, endoftags",python python3x list endoftags,python python3x list,python python3x list,1.0
52654607,2018-10-04,2018,5,Converting numpy array of strings to datetime,"<p>I have an array of strings, for example</p>

<pre><code>import numpy as np
foo = np.array( [b'2014-04-05', b'2014-04-06', b'2014-04-07'] )
</code></pre>

<p>To check for the datatype of the array, I print it with</p>

<pre><code>print( foo.dtype )
</code></pre>

<p>which results in <code>|S10</code>. Obviously, it consists of strings of length 10. I want to convert it into <code>numpy</code>'s <code>datetime64</code> type.</p>

<p>More precisely, I want to change the datatype of the array without looping through a for-loop and copying it element-wise into a new array (the real array is actually very large). Naive as I am, I thought the following might work</p>

<pre><code>[ np.datetime64(x) for x in foo ]
</code></pre>

<p>Spoiler: it does not. Printing the datatype of the array results in the same output as before (i.e. <code>|S10</code>). </p>

<p>Is there any memory efficient way to <em>convert</em> the datatype of the existing array without the necessity of <em>copying</em> everything to a new array? </p>
","['python', 'arrays', 'numpy']",52654647,"<p>Use <code>.astype</code>, with <code>copy=False</code> to avoid creating a copy:</p>

<pre><code>foo = np.array( [b'2014-04-05', b'2014-04-06', b'2014-04-07'] )

foo = foo.astype('datetime64',copy=False)

&gt;&gt;&gt; foo
array(['2014-04-05', '2014-04-06', '2014-04-07'], dtype='datetime64[D]')
</code></pre>
",Converting numpy array strings datetime I array strings example import numpy np foo np array b b b To check datatype array I print print foo dtype results S Obviously consists strings length I want convert numpy datetime type More precisely I want change datatype array without looping loop copying element wise new array real array actually large Naive I I thought following might work np datetime x x foo Spoiler Printing datatype array results output e S Is memory efficient way convert datatype existing array without necessity copying everything new array,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
52696414,2018-10-08,2018,2,What happens exactly if I use a class method as an udf in pyspark?,"<p>I understand whats happening if I define a Python function and use it for a PySpark UDF.</p>

<p>However, its not fully clear to me, what PySpark is doing when calling a method of an instantiated object like:</p>

<pre><code>class Foo:
  _const = 1
  def f(x):
    return x +_const

foo_object = Foo()

f_udf = pyspark.functions.udf(foo_object.f)
</code></pre>

<p>How is the function brought to the executors? Does the object stay on the manager or is it copied to the executors? </p>
","['python', 'apache-spark', 'pyspark']",52699896,"<p>A method call like</p>

<pre><code>foo_object.f(x)
</code></pre>

<p>is just a syntactic sugar for</p>

<pre><code>Foo.f(foo_object, x)
</code></pre>

<p>so effectively passing a method to <code>udf</code> is not really different from passing a plain function.</p>

<p>Spark will:</p>

<ul>
<li>Determine the closure.</li>
<li>Determine the arguments.</li>
<li>Serialized objects computed above as well as the called object itself.</li>
<li>Distribute this bundle among the workers.</li>
</ul>

<p>The only possible gotcha, that can encountered when you pass a plain function as well, is that serialization methods used in PySpark cannot serialize class definitions. This means that required classes have to present on <code>PYTHONPATH</code> of all workers. </p>
",What happens exactly I use class method udf pyspark I understand whats happening I define Python function use PySpark UDF However fully clear PySpark calling method instantiated object like class Foo const def f x return x const foo object Foo f udf pyspark functions udf foo object f How function brought executors Does object stay manager copied executors,"startoftags, python, apachespark, pyspark, endoftags",python discord discordpy endoftags,python apachespark pyspark,python discord discordpy,0.33
53071212,2018-10-30,2018,4,Stacking numpy arrays with padding,"<p>I have a list of 32 numpy arrays, each of which has shape <code>(n, 108, 108, 2)</code>, where <code>n</code> is different in each array. I want to stack all of them to create a numpy array of shape <code>(32, m, 108, 108, 2)</code>, where <code>m</code> is the maximum among the <code>n</code>s, and the shorter arrays are padded with zeros.</p>

<p>How do I do this?</p>

<p>I asked <a href=""https://stackoverflow.com/questions/53051560/stacking-numpy-arrays-of-different-length-using-padding/53052599?noredirect=1#comment93005810_53052599"">something similar</a> yesterday, but the answers there seem to break when using deep arrays like in my case.</p>

<p>Concretely, I went with this solution in the end, which produced the cleanest code:</p>

<pre><code>data = np.column_stack(zip_longest(*data, fillvalue=0))
</code></pre>

<p>But now it is throwing this error:</p>

<pre><code>ValueError: setting an array element with a sequence.
</code></pre>
","['python', 'arrays', 'numpy']",53071558,"<p>I have found a godly answer in <a href=""https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/"" rel=""nofollow noreferrer"">this webpage</a>.</p>

<p>The <code>pad_sequences</code> function is exactly what I needed.</p>

<pre><code>from tensorflow.python.keras.preprocessing.sequence import pad_sequences
result = pad_sequences(imgs, padding='post')
</code></pre>
",Stacking numpy arrays padding I list numpy arrays shape n n different array I want stack create numpy array shape maximum among ns shorter arrays padded zeros How I I asked something similar yesterday answers seem break using deep arrays like case Concretely I went solution end produced cleanest code data np column stack zip longest data fillvalue But throwing error ValueError setting array element sequence,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
53124105,2018-11-02,2018,3,create dataframe with datetime hourly frequency,"<p>I need to create a pandas dataframe with first column as date + time and with hourly frequency.</p>

<p>So in the dataframe, it will be complete year date with hourly time i.e 365 * 24 = 8760 rows in the first column.</p>

<p>sample data output:</p>

<pre><code>Hours
2018-01-01 00:00:00
2018-01-01 01:00:00
2018-01-01 02:00:00
...
...
...
2018-01-01 23:00:00
</code></pre>
","['python', 'pandas', 'dataframe']",53124169,"<p>Use <code>pd.date_range</code></p>

<pre><code>import pandas as pd

df = pd.DataFrame(
        {'Hours': pd.date_range('2018-01-01', '2019-01-01', freq='1H', closed='left')}
     )
</code></pre>

<h3>Output:</h3>

<pre><code>                   Hours
0    2018-01-01 00:00:00
1    2018-01-01 01:00:00
2    2018-01-01 02:00:00
3    2018-01-01 03:00:00
...                  ...
8759 2018-12-31 23:00:00

[8760 rows x 1 columns]
</code></pre>
",create dataframe datetime hourly frequency I need create pandas dataframe first column date time hourly frequency So dataframe complete year date hourly time e rows first column sample data output Hours,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
53138101,2018-11-04,2018,2,Regex build from list behaves abnormally,"<p>Below is my code:</p>

<pre><code>import re

class Example:
    def __init__(self):
        self.regex_list = ['omg', '\bwow']

    def print_regex(self):
        print('|'.join(self.regex_list))

e = Example()
e.print_regex()
</code></pre>

<p>Output:</p>

<pre><code>omgwow
</code></pre>

<p>Expected Output:</p>

<pre><code>omg|\bwow
</code></pre>

<p>So, I have a list of regexps that I want to join using OR operator. I do so expecting output to be OR joined string. </p>

<p>To my surprise it didn't join and it removed the \b too. This happens only inside the class while accessing self. </p>

<p>I tried directly joining outside of class which worked but I can't understand why this is not working. Could someone help me in understanding this?</p>

<p>EDIT:</p>

<pre><code>regex_list = ['omg', r'\bwow']
print('|'.join(regex_list)) # Works outside the class
</code></pre>
","['python', 'regex', 'python-3.x']",53138254,"<p><code>\b</code> is backspace character which is causing your pipe to be removed (just like when you press backspace). You need to bypass backspace.</p>

<p>you can bypass it by replacing <code>\bwow</code> with <code>\\bwow</code>.</p>

<pre><code>#output
omg|\bwow
</code></pre>
",Regex build list behaves abnormally Below code import class Example def init self self regex list omg bwow def print regex self print join self regex list e Example e print regex Output omgwow Expected Output omg bwow So I list regexps I want join using OR operator I expecting output OR joined string To surprise join removed b This happens inside class accessing self I tried directly joining outside class worked I understand working Could someone help understanding EDIT regex list omg r bwow print join regex list Works outside class,"startoftags, python, regex, python3x, endoftags",python django djangorestframework endoftags,python regex python3x,python django djangorestframework,0.33
53192602,2018-11-07,2018,14,Convert a Pandas DataFrame into a list of objects,"<p>I want to convert a Pandas DataFrame into a list of objects.</p>

<p>This is my class:</p>

<pre><code>class Reading:

    def __init__(self):
        self.HourOfDay: int = 0
        self.Percentage: float = 0
</code></pre>

<p>I read up on .to_dict, so I tried </p>

<pre><code>df.to_dict(into=Reading)
</code></pre>

<p>but it returned </p>

<pre><code>TypeError: unsupported type
</code></pre>

<p>I don't want a list of tuples, or a list of dicts, but a list of Readings. Every question I've found so far seems to be about these two scenarios. But I want my own typed objects.</p>

<p>Thanks</p>
","['python', 'pandas', 'dataframe']",55616777,"<p>having data frame with two column  HourOfDay and Percentage, and parameterized constructor of your class you could define a list of Object like this:</p>

<pre><code> class Reading:

   def __init__(self, h, p):
       self.HourOfDay = h 
       self.Percentage = p 

 listOfReading= [(Reading(row.HourOfDay,row.Percentage)) for index, row in df.iterrows() ]  
</code></pre>
",Convert Pandas DataFrame list objects I want convert Pandas DataFrame list objects This class class Reading def init self self HourOfDay int self Percentage float I read dict I tried df dict Reading returned TypeError unsupported type I want list tuples list dicts list Readings Every question I found far seems two scenarios But I want typed objects Thanks,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
53292432,2018-11-14,2018,12,Merging data based on matching first column in Python,"<p>I currently have two sets of data files that look like this:</p>

<p>File 1:</p>

<pre><code>test1 ba ab cd dh gf
test2 fa ab cd dh gf
test3 rt ty er wq ee
test4 er rt sf sd sa
</code></pre>

<p>and in file 2:</p>

<pre><code>test1 123 344 123
test1 234 567 787
test1 221 344 566
test3 456 121 677
</code></pre>

<p>I would like to combine the files based on mathching rows in the first column (so that ""tests"" match up)</p>

<p>like so:</p>

<pre><code>test1 ba ab cd dh gf 123 344 123
test1 ba ab cd dh gf 234 567 787
test1 ba ab cd dh gf 221 344 566
test3 rt ty er wq ee 456 121 677
</code></pre>

<p>I have this Code</p>

<pre><code>def combineFiles(file1,file2,outfile):
      def read_file(file):
         data = {}
         for line in csv.reader(file):
            data[line[0]] = line[1:]
         return data
      with open(file1, 'r') as f1, open(file2, 'r') as f2:
         data1 = read_file(f1)
         data2 = read_file(f2)
         with open(outfile, 'w') as out:
            wtr= csv.writer(out)
            for key in data1.keys():
               try:
                  wtr.writerow(((key), ','.join(data1[key]), ','.join(data2[key])))
               except KeyError:
                  pass
</code></pre>

<p>However the output ends up looking like this:</p>

<pre><code>test1 ba ab cd dh gf 123 344 123
test3 er rt sf sd sa 456 121 677
</code></pre>

<p>Can anyone help me with how to make the output so that test1 can be printed all three times?</p>

<p>Much Appreciated</p>
","['python', 'python-3.x', 'python-2.7']",53292922,"<p>While I would recommend <a href=""https://stackoverflow.com/a/53292483/1822698"">Brad Solomon's approach</a> as it's pretty succinct, you just need a small change in your code. </p>

<p>Since your second file is the one that has the ""final say"", you just need to create a dictionary for the first file. Then you can write the output file as you read from the second file, fetching values from the <code>data1</code> dictionary as you go:</p>

<pre><code>with open(file1, 'r') as f1, open(file2, 'r') as f2:
    data1 = read_file(f1)
    with open(outfile, 'w') as out:
        wtr = csv.writer(out, delimiter=' ')
        for line in csv.reader(f2, delimiter=' '):
            # only write if there is a corresponding line in file1
            if line[0] in data1:
                # as you write, get the corresponding file1 data
                wtr.writerow(line[0:] + data1[line[0]] + line[1:])
</code></pre>
",Merging data based matching first column Python I currently two sets data files look like File test ba ab cd dh gf test fa ab cd dh gf test rt ty er wq ee test er rt sf sd sa file test test test test I would like combine files based mathching rows first column tests match like test ba ab cd dh gf test ba ab cd dh gf test ba ab cd dh gf test rt ty er wq ee I Code def combineFiles file file outfile def read file file data line csv reader file data line line return data open file r f open file r f data read file f data read file f open outfile w wtr csv writer key data keys try wtr writerow key join data key join data key except KeyError pass However output ends looking like test ba ab cd,"startoftags, python, python3x, python27, endoftags",python python3x pandas endoftags,python python3x python27,python python3x pandas,0.67
53480403,2018-11-26,2018,2,How to merge Pandas DataFrame with dict of lists,"<p>What is the best way to merge a <code>df</code> like this:</p>
<pre><code>+------------+----------+
| domain     | username |
+------------+----------+
| @gmail.com | gagaga   |
+------------+----------+
| @mail.com  | bobo     |
+------------+----------+
</code></pre>
<p>with a dict like this:</p>
<pre><code>domain_to_app = {
    '@gmail.com': ['gmail', 'youtube', 'gdrive'],
    '@mail.com': ['email', 'dropbox']
}
</code></pre>
<p>to get this:</p>
<pre><code>+------------+----------+-----------+
| domain     | username | app       |
+------------+----------+-----------+
| @gmail.com | gagaga   | gmail     |
+------------+----------+-----------+
| @gmail.com | gagaga   | youtube   |
+------------+----------+-----------+
| @gmail.com | gagaga   | gdrive    |
+------------+----------+-----------+
| @mail.com  | bobo     | email     |
+------------+----------+-----------+
| @mail.com  | bobo     | dropbox   |
+------------+----------+-----------+
</code></pre>
<p>Is it recommended to convert the <code>dict</code> into a <code>df</code> with repeating rows and use <code>merge</code>, or should i use <code>map</code> then <a href=""https://stackoverflow.com/questions/42012152/unstack-a-pandas-column-containing-lists-into-multiple-rows"">unstack the app column</a>?</p>
","['python', 'pandas', 'dataframe']",53480568,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>map</code></a> for new <code>Series</code> and then <code>chain.from_iterable</code> with <code>repeat</code> for new <code>DataFrame</code>:</p>

<pre><code>s = df['domain'].map(domain_to_app)

from itertools import chain

lens = s.str.len()
df = pd.DataFrame({
    'domain' : df['domain'].values.repeat(lens),
    'username' : df['username'].values.repeat(lens),
     'app' : list(chain.from_iterable(s))
})

print (df)
       domain username      app
0  @gmail.com   gagaga    gmail
1  @gmail.com   gagaga  youtube
2  @gmail.com   gagaga   gdrive
3   @mail.com     bobo    email
4   @mail.com     bobo  dropbox
</code></pre>

<p>If need repeat multiple columns create <code>DaatFrame</code> from <code>mapped</code> values, reshape by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>stack</code></a> and 'repeat' by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html"" rel=""nofollow noreferrer""><code>join</code></a>:</p>

<pre><code>df['app'] = df['domain'].map(domain_to_app)

df = (df.join(pd.DataFrame(df.pop('app')
                            .values.tolist())
               .stack()
               .reset_index(level=1, drop=True)
               .rename('app'))).reset_index(drop=True)
print (df)
       domain username      app
0  @gmail.com   gagaga    gmail
1  @gmail.com   gagaga  youtube
2  @gmail.com   gagaga   gdrive
3   @mail.com     bobo    email
4   @mail.com     bobo  dropbox
</code></pre>
",How merge Pandas DataFrame dict lists What best way merge df like domain username gmail com gagaga mail com bobo dict like domain app gmail com gmail youtube gdrive mail com email dropbox get domain username app gmail com gagaga gmail gmail com gagaga youtube gmail com gagaga gdrive mail com bobo email mail com bobo dropbox Is recommended convert dict df repeating rows use merge use map unstack app column,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
53534151,2018-11-29,2018,3,How to sum total that refer to other dataframe,"<p>I would like to sum all combination from two DataFrames,   </p>

<pre><code>DataFrame A

         ColA    ColB   Sales
           1       A      10
           1       B      20
           1       C      100
           2       D      1000
           2       E      2000

DataFrame B
         ColA    ColB   
          1      A,B
          2      E
</code></pre>

<p>My Expect result for a DataFrame is </p>

<pre><code>        ColA     ColB   TotalSales
         1       A,B     30
         2       E       2000
</code></pre>
","['python', 'python-3.x', 'pandas', 'dataframe']",53537144,"<p>You can use <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer""><code>pd.DataFrame.apply</code></a> for a partially vectorised solution:</p>

<pre><code>def summer(x):
    m1 = df_a['ColA'].eq(x['ColA'])
    m2 = df_a['ColB'].isin(x['ColB'].split(','))
    return df_a.loc[m1 &amp; m2, 'Sales'].sum()

df_b['TotalSales'] = df_b.apply(summer, axis=1)

print(df_b)

   ColA ColB  TotalSales
0     1  A,B          30
1     2    E        2000
</code></pre>
",How sum total refer dataframe I would like sum combination two DataFrames DataFrame A ColA ColB Sales A B C D E DataFrame B ColA ColB A B E My Expect result DataFrame ColA ColB TotalSales A B E,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
53549030,2018-11-29,2018,3,"Comparing pd.Series and getting, what appears to be, unusual results when the series contains None","<p>I am wondering why comparing two identical series with <code>None</code> value returns False:</p>

<pre><code>pd.Series(['x', 'y', None]) == pd.Series(['x', 'y', None])

0     True
1     True
2    False
dtype: bool
</code></pre>

<p>I would expect all of results to be True. If I create an array, from the series, and compare I get the expected result:</p>

<pre><code>pd.Series(['x', 'y', None]).values == pd.Series(['x', 'y', None]).values

array([ True,  True,  True])
</code></pre>

<p>Why are the two identical series with <code>None</code> not equal to each other? Am I missing something? </p>

<p>I would expect this behavior with <code>np.nan</code> because <code>np.nan != np.nan</code>; however, <code>None == None</code></p>
","['python', 'python-3.x', 'pandas']",53549074,"<p>This is <a href=""https://github.com/pandas-dev/pandas/issues/20442#issuecomment-375247173"" rel=""nofollow noreferrer"">by design</a>:</p>

<blockquote>
  <p>see the warnings box: <a href=""http://pandas.pydata.org/pandas-docs/stable/missing_data.html"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/stable/missing_data.html</a></p>
  
  <p>This was done quite a while ago to make the behavior of nulls
  consistent, in that they don't compare equal. This puts <code>None</code> and
  <code>np.nan</code> on an equal (though not-consistent with python, BUT consistent
  with numpy) footing.</p>
  
  <p>So this is not a bug, rather a consequence of stradling 2 conventions.</p>
  
  <p>I suppose the documentation could be slightly enhanced.</p>
</blockquote>

<p>For equality of series containing null values, use <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.equals.html"" rel=""nofollow noreferrer""><code>pd.Series.equals</code></a>:</p>

<pre><code>pd.Series(['x', 'y', None]).equals(pd.Series(['x', 'y', None]))  # True
</code></pre>
",Comparing pd Series getting appears unusual results series contains None I wondering comparing two identical series None value returns False pd Series x None pd Series x None True True False dtype bool I would expect results True If I create array series compare I get expected result pd Series x None values pd Series x None values array True True True Why two identical series None equal Am I missing something I would expect behavior np nan np nan np nan however None None,"startoftags, python, python3x, pandas, endoftags",python pandas numpy endoftags,python python3x pandas,python pandas numpy,0.67
53611917,2018-12-04,2018,3,Swapping of elements in a PANDAS dataframe,"<p>Given below is a table :</p>

<pre><code>    A NUMBER    B NUMBER
    7042967611  9999574081
    12320       9999574081
    9999574081  9810256463
    9999574081  9716551924
    9716551924  9999574081  
    9999574081  8130945859
</code></pre>

<p>This was originally an excel sheet which has been converted into a dataframe. I wish to swap some of the elements such that the A number column has only 9999574081.
Therefore the output should look like :</p>

<pre><code>    A NUMBER    B NUMBER
    9999574081  7042967611  
    9999574081  12320       
    9999574081  9810256463
    9999574081  9716551924
    9999574081  9716551924  
    9999574081  8130945859
</code></pre>

<p>This is the code I have used :</p>

<pre><code>for i in list(df['A NUMBER']):
    j=0
    if i!= 9999574081:
        temp = df['B NUMBER'][j]
        df['B NUMBER'][j] = i
        df['A NUMBER'][j] = temp
    j+=1
</code></pre>

<p>However, I am not getting the desired result. Please help me out. Thanks:)</p>
","['python', 'pandas', 'dataframe']",53611971,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>DataFrame.loc</code></a> for swap only rows matched boolean mask, <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.values.html"" rel=""nofollow noreferrer""><code>values</code></a> is necessary for avoid align index values:</p>

<pre><code>m = df['A NUMBER'] != 9999574081

df.loc[m, ['A NUMBER','B NUMBER']] = df.loc[m, ['B NUMBER','A NUMBER']].values
</code></pre>

<p>Another solution with <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html"" rel=""nofollow noreferrer""><code>numpy.where</code></a>:</p>

<pre><code>df['B NUMBER'] = np.where(df['A NUMBER'] != 9999574081, df['A NUMBER'], df['B NUMBER'])
df['A NUMBER'] = 9999574081
</code></pre>

<hr>

<pre><code>print (df)
     A NUMBER    B NUMBER
0  9999574081  7042967611
1  9999574081       12320
2  9999574081  9810256463
3  9999574081  9716551924
4  9999574081  9716551924
5  9999574081  8130945859
</code></pre>
",Swapping elements PANDAS dataframe Given table A NUMBER B NUMBER This originally excel sheet converted dataframe I wish swap elements A number column Therefore output look like A NUMBER B NUMBER This code I used list df A NUMBER j temp df B NUMBER j df B NUMBER j df A NUMBER j temp j However I getting desired result Please help Thanks,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
53668823,2018-12-07,2018,3,Pandas: create column based on whether value exists in differents columns,"<p>I'd like to create a new column based on these conditions:</p>

<ul>
<li>if column 1 isn't empty then new column value is 1</li>
<li>if column 2 isn't empty then new column then value is 100</li>
<li>if both columns aren't empty then new column value is 101</li>
</ul>

<p>Is there a better way how to do this? Thx</p>

<pre><code>df = pd.DataFrame([['a', np.nan, 100], ['b', 20, np.nan], ['c', 30, 300], ['d', np.nan, np.nan]])


df['is_1'] =  np.where(df[1].notnull(), 1, 0)
df['is_2'] =  np.where(df[2].notnull(), 100, 0)
df['sum'] = df['is_1'] + df['is_2']
</code></pre>
","['python', 'pandas', 'numpy']",53668866,"<p>Just note Boolean values translate to <code>0</code> / <code>1</code> for computations:</p>

<pre><code>df['sum'] = df[1].notnull() + df[2].notnull() * 100
</code></pre>
",Pandas create column based whether value exists differents columns I like create new column based conditions column empty new column value column empty new column value columns empty new column value Is better way Thx df pd DataFrame np nan b np nan c np nan np nan df np df notnull df np df notnull df sum df df,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
53908253,2018-12-24,2018,3,How to pivot a pandas dataframe such that unique values across multiple columns become new columns?,"<p>I have a pandas dataframe of the form:</p>

<p><strong>df</strong></p>

<pre><code>    col_1      col_2      col_3      col_4
ID
1     A          B          C          A
2     B          D
3     A          C          B

df = pd.DataFrame({'col_1':['A','B','A'], 'col_2':['B','D','C'], 'col_3':['C',np.NaN,'B'], 'col_4':['A', np.NaN, np.NaN]}, index=[1,2,3])
</code></pre>

<p>Note that the values repeated across the columns are not accidental- they refer to the same entities (A in col_1 is the same as A in col_4, for instance). I am trying to pivot the values of this dataframe so that these unique values become the new columns. For instance, <strong>df</strong> would become:</p>

<p><strong>new_df</strong></p>

<pre><code>      A      B      C      D
ID
1     2      1      1      0
2     1      0      0      1
3     1      1      1      0
</code></pre>

<p>The new values represent counts. I have tried <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html"" rel=""nofollow noreferrer"">pd.get_dummies()</a> but it doesn't give me what I want. What is the most intuitive way to achieve this?</p>
","['python', 'pandas', 'numpy', 'dataframe']",53908279,"<p>IIUC using <code>stack</code> with <code>str.get_dummies</code></p>

<pre><code>df.stack().loc[lambda x : x!=''].str.get_dummies().sum(level=0)
    A  B  C  D
ID            
1   2  1  1  0
2   0  1  0  1
3   1  1  1  0
</code></pre>
",How pivot pandas dataframe unique values across multiple columns become new columns I pandas dataframe form df col col col col ID A B C A B D A C B df pd DataFrame col A B A col B D C col C np NaN B col A np NaN np NaN index Note values repeated across columns accidental refer entities A col A col instance I trying pivot values dataframe unique values become new columns For instance df would become new df A B C D ID The new values represent counts I tried pd get dummies give I want What intuitive way achieve,"startoftags, python, pandas, numpy, dataframe, endoftags",python pandas dataframe endoftags,python pandas numpy dataframe,python pandas dataframe,0.87
53980144,2018-12-30,2018,5,&#39;NoneType&#39; object has no attribute &#39;text&#39; in BeautifulSoup,"<p>I am trying to scrape Google results when I search ""<em>What is 2+2</em>"", but the following code is returning <code>'NoneType' object has no attribute 'text'</code>. Please help me in achieving the required goal.</p>

<pre><code>text=""What is 2+2""
search=text.replace("" "",""+"")
link=""https://www.google.com/search?q=""+search
headers={'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'}
source=requests.get(link,headers=headers).text
soup=BeautifulSoup(source,""html.parser"")
answer=soup.find('span',id=""cwos"")

self.respond(answer.text)
</code></pre>

<p>The only problem is with <code>id</code> in <code>soup.find</code>, however I have chosen this id very closely. I shouldn't be mistaken. I also tried <code>answer=soup.find('span',class_=""cwcot gsrt"")</code>, but neither worked.</p>
","['python', 'web-scraping', 'beautifulsoup']",53980529,"<p>Next time use the query string exactly as it is.</p>

<pre><code>import requests
from bs4 import BeautifulSoup
search=""2%2B2""
link=""https://www.google.com/search?q=""+search
headers={'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'}
source=requests.get(link,headers=headers).text
soup=BeautifulSoup(source,""html.parser"")
answer=soup.find('span',id=""cwos"")
print(answer.text)
</code></pre>

<p>Output:</p>

<pre><code> 4  
</code></pre>

<p>Visit these urls - they do not return the same result</p>

<p><a href=""https://www.google.com/search?q=What+is+2+2"" rel=""nofollow noreferrer"">https://www.google.com/search?q=What+is+2+2</a></p>

<p><a href=""https://www.google.com/search?q=2%2B2"" rel=""nofollow noreferrer"">https://www.google.com/search?q=2%2B2</a></p>

<p><a href=""https://www.google.com/search?q=2+2"" rel=""nofollow noreferrer"">https://www.google.com/search?q=2+2</a></p>
",NoneType object attribute text BeautifulSoup I trying scrape Google results I search What following code returning NoneType object attribute text Please help achieving required goal text What search text replace link https www google com search q search headers User Agent Mozilla X Linux x AppleWebKit KHTML like Gecko Chrome Safari source requests get link headers headers text soup BeautifulSoup source html parser answer soup find span id cwos self respond answer text The problem id soup find however I chosen id closely I mistaken I also tried answer soup find span class cwcot gsrt neither worked,"startoftags, python, webscraping, beautifulsoup, endoftags",python webscraping beautifulsoup endoftags,python webscraping beautifulsoup,python webscraping beautifulsoup,1.0
54141000,2019-01-11,2019,2,How to pivot a pandas Dataframe in Python?,"<p>I am trying to pivot the below dataframe. I want the column names to be added as rows. First row is a statis one but the Column names are not static since they will be calculated for the all numerical columns from the data frame. Could you please help.</p>

<p>This is my data frame:</p>

<p><a href=""https://i.stack.imgur.com/AHbSv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AHbSv.png"" alt=""enter image description here""></a></p>

<p>Expected Dataframe:</p>

<p><a href=""https://i.stack.imgur.com/rcuzs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rcuzs.png"" alt=""enter image description here""></a></p>
","['python', 'python-3.x', 'python-2.7']",54141160,"<p>You just add .T :) df.describe().T to transpose  your results:</p>

<pre><code>import pandas as pd
import numpy as np

#Create a Dictionary of series
d = {'Name':pd.Series(['Alisa','Bobby','Cathrine','Madonna','Rocky','Sebastian','Jaqluine',
   'Rahul','David','Andrew','Ajay','Teresa']),
   'Age':pd.Series([26,27,25,24,31,27,25,33,42,32,51,47]),
   'Score':pd.Series([89,87,67,55,47,72,76,79,44,92,99,69])}

#Create a DataFrame
pd.DataFrame(d).describe().T
</code></pre>

<p>Results:
<img src=""https://i.stack.imgur.com/jv3gz.png"" alt=""enter image description here""></p>
",How pivot pandas Dataframe Python I trying pivot dataframe I want column names added rows First row statis one Column names static since calculated numerical columns data frame Could please help This data frame Expected Dataframe,"startoftags, python, python3x, python27, endoftags",python pandas dataframe endoftags,python python3x python27,python pandas dataframe,0.33
54288196,2019-01-21,2019,3,Panda dataframe making every unique ID number NAT,"<p>I have a dataframe, and for every unqiue ID, make the first 'Diff' column NaT, for example my data starts off looking like</p>

<pre><code>index   DEVICE_ID      DIFF
0        12             Nat
1        12              20
2        12              30
3        13              40
4        13              40
5        13              21
6        14               9
7        14              10    
</code></pre>

<p>But I want the resulting dataframe to look like the one below</p>

<pre><code>index   DEVICE_ID      DIFF
0        12             Nat
1        12              20
2        12              30
3        13             Nat
4        13              40
5        13              21
6        14             Nat
7        14              10
</code></pre>

<p>Sorry for the poorly worded question</p>
","['python', 'pandas', 'dataframe']",54288347,"<p>Using <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.duplicated.html"" rel=""nofollow noreferrer""><code>df.duplicated()</code></a> and <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>df.loc</code></a></p>

<pre><code>df.loc[~df.DEVICE_ID.duplicated(),'DIFF'] = pd.NaT
&gt;&gt;df

   index  DEVICE_ID DIFF
0      0         12  NaT
1      1         12   20
2      2         12   30
3      3         13  NaT
4      4         13   40
5      5         13   21
6      6         14  NaT
7      7         14   10
</code></pre>
",Panda dataframe making every unique ID number NAT I dataframe every unqiue ID make first Diff column NaT example data starts looking like index DEVICE ID DIFF Nat But I want resulting dataframe look like one index DEVICE ID DIFF Nat Nat Nat Sorry poorly worded question,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
54465030,2019-01-31,2019,5,MonthEnd object result in &lt;11 * MonthEnds&gt; instead of number,"<p>In my pandas dataframe I want to find the difference between dates in months. The function <code>.dt.to_period('M')</code> results in a MonthEnd object like <code>&lt;11 * MonthEnds&gt;</code> instead of the month number.</p>

<p>I tried to change the column type with <code>pd.to_numeric()</code> and to remove the letters with <code>re.sub(""[^0-9]"", """", 'blablabla123bla')</code>. Both do not work on a <code>MonthEnd</code> object.</p>

<pre><code>df['duration_dataset'] = df['date_1'].dt.to_period('M') - df['date_2'].dt.to_period('M')
</code></pre>

<p>I expected 11, but the output is <code>&lt;11 * MonthEnds&gt;</code>.</p>

<p>Here is a minimum dataframe</p>

<pre><code>d = {'date_1': ['2018-03-31','2018-09-30'], 'date_2': ['2017-12-31','2017-12-31']}
df = pd.DataFrame(data=d)

df['date_1'] = pd.to_datetime(df['date_1'], format='%Y-%m-%d')
df['date_2'] = pd.to_datetime(df['date_2'], format='%Y-%m-%d')

df['duration_dataset'] = df['date_1'].dt.to_period('M') - df['date_2'].dt.to_period('M')

df
</code></pre>
","['python', 'pandas', 'dataframe']",54465409,"<p>This is <a href=""https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.24.0.html#period-subtraction"" rel=""nofollow noreferrer"">new behaviour in Pandas 0.24</a>, where subtracting <code>Period()</code> objects give you a <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.tseries.offsets.DateOffset.html"" rel=""nofollow noreferrer""><code>DateOffset</code> subclass</a>.</p>

<p>You can get the numeric value from the <code>DateOffset.n</code> attribute:</p>

<pre><code>from operator import attrgetter

df['duration_dataset'] = (
    df['date_1'].dt.to_period('M') -
    df['date_2'].dt.to_period('M')).apply(attrgetter('n'))
</code></pre>

<p>This produces</p>

<pre class=""lang-none prettyprint-override""><code>      date_1     date_2  duration_dataset
0 2018-03-31 2017-12-31                 3
1 2018-09-30 2017-12-31                 9
</code></pre>

<p>for your sample dataframe.</p>

<p>Rather than convert your dates to periods, you could instead convert them to a month count since the year 0, then subtract those numbers:</p>

<pre><code>df['duration_dataset'] = (
    df['date_1'].dt.year * 12 + df['date_1'].dt.month - 1 -
    (df['date_2'].dt.year * 12 + df['date_2'].dt.month - 1)
)
</code></pre>

<p>which can be simplified to</p>

<pre><code>df['duration_dataset'] = (
    12 * (df['date_1'].dt.year - df['date_2'].dt.year) +
    df['date_1'].dt.month - df['date_2'].dt.month
)
</code></pre>
",MonthEnd object result lt MonthEnds gt instead number In pandas dataframe I want find difference dates months The function dt period M results MonthEnd object like lt MonthEnds gt instead month number I tried change column type pd numeric remove letters sub blablabla bla Both work MonthEnd object df duration dataset df date dt period M df date dt period M I expected output lt MonthEnds gt Here minimum dataframe date date df pd DataFrame data df date pd datetime df date format Y df date pd datetime df date format Y df duration dataset df date dt period M df date dt period M df,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
54502043,2019-02-03,2019,2,Django Delete OneToOneField,"<p>I'm creating a Django web-app. I have an app named <code>vote</code>. I want to ""register"" this app via a OneToOne-Relationship to other apps. For example, I have an article app and I want to ""register"" vote:</p>

<pre><code>vote = models.OneToOneField(Vote, on_delete=models.CASCADE, default=None, null=True)
</code></pre>

<p>I changed the save method on article:</p>

<pre><code>def save(self, *args, **kwargs):
    self.vote = Vote.objects.create()
    super().save(*args, **kwargs)
</code></pre>

<p>Here's the problem:
I want the vote to be deleted when I delete article but that doesn't work.
When I delete article only article will be deleted and vote still exists.</p>
","['python', 'django', 'django-models']",54502147,"<p>That is correct behavior. you want to delete an article and want its votes to be deleted. so you should put your relation(One to One) on the <code>Vote</code> model, not the article. So replace:</p>

<pre><code>vote = models.OneToOneField(Vote, on_delete=models.CASCADE, default=None, null=True)
</code></pre>

<p>To:</p>

<pre><code>article = models.OneToOneField(Article, on_delete=models.CASCADE)
</code></pre>

<p>But on the vote model.</p>

<blockquote>
  <p>Note that: do not use default and null on this case.</p>
</blockquote>

<p>Also, you can read this <a href=""https://medium.com/@ajrbyers/django-fk-on-delete-defaults-to-cascade-1c1506aae7c7"" rel=""nofollow noreferrer"">Link</a> to understand where to put a relation and how cascade will delete it.</p>
",Django Delete OneToOneField I creating Django web app I app named vote I want register app via OneToOne Relationship apps For example I article app I want register vote vote models OneToOneField Vote delete models CASCADE default None null True I changed save method article def save self args kwargs self vote Vote objects create super save args kwargs Here problem I want vote deleted I delete article work When I delete article article deleted vote still exists,"startoftags, python, django, djangomodels, endoftags",python django djangorestframework endoftags,python django djangomodels,python django djangorestframework,0.67
54510814,2019-02-04,2019,3,"Python: I want to check if row has multiple same values for any of the columns in a dataframe and if yes, replace the repeated value with null","<p><em>I am new to stackoverflow, please excuse my formatting</em>  </p>

<p>My DataFrame looks like this:</p>

<pre><code>Col1    Col2    Col3    Col4    Col5

A       B       null    A       D
A       B       C       F       C
</code></pre>

<p>Since in the first row, Col4 has a repeated value i.e. A, I want to replace it with null.<br>
Similarly, in the second row, Col5 has a repeated value, I want to replace it with null.  </p>

<p>The final dataframe should look like:</p>

<pre><code>Col1    Col2    Col3    Col4    Col5

A       B       null    null    D
A       B       C       F       null
</code></pre>
","['python', 'pandas', 'dataframe']",54510982,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.drop_duplicates.html"" rel=""nofollow noreferrer""><code>Series.drop_duplicates</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer""><code>apply</code></a> and <code>axis=1</code> for processes by rows:</p>

<pre><code>df = df.apply(pd.Series.drop_duplicates, axis=1)
print (df)
  Col1 Col2 Col3 Col4 Col5
0    A    B  NaN  NaN    D
1    A    B    C    F  NaN
</code></pre>
",Python I want check row multiple values columns dataframe yes replace repeated value null I new stackoverflow please excuse formatting My DataFrame looks like Col Col Col Col Col A B null A D A B C F C Since first row Col repeated value e A I want replace null Similarly second row Col repeated value I want replace null The final dataframe look like Col Col Col Col Col A B null null D A B C F null,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
54827245,2019-02-22,2019,4,Remove first word if a sentence from pandas data frame column values,"<p>I have a data frame like this:</p>

<pre><code>df:
col1      col2
 A        blue berry
 B        nice water bottle
</code></pre>

<p>I want to remove first word from the col2 values, the final data frame will look like this:</p>

<pre><code>df1:
col1       col2
 A         berry
 B         water bottle
</code></pre>

<p>How to do this in most effective way using pandas </p>
","['python', 'pandas', 'dataframe']",54827316,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html"" rel=""nofollow noreferrer""><code>split</code></a> by first whitespace with <code>n=1</code> and then select second lists by indexing:</p>

<pre><code>df['col2'] = df['col2'].str.split(n=1).str[1]
print (df)
  col1          col2
0    A         berry
1    B  water bottle
</code></pre>

<p><strong>Detail</strong>:</p>

<pre><code>print (df['col2'].str.split(n=1))
0           [blue, berry]
1    [nice, water bottle]
Name: col2, dtype: object
</code></pre>

<p>If performance is important and no missing values convert solution to list comprehension:</p>

<pre><code>df['col2'] = [x.split(maxsplit=1)[1] for x in df['col2']]
</code></pre>
",Remove first word sentence pandas data frame column values I data frame like df col col A blue berry B nice water bottle I want remove first word col values final data frame look like df col col A berry B water bottle How effective way using pandas,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
54848064,2019-02-24,2019,2,HTML Scraping with Beautiful Soup - Unwanted line breaks,"<p>I've been trying to write a script to get data from a html page and save it to .csv file. However I've run into 3 minor problems. </p>

<p>First of all, when saving to .csv I get some unwanted line breaks which mess up the output file. </p>

<p>Secondly, players' names (the data concerns NBA players) appear twice.</p>

<pre><code>from bs4 import BeautifulSoup
import requests
import time


teams = ['atlanta-hawks', 'boston-celtics', 'brooklyn-nets']

seasons = []
a=2018
while (a&gt;2016):
    seasons.append(str(a))
    a-=1
print(seasons)  
for season in seasons:

    for team in teams:
        my_url = ' https://www.spotrac.com/nba/'+team+'/cap/'+ season +'/'

        headers = {""User-Agent"" : ""Mozilla/5.0""}

        response = requests.get(my_url)
        response.content

        soup = BeautifulSoup(response.content, 'html.parser')

        stat_table = soup.find_all('table', class_ = 'datatable')


        my_table = stat_table[0]

        plik = team + season + '.csv'   
        with open (plik, 'w') as r:
            for row in my_table.find_all('tr'):
                for cell in row.find_all('th'):
                    r.write(cell.text)
                    r.write("";"")

            for row in my_table.find_all('tr'):
                for cell in row.find_all('td'): 
                    r.write(cell.text)
                    r.write("";"")
</code></pre>

<p>Also, some of the numbers that are seperated by ""."" are being automatically converted to dates. </p>

<p>Any ideas how I could solve those problems?</p>

<p><a href=""https://i.stack.imgur.com/myyTf.png"" rel=""nofollow noreferrer"">Screenshot of output file </a></p>
","['python', 'web-scraping', 'beautifulsoup']",54849247,"<p>Richard provided a complete answer that works for the 3.6 + versions.
It executes <code>file.write()</code> for every cell, though, which is not necessary, so here's an alternative with str.format() which will work for python versions before 3.6, and writes once per row:</p>

<pre><code>from bs4 import BeautifulSoup
import requests
import time

teams = ['atlanta-hawks', 'boston-celtics', 'brooklyn-nets']
seasons = [2018, 2017]

for season in seasons:
    for team in teams:
        my_url = 'https://www.spotrac.com/nba/{}/cap/{}/'.format(team, season)
        headers = {""User-Agent"": ""Mozilla/5.0""}

        response = requests.get(my_url)
        response.content

        soup = BeautifulSoup(response.content, 'html.parser')
        stat_table = soup.find_all('table', class_ = 'datatable')
        my_table = stat_table[0]

        csv_file = '{}-{}.csv'.format(team, season)
        with open(csv_file, 'w') as r:
            for row in my_table.find_all('tr'):
                row_string = ''

                for cell in row.find_all('th'):
                    row_string='{}{};'.format(row_string, cell.text.strip())

                for i, cell in enumerate(row.find_all('td')):
                    cell_string = cell.a.text if i==0 else cell.text
                    row_string='{}{};'.format(row_string, cell_string)

                r.write(""{}\n"".format(row_string))
</code></pre>
",HTML Scraping Beautiful Soup Unwanted line breaks I trying write script get data html page save csv file However I run minor problems First saving csv I get unwanted line breaks mess output file Secondly players names data concerns NBA players appear twice bs import BeautifulSoup import requests import time teams atlanta hawks boston celtics brooklyn nets seasons gt seasons append str print seasons season seasons team teams url https www spotrac com nba team cap season headers User Agent Mozilla response requests get url response content soup BeautifulSoup response content html parser stat table soup find table class datatable table stat table plik team season csv open plik w r row table find tr cell row find th r write cell text r write row table find tr cell row find td r write cell text r write Also numbers seperated automatically converted dates Any ideas I could solve,"startoftags, python, webscraping, beautifulsoup, endoftags",python arrays numpy endoftags,python webscraping beautifulsoup,python arrays numpy,0.33
54853146,2019-02-24,2019,2,Overwrite some rows in pandas dataframe with ones from another dataframe based on index,"<p>I have a pandas dataframe, df1.</p>

<p>I want  to overwrite its values with values in df2, where the index and column name match.</p>

<p>I've found a few answers on this site, but nothing that quite does what I want.</p>

<p>df1</p>

<pre><code>   A   B   C
0  33  44  54
1  11  32   54
2  43  55  12
3  43  23  34

df2
   A
0  5555
</code></pre>

<p>output</p>

<pre><code>   A   B   C
0  5555  44  54
1  11  32   54
2  43  55  12
3  43  23  34
</code></pre>
","['python', 'pandas', 'dataframe']",54853169,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.combine_first.html"" rel=""nofollow noreferrer""><code>combine_first</code></a> with convert to integer if necessary:</p>

<pre><code>df = df2.combine_first(df1).astype(int)
print (df)
      A   B   C
0  5555  44  54
1    11  32  54
2    43  55  12
3    43  23  34
</code></pre>

<p>If need check intersection index and columns between both <code>DataFrame</code>s:</p>

<pre><code>df2= pd.DataFrame({'A':[5555, 2222],
                   'D':[3333, 4444]},index=[0, 10])

idx = df2.index.intersection(df1.index)
cols = df2.columns.intersection(df1.columns)

df = df2.loc[idx, cols].combine_first(df1).astype(int)
print (df)
      A   B   C
0  5555  44  54
1    11  32  54
2    43  55  12
3    43  23  34
</code></pre>
",Overwrite rows pandas dataframe ones another dataframe based index I pandas dataframe df I want overwrite values values df index column name match I found answers site nothing quite I want df A B C df A output A B C,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
55016520,2019-03-06,2019,2,Python melt dataframe based on values of comma-separated character vector column,"<p>I'm currently working on a test where I have different regions with some associated statistic, and a comma-separated list of genes that lie in those regions. This list will be variable in number, and may not contain anything (<code>""NA""</code>).</p>

<p>How can I ""melt"" this dataframe:</p>

<pre><code> region_id  statistic      genelist
          1        2.5       A, B, C
          2        0.5    B, C, D, E
          3        3.2          &lt;NA&gt;
          4        0.1          E, F
</code></pre>

<p>Into something like this:</p>

<pre><code>     region_id  statistic gene
           1       2.5    A
           1       2.5    B
           1       2.5    C
           2       0.5    B
           2       0.5    C
           2       0.5    D
           2       0.5    E
           3       3.2 &lt;NA&gt;
           4       0.1    E
           4       0.1    F
</code></pre>
","['python', 'pandas', 'dataframe']",55016590,"<p>Use:</p>

<pre><code># Splitting on , and joining with region_id and statistic columns
val = pd.concat([df.region_id, 
                 df.statistic, 
                 df.genelist.str.split(',', expand=True)], 
                axis=1)

# Unpivoting and ignoring variable column
m = pd.melt(val, id_vars=['region_id', 'statistic'])\
            .loc[:, ['region_id', 'statistic', 'value']]

# Ignoring Null values and sorting based on region_id
m[m.value.notnull()]\
.sort_values('region_id')\
.reset_index(drop=True)\
.rename(columns={'value':'gene'})

 region_id  statistic gene
       1       2.5    A
       1       2.5    B
       1       2.5    C
       2       0.5    B
       2       0.5    C
       2       0.5    D
       2       0.5    E
       3       3.2 &lt;NA&gt;
       4       0.1    E
       4       0.1    F
</code></pre>
",Python melt dataframe based values comma separated character vector column I currently working test I different regions associated statistic comma separated list genes lie regions This list variable number may contain anything NA How I melt dataframe region id statistic genelist A B C B C D E lt NA gt E F Into something like region id statistic gene A B C B C D E lt NA gt E F,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
55096836,2019-03-11,2019,2,How to redistribude the the value equaly between NaN values in pandas?,"<p>I have the following dataframe:</p>

<pre><code>                'B'         'C'
1/1/2017    'A' 
            BTC NaN       0.367392
            ETH NaN       0.367392
            XRP 0.164735  0.164735
            LTC 0.100481  0.100481
1/2/2017    BTC NaN       0.315265
            XRP NaN       0.315265
            ETH NaN       0.315265
            LTC 0.054204  0.054204
</code></pre>

<p>I want to redistribute (1 - df['B'].groupby(level=0).sum()) equally between NaN values.
Column 'C' is an example of an expected output. </p>
","['python', 'pandas', 'dataframe']",55097146,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.transform.html"" rel=""nofollow noreferrer""><code>GroupBy.transform</code></a> by first level of <code>MultiIndex</code> with <code>sum</code>, for second count number of NaNs by check missing values by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.isna.html"" rel=""nofollow noreferrer""><code>Series.isna</code></a> with <code>sum</code>, divide and replace missing values:</p>

<pre><code>print (df)
                     C         D
A        B                      
1/1/2017 BTC       NaN  0.367392
         ETH       NaN  0.367392
         XRP  0.164735  0.164735
         LTC  0.100481  0.100481
1/2/2017 BTC       NaN  0.315265
         XRP       NaN  0.315265
         ETH       NaN  0.315265
         LTC  0.054204  0.054204

sum1 = 1 - df['C'].groupby(level=0).transform('sum')
len1 = df['C'].isna().groupby(level=0).transform('sum')

df['E'] = df['C'].fillna(sum1 / len1)
print (df)
                     C         D         E
A        B                                
1/1/2017 BTC       NaN  0.367392  0.367392
         ETH       NaN  0.367392  0.367392
         XRP  0.164735  0.164735  0.164735
         LTC  0.100481  0.100481  0.100481
1/2/2017 BTC       NaN  0.315265  0.315265
         XRP       NaN  0.315265  0.315265
         ETH       NaN  0.315265  0.315265
         LTC  0.054204  0.054204  0.054204
</code></pre>
",How redistribude value equaly NaN values pandas I following dataframe B C A BTC NaN ETH NaN XRP LTC BTC NaN XRP NaN ETH NaN LTC I want redistribute df B groupby level sum equally NaN values Column C example expected output,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
55146871,2019-03-13,2019,4,Can numpy.rint to return an Int32?,"<p>I'm doing</p>

<pre><code>ret = np.rint(y * 4)
return ret
</code></pre>

<p>And I want it to return <code>Int32</code>. I tried adding <code>dtype='Int32'</code>, but it errors saying: <code>TypeError: No loop matching the specified signature and casting was found for ufunc rint</code></p>

<p>I apologize if this is a basic question, but I tried to search for an answer to no avail</p>
","['python', 'python-3.x', 'numpy']",55148031,"<p><code>ufuncs</code> have specific rules of what kinds of output they produce given the input.  For <code>rint</code> the rules are:</p>

<pre><code>In [41]: np.rint.types                                                          
Out[41]: ['e-&gt;e', 'f-&gt;f', 'd-&gt;d', 'g-&gt;g', 'F-&gt;F', 'D-&gt;D', 'G-&gt;G', 'O-&gt;O']
</code></pre>

<p>On top of that there are rules about what dtypes can be cast to other dtypes.  We can play with the <code>out</code> and the <code>casting</code> parameters to produce an integer output, but simply using <code>astype</code> after is simpler.</p>

<p>So <code>rint</code> normally returns a matching float, even though the values are rounded.</p>

<pre><code>In [43]: np.rint(np.linspace(0,10,8))                                           
Out[43]: array([ 0.,  1.,  3.,  4.,  6.,  7.,  9., 10.])
</code></pre>

<p>Simply providing an int <code>out</code> doesn't work:</p>

<pre><code>In [44]: np.rint(np.linspace(0,10,8),out=np.zeros(8,int))                       
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-44-e7f13fa29434&gt; in &lt;module&gt;
----&gt; 1 np.rint(np.linspace(0,10,8),out=np.zeros(8,int))

TypeError: ufunc 'rint' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''
</code></pre>

<p>We have to give it permission to do a float to int casting:</p>

<pre><code>In [45]: np.rint(np.linspace(0,10,8),out=np.zeros(8,int),casting='unsafe')      
Out[45]: array([ 0,  1,  3,  4,  6,  7,  9, 10])
</code></pre>

<p>The default <code>casting</code> for <code>astype</code> is 'unsafe'.</p>

<pre><code>In [55]: np.rint(np.linspace(0,10,8)).astype(int,casting='safe')                
TypeError: Cannot cast array from dtype('float64') to dtype('int64') according to the rule 'safe'
</code></pre>
",Can numpy rint return Int I ret np rint return ret And I want return Int I tried adding dtype Int errors saying TypeError No loop matching specified signature casting found ufunc rint I apologize basic question I tried search answer avail,"startoftags, python, python3x, numpy, endoftags",python arrays numpy endoftags,python python3x numpy,python arrays numpy,0.67
55181559,2019-03-15,2019,8,How the dtype of numpy array is calculated internally?,"<p>I was just messing around with numpy arrays when I realized the lesser known behavior of the <code>dtypes</code> parameter.</p>

<p>It seems to change as the input changes. For example,</p>

<pre><code>t = np.array([2, 2])
t.dtype
</code></pre>

<p>gives <code>dtype('int32')</code></p>

<p>However,</p>

<pre><code>t = np.array([2, 22222222222])
t.dtype
</code></pre>

<p>gives <code>dtype('int64')</code></p>

<p><em>So, my first question is: How is this calculated? Does it make the datatype suitable for the maximum element as a datatype for all the elements? If that is the case, don't you think it requires more space because it is unnecessarily storing excess memory to store 2 in the second array as a 64 bit integer?</em> </p>

<p>Now again, if I want to change the zeroth element of <code>array([2, 2])</code> like</p>

<pre><code>t = np.array([2, 2])
t[0] = 222222222222222
</code></pre>

<p>I get <code>OverflowError: Python int too large to convert to C long</code>. </p>

<p><em>My second question is: Why it does not support the same logic it did while creating the array if you change a particular value? Why does it not recompute and reevaluate?</em> </p>

<p>Any help is appreciated. Thanks in advance.</p>
","['python', 'arrays', 'numpy']",55186940,"<p>Let us try and find the relevant bits in the docs.</p>

<p>from the <code>np.array</code> doc string:</p>

<blockquote>
  <p>array(...)</p>
  
  <p>[...]</p>
  
  <h2>Parameters</h2>
  
  <p>[...]</p>
  
  <p>dtype : data-type, optional
      The desired data-type for the array.  If not given, then <strong>the type will
      be determined as the minimum type required to hold the objects in the
      sequence.</strong>  This argument can only be used to 'upcast' the array.  For
      downcasting, use the .astype(t) method.</p>
  
  <p>[...]</p>
</blockquote>

<p>(my emphasis)</p>

<p>It should be noted that this is not entirely accurate, for example for integer arrays the system (C) default integer is preferred over smaller integer types as is evident form your example.</p>

<p>Note that for numpy to be fast it is essential that all elements of an array be of the same size. Otherwise, how would you quickly locate the 1000th element, say? Also, mixing types  wouldn't save all that much space since you would have to store the types of every single element on top of the raw data.</p>

<p>Re your second question. First of all. There are type promotion rules in numpy. The best doc I could find for that is the <code>np.result_type</code> doc string:</p>

<blockquote>
  <p>result_type(...) result_type(*arrays_and_dtypes)</p>
  
  <p>Returns the type that results from applying the NumPy type promotion
  rules to the arguments.</p>
  
  <p>Type promotion in NumPy works similarly to the rules in languages like
  C++, with some slight differences.  When both scalars and arrays are
  used, the array's type takes precedence and the actual value of the
  scalar is taken into account.</p>
  
  <p>For example, calculating 3*a, where a is an array of 32-bit floats,
  intuitively should result in a 32-bit float output.  If the 3 is a
  32-bit integer, the NumPy rules indicate it can't convert losslessly
  into a 32-bit float, so a 64-bit float should be the result type. By
  examining the value of the constant, '3', we see that it fits in an
  8-bit integer, which can be cast losslessly into the 32-bit float.</p>
  
  <p>[...]</p>
</blockquote>

<p>I'm not quoting the entire thing here, refer to the doc string for more detail.</p>

<p>The exact way these rules apply are complicated and appear to represent a compromise between being intuitive and efficiency.</p>

<p>For example, the choice is based on inputs, not result</p>

<pre><code>&gt;&gt;&gt; A = np.full((2, 2), 30000, 'i2')
&gt;&gt;&gt; 
&gt;&gt;&gt; A
array([[30000, 30000],
       [30000, 30000]], dtype=int16)
# 1
&gt;&gt;&gt; A + 30000
array([[-5536, -5536],
       [-5536, -5536]], dtype=int16)
# 2
&gt;&gt;&gt; A + 60000
array([[90000, 90000],
       [90000, 90000]], dtype=int32)
</code></pre>

<p>Here efficiency wins. It would arguably be more intuitive to have #1 behave like #2. But this would be expensive.</p>

<p>Also, and more directly related to your question, type promotion only applies out-of-place, not in-place:</p>

<pre><code># out-of-place
&gt;&gt;&gt; A_new = A + 60000
&gt;&gt;&gt; A_new
array([[90000, 90000],
       [90000, 90000]], dtype=int32)
# in-place
&gt;&gt;&gt; A += 60000
&gt;&gt;&gt; A
array([[24464, 24464],
       [24464, 24464]], dtype=int16)
</code></pre>

<p>or</p>

<pre><code># out-of-place
&gt;&gt;&gt; A_new = np.where([[0, 0], [0, 1]], 60000, A)
&gt;&gt;&gt; A_new
array([[30000, 30000],
       [30000, 60000]], dtype=int32)
# in-place
&gt;&gt;&gt; A[1, 1] = 60000
&gt;&gt;&gt; A
array([[30000, 30000],
       [30000, -5536]], dtype=int16)
</code></pre>

<p>Again, this may seem rather non-intuitive. There are, however, compelling reasons for this choice.</p>

<p>And these should answer your second question:</p>

<p>Changing to a larger dtype would require allocating a larger buffer and copying over all the data. Not only would that be expensive for large arrays.</p>

<p>Many idioms in numpy rely on views and the fact that writing to a view directly modifies the base array (and other overlapping views). Therefore an array is not free to change its data buffer whenever it feels like it. To not break the link between views it would be necessary for an array to be aware of all views into its data buffer which would add a lot of admin overhead, and all those views would have to change their data pointers and metadata as well. And if the first array is itself a view (a slice, say) into another array things get even worse.</p>

<p>I suppose we can agree on that not being worth it and that is why types are not promoted in-place.</p>
",How dtype numpy array calculated internally I messing around numpy arrays I realized lesser known behavior dtypes parameter It seems change input changes For example np array dtype gives dtype int However np array dtype gives dtype int So first question How calculated Does make datatype suitable maximum element datatype elements If case think requires space unnecessarily storing excess memory store second array bit integer Now I want change zeroth element array like np array I get OverflowError Python int large convert C long My second question Why support logic creating array change particular value Why recompute reevaluate Any help appreciated Thanks advance,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
55279824,2019-03-21,2019,2,Frequency of a char at every position of inner list,"<p>I have a list of lists.</p>

<pre><code>[[A, B, C],[C, B, A],[B, B, B],[C, C, A],[B, C, A]]
</code></pre>

<p>I want to calculate the frequency ""A"" at every position of inner list.</p>

<p>Expected output:</p>

<pre><code>0.2, 0 , 0.6
</code></pre>

<p>The inner lists are equal in length.</p>

<pre><code>lst = []                                                            
with open(""Control_nucleosome.fasta"", ""r"", newline='\n') as report: 
    for line in report:                                             
        lst.append(line)                                            

for x in lst:                                                       
    if x[[0]]                                                       
</code></pre>

<p>I do not know how to approach at the second for loop.</p>
","['python', 'python-3.x', 'list']",55279931,"<pre><code>[inner.count('A')/len(inner) for inner in zip(*lst)]
</code></pre>
",Frequency char every position inner list I list lists A B C C B A B B B C C A B C A I want calculate frequency A every position inner list Expected output The inner lists equal length lst open Control nucleosome fasta r newline n report line report lst append line x lst x I know approach second loop,"startoftags, python, python3x, list, endoftags",python python3x list endoftags,python python3x list,python python3x list,1.0
55299078,2019-03-22,2019,2,Choosing non repetitive values in dataframe columns,"<p>I have the following dataframe.</p>

<pre><code>import pandas as pd
dates = pd.date_range('20130101', periods=10)
df = pd.DataFrame([1,1,1,-1,-1,-1,1,1,-1,1], index=dates, columns=list('A'))
</code></pre>

<p>Expected output from df</p>

<pre><code>df_out=pd.DataFrame([1,0,0,-1,0,0,1,0,-1,1], index=dates, columns=list('A'))
</code></pre>

<p>I want to choose alternate +1 and -1 and substitute zero when there is repetition.</p>

<p>df can be a big dataframe of 10 columns and I want this conversion on all the columns. What is the effective way without using for loop?
Please suggest the way forward. Thanking in anticipation.</p>
","['python', 'pandas', 'dataframe']",55299116,"<p>IIUC you could use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.diff.html"" rel=""nofollow noreferrer""><code>Series.diff</code></a> along with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ne.html"" rel=""nofollow noreferrer""><code>ne</code></a> to check which first differences are not <code>0</code>, or in other words, which subsequent values are not repeated, and replace those that are <code>False</code> with <code>0</code> using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.where.html"" rel=""nofollow noreferrer""><code>DataFrame.where</code></a>:</p>

<pre><code>df.where(df.A.diff().ne(0), 0)

            A
2013-01-01  1
2013-01-02  0
2013-01-03  0
2013-01-04 -1
2013-01-05  0
2013-01-06  0
2013-01-07  1
2013-01-08  0
2013-01-09 -1
2013-01-10  1
</code></pre>
",Choosing non repetitive values dataframe columns I following dataframe import pandas pd dates pd date range periods df pd DataFrame index dates columns list A Expected output df df pd DataFrame index dates columns list A I want choose alternate substitute zero repetition df big dataframe columns I want conversion columns What effective way without using loop Please suggest way forward Thanking anticipation,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
55324791,2019-03-24,2019,2,"ValueError: Input 0 is incompatible with layer batch_normalization_1: expected ndim=3, found ndim=2","<p>I am trying to use the implementetion of DeepTriage which is a deep learning approach for bug triaging. <a href=""http://bugtriage.mybluemix.net/"" rel=""nofollow noreferrer"">This website</a> includes dataset, source code and paper. I know that is a very specific area, but I'll try to make it simple. </p>

<p>In <a href=""http://bugtriage.mybluemix.net/#code"" rel=""nofollow noreferrer"">the source code</a> they define their approach ""DBRNN-A: Deep Bidirectional Recurrent Neural Network with Attention mechanism and with Long Short-Term Memory units (LSTM)"" with this code part:</p>

<pre><code>input = Input(shape=(max_sentence_len,), dtype='int32')
sequence_embed = Embedding(vocab_size, embed_size_word2vec, input_length=max_sentence_len)(input)

forwards_1 = LSTM(1024, return_sequences=True, dropout_U=0.2)(sequence_embed)
attention_1 = SoftAttentionConcat()(forwards_1)
after_dp_forward_5 = BatchNormalization()(attention_1)

backwards_1 = LSTM(1024, return_sequences=True, dropout_U=0.2, go_backwards=True)(sequence_embed)
attention_2 = SoftAttentionConcat()(backwards_1)
after_dp_backward_5 = BatchNormalization()(attention_2)

merged = merge([after_dp_forward_5, after_dp_backward_5], mode='concat', concat_axis=-1)
after_merge = Dense(1000, activation='relu')(merged)
after_dp = Dropout(0.4)(after_merge)
output = Dense(len(train_label), activation='softmax')(after_dp)                
model = Model(input=input, output=output)
model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy']) 
</code></pre>

<p><code>SoftAttentionConcat</code> implementation is from <a href=""https://gist.github.com/braingineer/27c6f26755794f6544d83dec2dd27bbb"" rel=""nofollow noreferrer"">here</a>. Rest of the functions are from <code>keras</code>. Also, in <a href=""http://bugtriage.mybluemix.net/#paper"" rel=""nofollow noreferrer"">the paper</a> they share the structure as:</p>

<p><a href=""https://i.stack.imgur.com/ABRhr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ABRhr.png"" alt=""DBRNN-A""></a></p>

<p>In the first batch normalization line, it throws this error:</p>

<pre><code>ValueError: Input 0 is incompatible with layer batch_normalization_1: expected ndim=3, found ndim=2
</code></pre>

<p>When I use <code>max_sentence_len=50</code> and <code>max_sentence_len=200</code> I look at the dimension until the error point, I see these shapes:</p>

<pre><code>Input               -&gt; (None, 50)
Embedding           -&gt; (None, 50, 200)
LSTM                -&gt; (None, None, 1024)
SoftAttentionConcat -&gt; (None, 2048) 
</code></pre>

<p>So, is there anybody seeing the problem here?</p>
","['python', 'tensorflow', 'keras']",56657745,"<p>I guess the problem is using TensorFlow code in a Keras structure or some version issues.</p>
<p>By using the question and the answers <a href=""https://stackoverflow.com/questions/42918446/how-to-add-an-attention-mechanism-in-keras"">here</a>, I implemented the attention mechanism in Keras as follows:</p>
<pre><code>attention_1 = Dense(1, activation=&quot;tanh&quot;)(forwards_1)
attention_1 = Flatten()(attention_1)  # squeeze (None,50,1)-&gt;(None,50)
attention_1 = Activation(&quot;softmax&quot;)(attention_1)
attention_1 = RepeatVector(num_rnn_unit)(attention_1)
attention_1 = Permute([2, 1])(attention_1)
attention_1 = multiply([forwards_1, attention_1])
attention_1 = Lambda(lambda xin: K.sum(xin, axis=1), output_shape=(num_rnn_unit,))(attention_1)

last_out_1 = Lambda(lambda xin: xin[:, -1, :])(forwards_1)
sent_representation_1 = concatenate([last_out_1, attention_1])
</code></pre>
<p>This works quite well. All the source code that I used for the implementation is available is in <a href=""https://github.com/hacetin/deep-triage"" rel=""nofollow noreferrer"">GitHub</a>.</p>
",ValueError Input incompatible layer batch normalization expected ndim found ndim I trying use implementetion DeepTriage deep learning approach bug triaging This website includes dataset source code paper I know specific area I try make simple In source code define approach DBRNN A Deep Bidirectional Recurrent Neural Network Attention mechanism Long Short Term Memory units LSTM code part input Input shape max sentence len dtype int sequence embed Embedding vocab size embed size word vec input length max sentence len input forwards LSTM return sequences True dropout U sequence embed attention SoftAttentionConcat forwards dp forward BatchNormalization attention backwards LSTM return sequences True dropout U go backwards True sequence embed attention SoftAttentionConcat backwards dp backward BatchNormalization attention merged merge dp forward dp backward mode concat concat axis merge Dense activation relu merged dp Dropout merge output Dense len train label activation softmax dp model Model input input output output model compile loss,"startoftags, python, tensorflow, keras, endoftags",python django djangorestframework endoftags,python tensorflow keras,python django djangorestframework,0.33
55483044,2019-04-02,2019,2,Python Function to Find Number of Days it Takes for a percent change to occur,"<p>I have a pandas dataframe of market price data.  It is the average trading price of a crypto currency (Ethereum) prices in USD.  </p>

<p>I want to create a function that will take parameter, a percent amount. The idea is that the function will then output a integer where that integer represents the total number of days that it took for that percent increase to be seen. </p>

<p>Pandas has a function called <code>pct_change(periods=x)</code> which will tell you the percent change over a certain period.  I am basically looking for the opposite of this function.  How many day it took to reach a certain percent growth. </p>

<p>our dataframe:<br>
df</p>

<pre><code>index   ts  quote   base    last_bid
0   2015-08-07  USD ETH 3.000000
1   2015-08-08  USD ETH 1.266663
2   2015-08-09  USD ETH 1.266663
3   2015-08-10  USD ETH 1.266663
4   2015-08-11  USD ETH 1.026667
</code></pre>

<pre><code>from datetime import datetime, timedelta
import pandas as pd

for index, row in df.iterrows():
    ndf = df['last_bid'].pct_change(periods=index)
    print(ndf[ndf &gt; .5])
</code></pre>

<p>This is giving me the every occurrence that each date performs over a 50% increase in price. However, I just want the First data where 50% is reached for each row. And then the loop will continue to the next row. </p>

<p>Ideally, All of this output would be in a single data frame with the same schema above. With 3 additional columns.  1.  The date where our percent gain occurred.  2. The actually percent change. 3. The date difference from between our start and end date.</p>
","['python', 'pandas', 'numpy']",55485903,"<p>Not having access to exactly your data frame, this code downloads the data, reformats it to look more like yours, and then looks for changes of <code>target_percent</code> or more.</p>

<pre><code>import csv

import requests
import pandas

from datetime import datetime, timedelta

url = 'https://etherscan.io/chart/etherprice?output=csv'

print ""Downloading CSV""

r = requests.get(url)

with open(""data.csv"", ""w"") as csv_handle:
    csv_handle.write(r.text)

df = pandas.read_csv(""data.csv"")

# First reformat the etherscan data to look like the data frame in the question
print ""Reformatting data""
df = df.drop(labels=[""UnixTimeStamp""], axis=1)
df = df.rename(columns={'Date(UTC)': 'ts', 'Value' : 'last_bid'})
df['ts'] = df['ts'].str.replace('/', '-')
df['ts'].replace(to_replace='^(\d+-\d+)-(\d\d\d\d)$', value='\g&lt;2&gt;-\g&lt;1&gt;',inplace=True,regex=True)

# Now process the reformatted data frame

target_percent = 50

found_dates    = []
date_diffs     = []
actual_changes = []

for i in range(len(df)):
    found_date = ""n/a""
    actual_change = ""n/a""
    date_diff = ""n/a""

    starting_date = df[""ts""][i]
    starting_bid = df[""last_bid""][i]

    for j in range(i+1, len(df)):
        if df[""last_bid""][j] &gt;= (1.0 + 0.01*target_percent) * starting_bid:
            actual_change = ""{:.1f}%"".format(100.0 * (df[""last_bid""][j] / starting_bid) - 100.0)
            found_date    = df[""ts""][j]
            date_diff = str(datetime.strptime(found_date, ""%Y-%m-%d"") - datetime.strptime(starting_date, ""%Y-%m-%d"")).split("","")[0]
            break


    date_diffs.append(date_diff)
    actual_changes.append(actual_change)
    found_dates.append(found_date)

df[""found_date""] = found_dates
df[""actual_change""] = actual_changes
df[""date_diff""] = date_diffs

print df
</code></pre>

<p>It gives screwy results for the first few records, with a price of zero, but looks okay after that:</p>

<pre><code>Downloading CSV
Reformatting data
           ts  last_bid found_date actual_change date_diff
1   2015-7-31      0.00   2015-8-1          nan%     1 day
2    2015-8-1      0.00   2015-8-2          nan%     1 day
3    2015-8-2      0.00   2015-8-3          nan%     1 day
4    2015-8-3      0.00   2015-8-4          nan%     1 day
5    2015-8-4      0.00   2015-8-5          nan%     1 day
6    2015-8-5      0.00   2015-8-6          nan%     1 day
7    2015-8-6      0.00   2015-8-7          inf%     1 day
8    2015-8-7      2.77  2016-2-10         57.0%  187 days
9    2015-8-8      0.81  2015-8-12         54.3%    4 days
10   2015-8-9      0.74  2015-8-12         68.9%    3 days
11  2015-8-10      0.68  2015-8-11         55.9%     1 day
12  2015-8-11      1.06  2015-8-13         67.9%    2 days
13  2015-8-12      1.25  2016-1-23         62.4%  164 days
14  2015-8-13      1.78   2016-2-7         68.5%  178 days
15  2015-8-14      1.79   2016-2-7         67.6%  177 days
16  2015-8-15      1.79   2016-2-7         67.6%  176 days
17  2015-8-16      1.37  2016-1-24         53.3%  161 days
</code></pre>
",Python Function Find Number Days Takes percent change occur I pandas dataframe market price data It average trading price crypto currency Ethereum prices USD I want create function take parameter percent amount The idea function output integer integer represents total number days took percent increase seen Pandas function called pct change periods x tell percent change certain period I basically looking opposite function How many day took reach certain percent growth dataframe df index ts quote base last bid USD ETH USD ETH USD ETH USD ETH USD ETH datetime import datetime timedelta import pandas pd index row df iterrows ndf df last bid pct change periods index print ndf ndf gt This giving every occurrence date performs increase price However I want First data reached row And loop continue next row Ideally All output would single data frame schema With additional columns The date percent gain occurred The actually,"startoftags, python, pandas, numpy, endoftags",python pandas numpy endoftags,python pandas numpy,python pandas numpy,1.0
55711581,2019-04-16,2019,2,Pandas DataFrame: Subtract columns with string datatype,"<p>How can I subtract two columns that contain values of type string? No values are indicated by '---' and should lead to a '---' in the result. The result should also be of value type string.</p>

<p><strong>Source</strong></p>

<pre><code>df1 = pd.DataFrame({'x': ['a', 'b', 'c'], 'y': ['5', '---', '7']})

    x   y
0   'a' '5'
1   'b' '---'
2   'c' '7'

df2 = pd.DataFrame({'x': ['a', 'b', 'c'], 'y': ['1', '2', '---']})

    x    y
0   'a'  '1'
1   'b'  '2'
2   'c'  '---'
</code></pre>

<p><strong>Target</strong></p>

<pre><code>df3 = df1 - df2

    x   y
0   'a' '4'
1   'b' '---'
2   'c' '---'
</code></pre>
","['python', 'pandas', 'dataframe']",55711619,"<p>Try with:</p>

<pre><code>df1.set_index('x').apply(lambda x: pd.to_numeric(x,errors='coerce')).sub(
      df2.set_index('x').apply(lambda x: pd.to_numeric(x,errors='coerce'))).fillna('--')\
                                                                .reset_index()
</code></pre>

<hr>

<pre><code>   x   y
0  a   4
1  b  --
2  c  --
</code></pre>
",Pandas DataFrame Subtract columns string datatype How I subtract two columns contain values type string No values indicated lead result The result also value type string Source df pd DataFrame x b c x b c df pd DataFrame x b c x b c Target df df df x b c,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
55753512,2019-04-18,2019,2,Pandas groupby rolling mean with custom window size,"<h2>Problem definition:</h2>

<p>For a Pandas DataFrame I'm trying to get a grouped by rolling mean with a changeable window size specified on each row that's relative to a date time index.</p>

<h3>Example:</h3>

<p>For the following <code>df</code> of weekly data:</p>

<pre><code>| week_start_date | material | location | quantity | window_size |
|-----------------|----------|----------|----------|-------------|
| 2019-01-28      | C        | A        | 870      | 1           |
| 2019-02-04      | C        | A        | 920      | 3           |
| 2019-02-18      | C        | A        | 120      | 1           |
| 2019-02-25      | C        | A        | 120      | 2           |
| 2019-03-04      | C        | A        | 120      | 1           |
| 2018-12-31      | D        | A        | 1200     | 8           |
| 2019-01-21      | D        | A        | 720      | 8           |
| 2019-01-28      | D        | A        | 480      | 8           |
| 2019-02-04      | D        | A        | 600      | 8           |
| 2019-02-11      | D        | A        | 720      | 8           |
| 2019-02-18      | D        | A        | 80       | 8           |
| 2019-02-25      | D        | A        | 600      | 8           |
| 2019-03-04      | D        | A        | 1200     | 8           |
| 2019-01-14      | E        | B        | 150      | 1           |
| 2019-01-28      | E        | B        | 1416     | 1           |
| 2019-02-04      | F        | B        | 1164     | 1           |
| 2019-01-28      | G        | B        | 11520    | 8           |
</code></pre>

<p>The window needs to be relative to the actual date set in <code>week_start_date</code>, rather than treating it like an integer index.</p>

<p>It needs to be grouped by <code>material</code> and <code>location</code>.</p>

<p>The rolling mean is for column <code>quantity</code>.</p>

<p>The window size needs to vary/change based on the value in the <code>window_size</code> column. This value changes over time - it represents the number of weeks back in time that quantity needs to be aggregated for.</p>

<p>When a row isn't available, the mean should assume that value is 0, i.e.:
when a week-dated row isn't available
<code>mean(null, null, null, 1000) = 1000</code>
but it should actually:
mean(0,0,0,1000)=250
However - this should only apply after the first observation has been measured.</p>

<h2>Fixed window, relative to date column:</h2>

<p>I can get a static window of 8 weeks (56 days) using the following:</p>

<pre><code>df.set_index('week_start_date').groupby(['material', 'location'])['quantity'].rolling('56D', min_periods=1).mean()
</code></pre>

<p>I've explored use of <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.expanding.html"" rel=""nofollow noreferrer"">expanding</a> but haven't been successful.</p>

<p>How can the window size be set relative to each row it reads?</p>

<h2>Sample Data:</h2>

<pre><code># Example Data
df = pd.DataFrame({'week_start_date': ['2019-01-28','2019-02-04','2019-02-18','2019-02-25','2019-03-04','2018-12-31','2019-01-21','2019-01-28','2019-02-04','2019-02-11','2019-02-18','2019-02-25','2019-03-04','2019-01-14','2019-01-28','2019-02-04','2019-01-28'],
'material': ['C','C','C','C','C','D','D','D','D','D','D','D','D','E','E','F','G'],
'location': ['A','A','A','A','A','A','A','A','A','A','A','A','A','B','B','B','B'],
'quantity': ['870','920','120','120','120','1200','720','480','600','720','80','600','1200','150','1416','1164','11520'],
'min_of_pdt_or_8_weeks': ['1','3','1','2','1','8','8','8','8','8','8','8','8','1','3','1','8']})
# Fix formats
df['week_start_date'] = pd.to_datetime(df['week_start_date'])
df['actual_week_qty'] = df['quantity'].astype(float)
</code></pre>

<h2>Expected result:</h2>

<pre><code>| material | location | week_start_date | quantity | 
| C        | A        | 2019-01-28      | 870      | 
| C        | A        | 2019-04-02      | 306.6667 | 
| C        | A        | 2019-02-18      | 520      | 
| C        | A        | 2019-02-25      | 386.6667 | 
| D        | A        | 2018-12-31      | 1200     | 
| D        | A        | 2019-01-21      | 960      | 
| D        | A        | 2019-01-28      | 800      | 
| D        | A        | 2019-04-02      | 600      | 
| D        | A        | 2019-11-02      | 720      | 
| D        | A        | 2019-02-18      | 400      | 
| D        | A        | 2019-02-25      | 466.6667 | 
| D        | A        | 2019-04-03      | 650      | 
| E        | B        | 2019-01-14      | 150      | 
| E        | B        | 2019-01-28      | 783      | 
| F        | B        | 2019-04-02      | 1164     | 
| G        | B        | 2019-01-28      | 11520    |
</code></pre>
","['python', 'pandas', 'pandas-groupby']",55756670,"<p>A naive way you might do this, is to do the 8 (assuming this is bounded!) calculations and merge the results:</p>

<pre><code>In [11]: d = {w: df.set_index('week_start_date')
                   .groupby(['material', 'location'])['quantity']
                   .rolling(f'{7*w}D', min_periods=1)
                   .mean()
                   .reset_index(name=""mean"")
                   .assign(window_size=w)
              for w in range(1, 9)}
</code></pre>

<p>then you can concat these DataFrames together and merge with the original, since we have the window_size column in both left and right it'll inner on that.</p>

<pre><code>In [12]: pd.concat(d.values()).merge(df, how=""inner"")
Out[12]:
   material location week_start_date          mean  window_size  quantity
0         C        A      2019-01-28    870.000000            1     870.0
1         C        A      2019-02-18    520.000000            1     120.0
2         C        A      2019-04-03    320.000000            1     120.0
3         E        B      2019-01-14    150.000000            1     150.0
4         F        B      2019-04-02   1164.000000            1    1164.0
5         C        A      2019-02-25    386.666667            2     120.0
6         C        A      2019-04-02    920.000000            3     920.0
7         E        B      2019-01-28    783.000000            3    1416.0
8         D        A      2018-12-31   1200.000000            8    1200.0
9         D        A      2019-01-21    960.000000            8     720.0
10        D        A      2019-01-28    800.000000            8     480.0
11        D        A      2019-04-02    600.000000            8     600.0
12        D        A      2019-11-02    720.000000            8     720.0
13        D        A      2019-02-18    400.000000            8      80.0
14        D        A      2019-02-25    466.666667            8     600.0
15        D        A      2019-04-03    650.000000            8    1200.0
16        G        B      2019-01-28  11520.000000            8   11520.0
</code></pre>

<p>Note: This assumes you've set the fillna of window_size to 8:</p>

<pre><code>df.window_size = df.window_size.replace('NaN', 8).astype(int)  # in your example
</code></pre>

<p>Further, you want to ensure you pass format to to_datetime to ensure you don't hit ambiguity, pandas <em>may</em> be able to do a good job here in infering it... but I wouldn't rely on it (use explicitly <code>format='%d/%m/%Y</code>). You want to get rid of the weird date formats as soon as you read it in, this can also be passed in read_csv (dayfirst=True) and friends.</p>

<hr>

<p>I'm not entirely convinced this is what you want, since there is a difference between your input df and expected (e.g. there's no G B in the expected...).</p>

<p>Regardless, I suspect there is a single shoot way to do this, <em>but</em> it will depend on the sparsity of the week/material/location (if it's dense it'll be much easier, if it's sparse this may be the best bet)...<br>
Now I think about it, you can do this completely on the material/location subDataFrame, can you simplify this problem to just be a function of that DataFrame (just week+value ignoring material/location) or will that apply be too slow?</p>
",Pandas groupby rolling mean custom window size Problem definition For Pandas DataFrame I trying get grouped rolling mean changeable window size specified row relative date time index Example For following df weekly data week start date material location quantity window size C A C A C A C A C A D A D A D A D A D A D A D A D A E B E B F B G B The window needs relative actual date set week start date rather treating like integer index It needs grouped material location The rolling mean column quantity The window size needs vary change based value window size column This value changes time represents number weeks back time quantity needs aggregated When row available mean assume value e week dated row available mean null null null actually mean However apply first observation measured Fixed window relative date column,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas numpy endoftags,python pandas pandasgroupby,python pandas numpy,0.67
55755739,2019-04-19,2019,2,How to merge different CSV file into a new CSV with one primary key,"<p>I have two huge CSV file and want them to join in one new CSV file with using python pandas, the primary key is id_student, it is ok that I successfully join different column together but when I output to a new CSV file, the whole bunch of data will only exist to the first row,  different column, for example, the row 1 column 1 will be id_student, it is like:</p>

<pre><code>0  12345
1  12344
</code></pre>

<p>then row 1 column will be final_result, the format will like:</p>

<pre><code>0  Pass
1  Pass
</code></pre>

<p>but my expected output will be like :</p>

<pre><code>0  12345 Pass
1  12344 Pass
</code></pre>

<p>Is there any way I can fix the output format?</p>

<pre class=""lang-py prettyprint-override""><code>def plotlyGraph(self):

    df = pandas.read_csv('studentAssessment.csv')
    dc = pandas.read_csv('studentInfo.csv')
    res = pandas.merge(df,dc, on=['id_student'], how='outer')
    a=res['id_student']
    b=res['final_result']
    c=res['score']
    d=res['id_assessment']
    e=res['region']

    with open(""new.csv"", ""w"", newline="""") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow([a,b,c,d,e])

</code></pre>
","['python', 'pandas', 'csv']",55755873,"<p>I am assuming your <code>df</code> has 2 columns: <code>id_student</code> and <code>id_assessment</code>, while the <code>dc</code> has 2 columns: <code>id_student</code> and <code>final_result</code>. Try this one:</p>

<pre><code>df = pandas.read_csv('studentAssessment.csv')
dc = pandas.read_csv('studentInfo.csv')

res = df.merge(dc, on=['id_student'], how='outer')
print(res)
</code></pre>

<p>Output</p>

<pre><code>   id_student id_assessment final_result
0           0       12345          pass
1           1       12344          pass
</code></pre>

<p>To store in <code>csv</code> file:</p>

<pre><code>res.to_csv(""new.csv"", index=False)
</code></pre>
",How merge different CSV file new CSV one primary key I two huge CSV file want join one new CSV file using python pandas primary key id student ok I successfully join different column together I output new CSV file whole bunch data exist first row different column example row column id student like row column final result format like Pass Pass expected output like Pass Pass Is way I fix output format def plotlyGraph self df pandas read csv studentAssessment csv dc pandas read csv studentInfo csv res pandas merge df dc id student outer res id student b res final result c res score res id assessment e res region open new csv w newline csvfile writer csv writer csvfile writer writerow b c e,"startoftags, python, pandas, csv, endoftags",python pandas matplotlib endoftags,python pandas csv,python pandas matplotlib,0.67
55796531,2019-04-22,2019,2,Dataframe.lookup and map combination resulting in column label error,"<p>I have a large dataframe of around (1200, 10) of mostly string where I have to append a new column say 'Z' based an existing reference column say 'Y', whose values are 'A', 'B', 'C', or unknown (NaN or other), from this I need to select one of three corresponding columns in the df say 'D', 'E', 'F', or output NaN and append this value as column 'Z'. I currently have the following code:</p>

<pre><code>df = pd.DataFrame({'T': {0: '.', 1: '.', 2: '.', 3: '.'}, 
                   'G': {0: '.', 1: '.', 2: '.', 3: '.'}, 
                   'D': {0: 4, 1: 1, 2: 5, 3: 3}, 
                   'E': {0: 6, 1: 2, 2: 7, 3: 2}, 
                   'F': {0: 8, 1: 3, 2: 9, 3: 1}, 
                   'K': {0: '.', 1: '.', 2: '.', 3:'.'}, 
                   'Y': {0: 'A', 1: 'B', 2: 'B', 3: np.nan}})

d = {'A': 'D', 'B': 'E', 'C': 'F'}
df['Z'] = df.lookup(df.index, df.Y.map(d))
</code></pre>

<p>The issue is that lookup breaks down where Y is an unknown value. and in the specific code, Y.unique() turns up something like (A, B, C, NaN, nan). So I was wondering if there would be a way to use a lookup-esque method that outputs Z to NaN where Y is NaN or unknown outside of the the given dict? </p>

<pre><code>
    T   G   D   E   F   K   Y   Z
0   .   .   4   6   8   .   A   4.0
1   .   .   1   2   3   .   B   2.0
2   .   .   5   7   9   .   B   7.0
3   .   .   3   2   1   .   NaN NaN
</code></pre>
","['python', 'pandas', 'dataframe']",55796656,"<p>You can use <code>stack</code> and <code>reindex</code> with zip for multiindexes:</p>

<pre><code>df['Z'] = df.stack().reindex(zip(df.index, df.Y.map(d))).reset_index(level=1, drop=True)
</code></pre>

<p>Output:</p>

<pre><code>   T  G  D  E  F  K    Y    Z
0  .  .  4  6  8  .    A    4
1  .  .  1  2  3  .    B    2
2  .  .  5  7  9  .    B    7
3  .  .  3  2  1  .  NaN  NaN
</code></pre>

<p>Details:</p>

<p>First, let's use <code>stack</code>, move the column header into the dataframe row index, creating a multiindex for the dataframe.  Where, level 0 is the original row index, and level 1 are the column headers.</p>

<p>Now, we can use <code>reindex</code> to filter down to only those indexes we need.  Multiindex are identified using tuples. (level0, level1) hence, we <code>zip</code> df.index and df.y.map(d) together creating the tuples used by reindex.</p>

<p>Lastly, we drop the level=1 of the multiindex creating a structure of the original index and assign a new column with those values.</p>
",Dataframe lookup map combination resulting column label error I large dataframe around mostly string I append new column say Z based existing reference column say Y whose values A B C unknown NaN I need select one three corresponding columns df say D E F output NaN append value column Z I currently following code df pd DataFrame T G D E F K Y A B B np nan A D B E C F df Z df lookup df index df Y map The issue lookup breaks Y unknown value specific code Y unique turns something like A B C NaN nan So I wondering would way use lookup esque method outputs Z NaN Y NaN unknown outside given dict T G D E F K Y Z A B B NaN NaN,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
55834095,2019-04-24,2019,2,what is the equivalent keras function for from_numpy in torch?,"<p>I found a code in torch and I have to change it to keras, but I could not find some equivalent for some of them. for example, I changed some of them as follow, but I am not sure they are true or not:</p>

<pre><code> `torch.tensor` to `K.variable` ( `K` is `from keras import backend as K`)
  unsqueez_(1) to K.expand_dims
  torch.empty((3,) + requested_shape) to K.zeros((3,) + requested_shape)
</code></pre>

<p>but I could not find anything for <code>torch.from_numpy</code>. now, my question is about the above changes I did that are they true? and something similar to <code>torch.from_numpy</code>? I appreciate your help.</p>
","['python', 'tensorflow', 'keras']",55834195,"<p>You can just initialize a variable with numpy array like this:</p>

<pre class=""lang-py prettyprint-override""><code>ary = np.random.normal(size=(2, 2))
v = K.variable(ary)
</code></pre>

<p>or use <code>cast()</code> to convert numpy array to a tensor:</p>

<pre class=""lang-py prettyprint-override""><code>ary = np.random.normal(size=(2, 2))
tensor = K.cast(ary, dtype='float32')
</code></pre>

<p>Beside this, the code that you used is correct.</p>
",equivalent keras function numpy torch I found code torch I change keras I could find equivalent example I changed follow I sure true torch tensor K variable K keras import backend K unsqueez K expand dims torch empty requested shape K zeros requested shape I could find anything torch numpy question changes I true something similar torch numpy I appreciate help,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
56109846,2019-05-13,2019,4,Why numpy&#39;s where operation is faster than apply function?,"<p>While creating a new column in pandas dataframe based on some condition, numpy's where method outperforms the apply method in terms of execution time, why is that so?</p>

<p>For example:</p>

<pre><code>df[""log2FC""] = df.apply(lambda x: np.log2(x[""C2Mean""]/x[""C1Mean""]) if x[""C1Mean""]&gt; 0 else np.log2(x[""C2Mean""]), axis=1)

df[""log2FC""] = np.where(df[""C1Mean""]==0,
                        np.log2(df[""C2Mean""]), 
                        np.log2(df[""C2Mean""]/df[""C1Mean""]))
</code></pre>
","['python', 'pandas', 'numpy']",56109954,"<p>This call to <code>apply</code> is row-wise iteration:</p>

<pre><code>df[""log2FC""] = df.apply(lambda x: np.log2(x[""C2Mean""]/x[""C1Mean""]) if x[""C1Mean""]&gt; 0 else np.log2(x[""C2Mean""]), axis=1)
</code></pre>

<p><code>apply</code> is just syntactic sugar for looping, you passed <code>axis=1</code> so it's row-wise.</p>

<p>Your other snippet</p>

<pre><code>df[""log2FC""] = np.where(df[""C1Mean""]==0,
                        np.log2(df[""C2Mean""]), 
                        np.log2(df[""C2Mean""]/df[""C1Mean""]))
</code></pre>

<p>is acting on the entire columns, so it's vectorised.</p>

<p>The other thing is that <code>pandas</code> is performing more checking, index-alignment, etc.. than <code>numpy</code>.</p>

<p>Your calls to <code>np.log2</code> are meaningless in this context as you pass scalar values:</p>

<pre><code> np.log2(x[""C2Mean""]/x[""C1Mean""])
</code></pre>

<p>performance-wise it would be the same as calling <code>math.log2</code></p>

<p>Explaining why numpy is significantly faster or what is vectorisation is beyond the scope of this question. You can see this: <a href=""https://stackoverflow.com/questions/47755442/what-is-vectorization"">What is vectorization?</a>.</p>

<p>The essential thing here is that numpy can and will use external libraries written in C or Fortran which are inherently faster than python.</p>
",Why numpy operation faster apply function While creating new column pandas dataframe based condition numpy method outperforms apply method terms execution time For example df log FC df apply lambda x np log x C Mean x C Mean x C Mean gt else np log x C Mean axis df log FC np df C Mean np log df C Mean np log df C Mean df C Mean,"startoftags, python, pandas, numpy, endoftags",python pandas numpy endoftags,python pandas numpy,python pandas numpy,1.0
56149847,2019-05-15,2019,3,How to transpose and merge same column names after transposing?,"<p>I have a dataframe that has column names in the index and values in a column next to it like so:</p>

<pre><code>      column
col1    a
col2    b
col3    c
col1    d
col2    e
col3    f
</code></pre>

<p>How do I flip and merge the index into columns like so? </p>

<pre><code>col1     col2    col3
a         b        c 
d         e        f 
</code></pre>

<p>I tried:</p>

<pre><code>new_df = pd.DataFrame(df).transpose()
</code></pre>

<p>new_df looks like this:</p>

<pre><code>col1    col2     col3     col1   col2  col3
a        b         c        d     e      f
</code></pre>
","['python', 'python-3.x', 'pandas']",56149886,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>DataFrame.set_index</code></a> with counter by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.cumcount.html"" rel=""nofollow noreferrer""><code>GroupBy.cumcount</code></a> and parameter <code>append=True</code> for <code>MultiIndex</code> and then reshape by first level by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unstack.html"" rel=""nofollow noreferrer""><code>Series.unstack</code></a>:</p>

<pre><code>df = df.set_index(df.groupby(level=0).cumcount(), append=True)['column'].unstack(0)
print (df)
    col1 col2 col3
0      a    b    c
1      d    e    f
</code></pre>
",How transpose merge column names transposing I dataframe column names index values column next like column col col b col c col col e col f How I flip merge index columns like col col col b c e f I tried new df pd DataFrame df transpose new df looks like col col col col col col b c e f,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
56351383,2019-05-28,2019,2,Count across dataframe columns based on str.contains (or similar),"<p>I would like to count the number of cells within each row that contain a particular character string, cells which have the particular string more than once should be counted once only.</p>

<p>I can count the number of cells across a row which equal a given value, but when I expand this logic to use str.contains, I have issues, as shown below</p>

<pre class=""lang-py prettyprint-override""><code>
d = {'col1': [""a#"", ""b"",""c#""], 'col2': [""a"", ""b"",""c#""]}
df = pd.DataFrame(d)

#can correctly count across rows using equality 
thisworks =( df ==""a#"" ).sum(axis=1)

#can count across  a column using str.contains
thisworks1=df['col1'].str.contains('#').sum()

#but cannot use str.contains with a dataframe so what is the alternative
thisdoesnt =( df.str.contains('#') ).sum(axis=1)
</code></pre>

<p>Output should be a series showing the number of cells in each row that contain the given character string.</p>
","['python', 'pandas', 'numpy']",56351516,"<p>A solution using <code>df.apply</code>:</p>

<pre><code>df = pd.DataFrame({'col1': [""a#"", ""b"",""c#""], 
                   'col2': [""a"", ""b"",""c#""]})
df
  col1 col2
0   a#    a
1    b    b
2   c#   c#

df['sum'] = df.apply(lambda x: x.str.contains('#'), axis=1).sum(axis=1)

  col1 col2  sum
0   a#    a    1
1    b    b    0
2   c#   c#    2
</code></pre>
",Count across dataframe columns based str contains similar I would like count number cells within row contain particular character string cells particular string counted I count number cells across row equal given value I expand logic use str contains I issues shown col b c col b c df pd DataFrame correctly count across rows using equality thisworks df sum axis count across column using str contains thisworks df col str contains sum cannot use str contains dataframe alternative thisdoesnt df str contains sum axis Output series showing number cells row contain given character string,"startoftags, python, pandas, numpy, endoftags",python python3x pandas endoftags,python pandas numpy,python python3x pandas,0.67
56351417,2019-05-28,2019,3,Transposing and Concatenating strings,"<p>How do I transpose and concatenate a pandas dataframe without using a for loop?</p>

<p>Here is the input data:</p>

<pre><code>input_data =  pandas.DataFrame({'a': ['fruit', 'fruit', 'fruit', 'food', 'food', 'food', 'food'],
                      'b': ['banana', '', 'apple', 'rice', '', 'yam', 'chicken']})
</code></pre>

<p>Resulting output should look like this:</p>

<pre><code>result = pandas.DataFrame({'a': ['fruit', 'food'],
                      'b': ['banana  apple', 'rice  yam  chicken']})
</code></pre>

<p>Here is my for loop solution:</p>

<pre><code>stuff_list = input_data.a.drop_duplicates().tolist()
result = pandas.DataFrame()

for s in stuff_list:
    step1 = input_data[input_data.a == s]
    step2 = ' '.join(step1.b.tolist())
    step3 = pandas.DataFrame({'a':[s], 'b':[step2]})
    result = result.append(step3)

print(result)
</code></pre>
","['python', 'python-3.x', 'pandas']",56351482,"<p>One way is to group by column <code>a</code> and apply a string join to column <code>b</code>:</p>

<pre><code>(input_data.groupby('a', sort=False)['b']
           .apply(lambda x: ' '.join(x))
           .reset_index())

       a                  b
0  fruit      banana  apple
1   food  rice  yam chicken
</code></pre>
",Transposing Concatenating strings How I transpose concatenate pandas dataframe without using loop Here input data input data pandas DataFrame fruit fruit fruit food food food food b banana apple rice yam chicken Resulting output look like result pandas DataFrame fruit food b banana apple rice yam chicken Here loop solution stuff list input data drop duplicates tolist result pandas DataFrame stuff list step input data input data step join step b tolist step pandas DataFrame b step result result append step print result,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
56800219,2019-06-28,2019,2,Pandas: Remove duplicate dates but keeping the last,"<p><code>(not a duplicate question)</code></p>

<p>I have the following datasets:</p>

<pre><code>GMT TIME, Value
2018-01-01 00:00:00,    1.2030   
2018-01-01 00:01:00,    1.2000 
2018-01-01 00:02:00,    1.2030   
2018-01-01 00:03:00,    1.2030   
.... , ....
2018-12-31 23:59:59,    1.2030   
</code></pre>

<p>I am trying to find a way to remove the following:</p>

<ul>
<li><code>hh:mm:ss</code> form the datetime</li>
<li>After removing the <code>time (hh:mm:ss)</code> section, we will have duplicate <code>date</code> entry like multiple <code>2018-01-01</code> and so on... so I need to remove the duplicate date data and only keep the last date, before the next date, eg <code>2018-01-02</code> and similarly keep the last <code>2018-01-02</code> before the next date <code>2018-01-03</code> and repeat...</li>
</ul>

<p>How can I do it with <code>Pandas</code>?</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",56800239,"<p>Suppose you have data:</p>

<pre><code>              GMT TIME  Value
0  2018-01-01 00:00:00  1.203
1  2018-01-01 00:01:00  1.200
2  2018-01-01 00:02:00  1.203
3  2018-01-01 00:03:00  1.203
4  2018-01-02 00:03:00  1.203
5  2018-01-03 00:03:00  1.203
6  2018-01-04 00:03:00  1.203
7  2018-12-31 23:59:59  1.203
</code></pre>

<p>Use <code>pandas.to_datetime.dt.date</code> with <code>pandas.DataFrame.groupby</code>:</p>

<pre><code>import pandas as pd

df['GMT TIME'] = pd.to_datetime(df['GMT TIME']).dt.date
df.groupby(df['GMT TIME']).last()
</code></pre>

<p>Output:</p>

<pre><code>            Value
GMT TIME         
2018-01-01  1.203
2018-01-02  1.203
2018-01-03  1.203
2018-01-04  1.203
2018-12-31  1.203
</code></pre>

<p>Or use <code>pandas.DataFrame.drop_duplicates</code>:</p>

<pre><code>df['GMT TIME'] = pd.to_datetime(df['GMT TIME']).dt.date
df.drop_duplicates('GMT TIME', 'last')
</code></pre>

<p>Output:</p>

<pre><code>     GMT TIME  Value
3  2018-01-01  1.203
4  2018-01-02  1.203
5  2018-01-03  1.203
6  2018-01-04  1.203
7  2018-12-31  1.203
</code></pre>
",Pandas Remove duplicate dates keeping last duplicate question I following datasets GMT TIME Value I trying find way remove following hh mm ss form datetime After removing time hh mm ss section duplicate date entry like multiple I need remove duplicate date data keep last date next date eg similarly keep last next date repeat How I Pandas,"startoftags, python, python3x, pandas, dataframe, endoftags",python python3x pandas endoftags,python python3x pandas dataframe,python python3x pandas,0.87
56841186,2019-07-01,2019,2,Dataframe Updating a column to category name based on a list of string values in that category,"<p>I have lists that are categorized by name, such as:</p>

<pre class=""lang-py prettyprint-override""><code>dining = ['CARLS', 'SUBWAY', 'PIZZA']
bank = ['TRANSFER', 'VENMO', 'SAVE AS YOU GO']
</code></pre>

<p>and I want to update a new column to the category name if any of those strings are found in the other column. An example from my other question <a href=""https://stackoverflow.com/questions/56828780/dataframe-how-to-update-a-column-based-many-str-values"">here</a>, I have the following data set (an example bank transactions list):</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

dining = ['CARLS', 'SUBWAY', 'PIZZA']
bank = ['TRANSFER', 'VENMO', 'SAVE AS YOU GO']

data = [
    [-68.23 , 'PAYPAL TRANSFER'],
    [-12.46, 'RALPHS #0079'],
    [-8.51, 'SAVE AS YOU GO'],
    [25.34, 'VENMO CASHOUT'],
    [-2.23 , 'PAYPAL TRANSFER'],
    [-64.29 , 'PAYPAL TRANSFER'],
    [-7.06, 'SUBWAY'],
    [-7.03, 'CARLS JR'],
    [-2.35, 'SHELL OIL'],
    [-35.23, 'CHEVRON GAS']
]

df = pd.DataFrame(data, columns=['amount', 'details'])
df['category'] = np.nan
df

    amount  details             category
0   -68.23  PAYPAL TRANSFER     NaN
1   -12.46  RALPHS #0079        NaN
2   -8.51   SAVE AS YOU GO      NaN
3   25.34   VENMO CASHOUT       NaN
4   -2.23   PAYPAL TRANSFER     NaN
5   -64.29  PAYPAL TRANSFER     NaN
6   -7.06   SUBWAY              NaN
7   -7.03   CARLS JR            NaN
8   -2.35   SHELL OIL           NaN
9   -35.23  CHEVRON GAS         NaN
</code></pre>

<p>Is there an efficient way for me update the category column to either 'dining' or 'bank' based on if the strings in the list are found in data.details?</p>

<pre class=""lang-py prettyprint-override""><code>I.e. Desired Output:
    amount  details             category
0   -68.23  PAYPAL TRANSFER     bank
1   -12.46  RALPHS #0079        NaN
2   -8.51   SAVE AS YOU GO      bank
3   25.34   VENMO CASHOUT       bank
4   -2.23   PAYPAL TRANSFER     bank
5   -64.29  PAYPAL TRANSFER     bank
6   -7.06   SUBWAY              dining
7   -7.03   CARLS JR            dining
8   -2.35   SHELL OIL           NaN
9   -35.23  CHEVRON GAS         NaN
</code></pre>

<p>From my previous question, so far I'm assuming I need to work with a new list that I create by using str.extract.</p>
","['python', 'pandas', 'numpy', 'dataframe']",56841430,"<p>We can do this with <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.select.html"" rel=""nofollow noreferrer""><code>np.select</code></a> since we have multiple conditions:</p>

<pre><code>dining = '|'.join(dining)
bank = '|'.join(bank)

conditions = [
    df['details'].str.contains(f'({dining})'),
    df['details'].str.contains(f'({bank})')
]

choices = ['dining', 'bank']

df['category'] = np.select(conditions, choices, default=np.NaN)
</code></pre>

<hr>

<pre><code>   amount          details category
0  -68.23  PAYPAL TRANSFER     bank
1  -12.46     RALPHS #0079      nan
2   -8.51   SAVE AS YOU GO     bank
3   25.34    VENMO CASHOUT     bank
4   -2.23  PAYPAL TRANSFER     bank
5  -64.29  PAYPAL TRANSFER     bank
6   -7.06           SUBWAY   dining
7   -7.03         CARLS JR   dining
8   -2.35        SHELL OIL      nan
9  -35.23      CHEVRON GAS      nan
</code></pre>
",Dataframe Updating column category name based list string values category I lists categorized name dining CARLS SUBWAY PIZZA bank TRANSFER VENMO SAVE AS YOU GO I want update new column category name strings found column An example question I following data set example bank transactions list import pandas pd import numpy np dining CARLS SUBWAY PIZZA bank TRANSFER VENMO SAVE AS YOU GO data PAYPAL TRANSFER RALPHS SAVE AS YOU GO VENMO CASHOUT PAYPAL TRANSFER PAYPAL TRANSFER SUBWAY CARLS JR SHELL OIL CHEVRON GAS df pd DataFrame data columns amount details df category np nan df amount details category PAYPAL TRANSFER NaN RALPHS NaN SAVE AS YOU GO NaN VENMO CASHOUT NaN PAYPAL TRANSFER NaN PAYPAL TRANSFER NaN SUBWAY NaN CARLS JR NaN SHELL OIL NaN CHEVRON GAS NaN Is efficient way update category column either dining bank based strings list found data details I e Desired Output amount details,"startoftags, python, pandas, numpy, dataframe, endoftags",python python3x pandas endoftags,python pandas numpy dataframe,python python3x pandas,0.58
56853092,2019-07-02,2019,4,Merge and combine 2 columns of different dataframe,"<p>I have 2 dataframes :</p>

<pre><code>ID             word
1              srv1
2              srv2
3              srv1
4              nan
5              srv3
6              srv1
7              srv5
8              nan
</code></pre>

<pre><code>ID             word
1              nan
2              srv12
3              srv10
4              srv8
5              srv4
6              srv7
7              nan
8              srv9
</code></pre>

<p>What I need is to merge thoses 2 dataframes on ID and combine the column word to get :</p>

<pre><code>ID             word
1              srv1 
2              srv2 , srv12
3              srv1 , srv10
4              srv8
5              srv3 , srv4
6              srv1 , srv7
7              srv5
8              srv9
</code></pre>

<p>With the following code</p>

<pre class=""lang-py prettyprint-override""><code>merge = pandas.merge(df1,df2,on=""ID"",how=""left"")
merge[""word""] = merge[word_x] + "" , "" + merge[""word_y""]
</code></pre>

<p>I am getting:</p>

<pre><code>ID             word
1              nan 
2              srv2 , srv12
3              srv1 , srv10
4              nan
5              srv3 , srv4
6              srv1 , srv7
7              nan
8              nan
</code></pre>

<p>Which it is not the correct solution.</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",56853268,"<p>You can use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.cat.html"" rel=""noreferrer""><code>Series.str.cat</code></a> and the <code>na_rep</code> option to populate the <code>word</code> column even if one of the source columns in <code>nan</code>, then use <code>str.strip</code> to trim any leading/trailing <code>' , '</code> not between words.</p>

<pre><code>m['word'] = m['word_x'].str.cat(m['word_y'], sep=' , ', na_rep='').str.strip(' , ')
</code></pre>

<p>returns</p>

<pre><code>   ID word_x word_y          word
0   1   srv1    NaN          srv1
1   2   srv2  srv12  srv2 , srv12
2   3   srv1  srv10  srv1 , srv10
3   4    NaN   srv8          srv8
4   5   srv3   srv4   srv3 , srv4
5   6   srv1   srv7   srv1 , srv7
6   7   srv5    NaN          srv5
7   8    NaN   srv9          srv9
</code></pre>
",Merge combine columns different dataframe I dataframes ID word srv srv srv nan srv srv srv nan ID word nan srv srv srv srv srv nan srv What I need merge thoses dataframes ID combine column word get ID word srv srv srv srv srv srv srv srv srv srv srv srv With following code merge pandas merge df df ID left merge word merge word x merge word I getting ID word nan srv srv srv srv nan srv srv srv srv nan nan Which correct solution,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
56888063,2019-07-04,2019,3,Split 2D numpy array horizontally based on percentages,"<p>I want to be able to split 2D <code>numpy</code> <strong>horizontally</strong> into two splits (80% and 20%). I have tried using <code>np.vsplit()</code> but it seems it is not made for such a case. For instance, suppose I have the following matrix of size (6,3). I want to split it horizontally into 80% and 20% [roughly (5,3), (1,3)], so I tried something like this:</p>

<pre><code>M = [[1,2,3],[4,5,6],[7,8,9], [10,11,12], [77,54,11], [424,78,98]]
M = np.asarray(M)
arr1 = np.vsplit(M, int(M.shape[0]* 0.8))[0]  # 80% of data goes to arr1
arr2 = np.vsplit(M, int(M.shape[0]* 0.2))[1]  # 20% of data goes to arr2
</code></pre>

<p>I know this try is incorrect but I can't fix it (actually still learning python). Kindly if someone can help to modify this code. Thank you</p>
","['python', 'python-3.x', 'numpy']",56888143,"<p>You can do it like this using <strong>Indexing</strong> (or use <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"" rel=""nofollow noreferrer"">train_test_split</a>):</p>

<pre><code>M = [[1,2,3],[4,5,6],[7,8,9], [10,11,12], [77,54,11], [424,78,98]]
M = np.asarray(M)

split_horizontally_idx = int(M.shape[0]* 0.8) # integer for line selection (horizontal selection)

array1 = M[:split_horizontally_idx , :] # indexing/selection of the 80%
array2 = M[split_horizontally_idx: , :] # indexing/selection of the remaining 20% 
</code></pre>
",Split D numpy array horizontally based percentages I want able split D numpy horizontally two splits I tried using np vsplit seems made case For instance suppose I following matrix size I want split horizontally roughly I tried something like M M np asarray M arr np vsplit M int M shape data goes arr arr np vsplit M int M shape data goes arr I know try incorrect I fix actually still learning python Kindly someone help modify code Thank,"startoftags, python, python3x, numpy, endoftags",python arrays numpy endoftags,python python3x numpy,python arrays numpy,0.67
56893150,2019-07-04,2019,2,Setting different errors for pandas plot bar,"<p>I'm trying to plot a probability distribution using a <code>pandas.Series</code> and I'm struggling to set different <code>yerr</code> for each bar. In summary, I'm plotting the following distribution:</p>

<p><a href=""https://i.stack.imgur.com/TEshn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TEshn.png"" alt=""enter image description here""></a></p>

<p>It comes from a <code>Series</code> and it is working fine, except for the <code>yerr</code>. It cannot overpass 1 or 0. So, I'd like to set different errors for each bar. Therefore, I went to the documentation, which is available <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.plot.html"" rel=""nofollow noreferrer"">here</a> and <a href=""https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.errorbar.html"" rel=""nofollow noreferrer"">here</a>.</p>

<p>According to them, I have 3 options to use either the <code>yerr</code> aor <code>xerr</code>:</p>

<ul>
<li><strong>scalar:</strong> Symmetric +/- values for all data points.</li>
<li><strong>scalar:</strong> Symmetric +/- values for all data points.</li>
<li><strong>shape(2,N):</strong> Separate - and + values for each bar. The first row contains the lower errors, the second row contains the upper errors.</li>
</ul>

<p>The case I need is the last one. In this case, I can use a <code>DataFrame</code>, <code>Series</code>, <code>array-like</code>, <code>dict</code> and <code>str</code>. Thus, I set the arrays for each <code>yerr</code> bar, however it's not working as expected. Just to replicate what's happening, I prepared the following examples:</p>

<p>First I set a <code>pandas.Series</code>:</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
se = pd.Series(data=[0.1,0.2,0.3,0.4,0.4,0.5,0.2,0.1,0.1],
 index=list('abcdefghi'))
</code></pre>

<p>Then, I'm replicating each case:</p>

<p>This works as expected:</p>

<pre class=""lang-py prettyprint-override""><code>err1 = [0.2]*9
se.plot(kind=""bar"", width=1.0, yerr=err1)
</code></pre>

<p><a href=""https://i.stack.imgur.com/OBk4c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OBk4c.png"" alt=""enter image description here""></a></p>

<p>This works as expected:</p>

<pre class=""lang-py prettyprint-override""><code>err2 = err1
err2[3] = 0.5
se.plot(kind=""bar"", width=1.0, yerr=err1)
</code></pre>

<p><a href=""https://i.stack.imgur.com/Ti91L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ti91L.png"" alt=""enter image description here""></a></p>

<p><strong>Now the problem:</strong> This doesn't works as expected! </p>

<pre class=""lang-py prettyprint-override""><code>err_up = [0.3]*9
err_low = [0.1]*9
err3 = [err_low, err_up]
se.plot(kind=""bar"", width=1.0, yerr=err3)
</code></pre>

<p><a href=""https://i.stack.imgur.com/41vTW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/41vTW.png"" alt=""enter image description here""></a></p>

<p>It's not setting different errors for low and up. I found an example <a href=""https://matplotlib.org/3.1.0/gallery/statistics/errorbar_features.html"" rel=""nofollow noreferrer"">here</a> and a similar SO question <a href=""https://stackoverflow.com/questions/13312820/how-do-i-plot-just-the-positive-error-bar-with-pyplot-bar"">here</a>, although they are using <code>matplotlib</code> instead of <code>pandas</code>, it should work here.</p>

<p>I'm glad if you have any solution about that.
Thank you.</p>
","['python', 'pandas', 'matplotlib']",56910372,"<p>Based on @Quanghoang comment, I started to think it was a a bug. So, I tried to change the <code>yerr</code> shape, and surprisely, the following code worked:</p>

<pre class=""lang-py prettyprint-override""><code>err_up = [0.3]*9
err_low = [0.1]*9
err3 = [[err_low, err_up]]
print (err3)
se.plot(kind=""bar"", width=1.0, yerr=err3)
</code></pre>

<p><a href=""https://i.stack.imgur.com/g7J4T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g7J4T.png"" alt=""enter image description here""></a></p>

<p>Observe I included a new axis in <code>err3</code>. Now it's a <code>(1,2,N)</code> array. However, <strong>the documentation says it should be</strong> <code>(2,N)</code>.</p>

<p>In addition, a possible work around that I found was set the <code>ax.ylim(0,1)</code>. It doesn't solve the problem, but plots the graph correctly.</p>
",Setting different errors pandas plot bar I trying plot probability distribution using pandas Series I struggling set different yerr bar In summary I plotting following distribution It comes Series working fine except yerr It cannot overpass So I like set different errors bar Therefore I went documentation available According I options use either yerr aor xerr scalar Symmetric values data points scalar Symmetric values data points shape N Separate values bar The first row contains lower errors second row contains upper errors The case I need last one In case I use DataFrame Series array like dict str Thus I set arrays yerr bar however working expected Just replicate happening I prepared following examples First I set pandas Series import pandas pd se pd Series data index list abcdefghi Then I replicating case This works expected err se plot kind bar width yerr err This works expected err err err,"startoftags, python, pandas, matplotlib, endoftags",python pandas matplotlib endoftags,python pandas matplotlib,python pandas matplotlib,1.0
56947819,2019-07-09,2019,9,Indexing numpy array with index array of lower dim yields array of higher dim than both,"<pre><code>a = np.zeros((5,4,3))
v = np.ones((5, 4), dtype=int)
data = a[v]
shp = data.shape
</code></pre>

<p>This code gives <code>shp==(5,4,4,3)</code></p>

<p>I don't understand why. How can a larger array be output? makes no sense to me and would love an explanation.</p>
","['python', 'arrays', 'numpy']",56948112,"<p>This is known as <a href=""https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing"" rel=""nofollow noreferrer"">advanced indexing</a>. Advanced indexing allows you to select arbitrary elements in the input array based on an N-dimensional index.</p>

<p>Let's use another example to make it clearer:</p>

<pre><code>a = np.random.randint(1, 5, (5,4,3))
v = np.ones((5, 4), dtype=int)
</code></pre>

<p>Say in this case <code>a</code> is:</p>

<pre><code>array([[[2, 1, 1],
        [3, 4, 4],
        [4, 3, 2],
        [2, 2, 2]],

       [[4, 4, 1],
        [3, 3, 4],
        [3, 4, 2],
        [1, 3, 1]],

       [[3, 1, 3],
        [4, 3, 1],
        [2, 1, 4],
        [1, 2, 2]],
        ...
</code></pre>

<p>By indexing with an array of <code>np.ones</code>:</p>

<pre><code>print(v)

array([[1, 1, 1, 1],
       [1, 1, 1, 1],
       [1, 1, 1, 1],
       [1, 1, 1, 1],
       [1, 1, 1, 1]])
</code></pre>

<p>You will simply be indexing <code>a</code> with <code>1</code> along the first axis as many times as <code>v</code>. Putting it in another way, when you do:</p>

<pre><code>a[1]

[[4, 4, 1],
 [3, 3, 4],
 [3, 4, 2],
 [1, 3, 1]]
</code></pre>

<p>You're indexing along the first axis, as no indexing is specified along the additional axes. It is the same as doing <code>a[1, ...]</code>, i.e taking a full slice along the remaining axes. Hence by indexing with a <code>2D</code> array of ones, you will have the above <code>2D</code> array <code>(5, 4)</code> times stacked together, resulting in an ndarray of shape <code>(5, 4, 4, 3)</code>. Or in other words, <code>a[1]</code>, of shape <code>(4,3)</code>, stacked <code>5*4=20</code> times.</p>

<p>Hence, in this case you'd be getting:</p>

<pre><code>array([[[[4, 4, 1],
         [3, 3, 4],
         [3, 4, 2],
         [1, 3, 1]],

        [[4, 4, 1],
         [3, 3, 4],
         [3, 4, 2],
         [1, 3, 1]],
         ...
</code></pre>
",Indexing numpy array index array lower dim yields array higher dim np zeros v np ones dtype int data v shp data shape This code gives shp I understand How larger array output makes sense would love explanation,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
56993825,2019-07-11,2019,3,Pandas: Replace column if 0 if it is not the max,"<p>What I want to do is replace a column value with ""0"" if, for a given row, that column is not maximal. And conversely, replace the column value with ""1"" if, for a given row, that column IS maximal. </p>

<p>My data looks like this:</p>

<pre><code>data = {
    ""A"": [1, 2, 3],
    ""B"": [3, 5, 1],
    ""Max"": [""B"", ""B"", ""A""]
}

data_df = pd.DataFrame(data)
print(data_df)
</code></pre>

<pre><code>   A  B Max
0  1  3   B
1  2  5   B
2  3  1   A
</code></pre>

<p>But I want it to look like </p>

<pre><code>   A  B Max
0  0  1   B
1  0  1   B
2  1  0   A
</code></pre>
","['python', 'python-3.x', 'pandas']",56993889,"<p>You can do:</p>

<pre><code>for col in ['A','B']:
    data_df[col] = data_df['Max'].eq(col).astype(int)
</code></pre>

<p>Or, you could do:</p>

<pre><code>data_df[['A', 'B']] = (pd.get_dummies(data_df['Max'])
                       .reindex(['A','B'], axis=1, fill_value=0)
                      )
</code></pre>
",Pandas Replace column max What I want replace column value given row column maximal And conversely replace column value given row column IS maximal My data looks like data A B Max B B A data df pd DataFrame data print data df A B Max B B A But I want look like A B Max B B A,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
57512376,2019-08-15,2019,3,How to check if a string is in a longer string in pandas DataFrame?,"<p>I know it's quite straightforward to use <code>df.str.contains()</code> to check if the column contains a certain substring. </p>

<p>What if I want to do the other way around: check if the column's value <strong>is contained</strong> by a longer string? I did a search but couldn't find an answer. I thought this should be easy, like in pure python we could simply <code>'a' in 'abc'</code></p>

<p>I tried to use <code>df.isin</code> but seems it's not designed for this purpose.</p>

<p>Say I have a df looks like this:</p>

<pre><code>       col1      col2
0     'apple'    'one'
1     'orange'   'two'
2     'banana'   'three'
</code></pre>

<p>I want to query this df on <code>col1</code> if <strong>is contained</strong> by a string <code>appleorangefruits</code>, it should return me the first two rows.</p>
","['python', 'pandas', 'dataframe']",57513128,"<p>You need:</p>

<pre><code>longstring = 'appleorangefruits'
df.loc[df['col1'].apply(lambda x: x in longstring)]
</code></pre>

<p>Output:</p>

<pre><code>    col1    col2
0   apple   one
1   orange  two
</code></pre>
",How check string longer string pandas DataFrame I know quite straightforward use df str contains check column contains certain substring What I want way around check column value contained longer string I search find answer I thought easy like pure python could simply abc I tried use df isin seems designed purpose Say I df looks like col col apple one orange two banana three I want query df col contained string appleorangefruits return first two rows,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
57604673,2019-08-22,2019,3,How can I swap two elements in one array?,"<p>I have an array like :</p>

<pre><code>a=np.array([2,7])

a=[2,7]
</code></pre>

<p>and I want to swap the same array like 7,2 is there anyway to do?</p>

<p>answer should be like 7,2</p>

<pre><code>a=[7,2]
</code></pre>
","['python', 'python-3.x', 'numpy']",57604838,"<pre><code>#a=np.array([2,7]) 
a=[2,7]

# Reversing a list using slice notation
print (a[::-1]) # [7, 2]

# The reversed() method
print (list(reversed(a))) # [7, 2]
</code></pre>

<p>swap two elements in a list: </p>

<pre><code># Swap function
def swapPositions(list, pos1, pos2):
    list[pos1], list[pos2] = list[pos2], list[pos1]
    return list


a=[2,7]
pos1, pos2 = 0, 1

print(swapPositions(a, pos1 - 1, pos2 - 1))
</code></pre>
",How I swap two elements one array I array like np array I want swap array like anyway answer like,"startoftags, python, python3x, numpy, endoftags",python python3x list endoftags,python python3x numpy,python python3x list,0.67
57944130,2019-09-15,2019,3,BeautifulSoup - How to extract email from a website?,"<p>I'm trying to extract some informations from a website, but I don't know how to scrape the email.</p>

<p>This code works for me :</p>

<pre><code>from urllib.request import urlopen as uReq
from bs4 import BeautifulSoup

url = ""https://www.eurocham-cambodia.org/member/476/2-LEau-Protection""
uClient = uReq(url)
page_html = uClient.read()
uClient.close()
soup = BeautifulSoup(page_html,""lxml"")

members = soup.findAll(""b"")
for member in members:
    member = members[0].text
print(member)
</code></pre>

<p>I wanted to extract the number and link with soup.findAll() but can't find a way to get the text properly so I used the SelectorGadget tool and tried this :</p>

<pre><code>numbers = soup.select(""#content li:nth-child(1)"")
for number in numbers:
    number = numbers[0].text
print(number)

links = soup.findAll("".icon-globe+ a"")
for link in links:
    link = links[0].text
print(link)
</code></pre>

<p>It prints correctly :</p>

<pre><code>2 L'Eau Protection
 (+33) 02 98 19 43 86
http://www.2leau-protection.com/
</code></pre>

<p>Now, when it comes to extract the email address i'm stuck. I'm new to this, any advice would be appreciate, thank you! </p>

<p>Attempt 1</p>

<pre><code>emails = soup.select(""#content li:nth-child(2)"")
for email in emails:
    email = emails[0].text
print(email)
</code></pre>

<p>I don't even know what it just prints</p>

<pre><code>//&lt;![CDATA[
var l=new Array();
l[0]='&gt;';l[1]='a';l[2]='/';l[3]='&lt;';l[4]='|109';l[5]='|111';l[6]='|99';l[7]='|46';l[8]='|110';l[9]='|111';l[10]='|105';l[11]='|116';l[12]='|99';l[13]='|101';l[14]='|116';l[15]='|111';l[16]='|114';l[17]='|112';l[18]='|45';l[19]='|117';l[20]='|97';l[21]='|101';l[22]='|108';l[23]='|50';l[24]='|64';l[25]='|110';l[26]='|111';l[27]='|105';l[28]='|116';l[29]='|97';l[30]='|109';l[31]='|114';l[32]='|111';l[33]='|102';l[34]='|110';l[35]='|105';l[36]='|32';l[37]='&gt;';l[38]='""';l[39]='|109';l[40]='|111';l[41]='|99';l[42]='|46';l[43]='|110';l[44]='|111';l[45]='|105';l[46]='|116';l[47]='|99';l[48]='|101';l[49]='|116';l[50]='|111';l[51]='|114';l[52]='|112';l[53]='|45';l[54]='|117';l[55]='|97';l[56]='|101';l[57]='|108';l[58]='|50';l[59]='|64';l[60]='|110';l[61]='|111';l[62]='|105';l[63]='|116';l[64]='|97';l[65]='|109';l[66]='|114';l[67]='|111';l[68]='|102';l[69]='|110';l[70]='|105';l[71]='|32';l[72]=':';l[73]='o';l[74]='t';l[75]='l';l[76]='i';l[77]='a';l[78]='m';l[79]='""';l[80]='=';l[81]='f';l[82]='e';l[83]='r';l[84]='h';l[85]=' ';l[86]='a';l[87]='&lt;';
for (var i = l.length-1; i &gt;= 0; i=i-1){
if (l[i].substring(0, 1) == '|') document.write(""&amp;#""+unescape(l[i].substring(1))+"";"");
else document.write(unescape(l[i]));}
//]]&gt;
</code></pre>

<p>Attempt 2</p>

<pre><code>emails = soup.select("".icon-mail~ a"") #follow the same logic
for email in emails:
    email = emails[0].text
print(email)
</code></pre>

<p>Error</p>

<pre><code>NameError: name 'email' is not defined
</code></pre>

<p>Attempt 3</p>

<pre><code>emails = soup.select("".icon-mail~ a"")
print(emails)
</code></pre>

<p>Print empty</p>

<pre><code>[]
</code></pre>

<p>Attempt 4,5,6</p>

<pre><code>email = soup.find(""a"",{""href"":""mailto:""}) # Print ""None""

email = soup.findAll(""a"",{""href"":""mailto:""}) # Print empty ""[]""

email = soup.select(""a"",{""href"":""mailto:""}) # Print a lot of informations but not the one that I need.
</code></pre>
","['python', 'web-scraping', 'beautifulsoup']",57944737,"<p>I see that you already have perfectly acceptable answers, but when I saw that obfuscation script I was fascinated, and just had to ""de-obfuscate"" it.</p>

<pre><code>from bs4 import BeautifulSoup
from requests import get
import re

page = ""https://www.eurocham-cambodia.org/member/476/2-LEau-Protection""

content = get(page).content
soup = BeautifulSoup(content, ""lxml"")

exp = re.compile(r""(?:.*?='(.*?)')"")
# Find any element with the mail icon
for icon in soup.findAll(""i"", {""class"": ""icon-mail""}):
    # the 'a' element doesn't exist, there is a script tag instead
    script = icon.next_sibling
    # the script tag builds a long array of single characters- lets gra
    chars = exp.findall(script.text)
    output = []
    # the javascript array is iterated backwards
    for char in reversed(list(chars)):
        # many characters use their ascii representation instead of simple text
        if char.startswith(""|""):
            output.append(chr(int(char[1:])))
        else:
            output.append(char)
    # putting the array back together gets us an `a` element
    link = BeautifulSoup("""".join(output))
    email = link.findAll(""a"")[0][""href""][8:]
    # the email is the part of the href after `mailto: `
    print(email)
</code></pre>
",BeautifulSoup How extract email website I trying extract informations website I know scrape email This code works urllib request import urlopen uReq bs import BeautifulSoup url https www eurocham cambodia org member LEau Protection uClient uReq url page html uClient read uClient close soup BeautifulSoup page html lxml members soup findAll b member members member members text print member I wanted extract number link soup findAll find way get text properly I used SelectorGadget tool tried numbers soup select content li nth child number numbers number numbers text print number links soup findAll icon globe link links link links text print link It prints correctly L Eau Protection http www leau protection com Now comes extract email address stuck I new advice would appreciate thank Attempt emails soup select content li nth child email emails email emails text print email I even know prints lt CDATA var l new Array,"startoftags, python, webscraping, beautifulsoup, endoftags",python webscraping beautifulsoup endoftags,python webscraping beautifulsoup,python webscraping beautifulsoup,1.0
57958432,2019-09-16,2019,15,How to add table title in python preferably with pandas,"<p>I would like to add a table title in the out put. It's relatively easy to do this in <code>r</code> with
<code>flextable</code>, you just have to use function <code>set_caption()</code>, is there a similar package in <code>python</code> with <code>pandas</code> dataframe (maybe <code>PTable</code>)?</p>
<p>This is what I have right now,
<a href=""https://i.stack.imgur.com/jiWQ8.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jiWQ8.png"" alt=""pic1"" /></a></p>
<p>and this is what I would like to have
<a href=""https://i.stack.imgur.com/TJ1pH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/TJ1pH.png"" alt=""pic2"" /></a></p>
<p>Please help, thanks guys!</p>
","['python', 'pandas', 'dataframe']",57958638,"<p>Have you tried this doing this?</p>

<pre><code>df.style.set_caption(""Hello World"")
</code></pre>

<p>Source: <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html"" rel=""noreferrer"">Pandas Styling</a></p>

<p>EDIT:</p>

<p>Here's an alternative way to present your table if you're okay with using <code>matplotlib</code></p>

<pre><code>import matplotlib.pyplot as plt
import pandas as pd

my_frame = pd.DataFrame(data={'simulation1':[71,4.8,65,4.7],
                              'simulation2':[71,4.8,69,4.7],
                              'simulation3':[70,3.8,68,4.9],
                              'experiment':[70.3,3.5,65,4.4]})
#my_frame Display pandas table

fig = plt.figure(figsize = (8, 2))
ax = fig.add_subplot(111)

ax.table(cellText = my_frame.values,
          rowLabels = my_frame.index,
          colLabels = my_frame.columns,
          loc = ""center""
         )
ax.set_title(""Top 10 Fields of Research by Aggregated Funding Amount"")

ax.axis(""off"");
</code></pre>
",How add table title python preferably pandas I would like add table title put It relatively easy r flextable use function set caption similar package python pandas dataframe maybe PTable This I right I would like Please help thanks guys,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
58095522,2019-09-25,2019,2,ValueError: too many values to unpack (expected 3) in url config,"<p>I have a <em>django</em> project with <em>django-rest-framework</em> named <strong>MyProject</strong> in which I have created an app <strong>accounts</strong>.</p>

<p>I have the following code inside <strong>MyProject/urls.py</strong>:</p>

<pre><code>from django.contrib import admin
from django.urls import path, include


urlpatterns = [
    path('', include('accounts.urls', namespace='accounts')),
    path('admin/', admin.site.urls),
]
</code></pre>

<p>Inside <strong>MyProject/accounts/urls.py</strong>, I have:</p>

<pre><code>from django.contrib import admin
from django.urls import path, include
from rest_framework import routers
from . import views

router = routers.DefaultRouter()
router.register('accounts', views.UserView)

urlpatterns = [
    path('', router.urls)
]
</code></pre>

<p>Inside <strong>MyProject/accounts/views.py</strong>:</p>

<pre><code>import sys
from django.shortcuts import render, redirect
from django.contrib.auth.models import User, auth
from django.contrib import messages
from rest_framework import viewsets

from .serializers import UserSerializer


class UserView(viewsets.ModelViewSet):
    queryset = User.objects.all()
    serializer_class = UserSerializer
</code></pre>

<p>I am getting the error:</p>

<pre><code>&gt; File
&gt; ""C:\Users\user\PycharmProjects\MyProject\accounts\urls.py"",
&gt; line 10, in &lt;module&gt;
&gt;     path('', router.urls)   File ""C:\Users\user\PycharmProjects\MyProject\venv\lib\site-packages\django\urls\conf.py"",
&gt; line 61, in _path
&gt;     urlconf_module, app_name, namespace = view ValueError: too many values to unpack (expected 3)
</code></pre>
","['python', 'django', 'django-rest-framework']",58095578,"<p>The <code>router.urls</code> contains a list of urls. You can simply set the <code>urlpatterns</code> to that list:</p>

<pre><code>router = routers.DefaultRouter()
router.register('accounts', views.UserView)

urlpatterns = <b>router.urls</b></code></pre>

<p>Or you can append the values if you want to make other paths:</p>

<pre><code>router = routers.DefaultRouter()
router.register('accounts', views.UserView)

urlpatterns = [
    # &hellip;
]

urlpatterns += <b>router.urls</b></code></pre>
",ValueError many values unpack expected url config I django project django rest framework named MyProject I created app accounts I following code inside MyProject urls py django contrib import admin django urls import path include urlpatterns path include accounts urls namespace accounts path admin admin site urls Inside MyProject accounts urls py I django contrib import admin django urls import path include rest framework import routers import views router routers DefaultRouter router register accounts views UserView urlpatterns path router urls Inside MyProject accounts views py import sys django shortcuts import render redirect django contrib auth models import User auth django contrib import messages rest framework import viewsets serializers import UserSerializer class UserView viewsets ModelViewSet queryset User objects serializer class UserSerializer I getting error gt File gt C Users user PycharmProjects MyProject accounts urls py gt line lt module gt gt path router urls File C Users user PycharmProjects MyProject venv,"startoftags, python, django, djangorestframework, endoftags",python tensorflow keras endoftags,python django djangorestframework,python tensorflow keras,0.33
58204710,2019-10-02,2019,2,"How can I train a sequential model in keras, giving a list as outputs and inputs?","<p>I am very new to Keras and to machine learning in general, but here is what I want. I have a list of inputs (1 value for every input node) and a list of targets (1 value for every output node).</p>

<pre><code>    input_list = [1, 0, 1, 0, 1, 0] # maybe longer

    wanted_output_list = [1, 0, 0, 0] # also maybe longer
</code></pre>

<p>And now I want to give these as input to train my neural network:</p>

<pre><code>    # create model
    model = Sequential()

    # get number of columns in training data
    n_cols = 6

    # add model layers
    model.add(Dense(6, activation='relu', input_shape= (n_cols,)))
    model.add(Dense(2, activation='relu'))
    model.add(Dense(2, activation='relu'))
    model.add(Dense(3))

    # compile the model
    model.compile(optimizer='adam', loss='mean_squared_error')

    # train model
    model.fit(input_list, wanted_output_list, validation_split=0.2, epochs=30)
</code></pre>

<p>However I get this error:</p>

<pre><code>    ValueError: Error when checking input: dense_1_input to have shape (6,) but got with shape (1,)
</code></pre>

<p>Can anyone please tell me why and how I can fix this?</p>
","['python', 'tensorflow', 'keras']",58206126,"<p>When defining your model, you specified a model that accepts an input with 6 features, and output a vector with 3 component. You training data, however, is not shaped correctly (nor your labels, by the way). You should shape your data the way you have defined your model. In this case, that means that each sample of your training data is a vector with 6 components, and each label is a vector with 3 components. </p>

<p>Keras expects a list of <code>numpy array</code> (or a 2D array) when training a model with multiple inputs, see the <a href=""https://keras.io/models/sequential/#fit"" rel=""nofollow noreferrer"">documentation</a>.</p>

<blockquote>
  <p><strong>x</strong> : Input data. It could be:</p>
  
  <ul>
  <li>A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs).</li>
  </ul>
</blockquote>

<p>So per your model definition, you could shape your training data the following way : </p>

<pre><code>import numpy as np
# your data, in this case 2 datapoints
X = np.array([[1, 0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 1]])
# the corresponding labels
y = np.array([[1, 0, 0], [0, 1, 0]]) 
</code></pre>

<p>And then train your model by calling <code>fit</code>.    </p>

<pre><code>model.fit(x, y, epochs=30)
</code></pre>
",How I train sequential model keras giving list outputs inputs I new Keras machine learning general I want I list inputs value every input node list targets value every output node input list maybe longer wanted output list also maybe longer And I want give input train neural network create model model Sequential get number columns training data n cols add model layers model add Dense activation relu input shape n cols model add Dense activation relu model add Dense activation relu model add Dense compile model model compile optimizer adam loss mean squared error train model model fit input list wanted output list validation split epochs However I get error ValueError Error checking input dense input shape got shape Can anyone please tell I fix,"startoftags, python, tensorflow, keras, endoftags",python tensorflow keras endoftags,python tensorflow keras,python tensorflow keras,1.0
58335548,2019-10-11,2019,2,"Python - get first and last observation per week, month, quarter, six months, year, etc","<p>I have a dataset (Pandas dataframe) that looks like this:</p>

<pre><code>         Date      Open      High  ...       date  year  month
0  2002-05-23  1.156429  1.242857  ... 2002-05-23  2002      5
1  2002-05-24  1.214286  1.225000  ... 2002-05-24  2002      5
2  2002-05-28  1.213571  1.232143  ... 2002-05-28  2002      5
3  2002-05-29  1.164286  1.164286  ... 2002-05-29  2002      5
4  2002-05-30  1.107857  1.107857  ... 2002-05-30  2002      5
</code></pre>

<p>How can I get the first and last observation per week, month, quarter, 6 months, year etc? I would like to get the prices from the dataset and then calculate the return for each respective period.</p>

<p>EDIT:</p>

<p>Your code gives me the following output for df1:</p>

<pre><code>Index        first    last
2002-05-26   1.15643  1.21429
2002-06-02   1.21357  1.07857
</code></pre>

<p>I would like to have:</p>

<pre><code>First date       Last date     First value    Last value
2002-05-26       2005-06-01    1.15643        1.21429
2002-06-02       2002-06-09    1.21357        1.07857
</code></pre>
","['python', 'pandas', 'dataframe']",58335588,"<p>I believe you need <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html"" rel=""nofollow noreferrer""><code>DataFrame.resample</code></a> with aggregate <code>first</code> and <code>last</code>: </p>

<pre><code>df1 = df.resample('W', on='Date')['Open'].agg(['first','last'])
df2 = df.resample('M', on='Date')['Open'].agg(['first','last'])
df3 = df.resample('Q', on='Date')['Open'].agg(['first','last'])
df4 = df.resample('6M', on='Date')['Open'].agg(['first','last'])
df5 = df.resample('Y', on='Date')['Open'].agg(['first','last'])
</code></pre>

<p>EDIT:</p>

<pre><code>print (df)
         Date  Open
0  2002-05-23   1.1
1  2002-05-24   1.2
2  2002-05-28   1.3
3  2002-05-29   1.4
4  2002-05-30   1.5
5  2002-05-31   1.6
6  2002-06-01   1.7
7  2002-06-02   1.8
8  2002-06-03   1.9
9  2002-06-04   2.0

df['Date'] = pd.to_datetime(df['Date'])
df1 = df.resample('W', on='Date')['Open'].agg(['first','last'])
print (df1)
            first  last
Date                   
2002-05-26    1.1   1.2
2002-06-02    1.3   1.8
2002-06-09    1.9   2.0

df1 = df.set_index('Date').resample('W')['Open'].agg([('First date', lambda x: x.index[0]),
                                                      ('Last date', lambda x: x.index[-1]),
                                                      ('First value','first'),
                                                      ('Last value','last')])
print (df1)
           First date  Last date  First value  Last value
Date                                                     
2002-05-26 2002-05-23 2002-05-24          1.1         1.2
2002-06-02 2002-05-28 2002-06-02          1.3         1.8
2002-06-09 2002-06-03 2002-06-04          1.9         2.0
</code></pre>
",Python get first last observation per week month quarter six months year etc I dataset Pandas dataframe looks like Date Open High date year month How I get first last observation per week month quarter months year etc I would like get prices dataset calculate return respective period EDIT Your code gives following output df Index first last I would like First date Last date First value Last value,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
58566113,2019-10-25,2019,2,replacing NaN values in dataframe with pandas,"<p>I want to create a function that takes a dataframe and replaces NaN with the mode in categorical columns, and replaces NaN in numerical columns with the mean of that column. If there are more than one mode in the categorical columns, then it should use the first mode.</p>

<p>I have managed to do it with following code:</p>

<pre><code>def exercise4(df):
    df1 = df.select_dtypes(np.number)
    df2 = df.select_dtypes(exclude = 'float')
    mode = df2.mode()
    df3 = df1.fillna(df.mean())
    df4 = df2.fillna(mode.iloc[0,:])
    new_df = [df3,df4]
    df5 = pd.concat(new_df,axis=1)
    new_cols = list(df.columns)
    df6 = df5[new_cols]
    return df6
</code></pre>

<p>But i am sure there is a far easier method to do this?</p>
","['python', 'pandas', 'dataframe']",58569181,"<p>You can use:</p>

<pre><code>df = pd.DataFrame({
        'A':list('abcdec'),
         'B':[4,5,4,5,5,4],
         'C':[7,8,9,4,2,3],
         'D':[1,3,5,7,1,0],
         'E':list('bbcdeb'),
})
df.iloc[[1,3], [1,2,0,4]] = np.nan

print (df)
     A    B    C  D    E
0    a  4.0  7.0  1    b
1  NaN  NaN  NaN  3  NaN
2    c  4.0  9.0  5    c
3  NaN  NaN  NaN  7  NaN
4    e  5.0  2.0  1    e
5    c  4.0  3.0  0    b
</code></pre>

<p>Idea is use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html"" rel=""nofollow noreferrer""><code>DataFrame.select_dtypes</code></a> for non numeric columns with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mode.html"" rel=""nofollow noreferrer""><code>DataFrame.mode</code></a> and select first row by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html"" rel=""nofollow noreferrer""><code>DataFrame.iloc</code></a> for positions, then count <code>means</code> - non numeric are expluded by default, so possible use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.append.html"" rel=""nofollow noreferrer""><code>Series.append</code></a> for Series with all values for replacement passed to <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html"" rel=""nofollow noreferrer""><code>DataFrame.fillna</code></a>:</p>

<pre><code>modes = df.select_dtypes(exclude=np.number).mode().iloc[0]
means = df.mean()
both = modes.append(means)
print (both)
A          c
E          b
B       4.25
C       5.25
D    2.83333
dtype: object

df.fillna(both, inplace=True)
print (df)
   A     B     C  D  E
0  a  4.00  7.00  1  b
1  c  4.25  5.25  3  b
2  c  4.00  9.00  5  c
3  c  4.25  5.25  7  b
4  e  5.00  2.00  1  e
5  c  4.00  3.00  0  b
</code></pre>

<p>Passed to function with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pipe.html"" rel=""nofollow noreferrer""><code>DataFrame.pipe</code></a>:</p>

<pre><code>def exercise4(df):
    modes = df.select_dtypes(exclude=np.number).mode().iloc[0]
    means = df.mean()
    both = modes.append(means)
    df.fillna(both, inplace=True)
    return df

df = df.pipe(exercise4)
#alternative
#df = exercise4(df)
print (df)
   A     B     C  D  E
0  a  4.00  7.00  1  b
1  c  4.25  5.25  3  b
2  c  4.00  9.00  5  c
3  c  4.25  5.25  7  b
4  e  5.00  2.00  1  e
5  c  4.00  3.00  0  b
</code></pre>

<p>Another idea is use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer""><code>DataFrame.apply</code></a>, but is necessary <a href=""https://stackoverflow.com/questions/34917404/why-does-apply-change-dtype-in-pandas-dataframe-columns""><code>result_type='expand'</code></a> parameter with test dtypes by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.api.types.is_numeric_dtype.html"" rel=""nofollow noreferrer""><code>types.is_numeric_dtype</code></a>:</p>

<pre><code>from pandas.api.types import is_numeric_dtype

f = lambda x: x.mean() if is_numeric_dtype(x.dtype) else x.mode()[0]
df.fillna(df.apply(f, result_type='expand'), inplace=True)
print (df)
   A     B     C  D  E
0  a  4.00  7.00  1  b
1  c  4.25  5.25  3  b
2  c  4.00  9.00  5  c
3  c  4.25  5.25  7  b
4  e  5.00  2.00  1  e
5  c  4.00  3.00  0  b
</code></pre>

<p>Passed to function:</p>

<pre><code>from pandas.api.types import is_numeric_dtype

def exercise4(df):
    f = lambda x: x.mean() if is_numeric_dtype(x.dtype) else x.mode()[0]
    df.fillna(df.apply(f, result_type='expand'), inplace=True)
    return df

df = df.pipe(exercise4)
#alternative
#df = exercise4(df)
print (df)
</code></pre>
",replacing NaN values dataframe pandas I want create function takes dataframe replaces NaN mode categorical columns replaces NaN numerical columns mean column If one mode categorical columns use first mode I managed following code def exercise df df df select dtypes np number df df select dtypes exclude float mode df mode df df fillna df mean df df fillna mode iloc new df df df df pd concat new df axis new cols list df columns df df new cols return df But sure far easier method,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
58649740,2019-10-31,2019,2,How to label ascending consecutive numbers onto values in a column?,"<p>I have a column that looks something like this: </p>

<pre><code>1
0
0
1
0
0
0
1
</code></pre>

<p>I want the output to look something like this:</p>

<pre><code>1 &lt;--
0
0
2 &lt;--
0
0
0
3 &lt;--
</code></pre>

<p>And so forth. I'm not sure where to begin. There about 10,000 rows and I feel like making a if statement might take awhile. How do I achieve this output?</p>
","['python', 'pandas', 'numpy']",58649938,"<p>Efficient and concise:</p>

<pre><code>s.cumsum()*s
</code></pre>

<p></p>

<pre><code>0    1       
1    0       
2    0       
3    2       
4    0       
5    0       
6    0       
7    3       
dtype: int64 
</code></pre>
",How label ascending consecutive numbers onto values column I column looks something like I want output look something like lt lt lt And forth I sure begin There rows I feel like making statement might take awhile How I achieve output,"startoftags, python, pandas, numpy, endoftags",python python3x pandas endoftags,python pandas numpy,python python3x pandas,0.67
58681281,2019-11-03,2019,4,"How do I group by a column, and count values in separate columns (Pandas)","<p>Here's an example data:</p>

<pre><code>data = [['a1', 1, 'a'], ['b1', 2, 'b'], ['a1', 3, 'a'], ['c1', 4, 'c'], ['b1', 5, 'a'], ['a1', 6, 'b'], ['c1', 7, 'a'], ['a1', 8, 'a']] 

df = pd.DataFrame(data, columns = ['user', 'house', 'type']) 

user house type
a1     1    a
b1     2    b
a1     3    a
c1     4    c
b1     5    a
a1     6    b
c1     7    a
a1     8    a
</code></pre>

<p>The final output that I want is this (the types need to be their own columns):</p>

<pre><code>user houses a b c    
a1      4   3 1 0
b1      2   1 1 0
c1      2   1 0 1
</code></pre>

<p>Currently, I'm able to get it by using the following code:</p>

<pre><code>house = df.groupby(['user']).agg(houses=('house', 'count'))
a = df[df['type']=='a'].groupby(['user']).agg(a=('type', 'count'))
b = df[df['type']=='b'].groupby(['user']).agg(b=('type', 'count'))
c = df[df['type']=='c'].groupby(['user']).agg(c=('type', 'count'))

final = house.merge(a,on='user', how='left').merge(b,on='user', how='left').merge(c,on='user', how='left')
</code></pre>

<p>Is there a simpler, cleaner way to do this?</p>
","['python', 'pandas', 'dataframe']",58681461,"<p>Using <code>GroupBy.size</code> with <code>pd.crosstab</code> and <code>join</code>:</p>

<pre><code>grps = pd.crosstab(df['user'], df['type']).join(df.groupby('user')['house'].size())

      a  b  c  house
user                
a1    3  1  0      4
b1    1  1  0      2
c1    1  0  1      2
</code></pre>

<hr>

<p>If you want <code>user</code> back as column, use <code>reset_index</code>:</p>

<pre><code>print(grps.reset_index())

  user  a  b  c  house
0   a1  3  1  0      4
1   b1  1  1  0      2
2   c1  1  0  1      2
</code></pre>
",How I group column count values separate columns Pandas Here example data data b b c c b b c df pd DataFrame data columns user house type user house type b b c c b b c The final output I want types need columns user houses b c b c Currently I able get using following code house df groupby user agg houses house count df df type groupby user agg type count b df df type b groupby user agg b type count c df df type c groupby user agg c type count final house merge user left merge b user left merge c user left Is simpler cleaner way,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
58808269,2019-11-11,2019,3,Pandas reset index after operations on rows,"<p>I want to know if there is a way to assign new <strong>.loc</strong> values to a dataframe in order to index this row. 
I was writing code where I was indexing rows by the <strong>.loc[]</strong>, but now I have randomly shuffled the dataframe in to two sets and so when I index the row by <strong>.loc[]</strong>, i get a key error as the row might be in the other dataset. </p>

<p>I want to be able to assign a new .loc[] index to the data right after I shuffle so I can still index as I always have.</p>

<p>Example, I have a dataframe:</p>

<pre><code>          length    height...                  water      type
    4     15.85  14.7240  ...               0.173     orange
    92    20.06  17.3565  ...               0.171     orange
    155   22.71  15.8040  ...               0.169     apple
    142   11.76  12.2355  ...               0.175     pear
    91    20.33  16.0785  ...               0.175      apple
</code></pre>

<p>The index given is displayed on the left (i.e 4 to 91), I want to change these index values to what I want to assign them, which is in sequential order (i.e 0 to 4). So that when I call .loc[0] it will return the first row and not give me a KeyError as that row is in another dataset</p>

<p>Thanks.</p>
","['python', 'pandas', 'dataframe']",59704987,"<p>From Pandas documentation:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([('bird', 389.0),
...                    ('bird', 24.0),
...                    ('mammal', 80.5),
...                    ('mammal', np.nan)],
...                   index=['falcon', 'parrot', 'lion', 'monkey'],
...                   columns=('class', 'max_speed'))
&gt;&gt;&gt; df
         class  max_speed
falcon    bird      389.0
parrot    bird       24.0
lion    mammal       80.5
monkey  mammal        NaN
</code></pre>

<p>using reset_index with drop parameter:</p>

<pre><code>&gt;&gt;&gt; df.reset_index(drop=True)
    class  max_speed
0    bird      389.0
1    bird       24.0
2  mammal       80.5
3  mammal        NaN
</code></pre>
",Pandas reset index operations rows I want know way assign new loc values dataframe order index row I writing code I indexing rows loc I randomly shuffled dataframe two sets I index row loc get key error row might dataset I want able assign new loc index data right I shuffle I still index I always Example I dataframe length height water type orange orange apple pear apple The index given displayed left e I want change index values I want assign sequential order e So I call loc return first row give KeyError row another dataset Thanks,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
58810517,2019-11-12,2019,3,pd.merge_asof with multiple matches per time period?,"<p>I'm trying to merge two dataframes by time with  multiple matches. I'm looking for all the instances of df2 whose <code>timestamp</code> falls 7 days or less before <code>endofweek</code> in df1. There may be more than one record that fits the case, and I want all of the matches, not just the first or last (which pd.merge_asof does).</p>

<pre><code>import pandas as pd
df1 = pd.DataFrame({'endofweek': ['2019-08-31', '2019-08-31', '2019-09-07', '2019-09-07', '2019-09-14', '2019-09-14'], 'GroupCol': [1234,8679,1234,8679,1234,8679]})
df2 = pd.DataFrame({'timestamp': ['2019-08-30 10:00', '2019-08-30 10:30', '2019-09-07 12:00', '2019-09-08 14:00'], 'GroupVal': [1234, 1234, 8679, 1234], 'TextVal': ['1234_1', '1234_2', '8679_1', '1234_3']})
df1['endofweek'] = pd.to_datetime(df1['endofweek'])
df2['timestamp'] = pd.to_datetime(df2['timestamp'])
</code></pre>

<p>I've tried </p>

<pre><code>pd.merge_asof(df1, df2, tolerance=pd.Timedelta('7d'), direction='backward', left_on='endofweek', right_on='timestamp', left_by='GroupCol', right_by='GroupVal')
</code></pre>

<p>but that gets me </p>

<pre><code>   endofweek  GroupCol           timestamp  GroupVal TextVal
0 2019-08-31      1234 2019-08-30 10:30:00    1234.0  1234_2
1 2019-08-31      8679                 NaT       NaN     NaN
2 2019-09-07      1234                 NaT       NaN     NaN
3 2019-09-07      8679                 NaT       NaN     NaN
4 2019-09-14      1234 2019-09-08 14:00:00    1234.0  1234_3
5 2019-09-14      8679 2019-09-07 12:00:00    8679.0  8679_1
</code></pre>

<p>I'm losing the text 1234_1. Is there way to do a sort of outer join for <code>pd.merge_asof</code>, where I can keep all of the instances of <code>df2</code> and not just the first or last?</p>

<p>My ideal result would look like this (assuming that the <code>endofweek</code> times are treated like 00:00:00 on that date):</p>

<pre><code>   endofweek  GroupCol           timestamp  GroupVal TextVal
0 2019-08-31      1234 2019-08-30 10:00:00    1234.0  1234_1
1 2019-08-31      1234 2019-08-30 10:30:00    1234.0  1234_2
2 2019-08-31      8679                 NaT       NaN     NaN
3 2019-09-07      1234                 NaT       NaN     NaN                 
4 2019-09-07      8679                 NaT       NaN     NaN 
5 2019-09-14      1234 2019-09-08 14:00:00    1234.0  1234_3
6 2019-09-14      8679 2019-09-07 12:00:00    8679.0  8679_1
</code></pre>
","['python', 'pandas', 'dataframe']",59218917,"<p><code>pd.merge_asof</code> only does a left join. After a lot of frustration trying to speed up the <code>groupby</code>/<code>merge_ordered</code> example, it's more intuitive and faster to do <code>pd.merge_asof</code> on both data sources in different directions, and then do an outer join to combine them.</p>

<pre class=""lang-py prettyprint-override""><code>left_merge = pd.merge_asof(df1, df2,
    tolerance=pd.Timedelta('7d'), direction='backward', 
    left_on='endofweek', right_on='timestamp', 
    left_by='GroupCol', right_by='GroupVal')

right_merge = pd.merge_asof(df2, df1, 
    tolerance=pd.Timedelta('7d'), direction='forward', 
    left_on='timestamp', right_on='endofweek',
    left_by='GroupVal', right_by='GroupCol')

merged = (left_merge.merge(right_merge, how=""outer"")
    .sort_values(['endofweek', 'GroupCol', 'timestamp'])
    .reset_index(drop=True))

merged

   endofweek  GroupCol           timestamp  GroupVal TextVal
0 2019-08-31      1234 2019-08-30 10:00:00    1234.0  1234_1
1 2019-08-31      1234 2019-08-30 10:30:00    1234.0  1234_2
2 2019-08-31      8679                 NaT       NaN     NaN
3 2019-09-07      1234                 NaT       NaN     NaN
4 2019-09-07      8679                 NaT       NaN     NaN
5 2019-09-14      1234 2019-09-08 14:00:00    1234.0  1234_3
6 2019-09-14      8679 2019-09-07 12:00:00    8679.0  8679_1
</code></pre>

<p>In addition, it is much faster than my other answer:</p>

<pre class=""lang-py prettyprint-override""><code>import time
n=1000
start=time.time()
for i in range(n):
    left_merge = pd.merge_asof(df1, df2,
        tolerance=pd.Timedelta('7d'), direction='backward', 
        left_on='endofweek', right_on='timestamp', 
        left_by='GroupCol', right_by='GroupVal')
    right_merge = pd.merge_asof(df2, df1, 
        tolerance=pd.Timedelta('7d'), direction='forward', 
        left_on='timestamp', right_on='endofweek',
        left_by='GroupVal', right_by='GroupCol')
    merged = (left_merge.merge(right_merge, how=""outer"")
        .sort_values(['endofweek', 'GroupCol', 'timestamp'])
        .reset_index(drop=True))

end = time.time()

end-start
15.040804386138916
</code></pre>
",pd merge asof multiple matches per time period I trying merge two dataframes time multiple matches I looking instances df whose timestamp falls days less endofweek df There may one record fits case I want matches first last pd merge asof import pandas pd df pd DataFrame endofweek GroupCol df pd DataFrame timestamp GroupVal TextVal df endofweek pd datetime df endofweek df timestamp pd datetime df timestamp I tried pd merge asof df df tolerance pd Timedelta direction backward left endofweek right timestamp left GroupCol right GroupVal gets endofweek GroupCol timestamp GroupVal TextVal NaT NaN NaN NaT NaN NaN NaT NaN NaN I losing text Is way sort outer join pd merge asof I keep instances df first last My ideal result would look like assuming endofweek times treated like date endofweek GroupCol timestamp GroupVal TextVal NaT NaN NaN NaT NaN NaN NaT NaN NaN,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
58940202,2019-11-19,2019,2,How do I create a hitbox for all shapes in my program and a way to detect collision?,"<p>I am fairly new to python and the module pygame. I also recently started work on a small project where the purpose of the game is to if a circle touches the ""player"" (the square) then the game will end or present a message saying you loose (still trying to solve the hitbox issue so still thinking of what to do after). The problem I am having is I am having trouble creating a way to detect the collision of the hitboxes and every time I try to make a way to test if the hitboxes I have collided it just won't work, so if someone can tell me how to make the classes and hitbox, another method, or even how to fix my code so it won't crash with a player class. Thank you, for any help, I may receive.</p>

<p>Here is the code (apologies if it is horrible on the eyes I kept on deleting and adding stuff while trying to find a solution and also sorry if I did something improper this is one of my first questions).</p>

<pre><code>import pygame
import random

pygame.init()

width, height = 800, 800
hbox, vbox = 15, 15
rect = pygame.Rect(500, 600, hbox, vbox)
velocity = (0, 0)
frames = 40
ball_size = 12
white = (255, 255, 255)
black = (0, 0, 0)
hp = 100

screen = pygame.display.set_mode((width, height))


pygame.display.set_caption(""Space Invaders"")
icon = pygame.image.load('ufo.png')
pygame.display.set_icon(icon)

is_blue = True


clock = pygame.time.Clock()


class Ball:


    def __init__(self):
        self.x = 0
        self.y = 0
        self.change_x = 0
        self.change_y = 0
        self.hitbox = (self.x + 1, self.y + 2, 31, 57)


def make_ball():

    ball = Ball()
    Ball.hitbox = ball.hitbox

    ball.x = random.randrange(ball_size, width - ball_size)
    ball.y = random.randrange(ball_size, height - ball_size)

    ball.change_x = random.randrange(-2, 3)
    ball.change_y = random.randrange(-2, 3)

    return ball


ball_list = []

ball = make_ball()
ball_list.append(ball)


class player(object):
    def __init__(self, ):
        self.x = rect.x
        self.y = rect.y
        self.box_width = hbox
        self.box_hieght = vbox
        self.speed = move
        player.hitbox = (self.x + 1, self.y + 11, 29, 52)

    def draw(self, is_blue):
        if is_blue:
            color = (0, 128, 255)
        else:
            color = (255, 100, 0)
        if event.type == pygame.KEYDOWN and event.key == pygame.K_c:
            is_blue = not is_blue
        Player = player
        Player = pygame.draw.rect(screen, color, rect)

    def move(self):
        surface = pygame.Surface((100, 100))

        keys = pygame.key.get_pressed()
        if keys[pygame.K_LSHIFT] or keys[pygame.K_RSHIFT]:
            move = 8
        else:
            move = 4
        if keys[pygame.K_w]:
            rect.y -= move
        if rect.y &lt; 0:
            rect.y = 0
        if keys[pygame.K_s]:
            rect.y += move
        if rect.y &gt; height - hbox:
            rect.y = height - vbox
        if keys[pygame.K_a]:
            rect.x -= move
        if rect.x &lt; 0:
            rect.x = 0
        if keys[pygame.K_d]:
            rect.x += move
        if rect.x &gt; width - hbox:
            rect.x = width - hbox



running = True
while running:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            running = False
        elif event.type == pygame.KEYDOWN:
            # Space bar! Spawn a new ball.
            if event.key == pygame.K_SPACE:
                ball = make_ball()
                ball_list.append(ball)
        if rect.x == ball.x and rect.y == ball.y:
            hp -100
            if hp == 0:
                pygame.quit()


    if event.type == pygame.KEYDOWN and event.key == pygame.K_c:
        is_blue = not is_blue

    surface = pygame.Surface((100, 100))

    keys = pygame.key.get_pressed()
    if keys[pygame.K_LSHIFT] or keys[pygame.K_RSHIFT]:
        move = 8
    else:
        move = 4
    if keys[pygame.K_w]:
        rect.y -= move
    if rect.y &lt; 0:
        rect.y = 0
    if keys[pygame.K_s]:
        rect.y += move
    if rect.y &gt; height - hbox:
        rect.y = height - vbox
    if keys[pygame.K_a]:
        rect.x -= move
    if rect.x &lt; 0:
        rect.x = 0
    if keys[pygame.K_d]:
        rect.x += move
    if rect.x &gt; width - hbox:
        rect.x = width - hbox


    screen.fill((0, 0, 0))

    if is_blue:
        color = (0, 128, 255)
    else:
        color = (255, 100, 0)

    color_2 = (255, 0, 0)
    for ball in ball_list:

        ball.x += ball.change_x
        ball.y += ball.change_y

        if ball.y &gt; height - ball_size or ball.y &lt; ball_size:
            ball.change_y *= -1
        if ball.x &gt; width - ball_size or ball.x &lt; ball_size:
            ball.change_x *= -1



    screen.fill(black)


    for ball in ball_list:
        pygame.draw.circle(screen, white, [ball.x, ball.y], ball_size)


    Rectangle = pygame.draw.rect(screen, color, rect)


    pygame.display.flip()

    clock.tick(30)


` 
</code></pre>
","['python', 'python-3.x', 'pygame']",58940565,"<p>In general I recommend to use <a href=""https://www.pygame.org/docs/ref/sprite.html#pygame.sprite.Sprite"" rel=""nofollow noreferrer""><code>pygame.sprite.Sprite</code></a> and <a href=""https://www.pygame.org/docs/ref/sprite.html#pygame.sprite.Group"" rel=""nofollow noreferrer""><code>pygame.sprite.Group</code></a>, but I'll show you a solution, which is closer to your current code.</p>

<p>Create a <code>Ball</code> class, which can move and draw the ball object. The class has a <code>.rect</code> attribute of type <a href=""https://www.pygame.org/docs/ref/rect.html"" rel=""nofollow noreferrer""><code>pygame.Rect</code></a> instead of the attributes <code>.x</code> and <code>.y</code> and can be used for the ""hitbox"", too.<br>
The rectangle is updated in the instance method <code>move()</code>:  </p>

<pre class=""lang-py prettyprint-override""><code>class Ball:

    def __init__(self):
        x = random.randrange(ball_size, width - ball_size)
        y = random.randrange(ball_size, height - ball_size)
        self.change_x, self.change_y = 0, 0
        while self.change_x == 0 and self.change_y == 0:
            self.change_x = random.randrange(-2, 3)
            self.change_y = random.randrange(-2, 3)
        self.rect = pygame.Rect(x-ball_size, y-ball_size, ball_size*2, ball_size*2)

    def move(self):
        self.rect = self.rect.move(self.change_x, self.change_y)
        if self.rect.right &gt;= height or self.rect.left &lt; 0:
            self.change_x *= -1
        if self.rect.bottom &gt;= width or self.rect.top &lt;= 0:
            self.change_y *= -1

    def draw(self, surface): 
        pygame.draw.circle(surface, white, self.rect.center, ball_size)

def make_ball():
    ball = Ball()
    return ball
</code></pre>

<p>In the main application loop, the balls can be moved and drawn in a <code>for</code>-loop and the collision test can be done by <a href=""https://www.pygame.org/docs/ref/rect.html#pygame.Rect.colliderect"" rel=""nofollow noreferrer""><code>.colliderect()</code></a>: </p>

<pre class=""lang-py prettyprint-override""><code>hits = 0
running = True
while running:

    # [...]

    for ball in ball_list:
        if ball.rect.colliderect(rect):
            hits += 1
            print(""hit "" + str(hits))   

    # [...]

    for ball in ball_list:
        ball.move()

    screen.fill(black)

    for ball in ball_list:
        ball.draw(screen)

    Rectangle = pygame.draw.rect(screen, color, rect)

    pygame.display.flip()
    clock.tick(30)
</code></pre>
",How I create hitbox shapes program way detect collision I fairly new python module pygame I also recently started work small project purpose game circle touches player square game end present message saying loose still trying solve hitbox issue still thinking The problem I I trouble creating way detect collision hitboxes every time I try make way test hitboxes I collided work someone tell make classes hitbox another method even fix code crash player class Thank help I may receive Here code apologies horrible eyes I kept deleting adding stuff trying find solution also sorry I something improper one first questions import pygame import random pygame init width height hbox vbox rect pygame Rect hbox vbox velocity frames ball size white black hp screen pygame display set mode width height pygame display set caption Space Invaders icon pygame image load ufo png pygame display set icon icon blue True clock,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
59093541,2019-11-28,2019,2,Filter rows in a pandas DataFrame,"<p>I am looking for a way to filter rows in a DataFrame. I have the following data:</p>

<pre><code>data = [
    {'year':2015, 'v1':'str1', 'v2':'str2', 'v3':'str3', 'val': 6}, 
    {'year':2016, 'v1':'str1', 'v2':'str2', 'v3':'str3', 'val': 5}, 
    {'year':2017, 'v1':'str1', 'v2':'str2', 'v3':'str3', 'val': 3},
    {'year':2015, 'v1':'str11', 'v2':'str2', 'v3':'str3', 'val': 4},
    {'year':2016, 'v1':'str11', 'v2':'str2', 'v3':'str3', 'val': 9},
    {'year':2017, 'v1':'str12', 'v2':'str2', 'v3':'str3', 'val': 1},
    {'year':2016, 'v1':'str1', 'v2':'str21', 'v3':'str3', 'val': 9},
    {'year':2017, 'v1':'str1', 'v2':'str21', 'v3':'str3', 'val': 7},
    {'year':2018, 'v1':'str1', 'v2':'str21', 'v3':'str3', 'val': 8},
    {'year':2015, 'v1':'str1', 'v2':'str2', 'v3':'str31', 'val': 6}, 
    {'year':2016, 'v1':'str1', 'v2':'str2', 'v3':'str31', 'val': 5},
    {'year':2016, 'v1':'str1', 'v2':'str2', 'v3':'str31', 'val': 6}, 
    {'year':2017, 'v1':'str1', 'v2':'str2', 'v3':'str31', 'val': 3},
    {'year':2018, 'v1':'str1', 'v2':'str2', 'v3':'str31', 'val': 4}
]
</code></pre>

<p>The filtering rule: if there are not at least three subsequent years, starting with 2015, with rows which match in v1, v2 and v3, then those rows should be removed. The rows which match in v1, v2 and v3 for at least three subsequent years from 2015 on, should be kept.</p>

<p>The expected output after filtering for the example above is:</p>

<pre><code>import pandas as pd
df = pd.DataFrame(data)
# filtering step
print(df)

    year     v1     v2     v3  val
0   2015   str1   str2   str3    6
1   2016   str1   str2   str3    5
2   2017   str1   str2   str3    3
3   2015   str1   str2  str31    6
4   2016   str1   str2  str31    5
5   2016   str1   str2  str31    6
6   2017   str1   str2  str31    3
7   2018   str1   str2  str31    4
</code></pre>

<p>Any ideas?</p>
","['python', 'pandas', 'dataframe']",59094203,"<p>I feel like we can short the <code>filter</code> as below </p>

<pre><code>df.groupby(['v1','v2','v3']).filter(lambda x : pd.Series([2015,2016,2017]).isin(x['year']).all())
Out[142]: 
    year    v1    v2     v3  val
0   2015  str1  str2   str3    6
1   2016  str1  str2   str3    5
2   2017  str1  str2   str3    3
9   2015  str1  str2  str31    6
10  2016  str1  str2  str31    5
11  2016  str1  str2  str31    6
12  2017  str1  str2  str31    3
13  2018  str1  str2  str31    4
</code></pre>
",Filter rows pandas DataFrame I looking way filter rows DataFrame I following data data year v str v str v str val year v str v str v str val year v str v str v str val year v str v str v str val year v str v str v str val year v str v str v str val year v str v str v str val year v str v str v str val year v str v str v str val year v str v str v str val year v str v str v str val year v str v str v str val year v str v str v str val year v str v str v str val The filtering rule least three subsequent years starting rows match v v v rows removed The rows match v v v least three subsequent,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
59177105,2019-12-04,2019,3,Django filters with a string variable field,"<p>I want to filter a model with a field but I want to pass the field as a string variable. How can I do it? </p>

<p>For example:</p>

<pre><code>the_field = 'name'
TheModel.objects.filter(the_field='Gazelle')
</code></pre>

<p>What should I replace <em>the_field</em> with?</p>
","['python', 'django', 'django-models']",59177116,"<p>You can use <em>dictionary unpacking</em>:</p>

<pre><code>the_field = 'name'
TheModel.objects.filter(<b>**{</b>the_field<b>:</b> 'Gazelle'<b>}</b>)</code></pre>

<p>Notice the two asterisks (<code>**</code>) in front of the dictionary. If you call a function with <code>f(**{'a': 4})</code>, that is equivalent to calling it with <code>f(a=4)</code>.</p>

<p>or you can make use of a <code>Q</code> object, and pass it a 2-tuple that represents the key and value:</p>

<pre><code>from django.db.models import <b>Q</b>
the_field = 'name'
TheModel.objects.filter(<b>Q((</b>the_field<b>,</b> 'Gazelle'<b>))</b>)</code></pre>
",Django filters string variable field I want filter model field I want pass field string variable How I For example field name TheModel objects filter field Gazelle What I replace field,"startoftags, python, django, djangomodels, endoftags",python django djangorestframework endoftags,python django djangomodels,python django djangorestframework,0.67
59244462,2019-12-09,2019,3,How to using pandas to cell merge,"<p>I want to combine the cells as follows.</p>

<p>Before:</p>

<pre><code>|   | test1 | test2 | test3 |
| -:|:----- |:----- | -----:|
| 0 | value | value | value |
| 1 | test4 | test5 |
| 2 | value | value |
| 3 | test6 | test7 | test8 |
| 4 | value | value | value |
| 5 | test9 | test0 |
| 6 | value | value |
</code></pre>

<p>After:</p>

<pre><code>|   | test1 | test2 | test3 | test4 | test5 | test6 | test7 | test8 | test9 | test0 |
| -:|:----- |:----- | ----- |:----- |:----- |:----- |:----- |:----- |:----- | -----:|
| 0 | value | value | value | value | value | value | value | value | value | value |
</code></pre>

<p>I want to change the cells using the Python code with the help of pandas. Please help me with this. Thank you.</p>
","['python', 'excel', 'pandas']",59245620,"<p>Here is something you can do. </p>

<pre><code>import pandas as pd

df = pd.DataFrame({'test1 ':['15','test4','79', 'test6', '34', 'test9', '323'],
                   'test2 ':['78','test5','45', 'test7', '4', 'test10', '34'],
                   'test3 ':['8','','', 'test8', '56', '', '']})
print(""Original Dataframe"")
print(df)

df1 = pd.DataFrame()
col_names = []
col_names = df.iloc[1::2, :].to_numpy('str').tolist()
row_values = df.iloc[2::2, :].to_numpy('str').tolist()
col_names = [j for sub in col_names for j in sub if j!= '']
row_values = [j for sub in row_values for j in sub if j!= '']
df1 = pd.DataFrame([row_values], columns= col_names)
print(""Dataframe 1"")
print(df1)

df2 = df.iloc[[0, ]]
print(""Dataframe 2"")
print(df2)

df3 = pd.concat([df2, df1], axis=1)
print(""Dataframe Result"")
print(df3)
</code></pre>

<p>The <code>df1</code> contains all the data except for the first one. The <code>df2</code> contains only original columns and first row. Finally, you concatenate <code>df1 and df2</code> to form <code>df3</code>.
<br/>
This gives you an output as:</p>

<pre><code>Original Dataframe
  test1   test2  test3 
0     15      78      8
1  test4   test5       
2     79      45       
3  test6   test7  test8
4     34       4     56
5  test9  test10       
6    323      34       
Dataframe 1
  test4 test5 test6 test7 test8 test9 test10
0    79    45    34     4    56   323     34
Dataframe 2
  test1  test2  test3 
0     15     78      8
Dataframe Result
  test1  test2  test3  test4 test5 test6 test7 test8 test9 test10
0     15     78      8    79    45    34     4    56   323     34
</code></pre>
",How using pandas cell merge I want combine cells follows Before test test test value value value test test value value test test test value value value test test value value After test test test test test test test test test test value value value value value value value value value value I want change cells using Python code help pandas Please help Thank,"startoftags, python, excel, pandas, endoftags",python pandas dataframe endoftags,python excel pandas,python pandas dataframe,0.67
59392122,2019-12-18,2019,2,how to calculate percentage for each cell and replace tha value with result(%) using pandas dataframe?,"<p>here is the sample example:</p>

<pre><code>input:
sub del wait 
12  55  11   
13  33  71   
</code></pre>

<p>ex:
1st row:
del=55*100/12=458.33
wait=11*100/12=91.66</p>

<p>2nd row:
del=33*100/13=253.3
wait=71*100/13=546.01</p>

<p><strong>final output should be:</strong></p>

<pre><code>sub del    wait     
12  458.33  91.66   
13  253.3   546.01  

</code></pre>
","['python', 'pandas', 'dataframe']",59392171,"<p>Select both columns, multiple by <code>100</code> and divide by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.div.html"" rel=""nofollow noreferrer""><code>DataFrame.div</code></a>, then assign columns back with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.round.html"" rel=""nofollow noreferrer""><code>DataFrame.round</code></a> if necessary:</p>

<pre><code>df[['del','wait']] = df[['del','wait']].mul(100).div(df['sub'], axis=0).round(2)
print (df)
   sub     del    wait
0   12  458.33   91.67
1   13  253.85  546.15
</code></pre>

<p>Or divide by numpy array created by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.to_numpy.html"" rel=""nofollow noreferrer""><code>Series.to_numpy</code></a> with shape (N x 1) for divide per rows:</p>

<pre><code>df[['del','wait']] = (df[['del','wait']] * 100 / df['sub'].to_numpy()[:, None]).round(2)
print (df)
   sub         del        wait
0   12  458.333333   91.666667
1   13  253.846154  546.153846
</code></pre>
",calculate percentage cell replace tha value result using pandas dataframe sample example input sub del wait ex st row del wait nd row del wait final output sub del wait,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
59437683,2019-12-21,2019,2,My button is not doing what I expected it to do,"<p>I have created 2 screens and a button to allow you to play again should you either get caught by the monster or win. However, despite the button (on both screens) showing and changing colour when the mouse is hovering over it, the click/link isn't working properly. When on the 'caught' screen, if I click the button several times it will eventually return to the game but at the exact point where you were caught by the monster, rather than go back to the beginning as I intended. And when on the win screen, the click/link isn't working at all. I copied the code from another game where it works fine, you click the button and the game restarts, thus I'm really not sure what the issue it. I'm including all of the code as I think it might help. Thank you in advance!</p>

<pre><code>import pygame
import random
import os

os.environ['SDL_VIDEO_CENTERED'] = '1'
#Initalise pygame
pygame.init()

#Colours required
black = (0,0,0)
blue = (0,0,255)
green = (0,200,0)
red = (255,0,0)
orange = (255,200,0)
bright_green = (0,255,0)

#Set up the display
width = 640
height = 480
screen = pygame.display.set_mode((width, height))
background = pygame.image.load('path.png')
caption = pygame.display.set_caption('Escape from the mazes!')
clock = pygame.time.Clock()

#Class for the player rect
class Player(object):
    def __init__(self, pos):
        self.rect = pygame.Rect(pos[0], pos[1], 32, 32) #x-axis, y-axis, width, height
        self.image = pygame.image.load('player.png')

    def move(self, dx, dy):
        #Move each axis separately. NB this checks for collisions both times
        if dx != 0:
            self.move_single_axis(dx, 0)
        if dy != 0:
            self.move_single_axis(0, dy)

    def move_single_axis(self, dx, dy):
        #Move the rect
        self.rect.x += dx
        self.rect.y += dy

        #If you collide with a wall, move out based on velocity
        for wall in walls:
            if self.rect.colliderect(wall.rect):
                if dx &gt; 0: #Moving right, hit the left side of wall
                    self.rect.right = wall.rect.left
                if dx &lt; 0: #Moving left, hit the right side of wall
                    self.rect.left = wall.rect.right
                if dy &gt; 0: #Moving down, hit the top side of wall
                    self.rect.bottom = wall.rect.top
                if dy &lt; 0: #Moving up, hit the bottom side of wall
                    self.rect.top = wall.rect.bottom

#Class for the monster rect
class Monster(object):
    def __init__(self, pos):
        self.rect = pygame.Rect(pos[0], pos[1], 32, 32)
        self.image = pygame.image.load('monster.png')
        self.dist = 3
        self.direction = random.randint(0, 3)
        self.steps = random.randint(3, 9) * 32

    def move(self):
        direction_list = ((-1,0), (1,0), (0,-1), (0,1))
        dx, dy = direction_list[self.direction]
        self.rect.x += dx
        self.rect.y += dy

        collide = False
        for wall in walls:
            if self.rect.colliderect(wall.rect):
                collide = True
                if dx &gt; 0:
                    self.rect.right = wall.rect.left
                if dx &lt; 0:
                    self.rect.left = wall.rect.right
                if dy &gt; 0:
                    self.rect.bottom = wall.rect.top
                if dy &lt; 0:
                    self.rect.top = wall.rect.bottom

        self.steps -= 1
        if collide or self.steps == 0:
            #New random direction
            self.direction = random.randint(0, 3)
            self.steps = random.randint(3, 9) * 32

#Class for the wall rect
class Wall(object):
    def __init__(self, pos):
        self.rect = pygame.Rect(pos[0], pos[1], 32, 32)
        self.image = pygame.image.load('hedge.png')

#Class for end rect
class Finish(object):
    def __init__(self, pos):
        self.rect = pygame.Rect(pos[0], pos[1], 32, 32)
        self.image = pygame.image.load('gate.png')

#Variables
currentLevel = 0

#Holds the level layout in a list of strings
levels = [[
'WWWWWWWWWWWWWWWWWWWW',
'WP          W      W',
'W         WWWWWW   W',
'W   WWWW       W   W',
'W   W        WWWW  W',
'W WWW  WWWW        W',
'W   W    MW W      W',
'W   W     W   WWW WW',
'W   WWW WWW   W W  W',
'W     W   W   W W  W',
'WWW   W   WWWWW W  W',
'W W      WW        W',
'W W   WWWW   WWWWWWW',
'W     W           FW',
'WWWWWWWWWWWWWWWWWWWW',
],
[
'WWWWWWWWWWWWWWWWWWWW',
'W W     W     W   FW',
'W W     W     W    W',
'W W  W  W  W  W  WWW',
'W W  W  W  W  W    W',
'W W  W  W  W  W    W',
'W W  W  W  W  W  W W',
'W    W  M  W  W  W W',
'W    W     W  W  WWW',
'WWW  WWWWWWW  W    W',
'W      W      W    W',
'W      W  WWWWWWW  W',
'WWWWW  W           W',
'WP     W           W',
'WWWWWWWWWWWWWWWWWWWW',
],
[
'WWWWWWWWWWWWWWWWWWWW',
'WP W           W   W',
'W  W  WWWW  WWWW W W',
'W  W     W       W W',
'W  WWWW  W    WWWW W',
'W     W  W  WW     W',
'W     W  W  W  WWWWW',
'WWWW  W  W MW     FW',
'W  W  W  W  WWWWWWWW',
'W  W     W         W',
'W  W     W         W',
'W  W  WWWWWWWWWWW  W',
'W               W  W',
'W               W  W',
'WWWWWWWWWWWWWWWWWWWW',
],
[
'WWWWWWWWWWWWWWWWWWWW',
'W     W            W',
'W     W   M        W',
'W  W  W  WWWWWWWW  W',
'W  W  W  W         W',
'W  W  W  W         W',
'W  W  W  W  WWWWWWWW',
'WP W  W  W         W',
'WWWW  W  W         W',
'W     W  WWWWWWWW  W',
'W     W  W         W',
'W  WWWW  W         W',
'W        W  WWWWWWWW',
'W        W        FW',
'WWWWWWWWWWWWWWWWWWWW',
],
[
'WWWWWWWWWWWWWWWWWWWW',
'W  W        W     FW',
'W  W        WWW W  W',
'W  W  WWWW    W WWWW',
'W  W     W    W    W',
'W  WWWW  WWWW W  W W',
'W    MW     W WWWW W',
'W     W     W      W',
'W  W  WWWW  WWWWWWWW',
'W  W     W         W',
'W  W     W         W',
'WWWW  W  WWWWWWWW  W',
'W     W            W',
'WP    W            W',
'WWWWWWWWWWWWWWWWWWWW',
],
[
'WWWWWWWWWWWWWWWWWWWW',
'WP     W     W   W W',
'WWWWW  W     W W W W',
'W  W  W  WWWWWWW W W',
'W W  W M W         W',
'WW  W  W W WWWWW   W',
'W  W   W W     W WWW',
'W  WWWWW WWWWW W WFW',
'W      W       W W W',
'WWWWWW W WWWWWWW W W',
'W      W W       W W',
'W W  WWW W       W W',
'W W      W WWWWWWW W',
'W W      W         W',
'WWWWWWWWWWWWWWWWWWWW',
]]

def load_level(level):
    walls = []
    players = []
    monsters = []
    finishes = []

    #Parse the level string above. W = wall, F = exit, P = player, M = monster
    x = y = 0
    for row in levels[level]:
        for col in row:
            if col == 'W':
                walls.append(Wall((x, y)))
            if col == 'P':
                players.append(Player((x, y)))
            if col == 'M':
                monsters.append(Monster((x, y)))
            if col == 'F':
                finishes.append(Finish((x, y)))
            x += 32
        y += 32
        x = 0
    return walls, players, monsters, finishes

walls, players, monsters, finishes = load_level(currentLevel)
Highest_level = len(levels)-1 #index of last level

def button(msg,x,y,w,h,ic,ac,action=None):
    mouse = pygame.mouse.get_pos()
    click = pygame.mouse.get_pressed()
    if x+w &gt; mouse[0] &gt; x and y+h &gt; mouse[1] &gt; y:
        pygame.draw.rect(screen, ac, (x,y,w,h))
        if click[0] == 1 and action != None:
            action()
    else:
        pygame.draw.rect(screen, ic, (x,y,w,h))

    smallText = pygame.font.Font(""freesansbold.ttf"", 20)
    textSurf, textRect = text_objects(msg, smallText)
    textRect.center = ((x+(w/2)), (y+(h/2)))
    screen.blit(textSurf, textRect)

def text_objects(text, font):
    textSurface = font.render(text, True, black)
    return textSurface, textSurface.get_rect()

def win():
    screen.fill(blue)
    largeText = pygame.font.Font('freesansbold.ttf', 90)
    TextSurf, TextRect = text_objects('You Escaped!', largeText)
    TextRect.center = (int((width/2)), int((height/2)))
    screen.blit(TextSurf, TextRect)

    while True:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                quit()

        button('Play Again',280,350,110,50,green,bright_green,main)

        pygame.display.update()
        clock.tick(15)

def caught():
    screen.fill(red)
    largeText = pygame.font.Font('freesansbold.ttf', 75)
    TextSurf, TextRect = text_objects('You Got Caught!', largeText)
    TextRect.center = (int((width/2)), int((height/2)))
    screen.blit(TextSurf, TextRect)

    while True:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                quit()

        button('Try Again',280,350,100,50,green,bright_green,main)

        pygame.display.update()
        clock.tick(15)

def main():
    global walls, players, monsters, currentLevel, finishes
    running = True
    while running:
        clock.tick(60)
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                quit()
            if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:
                pygame.quit()
                quit()

        #Move the player if an arrow key is pressed
        key = pygame.key.get_pressed()
        if key[pygame.K_LEFT]:
            player.move(-2, 0)
        if key[pygame.K_RIGHT]:
            player.move(2, 0)
        if key[pygame.K_UP]:
            player.move(0, -2)
        if key[pygame.K_DOWN]:
            player.move(0, 2)

        #Move monster
        for monster in monsters:
            monster.move()

        #Moving to next level/win
        for player in players:
            for finish in finishes:
                if player.rect.colliderect(finish.rect):
                    if currentLevel &lt; Highest_level:
                        currentLevel +=1
                        walls, players, monsters, finishes = load_level(currentLevel)
                    else:
                        win()

        #Getting caught by the monster
        for player in players:
            for monster in monsters:
                if player.rect.colliderect(monster.rect):
                    caught()

        #Draw the scene
        screen.fill(blue)
        screen.blit(background, (0,0))
        for wall in walls:
            #pygame.draw.rect(screen, green, wall.rect)
            screen.blit(wall.image, wall.rect)
        for player in players:
            #pygame.draw.rect(screen, orange, player.rect)
            screen.blit(player.image, player.rect)
        for monster in monsters:
            #pygame.draw.rect(screen, bright_green, monster.rect)
            screen.blit(monster.image, monster.rect)
        for finish in finishes:
            #pygame.draw.rect(screen, red, finish.rect)
            screen.blit(finish.image, finish.rect)
        pygame.display.update()

main()
pygame.quit()
quit()

</code></pre>
","['python', 'python-3.x', 'pygame']",59438145,"<p>Sorry, but that does not work like that. The action of the button can not be the main application loop (function <code>main</code>).<br>
What you actually do is to execute the main application loop. If the player is caught, then the function <code>caught</code> is called.<br>
In <code>caught</code> is another loop, which is executed endless. If the button is pressed, then the <code>main</code> is called. 
At this point you've the following situation:</p>

<pre class=""lang-none prettyprint-override""><code>main
  |
  +--caught
       |
       +--main
</code></pre>

<p>Since the game state has not changed, the player is  caught immediately in the inner (<code>main</code>) application loop and <code>caught</code> is called again:</p>

<pre class=""lang-none prettyprint-override""><code>main
  |
  +--caught
       |
       +--main
            |
            +-caught
</code></pre>

<p>That continues infinitely.</p>

<hr>

<p>I'll show you haw to fix this for <code>caught</code>.</p>

<p>Create an function, which resets the current game state. The function resets the current level to its begin:</p>

<pre class=""lang-py prettyprint-override""><code>def load_current_level():
    global walls, players, monsters, finishes
    walls, players, monsters, finishes = load_level(currentLevel)
</code></pre>

<p>Change the function <code>button</code> in that way, that  it returns <code>True</code> when the button was clicked and else <code>False</code>:</p>

<pre class=""lang-py prettyprint-override""><code>def button(msg,x,y,w,h,ic,ac,action=None):

    clicked = False
    mouse = pygame.mouse.get_pos()
    click = pygame.mouse.get_pressed()
    if x+w &gt; mouse[0] &gt; x and y+h &gt; mouse[1] &gt; y:
        pygame.draw.rect(screen, ac, (x,y,w,h))
        if click[0] == 1 and action != None:
            action()
            clicked = True
    else:
        pygame.draw.rect(screen, ic, (x,y,w,h))

    smallText = pygame.font.Font(""freesansbold.ttf"", 20)
    textSurf, textRect = text_objects(msg, smallText)
    textRect.center = ((x+(w/2)), (y+(h/2)))
    screen.blit(textSurf, textRect)

    return clicked
</code></pre>

<p>Terminate the loop in <code>caught</code>, when the button was pressed. The action which is associated to the button is <code>load_current_level</code>:</p>

<pre class=""lang-py prettyprint-override""><code>def caught():
    screen.fill(red)
    largeText = pygame.font.Font('freesansbold.ttf', 75)
    TextSurf, TextRect = text_objects('You Got Caught!', largeText)
    TextRect.center = (int((width/2)), int((height/2)))
    screen.blit(TextSurf, TextRect)

    try_again = False
    while not try_again:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                quit()

        try_again = button('Try Again',280,350,100,50,green,bright_green,load_current_level)

        pygame.display.update()
        clock.tick(15)
</code></pre>

<p>When the button is pressed, then the level is reset and the loop in <code>caught</code> is terminated. <code>caught</code> returns to the main application loop and you can retry the level.</p>

<hr>

<p>Do something similar in <code>win</code>:</p>

<pre class=""lang-py prettyprint-override""><code>def load_first_level():
    global walls, players, monsters, finishes, currentLevel
    currentLevel = 0
    walls, players, monsters, finishes = load_level(currentLevel)
</code></pre>

<pre class=""lang-py prettyprint-override""><code>def win():
    screen.fill(blue)
    largeText = pygame.font.Font('freesansbold.ttf', 90)
    TextSurf, TextRect = text_objects('You Escaped!', largeText)
    TextRect.center = (int((width/2)), int((height/2)))
    screen.blit(TextSurf, TextRect)

    play_again = False
    while not play_again:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                quit()

        play_again = button('Play Again',280,350,110,50,green,bright_green,load_first_level)

        pygame.display.update()
        clock.tick(15)
</code></pre>
",My button I expected I created screens button allow play either get caught monster win However despite button screens showing changing colour mouse hovering click link working properly When caught screen I click button several times eventually return game exact point caught monster rather go back beginning I intended And win screen click link working I copied code another game works fine click button game restarts thus I really sure issue I including code I think might help Thank advance import pygame import random import os os environ SDL VIDEO CENTERED Initalise pygame pygame init Colours required black blue green red orange bright green Set display width height screen pygame display set mode width height background pygame image load path png caption pygame display set caption Escape mazes clock pygame time Clock Class player rect class Player object def init self pos self rect pygame Rect pos pos x axis,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
59817336,2020-01-20,2020,3,Display sum function value for only one category,"<pre><code>data = {'Person': ['a','b','c','d','a','b','c','d','b','c'],
        'months':['Jan','Jan','Jan','Jan','Feb','Feb','Feb','Feb','March','March'],
        'income':[100,75,80,56,48,56,37,48,95,65]}
df = pd.DataFrame(data)

df.groupby(['Person'])['income'].sum()
</code></pre>

<p>Output:  </p>

<pre><code>Person  
a    148  
b    226  
c    182  
d    104  
Name: income, dtype: int64
</code></pre>

<p>But I want to display data for only <code>a</code>. How can I do that?</p>
","['python', 'pandas', 'dataframe']",59817391,"<p>Why use <code>groupby</code> if you only need <code>a</code>?</p>

<pre><code>df.loc[df[""Person""].eq(""a""),""income""].sum()

#148
</code></pre>
",Display sum function value one category data Person b c b c b c months Jan Jan Jan Jan Feb Feb Feb Feb March March income df pd DataFrame data df groupby Person income sum Output Person b c Name income dtype int But I want display data How I,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
59821502,2020-01-20,2020,2,replace just first biggest value of dataframe line in pandas?,"<p>i'm having trouble trying to replace(with 0 ) just the first instance of a max value in dataframe.
for example:</p>

<pre><code>NAME   1ST MONTH    2ND MONTH   3RD MONTH....
Joe        3            3            2
Erik       5            7            7
</code></pre>

<p>I need to replace just the first instance of the max value of every line in the df.
The output i need is:</p>

<pre><code>NAME   1ST MONTH    2ND MONTH   3RD MONTH....
Joe        0            3            2
Erik       5            0            7
</code></pre>

<p>But i'm using:</p>

<pre><code>df_temp1.apply(lambda x: x.replace(max(x), 0), axis = 1)
</code></pre>

<p>And this gives me the following df:</p>

<pre><code>NAME   1ST MONTH    2ND MONTH   3RD MONTH....
Joe        0            0            2
Erik       5            0            0
</code></pre>
","['python', 'pandas', 'dataframe']",59821968,"<p>You can use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.nlargest.html"" rel=""nofollow noreferrer""><code>nlargest()</code></a> with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.replace.html"" rel=""nofollow noreferrer""><code>replace()</code></a></p>

<pre><code>df = pd.DataFrame([[3, 3, 2], [5, 7, 7]], columns=['a', 'b', 'c'])

df = df.apply(lambda x: x.replace(x.nlargest(1), 0), axis=1)

print(df)

    a   b   c
0   0   3   2
1   5   0   7
</code></pre>
",replace first biggest value dataframe line pandas trouble trying replace first instance max value dataframe example NAME ST MONTH ND MONTH RD MONTH Joe Erik I need replace first instance max value every line df The output need NAME ST MONTH ND MONTH RD MONTH Joe Erik But using df temp apply lambda x x replace max x axis And gives following df NAME ST MONTH ND MONTH RD MONTH Joe Erik,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
59886994,2020-01-23,2020,2,How to reshape pandas dataframe with pivot?,"<p>I have a dataframe,</p>

<pre><code>    Year  Start  End   Name       Price
0   nan   0101   0331  Squirtle    876
1  2021   0101   1231  Squirtle    200
2   nan   0101   0331  Wartortle   1000
3  2021   0101   1231  Wartortle   1200
4   nan   0101   0331  Blastoise   3100
5  2021   0101   1231  Blastoise   4200
6  2022   0101   1231  Blastoise   10000
</code></pre>

<p>I want to reshape it like this,</p>

<pre><code>                   Name    Squirtle      Wartortle       Blastoise
Year  Start End
nan   0101  0331              876           1000            3100
2021  0101  1231              200           1200            4200
2022  0101  1231                                            10000
</code></pre>

<p>I tried,
<code>df.pivot(index=['Year', 'Start', 'End'], columns='Name', values='Price')</code>. But didn't get any luck.
Any help would be appreciated!</p>
","['python', 'pandas', 'dataframe']",59887385,"<p>You are pretty close.  Use <code>pivot_table</code> instead of <code>pivot</code> to get the grouping you want.  The only caveat is you will need to replace the <code>NA</code> values (if they are actually <code>NA</code> and not the string <code>'nan'</code>).</p>

<pre class=""lang-py prettyprint-override""><code>df.fillna('NA').pivot_table(index=['Year', 'Start', 'End'], columns='Name', values='Price')
# returns:
Name               Blastoise  Squirtle  Wartortle
Year   Start End
2021.0 101   1231     4200.0     200.0     1200.0
2022.0 101   1231    10000.0       NaN        NaN
NA     101   331      3100.0     876.0     1000.0
</code></pre>
",How reshape pandas dataframe pivot I dataframe Year Start End Name Price nan Squirtle Squirtle nan Wartortle Wartortle nan Blastoise Blastoise Blastoise I want reshape like Name Squirtle Wartortle Blastoise Year Start End nan I tried df pivot index Year Start End columns Name values Price But get luck Any help would appreciated,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
59952078,2020-01-28,2020,3,Matplotlib side by side bar plot,"<p>I am trying to plot the following dataframe using matplotlib:</p>

<pre><code>df = pd.DataFrame({'X': [""A"", ""A"", ""B"", ""B""], 'Z': [""a"", ""b"", ""a"", ""b""], 'Y': [5, 1, 10, 5]})
df

    X   Z   Y
0   A   a   5
1   A   b   1
2   B   a   10
3   B   b   5
</code></pre>

<p>What I want is two bar plots where the bars are next to each other rather than on top of each other. When I run this the bars are placed on top of each other:</p>

<pre><code>plt.barh(df['X'][df['Z'] == ""a""], df['Y'][df['Z'] == ""a""], color = 'blue')
plt.barh(df['X'][df['Z'] == ""b""], df['Y'][df['Z'] == ""b""], color = 'red')
</code></pre>

<p>And whe I try changing the position of the bars I get the error: <code>can only concatenate str (not ""float"") to str</code>. How can I work around this?</p>
","['python', 'pandas', 'matplotlib']",59952355,"<p>Not sure exactly what you want but you can try this:</p>

<pre><code>df.set_index(['X','Z'])['Y'].unstack().plot.barh()
</code></pre>

<p><a href=""https://i.stack.imgur.com/Dziv6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dziv6.png"" alt=""enter image description here""></a></p>

<p>Or</p>

<pre><code>df.set_index(['X','Z'])['Y'].unstack().plot.bar()
</code></pre>

<p><a href=""https://i.stack.imgur.com/arTTT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/arTTT.png"" alt=""enter image description here""></a></p>

<p>Or</p>

<pre><code>df.set_index(['X','Z'])['Y'].unstack().plot.barh(subplots=True, layout=(1,2))
</code></pre>

<p><a href=""https://i.stack.imgur.com/FLXfY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FLXfY.png"" alt=""enter image description here""></a></p>

<p>or</p>

<pre><code>df.set_index(['X','Z'])['Y'].unstack().plot.bar(subplots=True, layout=(1,2))
</code></pre>

<p><a href=""https://i.stack.imgur.com/wFRab.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wFRab.png"" alt=""enter image description here""></a></p>
",Matplotlib side side bar plot I trying plot following dataframe using matplotlib df pd DataFrame X A A B B Z b b Y df X Z Y A A b B B b What I want two bar plots bars next rather top When I run bars placed top plt barh df X df Z df Y df Z color blue plt barh df X df Z b df Y df Z b color red And whe I try changing position bars I get error concatenate str float str How I work around,"startoftags, python, pandas, matplotlib, endoftags",python pandas matplotlib endoftags,python pandas matplotlib,python pandas matplotlib,1.0
60042289,2020-02-03,2020,3,Multi-column to single column in Pandas,"<p>I have the following data frame :</p>

<pre><code>    parent          0        1      2   3
0   14026529    14062504     0      0   0
1   14103793    14036094     0      0   0
2   14025454    14036094     0      0   0
3   14030252    14030253  14062647  0   0
4   14034704    14086964     0      0   0
</code></pre>

<p>And I need this :</p>

<pre><code>    parent_id   child_id
 0   14026529   14062504
 1   14025454   14036094
 2   14030252   14030253  
 3   14030252   14062647
 4   14103793   14036094
 5   14034704   14086964
</code></pre>

<p>This is just a basic example, the real deal can have over 60 children.</p>
","['python', 'pandas', 'dataframe']",60042517,"<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.where.html"" rel=""nofollow noreferrer""><code>DataFrame.where</code></a>, <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>stack</code></a> and <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>reset_index</code></a>.<br>
Casting as <code>Int64</code> first will prevent child_Id's being cast to floats during the stacking process.</p>

<pre><code>(df.astype('Int64').where(df.ne(0))
 .set_index('parent')
 .stack()
 .reset_index(level=0, name='child'))
</code></pre>

<p>[out]</p>

<pre><code>     parent     child
0  14026529  14062504
0  14103793  14036094
0  14025454  14036094
0  14030252  14030253
1  14030252  14062647
0  14034704  14086964
</code></pre>
",Multi column single column Pandas I following data frame parent And I need parent id child id This basic example real deal children,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
60098237,2020-02-06,2020,3,How to extract hourly data from a df in python?,"<p>I have the following <code>df</code></p>

<pre><code>     dates         Final
2020-01-01 00:15:00 94.7
2020-01-01 00:30:00 94.1
2020-01-01 00:45:00 94.1
2020-01-01 01:00:00 95.0
2020-01-01 01:15:00 96.6
2020-01-01 01:30:00 98.4
2020-01-01 01:45:00 99.8
2020-01-01 02:00:00 99.8
2020-01-01 02:15:00 98.0
2020-01-01 02:30:00 95.1
2020-01-01 02:45:00 91.9
2020-01-01 03:00:00 89.5
</code></pre>

<p>The entire dataset is till <code>2021-01-01 00:00:00 95.6</code> with a gap of 15mins.</p>

<p>Since the freq is 15mins, I would like to change it to 1 hour and maybe drop the middle values</p>

<p><strong>Expected output</strong></p>

<pre><code>      dates        Final
2020-01-01 01:00:00 95.0
2020-01-01 02:00:00 99.8
2020-01-01 03:00:00 89.5
</code></pre>

<p>With the last row being <code>2021-01-01 00:00:00 95.6</code></p>

<p>How can this be done?</p>

<p>Thanks </p>
","['python', 'python-3.x', 'pandas']",60098319,"<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.minute.html"" rel=""nofollow noreferrer""><code>Series.dt.minute</code></a> to performance a <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer""><code>boolean indexing</code></a>:</p>

<pre><code>df_filtered = df.loc[df['dates'].dt.minute.eq(0)]
#if necessary
#df_filtered = df.loc[pd.to_datetime(df['dates']).dt.minute.eq(0)]
print(df_filtered)
                 dates  Final
3  2020-01-01 01:00:00   95.0
7  2020-01-01 02:00:00   99.8
11 2020-01-01 03:00:00   89.5
</code></pre>
",How extract hourly data df python I following df dates Final The entire dataset till gap mins Since freq mins I would like change hour maybe drop middle values Expected output dates Final With last row How done Thanks,"startoftags, python, python3x, pandas, endoftags",python pandas numpy endoftags,python python3x pandas,python pandas numpy,0.67
60149936,2020-02-10,2020,2,Pandas.DataFrame: find the index of the row whose value in a given column is closest to (but below) a specified value,"<p>In a Pandas.DataFrame, I would like to find the index of the row whose value in a given column is closest to (but below) a specified value. Specifically, say I am given the number 40 and the DataFrame df:</p>

<pre><code>|    |   x |
|---:|----:|
|  0 |  11 |
|  1 |  15 |
|  2 |  17 |
|  3 |  25 |
|  4 |  54 |
</code></pre>

<p>I want to find the index of the row such that df[""x""] is lower but as close as possible to 40. Here, the answer would be 3 because df[3,'x']=25 is smaller than the given number 40 but closest to it.
My dataframe has other columns, but I can assume that the column ""x"" is increasing.</p>

<p>For an exact match, I did (correct me if there is a better method):</p>

<pre><code>    list = df[(df.x == number)].index.tolist()
    if list:
        result = list[0]
</code></pre>

<p>But for the general case, I do not know how to do it in a ""vectorized"" way.</p>
","['python', 'pandas', 'dataframe']",60150050,"<p>Filter rows below 40 by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.lt.html"" rel=""nofollow noreferrer""><code>Series.lt</code></a> in <a href=""http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing"" rel=""nofollow noreferrer""><code>boolean indexing</code></a> and get mximal index value by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.idxmax.html"" rel=""nofollow noreferrer""><code>Series.idxmax</code></a>:</p>

<pre><code>a = df.loc[df['x'].lt(40), 'x'].idxmax()
print (a)
3
</code></pre>

<hr>

<p>For improve performance is possible use <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html"" rel=""nofollow noreferrer""><code>numpy.where</code></a> with <code>np.max</code>, solution working if default index:</p>

<pre><code>a = np.max(np.where(df['x'].lt(40))[0])
print (a)
3
</code></pre>

<p>If not default <code>RangeIndex</code>:</p>

<pre><code>df = pd.DataFrame({'x':[11,15,17,25,54]}, index=list('abcde'))

a = np.max(np.where(df['x'].lt(40))[0])
print (a)
3

print (df.index[a])
d
</code></pre>
",Pandas DataFrame find index row whose value given column closest specified value In Pandas DataFrame I would like find index row whose value given column closest specified value Specifically say I given number DataFrame df x I want find index row df x lower close possible Here answer would df x smaller given number closest My dataframe columns I assume column x increasing For exact match I correct better method list df df x number index tolist list result list But general case I know vectorized way,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
60174442,2020-02-11,2020,2,Show Model Validation Progress with Keras model.fit(),"<p>I am training a CNN model using <code>tf.keras</code> passing training and validation generators as follows:</p>

<pre><code>model.fit(
    x=training_data_generator,
    validation_data=validation_data_generator,
    epochs=n_epochs,
    use_multiprocessing=False,
    max_queue_size=100,
    workers=50
)
</code></pre>

<p>The generators are based on <code>tf.keras.Sequence</code>.</p>

<p>The problem is, my data set is huge. Training one epoch takes about a day (despite training on two Titan RTX GPUs) and validation after each epoch takes a few hours.</p>

<p>During training I can see the progress displayed, but during validation all I see is the last snapshot of the training progress bar:</p>

<p><code>130339/130340 [==============================] - 147432s 1s/step</code></p>

<p>until the validation finishes and finally I see my validation acuracy, loss etc.</p>

<p><em>Is there a way to display a progress bar for validation?</em></p>

<p>I'm thinking of doing something like this:</p>

<pre><code>for epoch in range(n_epochs):
    model.fit(
        x=training_data_generator,
        epochs=1,
        use_multiprocessing=False,
        max_queue_size=100,
        workers=50
    )
    validation_results = model.evaluate(
        x=validation_data_generator,
        use_multiprocessing=False,
        max_queue_size=100,
        workers=50
    )
    print(validation_results)
</code></pre>

<p>Another option I was considering is to create a custom callback that validates the model <code>on_epoch_end</code>, but this seems very non-standard.</p>

<p>Is there a better approach to this?</p>
","['python', 'tensorflow', 'keras']",61378642,"<p>You can set a <strong>steps_per_epoch</strong> on the fit method.
<br>
Based on the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model"" rel=""nofollow noreferrer"">documentation</a>: <br/>
<em>Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, <br/>the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. This argument is not supported with array inputs.<br/></em></p>

<p>By this, you can limit the per epoch steps, so setting it with a lower value will immediately give you the <strong>validation loss &amp; accuracy per epoch</strong> <br>
By setting the steps_per_epoch to a lower size means you need to increase the epoch.<br></p>

<p>Every 1000 steps or epoch, it will show you the <strong>training and validation loss &amp; accuracy</strong> after finishing 1000 steps rather than exhausting the entire dataset first then showing the results.<br></p>

<pre><code>history = model.fit(x_train, y_train,
                    batch_size=2,
                    epochs=30,
                    steps_per_epoch=1000,
                    # We pass some validation for
                    # monitoring validation loss and metrics
                    # at the end of each epoch
                    validation_data=(x_val, y_val)) 
</code></pre>
",Show Model Validation Progress Keras model fit I training CNN model using tf keras passing training validation generators follows model fit x training data generator validation data validation data generator epochs n epochs use multiprocessing False max queue size workers The generators based tf keras Sequence The problem data set huge Training one epoch takes day despite training two Titan RTX GPUs validation epoch takes hours During training I see progress displayed validation I see last snapshot training progress bar step validation finishes finally I see validation acuracy loss etc Is way display progress bar validation I thinking something like epoch range n epochs model fit x training data generator epochs use multiprocessing False max queue size workers validation results model evaluate x validation data generator use multiprocessing False max queue size workers print validation results Another option I considering create custom callback validates model epoch end seems non standard,"startoftags, python, tensorflow, keras, endoftags",python django djangorestframework endoftags,python tensorflow keras,python django djangorestframework,0.33
60211105,2020-02-13,2020,3,Ordering dataframe rows based on matching values in parallel columns,"<p>EDIT: As requested here are what the two dataframes look like before I concat them, and as they have the same column types so its literally:</p>

<pre><code># The Larger dataset, there are 4 other columns after, which the smaller dataframe shares in type

      id      Port  
    123ABC  Boston Port

# The smaller dataset

      id      Port  
    456DEF  Boston Port
</code></pre>

<p>I have a dataframe which is comparing two different datasets that have been merged into the same dataframe. An important note is that the two dataframes are of same number of columns but different number of rows, so one is 2979 rows and the other 791 rows. </p>

<p>The smaller dataset has matching NAME values in the larger one and I'm trying to get the rows to match based on the name. This is simple to do if the values are unique, its just using a dict map, however a caveat being something like this:</p>

<p>There are two <code>Boston Ports</code> however they have different ID numbers (since one port is located in Boston US, and there is one in the UK), the dict/map ignores this and places the first occurring one as match.</p>

<p>Example df:</p>

<pre><code>  id      id2       Port1            Port2
123ABC   456DEF  Boston Port      Boston Port
789GHI   101JKL  Boston Port      Bridport Harbour
</code></pre>

<p>The second dataset (smaller one) was created based off of matching names inside a larger database, however the names arent unique, but other attributes are which I used to differentiate between them later on down in my code workflow. </p>

<p>The basic task here is to make sure any values in <code>Port1</code> and <code>Port2</code> align into the same row, and any duplicates get attached matched to <code>Port1</code> so a desired output ideally looks something like this:</p>

<pre><code>  id      id2       Port1            Port2
123ABC   456DEF  Boston Port      Boston Port
789GHI   456DEF  Boston Port      Boston Port
</code></pre>

<p>This is the code snippet I'm using to map the two columns onto the same dataframe:</p>

<pre><code>all_ents = pd.concat([big_dataset, small_dataset], axis=1)  # Concat the two dataframes 

big_name_mapper = dict(zip(big_dataset.id1, big_dataset.name))  # Create Mapping dictionary from larger dataframe
small_name_mapper = dict(zip(small_dataset.id2, small_dataset.name))  # Create Mapping dictionary from smaller dataframe

all_ents['Port1'] = all_ents.id1.map(big_name_mapper)  # Create New column based on big_name_mapper
all_ents['Port2'] = all_ents.id2.map(small_name_mapper)  # Create New column based on small_name_mapper
</code></pre>

<p>Anyone have any experience handling this? Thanks!</p>

<p>EDIT2: Final code, </p>

<p>Thanks @Dave! </p>

<p>This is the final code I used based on his answer with some filtering too since I don't want rows that dont match in this case.</p>

<pre><code>Really_Big = BIG.merge(SMALL, how=""outer"", on=""port"", suffixes=(1, 2))

Really_Big = Really_Big[Really_Big['id1'].notnull()]
Really_Big = Really_Big[Really_Big['id2'].notnull()]  # Theres absolutely a much more efficient way to do this filter but for times sake this'll work.
</code></pre>
","['python', 'python-3.x', 'pandas']",60211680,"<p>It looks a lot like you want to join big and small datasets.
Suppose you have small:</p>

<pre><code>       id         Port
0  456DEF  Boston Port
</code></pre>

<p>and big:</p>

<pre><code>       id              Port
0  123ABC       Boston Port
1  789GHI       Boston Port
2  101JKL  Bridport Harbour
</code></pre>

<p>Merging them on <code>Port</code> will give you every pair of <code>id</code>s from small and big that share a <code>Port</code>:</p>

<pre class=""lang-py prettyprint-override""><code>big_dataset.merge(small_dataset, how=""outer"", on=""Port"", suffixes=(1, 2)) 
</code></pre>

<p>If the port appears in only one dataset, it will show up once in the output.</p>

<pre><code>      id1              Port     id2
0  123ABC       Boston Port  456DEF
1  789GHI       Boston Port  456DEF
2  101JKL  Bridport Harbour     NaN
</code></pre>
",Ordering dataframe rows based matching values parallel columns EDIT As requested two dataframes look like I concat column types literally The Larger dataset columns smaller dataframe shares type id Port ABC Boston Port The smaller dataset id Port DEF Boston Port I dataframe comparing two different datasets merged dataframe An important note two dataframes number columns different number rows one rows rows The smaller dataset matching NAME values larger one I trying get rows match based name This simple values unique using dict map however caveat something like There two Boston Ports however different ID numbers since one port located Boston US one UK dict map ignores places first occurring one match Example df id id Port Port ABC DEF Boston Port Boston Port GHI JKL Boston Port Bridport Harbour The second dataset smaller one created based matching names inside larger database however names arent unique attributes I used differentiate,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
60257377,2020-02-17,2020,14,Encountering &quot; WARN ProcfsMetricsGetter: Exception when trying to compute pagesize&quot; error when running Spark,"<p>I installed spark and when trying to run it, I am getting the error:
 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped</p>

<p>Can someone help me with that?</p>
","['python', 'apache-spark', 'pyspark']",61081901,"<p>The same problem occured with me because python path was not added to system environment. I added this in environment and now it works perfectly.</p>
<p>Adding <code>PYTHONPATH</code> environment variable with value as:</p>
<pre class=""lang-sh prettyprint-override""><code>%SPARK_HOME%\python;%SPARK_HOME%\python\lib\py4j-&lt;version&gt;-src.zip;%PYTHONPATH%
</code></pre>
<p>helped resolve this issue. Just check what py4j version you have in your <code>spark/python/lib folder</code>.</p>
",Encountering quot WARN ProcfsMetricsGetter Exception trying compute pagesize quot error running Spark I installed spark trying run I getting error WARN ProcfsMetricsGetter Exception trying compute pagesize result reporting ProcessTree metrics stopped Can someone help,"startoftags, python, apachespark, pyspark, endoftags",python pandas numpy endoftags,python apachespark pyspark,python pandas numpy,0.33
60657816,2020-03-12,2020,3,Series markers in pandas dataframe plots,"<p>I'm plotting a pandas dataframe which contains multiple time series.
I have more series than the number of colors matplotlib chooses from, so there is ambiguity in mapping legend colors to plots.<br>
I haven't seen any matplotlib examples that assigns markers as a batch across all series and I'm wondering if there's a way to pass a list of marker styles that df.plot() can rotate through in the same way it chooses colors.</p>

<p><code>df.plot(markers = ??)</code></p>
","['python', 'pandas', 'matplotlib']",60657903,"<p>A for loop would be sufficient:</p>

<pre><code>df = pd.DataFrame(np.arange(16).reshape(4,-1))

for c,m in zip(df,'oxds'):
    df[c].plot(marker=m)

plt.legend()
</code></pre>

<p>Output:</p>

<p><a href=""https://i.stack.imgur.com/xMDCj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xMDCj.png"" alt=""enter image description here""></a></p>
",Series markers pandas dataframe plots I plotting pandas dataframe contains multiple time series I series number colors matplotlib chooses ambiguity mapping legend colors plots I seen matplotlib examples assigns markers batch across series I wondering way pass list marker styles df plot rotate way chooses colors df plot markers,"startoftags, python, pandas, matplotlib, endoftags",python pandas matplotlib endoftags,python pandas matplotlib,python pandas matplotlib,1.0
61051920,2020-04-06,2020,3,Pandas action on column between two numbers,"<p>Currently using Pandas and Numpy. I have a dataframe named 'df'. Lets say I have the below data, how can I give the third column a value based on a between clause? I'd like to <strong><em>treat this as a vectorised approach</em></strong> if possible to maintain the speed of what I already have.   </p>

<p>I've tried lambda functions, but frankly I don't understand what I'm doing and I'm getting errors such as the object has no attribute 'between'.   </p>

<p>General approach - using a non vectorised approach:  </p>

<pre><code>NOTE: I am looking for a way to make this vectorised.

If df.['Col2'] is between 0 and 10
   df.['Col 3'] = 1
Elseif df.['Col2'] is between 10.01 and 20
   df.['Col3']  = 2
Else if df.['Col2'] is between 20.1 and 30
   df.['Col3']  = 3
</code></pre>

<p>Sample set</p>

<pre><code>+------+------+------+
| Col1 | Col2 | Col3 |
+------+------+------+
| a    |    5 |    1 |
| b    |   10 |    1 |
| c    |   15 |    2 |
| d    |   20 |    2 |
| e    |   25 |    3 |
| f    |   30 |    3 |
| g    |    1 |    1 |
| h    |   11 |    2 |
| i    |   21 |    3 |
| j    |    7 |    1 |
+------+------+------+


</code></pre>

<p>Many thanks</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",61051996,"<h1>Solution reusing your current code:</h1>

<pre class=""lang-py prettyprint-override""><code>def cust_func(row):
    r = row['Col2']
    if  r &gt;=0 AND r&lt;=10:
        val = 1
    elif r &gt;=10.01 AND r&lt;=20:
        val = 2
    elseif r&gt;=20.01 AND r&lt;=30:
        val = 3
    return val

df['Col3'] = df.apply(cust_func, axis=1)
</code></pre>

<h1>Optimal solution:</h1>

<pre class=""lang-py prettyprint-override""><code>cut_labels = [1, 2, 3]
cut_bins = [0, 10, 20,30]
df['Col3'] = pd.cut(df['Col2'], bins=cut_bins, labels=cut_labels)
</code></pre>
",Pandas action column two numbers Currently using Pandas Numpy I dataframe named df Lets say I data I give third column value based clause I like treat vectorised approach possible maintain speed I already I tried lambda functions frankly I understand I I getting errors object attribute General approach using non vectorised approach NOTE I looking way make vectorised If df Col df Col Elseif df Col df Col Else df Col df Col Sample set Col Col Col b c e f g h j Many thanks,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas numpy endoftags,python python3x pandas dataframe,python pandas numpy,0.58
61104122,2020-04-08,2020,4,Converting long dataframe and extracting string,"<p>Hi I have Dataframe like this:</p>

<pre><code>    Date A_2002 B_2003 C_2004 D_2005 Type
03-2002   20     30      12     42    X
04-2002   12     321     12     23    X
03-2002   10     31      2      3     Y
</code></pre>

<p>I want to convert it to long version and extract the string type from it so the end result would be this:</p>

<pre><code>   Date NewCol Extracted Type Value
03-2002 A       2002      X    20
03-2002 B       2003      X    30
03-2002 C       2004      X    12
03-2002 D       2005      X    42
04-2002 A       2002      X    12
04-2002 B       2003      X    321
04-2002 C       2004      X    12
04-2002 D       2005      X    23
03-2002 A       2002      Y    10
03-2002 B       2003      Y    31
03-2002 C       2004      Y    2
03-2002 D       2005      Y    3
</code></pre>

<p>So the end result will convert value from column name into tow new values and melt the data as seen above. Is it possible with pandas?</p>
","['python', 'pandas', 'dataframe']",61104270,"<p>you can do <code>stack</code> after <code>set_index</code> and <code>str.split</code>:</p>

<pre><code>m = df.set_index(['Date','Type'])
m.columns = m.columns.str.split('_',expand=True)
out = (m.stack([0,1]).rename('Value').reset_index()
     .rename(columns={'level_2':'NewCol','level_3':'Extracted'}))
</code></pre>

<hr>

<pre><code>       Date Type NewCol Extracted  Value
0   03-2002    X      A      2002   20.0
1   03-2002    X      B      2003   30.0
2   03-2002    X      C      2004   12.0
3   03-2002    X      D      2005   42.0
4   04-2002    X      A      2002   12.0
5   04-2002    X      B      2003  321.0
6   04-2002    X      C      2004   12.0
7   04-2002    X      D      2005   23.0
8   03-2002    Y      A      2002   10.0
9   03-2002    Y      B      2003   31.0
10  03-2002    Y      C      2004    2.0
11  03-2002    Y      D      2005    3.0
</code></pre>
",Converting long dataframe extracting string Hi I Dataframe like Date A B C D Type X X Y I want convert long version extract string type end result would Date NewCol Extracted Type Value A X B X C X D X A X B X C X D X A Y B Y C Y D Y So end result convert value column name tow new values melt data seen Is possible pandas,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
61150879,2020-04-11,2020,2,Mapping a Dataframe into another using conditionals,"<p>I would like to map one dataframe into another, though it is not so simple because I am using 2 conditions to execute the mapping - I will explain them below. Basically, what I am trying to do is given two dataframes, df1 and df2, such that:</p>

<p>df1:</p>

<pre><code>A       B        Type
Heart   Spades   Boo
Heart   Clubs    Fog
Spades  Diamonds Bler
</code></pre>

<p>df2:</p>

<pre><code>A       B        Boo    Fog     Bler
Heart   Spades   True   True    True
Spades  Diamonds True   False   True
Heart   Spades   True   True    False
</code></pre>

<p>I could map the values contained in the columns 'Boo','Fog,'Bler' into a new column in df1 called 'Verification', resulting in:</p>

<pre><code>A       B           Type    Verification
Heart   Spades      Boo     True
Heart   Clubs       Fog 
Spades  Diamonds    Bler    True
</code></pre>

<p>Then, to do this process I have 2 conditions that need to be filled: the values in df1 and the values in df2 for the columns A and B must be equal - as they were acting as keys, and the mapping should take the values in some column of df2 based on the value in the type of df1. I am having two difficulties:</p>

<ol>
<li>The mapping requires two columns so I am not able to figure out a way to use pandas.series.map; furthermore I was not able to apply Dataframe.loc[conditions] in this context so that the conditions compare df1 and df2.</li>
<li>The example above is quite short, but the data set that I am working on has several combinations from the values of A and B, hence is unreasonable to write a function of association between A,B and value to each type.</li>
</ol>

<p>Do you have any suggestions?</p>
","['python', 'pandas', 'dataframe']",61150995,"<p>Try <code>melt</code> and <code>drop_duplicates</code> on <code>df2</code>. Finally, left <code>merge</code> df1 to the result of <code>melt</code> and <code>drop_duplicates</code></p>

<pre><code>df_final = (df1.merge(df2.melt(['A','B'], var_name='Type', value_name='Verification')
                         .drop_duplicates(['A','B','Type']), how='left'))

Out[240]:
        A         B  Type Verification
0   Heart    Spades   Boo         True
1   Heart     Clubs   Fog          NaN
2  Spades  Diamonds  Bler         True
</code></pre>

<hr>

<p><strong>Note</strong>: on <code>df2</code>, the value of <code>bler</code> for <code>Spades  Diamonds</code> (2nd row) is <code>True</code>, so its <code>Verification</code> is <code>True</code> in the output</p>
",Mapping Dataframe another using conditionals I would like map one dataframe another though simple I using conditions execute mapping I explain Basically I trying given two dataframes df df df A B Type Heart Spades Boo Heart Clubs Fog Spades Diamonds Bler df A B Boo Fog Bler Heart Spades True True True Spades Diamonds True False True Heart Spades True True False I could map values contained columns Boo Fog Bler new column df called Verification resulting A B Type Verification Heart Spades Boo True Heart Clubs Fog Spades Diamonds Bler True Then process I conditions need filled values df values df columns A B must equal acting keys mapping take values column df based value type df I two difficulties The mapping requires two columns I able figure way use pandas series map furthermore I able apply Dataframe loc conditions context conditions compare df df The example quite,"startoftags, python, pandas, dataframe, endoftags",python python3x pandas endoftags,python pandas dataframe,python python3x pandas,0.67
61322281,2020-04-20,2020,3,how to add another paired column in pandas dataframe?,"<pre><code>df = pd.DataFrame({'col1':['a-b', 'c-d', 'e-f', 'g-h', 'i-j', 'k-l','b-a', 'd-c', 'f-e', 'h-g', 'j-i', 'l-k']})
</code></pre>

<p>You have one column that has a combination overlapped despite the fact that the order seems reversed. But how to add another column on the other side to actually show that they are the same like the below:</p>

<pre><code>df2 = pd.DataFrame({'col1':['a-b', 'c-d', 'e-f', 'g-h', 'i-j', 'k-l','b-a', 'd-c', 'f-e', 'h-g', 'j-i', 'l-k']
,'col2':['a-b', 'c-d', 'e-f', 'g-h', 'i-j', 'k-l','a-b', 'c-d', 'e-f', 'g-h', 'i-j', 'k-l']})
</code></pre>

<p>The real data is not predictable as the sequence of alpha beta, so I got a headache. </p>

<p>Thank you in advance.</p>
","['python', 'pandas', 'dataframe']",61323267,"<p>IIUC</p>

<pre><code>df.groupby(df.col1.str.split('-').map(lambda x : tuple(sorted(x)))).col1.transform('first')
0     a-b
1     c-d
2     e-f
3     g-h
4     i-j
5     k-l
6     a-b
7     c-d
8     e-f
9     g-h
10    i-j
11    k-l
Name: col1, dtype: object
</code></pre>
",add another paired column pandas dataframe df pd DataFrame col b c e f g h j k l b c f e h g j l k You one column combination overlapped despite fact order seems reversed But add another column side actually show like df pd DataFrame col b c e f g h j k l b c f e h g j l k col b c e f g h j k l b c e f g h j k l The real data predictable sequence alpha beta I got headache Thank advance,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
61576749,2020-05-03,2020,2,Pandas Groupby on a specific value of a column,"<p>I have this sample data here that I'm trying to aggregate:</p>

<pre><code>Ticket ID   User    Date        Category
1926        a       1/1/2020    cat_7
1947        a       1/1/2020    cat_6
1901        c       1/2/2020    cat_7
1067        a       1/3/2020    cat_1
1683        a       1/4/2020    cat_3
1281        a       1/4/2020    cat_3
1561        a       1/5/2020    cat_5
1932        a       1/5/2020    cat_5
1234        c       1/5/2020    cat_6
1013        c       1/7/2020    cat_7
1575        b       1/9/2020    cat_8
1152        b       1/10/2020   cat_4
1235        c       1/10/2020   cat_7
1596        b       1/11/2020   cat_4
1523        c       1/11/2020   cat_1
1447        b       1/12/2020   cat_4
1576        b       1/12/2020   cat_5
1260        c       1/13/2020   cat_2
1556        b       1/15/2020   cat_5
1838        b       1/16/2020   cat_5
1182        b       1/17/2020   cat_5
</code></pre>

<p><strong>Into this expected output:</strong></p>

<pre><code>User    Category 1  Next Category   Count
a       cat_1       cat_3           2
                    cat_5           2
b       cat_1       cat_4           3
                    cat_5           4
c       cat_1       cat_2           1
</code></pre>

<p>I'm not sure if a specific value from a column can be extracted and used to groupby the actvities happened after <strong>cat_1</strong></p>

<p>My failed attempt:</p>

<pre><code>df.groupby([""User"", ""Category""])[""Ticket ID""].count()
</code></pre>

<p>Which resulted to this:</p>

<pre><code>User  Category    Count
a     cat_1       1
      cat_3       2
      cat_5       2
      cat_6       1
      cat_7       1
b     cat_1       1
      cat_4       3
      cat_5       4
c     cat_1       1
      cat_2       1
      cat_6       1
      cat_7       3
      cat_8       1
</code></pre>
","['python', 'pandas', 'pandas-groupby']",61578325,"<p>You can try:</p>

<pre class=""lang-py prettyprint-override""><code>def f(x):
    x = x.reset_index(drop=True)
    return x.iloc[x[x[""Category""].eq('cat_1')].index[0]+1:]

df.groupby(""User"")\
    .apply(f) \
    .reset_index(drop=True) \
    .groupby([""User"", ""Category""]) \
    .agg({""Ticket ID"": ""count""}) \
    .assign(Category_1=""cat_1"") \
    .set_index(""Category_1"", append=True)\
    .reorder_levels([0, 2, 1])
</code></pre>

<p><strong>Explanations</strong>:</p>

<p>There are two main steps:</p>

<ul>
<li>For each <code>User</code> group, remove rows before the first <code>cat_1</code> Category value</li>
<li>Compute the <code>count</code> on each <code>[""User"", ""Category""]</code></li>
</ul>

<p>The steps are:</p>

<ol>
<li>Group the dataset according the <code>""User""</code> column using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>groupby</code></a></li>
<li><p>Filter all rows to only select rows after the first <code>cat_1</code> occurrence with the <code>f</code> function.</p>

<ol>
<li>Firstly reset index using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>reset_index</code></a>.</li>
<li>Select the index of the first <code>cat_1</code> row using <code>x[x[""Category""].eq('cat_1')].index[0]</code></li>
<li>Add <code>+1</code> to the index from previous stap as we don't want the <code>cat_1</code> row.</li>
<li>Using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html"" rel=""nofollow noreferrer""><code>iloc</code></a> to slice rows before the index defined step 2.3. </li>
</ol></li>
<li><p>Remove the redundant <code>User</code> index with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>reset_index</code></a> and <code>drop=True</code></p></li>
<li><p>Group the dataframe according <code>User</code> and <code>Category</code> column using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>groupby</code></a></p></li>
<li><p>Aggregate the columns using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html"" rel=""nofollow noreferrer""><code>agg</code></a> and count all the <code>Ticket_id</code>.</p></li>
</ol>

<p>Here we have the output values. The next steps are here to match the desired <em>expected output</em>.</p>

<ol start=""6"">
<li><p>Rename the <code>count</code> output to <code>count</code> using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html"" rel=""nofollow noreferrer""><code>rename</code></a>.</p></li>
<li><p>Add the <code>Category_1</code> column using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html"" rel=""nofollow noreferrer""><code>assign</code></a></p></li>
<li><p>Set the <code>Category_1</code> column as index using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>set_index</code></a> with <code>append=True</code></p></li>
<li><p>Reorder the index levels using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reorder_levels.html"" rel=""nofollow noreferrer""><code>reorder_levels</code></a></p></li>
</ol>

<hr>

<p><strong>Full code + illustration</strong></p>

<pre class=""lang-py prettyprint-override""><code>def f(x):
    x = x.reset_index(drop=True)
    return x.iloc[x[x[""Category""].eq('cat_1')].index[0]+1:]


# Step 2
print(df.groupby(""User"")
        .apply(f))
#         Ticket ID User       Date Category
# User
# a    3       1683    a   1/4/2020    cat_3
#      4       1281    a   1/4/2020    cat_3
#      5       1561    a   1/5/2020    cat_5
#      6       1932    a   1/5/2020    cat_5
# b    1       1152    b  1/10/2020    cat_4
#      2       1596    b  1/11/2020    cat_4
#      3       1447    b  1/12/2020    cat_4
#      4       1576    b  1/12/2020    cat_5
#      5       1556    b  1/15/2020    cat_5
#      6       1838    b  1/16/2020    cat_5
#      7       1182    b  1/17/2020    cat_5
# c    6       1260    c  1/13/2020    cat_2

# Step 3
print(df.groupby(""User"")
        .apply(f)
        .reset_index(drop=True))
#     Ticket ID User       Date Category
# 0        1683    a   1/4/2020    cat_3
# 1        1281    a   1/4/2020    cat_3
# 2        1561    a   1/5/2020    cat_5
# 3        1932    a   1/5/2020    cat_5
# 4        1152    b  1/10/2020    cat_4
# 5        1596    b  1/11/2020    cat_4
# 6        1447    b  1/12/2020    cat_4
# 7        1576    b  1/12/2020    cat_5
# 8        1556    b  1/15/2020    cat_5
# 9        1838    b  1/16/2020    cat_5
# 10       1182    b  1/17/2020    cat_5
# 11       1260    c  1/13/2020    cat_2

# Step 5
print(df.groupby(""User"")
        .apply(f)
        .reset_index(drop=True)
        .groupby([""User"", ""Category""])
        .agg({""Ticket ID"": ""count""}))
#                Ticket ID
# User Category
# a    cat_3             2
#      cat_5             2
# b    cat_4             3
#      cat_5             4
# c    cat_2             1

# Step 6
print(df.groupby(""User"")
        .apply(f)
        .reset_index(drop=True)
        .groupby([""User"", ""Category""])
        .agg({""Ticket ID"": ""count""})
        .rename(columns={""Ticket ID"": ""count""}))
#                count
# User Category
# a    cat_3         2
#      cat_5         2
# b    cat_4         3
#      cat_5         4
# c    cat_2         1

print(df.groupby(""User"")
        .apply(f)
        .reset_index(drop=True)
        .groupby([""User"", ""Category""])
        .agg({""Ticket ID"": ""count""})
        .rename(columns={""Ticket ID"": ""count""})
        .assign(Category_1=""cat_1"")
        .set_index(""Category_1"", append=True)
        .reorder_levels([0, 2, 1]))
#                           count
# User Category_1 Category
# a    cat_1      cat_3         2
#                 cat_5         2
# b    cat_1      cat_4         3
#                 cat_5         4
# c    cat_1      cat_2         1
#                count Category_1

# Step 7
print(df.groupby(""User"")
        .apply(f)
        .reset_index(drop=True)
        .groupby([""User"", ""Category""])
        .agg({""Ticket ID"": ""count""})
        .rename(columns={""Ticket ID"": ""count""})
        .assign(Category_1=""cat_1""))
# User Category
# a    cat_3         2      cat_1
#      cat_5         2      cat_1
# b    cat_4         3      cat_1
#      cat_5         4      cat_1
# c    cat_2         1      cat_1
#                           count

# Step 8
print(df.groupby(""User"")
        .apply(f)
        .reset_index(drop=True)
        .groupby([""User"", ""Category""])
        .agg({""Ticket ID"": ""count""})
        .rename(columns={""Ticket ID"": ""count""})
        .assign(Category_1=""cat_1"")
        .set_index(""Category_1"", append=True))
# User Category Category_1
# a    cat_3    cat_1           2
#      cat_5    cat_1           2
# b    cat_4    cat_1           3
#      cat_5    cat_1           4
# c    cat_2    cat_1           1
#                           count

# Step 9
print(df.groupby(""User"")
        .apply(f)
        .reset_index(drop=True)
        .groupby([""User"", ""Category""])
        .agg({""Ticket ID"": ""count""})
        .rename(columns={""Ticket ID"": ""count""})
        .assign(Category_1=""cat_1"")
        .set_index(""Category_1"", append=True)
        .reorder_levels([0, 2, 1]))
# User Category_1 Category
# a    cat_1      cat_3         2
#                 cat_5         2
# b    cat_1      cat_4         3
#                 cat_5         4
# c    cat_1      cat_2         1
</code></pre>
",Pandas Groupby specific value column I sample data I trying aggregate Ticket ID User Date Category cat cat c cat cat cat cat cat cat c cat c cat b cat b cat c cat b cat c cat b cat b cat c cat b cat b cat b cat Into expected output User Category Next Category Count cat cat cat b cat cat cat c cat cat I sure specific value column extracted used groupby actvities happened cat My failed attempt df groupby User Category Ticket ID count Which resulted User Category Count cat cat cat cat cat b cat cat cat c cat cat cat cat cat,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
61789867,2020-05-14,2020,5,What is the role of preprocess_input() function in Keras VGG model?,"<p>This question is kind of a follow up for the discussion in comments of <a href=""https://stackoverflow.com/a/61768062/3337089"">this answer</a>.</p>

<p>From what I understand, the <code>preprocess_input()</code> function does mean subtraction and std-dev dvision for the input images. The mean are those that are computed on ImageNet-1K database when training VGG.</p>

<p>But <a href=""https://stackoverflow.com/a/61768062/3337089"">this answer</a> says that when using VGG features as a loss function, <code>preprocess_input()</code> is not required and we just need to normalize the image to <code>[0,1]</code> range before passing to VGG. This confuses me...</p>

<ol>
<li>If we don't preprocess, then the input will be in different range compared to those images used to train VGG. How are the VGG features still valid?</li>
<li>From what I understand from <a href=""https://stackoverflow.com/a/47556342/3337089"">this answer</a>, we should have images in <code>[0,255]</code> range and <code>preprocess_input()</code> function takes care of the normalization and all. From the <a href=""https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/applications/imagenet_utils.py#L158-L229"" rel=""nofollow noreferrer"">source code</a>, I understand that for caffe models, normalization to <code>[0,1]</code> range is not done. Instead mean is subtracted and std-dev is divided. How would just normalizing network output to <code>[0,1]</code> range as suggested in the comments of <a href=""https://stackoverflow.com/a/61768062/3337089"">this answer</a> achieve the same?</li>
</ol>

<p><strong>Edit 1</strong>:<br>
I'm considering the models which output images. It is not specific to a single model. One example is image denoising network. The input to my network is a noisy image and its output is a denoised image. I want to minimize MSE between denoised image and ground truth image in VGG feature space. Whatever be the range of my network's output, I can easily change it to <code>[0,255]</code> by multiplying by appropriate factors. Similarly I can do any preprocessing required on my network's output (subtract mean, divide by std-dev).</p>

<p>Empirically I found that the output of preprocess function is in approx range <code>[-128,151]</code>. So VGG network is trained on images in this range. Now, if I feed it with images (or tensors from my network output) in the range <code>[0,1]</code>, convolution would be fine but biases will cause problem right? To elaborate, for images in range <code>[-128,151]</code>, a layer of VGG network may have learnt a bias of 5. When I feed an image in the range <code>[-1,1]</code> to the VGG network, the bias disrupts everything, right?</p>

<p>I'm not training VGG model. I'm using the weights from the model trained on ImageNet-1k database.</p>
","['python', 'tensorflow', 'keras']",61791837,"<p>In general you should not ignore or change the normalization of the data in which a model was trained. It could break the model in unexpected ways and since you are using the features in another learning model, it appears to work, but you have now hidden any changes in performance.</p>

<p>This is true specially for models that use saturating activations, for example with a ReLU you might get more zeros than with using normalized data.</p>

<p>Answer to your specific questions:</p>

<ol>
<li><p>Yes features would be in a different range for VGG and other networks, whether they are valid is another issue, there is a performance loss since normalization was not used.</p></li>
<li><p>Changing the normalization scheme does not produce the same kind of normalization as the original, so it is not achieving the same. The code in the answer works but conceptually it is not doing the right thing.</p></li>
</ol>
",What role preprocess input function Keras VGG model This question kind follow discussion comments answer From I understand preprocess input function mean subtraction std dev dvision input images The mean computed ImageNet K database training VGG But answer says using VGG features loss function preprocess input required need normalize image range passing VGG This confuses If preprocess input different range compared images used train VGG How VGG features still valid From I understand answer images range preprocess input function takes care normalization From source code I understand caffe models normalization range done Instead mean subtracted std dev divided How would normalizing network output range suggested comments answer achieve Edit I considering models output images It specific single model One example image denoising network The input network noisy image output denoised image I want minimize MSE denoised image ground truth image VGG feature space Whatever range network output I easily change,"startoftags, python, tensorflow, keras, endoftags",python django djangorestframework endoftags,python tensorflow keras,python django djangorestframework,0.33
61931704,2020-05-21,2020,3,Update row values in a dataframe based on another row&#39;s values?,"<p>I have a dataframe with two columns: <code>a</code> and <code>b</code> </p>

<p>df</p>

<pre><code>         a         b
0       john      123
1       john
2       mark     
3       mark      456
4       marcus    789
</code></pre>

<p>I want to update values of <code>b</code> column based on <code>a</code> column. </p>

<pre><code>         a         b
0       john      123
1       john      123
2       mark      456
3       mark      456
4       marcus    789
</code></pre>

<p>If <code>john</code> has value <code>123</code> in <code>b</code>. Remaining <code>john</code> also must have same value.</p>
","['python', 'pandas', 'numpy']",61932101,"<p>Assuming your dataframe is:</p>

<pre><code>df = pd.DataFrame({'a': ['john', 'john', 'mark', 'mark', 'marcus'], 'b': [123, '', '', 456, 789]})
</code></pre>

<p>You can <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>df.groupby</code></a> the dataframe on column <code>a</code> and then apply <code>transform</code> on the column <code>b</code> of the grouped dataframe returning the first non empty value in the grouped column <code>b</code>.</p>

<p>Use:</p>

<pre><code>df['b'] = (
    df.groupby('a')['b']
    .transform(lambda s: s[s.ne('')].iloc[0] if s.ne('').any() else s)
)
</code></pre>

<p>Result:</p>

<pre><code># print(df)

        a    b
0    john  123
1    john  123
2    mark  456
3    mark  456
4  marcus  789
</code></pre>
",Update row values dataframe based another row values I dataframe two columns b df b john john mark mark marcus I want update values b column based column b john john mark mark marcus If john value b Remaining john also must value,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
61980969,2020-05-24,2020,2,Validating a choice field with ModelSerializer with Django,"<p>I have a model <strong>BaseUser</strong> which inherits from <strong>AbstractUser</strong> and has two additional fields: complex_list and active_complex. complex_list is a ManyToManyField connected to the <strong>BaseComplex</strong> model through an enrollment table.</p>

<p>I want the default value for active_complex to be null which can be taken care of at the model initiation. I also need the user to be able to choose the active_complex only from the values within the complex_list. For that, I tried to use a customized ModelSerializer:</p>

<pre><code>from rest_framework import serializers
from .models import BaseUser



class ActiveComplexSerializer(serializers.ModelSerializer):
    this_user = serializers.SerializerMethodField('get_this_user')

    choice =[]
    qs = BaseUser.objects.get(username = this_user)
    for cmplx in qs.complex_list:
        choice.append((cmplx.id, cmplx.name))

    active_complex = serializers.ChoiceField(choices = choice)


    class Meta:
        model = BaseUser
        fields = ('active_complex')

    def get_this_user(self, request):

        return request.user

</code></pre>

<p>I know I'm doing something wrong because I get a query not found ERROR. I would appreciate it if you could correct this or suggest a better approach to achieving the same goal.</p>
","['python', 'django', 'django-rest-framework']",61982503,"<p>The issue with the current code is that when you're calling</p>

<pre><code>qs = BaseUser.objects.get(username = this_user)
</code></pre>

<p><code>this_user</code> is not bound to any specific value. This code runs only once when the Class is first initialized, <em>not</em> on every new instance of the class.</p>

<p>You instead want to get the user when the serializer class is initialized and then populate the choices.</p>

<p>Here's a rough change, I have not tested this, but it should more or less work.</p>

<pre><code>class ActiveComplexSerializer(serializers.ModelSerializer):
    this_user = serializers.SerializerMethodField('get_this_user')
    active_complex = serializers.ChoiceField(choices=[])

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        choice =[]
        qs = BaseUser.objects.get(username = self.fields['this_user'])
        for cmplx in qs.complex_list:
            choice.append((cmplx.id, cmplx.name))
        self.fields['active_complex'].choices = choice

    class Meta:
        model = BaseUser
        fields = ('active_complex')

    def get_this_user(self, request):
        return request.user
</code></pre>

<p>Also on a side note check out <code>get_user_model()</code> from <a href=""https://docs.djangoproject.com/en/3.0/topics/auth/customizing/#django.contrib.auth.get_user_model"" rel=""nofollow noreferrer"">https://docs.djangoproject.com/en/3.0/topics/auth/customizing/#django.contrib.auth.get_user_model</a> - it is preferred to use that function instead of direclty importing user model.</p>
",Validating choice field ModelSerializer Django I model BaseUser inherits AbstractUser two additional fields complex list active complex complex list ManyToManyField connected BaseComplex model enrollment table I want default value active complex null taken care model initiation I also need user able choose active complex values within complex list For I tried use customized ModelSerializer rest framework import serializers models import BaseUser class serializers ModelSerializer user serializers get user choice qs BaseUser objects get username user cmplx qs complex list choice append cmplx id cmplx name active complex serializers ChoiceField choices choice class Meta model BaseUser fields active complex def get user self request return request user I know I something wrong I get query found ERROR I would appreciate could correct suggest better approach achieving goal,"startoftags, python, django, djangorestframework, endoftags",python django djangorestframework endoftags,python django djangorestframework,python django djangorestframework,1.0
62024575,2020-05-26,2020,5,mapping json columns to pandas dataframe columns,"<p>I have the below which takes some JSON input and converts to a Pandas dataframe.</p>

<p>But, because the JSON doesn't have a consistent schema, it's all misaligned. (if a field doesn't exist in one entry, it shifts everything to the left)</p>

<p>Is there any way I can say the below &amp; explicitly define it?</p>

<pre><code>df.field1 = json.field1
</code></pre>

<p>If I can define by their names, I can do it nicely :)</p>

<p>Thanks</p>

<pre><code>output = subprocess.check_output(command, shell=True)

# output of subprocess will be bytes, converting to string.
if isinstance(output, bytes):
    output = output.decode()

output = json.loads(output)
df = pd.DataFrame(output['apps']['app'])
df = df.loc[df['startedTime'] &gt; starttime]
df.to_csv('yarn_output.csv')
</code></pre>

<p>Sample Input JSON</p>

<pre><code>{""apps"":{""app"":[{""id"":""application_1589431105417_21534"",""user"":""udsldr"",""name"":""HIVE-61a4ee14-1d26-4c7b-bf0d-1cc2a990557d"",""queue"":""udsldr"",""state"":""FINISHED"",""finalStatus"":""SUCCEEDED"",""progress"":100.0,""trackingUI"":""History"",""trackingUrl"":""http://uds-far-mn4.dab.02.net:8088/proxy/application_1589431105417_21534/"",""diagnostics"":""Session stats:submittedDAGs=0, successfulDAGs=0, failedDAGs=0, killedDAGs=0\n"",""clusterId"":1589431105417,""applicationType"":""TEZ"",""applicationTags"":"""",""priority"":0,""startedTime"":1590294649069,""finishedTime"":1590294666011,""elapsedTime"":16942,""amContainerLogs"":""http://uds-far-dn150.dab.02.net:8042/node/containerlogs/container_e66_1589431105417_21534_01_000001/udsldr"",""amHostHttpAddress"":""uds-far-dn150.dab.02.net:8042"",""allocatedMB"":-1,""allocatedVCores"":-1,""runningContainers"":-1,""memorySeconds"":144531,""vcoreSeconds"":17,""queueUsagePercentage"":0.0,""clusterUsagePercentage"":0.0,""preemptedResourceMB"":0,""preemptedResourceVCores"":0,""numNonAMContainerPreempted"":0,""numAMContainerPreempted"":0,""logAggregationStatus"":""SUCCEEDED"",""unmanagedApplication"":false,""amNodeLabelExpression"":""""},{""id"":""application_1589431105417_21535"",""user"":""nifildr"",""name"":""HIVE-850812d7-9d22-4be8-a225-7b341f6ea980"",""queue"":""default"",""state"":""FINISHED"",""finalStatus"":""SUCCEEDED"",""progress"":100.0,""trackingUI"":""History"",""trackingUrl"":""http://uds-far-mn4.dab.02.net:8088/proxy/application_1589431105417_21535/"",""diagnostics"":""Session stats:submittedDAGs=0, successfulDAGs=1, failedDAGs=0, killedDAGs=0\n"",""clusterId"":1589431105417,""applicationType"":""TEZ"",""applicationTags"":"""",""priority"":0,""startedTime"":1590294664397,""finishedTime"":1590294801090,""elapsedTime"":136693,""amContainerLogs"":""http://uds-far-dn129.dab.02.net:8042/node/containerlogs/container_e66_1589431105417_21535_01_000001/nifildr"",""amHostHttpAddress"":""uds-far-dn129.dab.02.net:8042"",""allocatedMB"":-1,""allocatedVCores"":-1,""runningContainers"":-1,""memorySeconds"":18279340,""vcoreSeconds"":4248,""queueUsagePercentage"":0.0,""clusterUsagePercentage"":0.0,""preemptedResourceMB"":0,""preemptedResourceVCores"":0,""numNonAMContainerPreempted"":0,""numAMContainerPreempted"":0,""logAggregationStatus"":""TIME_OUT"",""unmanagedApplication"":false,""amNodeLabelExpression"":""""},{""id"":""application_1589431105417_21532"",""user"":""udsldr"",""name"":""HIVE-73e0c359-32a5-4334-89da-4a8ae2bb1037"",""queue"":""udsldr"",""state"":""FINISHED"",""finalStatus"":""SUCCEEDED"",""progress"":100.0,""trackingUI"":""History"",""trackingUrl"":""http://uds-far-mn4.dab.02.net:8088/proxy/application_1589431105417_21532/"",""diagnostics"":""Session stats:submittedDAGs=0, successfulDAGs=0, failedDAGs=0, killedDAGs=0\n"",""clusterId"":1589431105417,""applicationType"":""TEZ"",""applicationTags"":"""",""priority"":0,""startedTime"":1590294622244,""finishedTime"":1590294643808,""elapsedTime"":21564,""amContainerLogs"":""http://uds-far-dn35.dab.02.net:8042/node/containerlogs/container_e66_1589431105417_21532_01_000001/udsldr"",""amHostHttpAddress"":""uds-far-dn35.dab.02.net:8042"",""allocatedMB"":-1,""allocatedVCores"":-1,""runningContainers"":-1,""memorySeconds"":182247,""vcoreSeconds"":22,""queueUsagePercentage"":0.0,""clusterUsagePercentage"":0.0,""preemptedResourceMB"":0,""preemptedResourceVCores"":0,""numNonAMContainerPreempted"":0,""numAMContainerPreempted"":0,""logAggregationStatus"":""SUCCEEDED"",""unmanagedApplication"":false,""amNodeLabelExpression"":""""},{""id"":""application_1589431105417_21533"",""user"":""udssupport"",""name"":""tcs.uds.webstats"",""queue"":""udssystem"",""state"":""FINISHED"",""finalStatus"":""SUCCEEDED"",""progress"":100.0,""trackingUI"":""History"",""trackingUrl"":""http://uds-far-mn4.dab.02.net:8088/proxy/application_1589431105417_21533/"",""diagnostics"":"""",""clusterId"":1589431105417,""applicationType"":""SPARK"",""applicationTags"":"""",""priority"":0,""startedTime"":1590294631138,""finishedTime"":1590295670552,""elapsedTime"":1039414,""amContainerLogs"":""http://uds-far-dn148.dab.02.net:8042/node/containerlogs/container_e66_1589431105417_21533_01_000001/udssupport"",""amHostHttpAddress"":""uds-far-dn148.dab.02.net:8042"",""allocatedMB"":-1,""allocatedVCores"":-1,""runningContainers"":-1,""memorySeconds"":4762538052,""vcoreSeconds"":775756,""queueUsagePercentage"":0.0,""clusterUsagePercentage"":0.0,""preemptedResourceMB"":0,""preemptedResourceVCores"":0,""numNonAMContainerPreempted"":0,""numAMContainerPreempted"":0,""logAggregationStatus"":""TIME_OUT"",""unmanagedApplication"":false,""amNodeLabelExpression"":""""},{""id"":""application_1589431105417_21530"",""user"":""nifildr"",""name"":""HIVE-e9a64e12-11f0-4ba8-b069-3be0ce561137"",""queue"":""default"",""state"":""FINISHED"",""finalStatus"":""SUCCEEDED"",""progress"":100.0,""trackingUI"":""History"",""trackingUrl"":""http://uds-far-mn4.dab.02.net:8088/proxy/application_1589431105417_21530/"",""diagnostics"":""Session stats:submittedDAGs=0, successfulDAGs=3, failedDAGs=0, killedDAGs=0\n"",""clusterId"":1589431105417,""applicationType"":""TEZ"",""applicationTags"":"""",""priority"":0,""startedTime"":1590294606965,""finishedTime"":1590295033193,""elapsedTime"":426228,""amContainerLogs"":""http://uds-far-dn75.dab.02.net:8042/node/containerlogs/container_e66_1589431105417_21530_01_000001/nifildr"",""amHostHttpAddress"":""uds-far-dn75.dab.02.net:8042"",""allocatedMB"":-1,""allocatedVCores"":-1,""runningContainers"":-1,""memorySeconds"":114397555,""vcoreSeconds"":27175,""queueUsagePercentage"":0.0,""clusterUsagePercentage"":0.0,""preemptedResourceMB"":0,""preemptedResourceVCores"":0,""numNonAMContainerPreempted"":0,""numAMContainerPreempted"":0,""logAggregationStatus"":""TIME_OUT"",""unmanagedApplication"":false,""amNodeLabelExpression"":""""},{""id"":""application_1589431105417_21531"",""user"":""nifi"",""name"":""HIVE-a063ddd1-5bf8-47b4-8ce3-8497c93b79a5"",""queue"":""default"",""state"":""FINISHED"",""finalStatus"":""SUCCEEDED"",""progress"":100.0,""trackingUI"":""History"",""trackingUrl"":""http://uds-far-mn4.dab.02.net:8088/proxy/application_1589431105417_21531/"",""diagnostics"":""Session stats:submittedDAGs=0, successfulDAGs=0, failedDAGs=0, killedDAGs=0\n"",""clusterId"":1589431105417,""applicationType"":""TEZ"",""applicationTags"":"""",""priority"":0,""startedTime"":1590294613578,""finishedTime"":1590294655173,""elapsedTime"":41595,""amContainerLogs"":""http://uds-far-dn56.dab.02.net:8042/node/containerlogs/container_e66_1589431105417_21531_01_000001/nifi"",""amHostHttpAddress"":""uds-far-dn56.dab.02.net:8042"",""allocatedMB"":-1,""allocatedVCores"":-1,""runningContainers"":-1,""memorySeconds"":345792,""vcoreSeconds"":42,""queueUsagePercentage"":0.0,""clusterUsagePercentage"":0.0,""preemptedResourceMB"":0,""preemptedResourceVCores"":0,""numNonAMContainerPreempted"":0,""numAMContainerPreempted"":0,""logAggregationStatus"":""SUCCEEDED"",""unmanagedApplication"":false,""amNodeLabelExpression"":""""},{""id"":""application_1589431105417_21528"",""user"":""udsldr"",""name"":""com.cardinality.LocationDB"",""queue"":""udsldr"",""state"":""FINISHED"",""finalStatus"":""SUCCEEDED"",""progress"":100.0,""trackingUI"":""History"",""trackingUrl"":""http://uds-far-mn4.dab.02.net:8088/proxy/application_1589431105417_21528/"",""diagnostics"":"""",""clusterId"":1589431105417,""applicationType"":""SPARK"",""applicationTags"":""5ec9f8480000f1697e683969"",""priority"":0,""startedTime"":1590294605875,""finishedTime"":1590294782281,""elapsedTime"":176406,""amContainerLogs"":""http://uds-far-dn167.dab.02.net:8042/node/containerlogs/container_e66_1589431105417_21528_01_000001/udsldr"",""amHostHttpAddress"":""uds-far-dn167.dab.02.net:8042"",""allocatedMB"":-1,""allocatedVCores"":-1,""runningContainers"":-1,""memorySeconds"":43389139,""vcoreSeconds"":5239,""queueUsagePercentage"":0.0,""clusterUsagePercentage"":0.0,""preemptedResourceMB"":0,""preemptedResourceVCores"":0,""numNonAMContainerPreempted"":0,""numAMContainerPreempted"":0,""logAggregationStatus"":""TIME_OUT"",""unmanagedApplication"":false,""amNodeLabelExpression"":""""},{""id"":""application_1589431105417_21529"",""user"":""keenek1"",""name"":""Clean DPI Report"",""queue"":""default"",""state"":""FINISHED"",""finalStatus"":""SUCCEEDED"",""progress"":100.0,""trackingUI"":""History"",""trackingUrl"":""http://uds-far-mn4.dab.02.net:8088/proxy/application_1589431105417_21529/"",""diagnostics"":"""",""clusterId"":1589431105417,""applicationType"":""SPARK"",""applicationTags"":"""",""priority"":0,""startedTime"":1590294607111,""finishedTime"":1590295032105,""elapsedTime"":424994,""amContainerLogs"":""http://uds-far-dn62.dab.02.net:8042/node/containerlogs/container_e66_1589431105417_21529_01_000001/keenek1"",""amHostHttpAddress"":""uds-far-dn62.dab.02.net:8042"",""allocatedMB"":-1,""allocatedVCores"":-1,""runningContainers"":-1,""memorySeconds"":2114077299,""vcoreSeconds"":344079,""queueUsagePercentage"":0.0,""clusterUsagePercentage"":0.0,""preemptedResourceMB"":0,""preemptedResourceVCores"":0,""numNonAMContainerPreempted"":0,""numAMContainerPreempted"":0,""logAggregationStatus"":""TIME_OUT"",""unmanagedApplication"":false,""amNodeLabelExpression"":""""},{""id"":""application_1589431105417_21542"",""user"":""murugaa1"",""name"":""HIVE-a1a5aadb-254c-4289-ad22-e9c7ce5e9814"",""queue"":""default"",""state"":""FINISHED"",""finalStatus"":""SUCCEEDED"",""progress"":100.0,""trackingUI"":""History"",""trackingUrl"":""http://uds-far-mn4.dab.02.net:8088/proxy/application_1589431105417_21542/"",""diagnostics"":""Session stats:submittedDAGs=0, successfulDAGs=1, failedDAGs=0, killedDAGs=0\n"",""clusterId"":1589431105417,""applicationType"":""TEZ"",""applicationTags"":"""",""priority"":0,""startedTime"":1590295275713,""finishedTime"":1590295297948,""elapsedTime"":22235,""amContainerLogs"":""http://uds-far-dn46.dab.02.net:8042/node/containerlogs/container_e66_1589431105417_21542_01_000001/murugaa1"",""amHostHttpAddress"":""uds-far-dn46.dab.02.net:8042"",""allocatedMB"":-1,""allocatedVCores"":-1,""runningContainers"":-1,""memorySeconds"":999465,""vcoreSeconds"":217,""queueUsagePercentage"":0.0,""clusterUsagePercentage"":0.0,""preemptedResourceMB"":0,""preemptedResourceVCores"":0,""numNonAMContainerPreempted"":0,""numAMContainerPreempted"":0,""logAggregationStatus"":""SUCCEEDED"",""unmanagedApplication"":false,""amNodeLabelExpression"":""""},{""id"":""application_1589431105417_21543"",""user"":""murugaa1"",""name"":""HIVE-cdc8a5da-f880-4f8e-9baf-b306095b9efb"",""queue"":""default"",""state"":""FINISHED"",""finalStatus"":""SUCCEEDED"",""progress"":100.0,""trackingUI"":""History"",""trackingUrl"":""http://uds-far-mn4.dab.02.net:8088/proxy/application_1589431105417_21543/"",""diagnostics"":""Session stats:submittedDAGs=0, successfulDAGs=1, failedDAGs=0, killedDAGs=0\n"",""clusterId"":1589431105417,""applicationType"":""TEZ"",""applicationTags"":"""",""priority"":0,""startedTime"":1590295277611,""finishedTime"":1590295301515,""elapsedTime"":23904,""amContainerLogs"":""http://uds-far-dn41.dab.02.net:8042/node/containerlogs/container_e66_1589431105417_21543_01_000001/murugaa1"",""amHostHttpAddress"":""uds-far-dn41.dab.02.net:8042"",""allocatedMB"":-1,""allocatedVCores"":-1,""runningContainers"":-1,""memorySeconds"":1077860,""vcoreSeconds"":228,""queueUsagePercentage"":0.0,""clusterUsagePercentage"":0.0,""preemptedResourceMB"":0,""preemptedResourceVCores"":0,""numNonAMContainerPreempted"":0,""numAMContainerPreempted"":0,""logAggregationStatus"":""SUCCEEDED"",""unmanagedApplication"":false,""amNodeLabelExpression"":""""}]}}
</code></pre>

<p>CSV output:</p>

<pre><code>allocatedMB|applicationType|diagnostics|finalStatus|finishedTime|memorySeconds|queue|startedTime|user|vcoreSeconds
-1|TEZ|""Session stats:submittedDAGs=0, successfulDAGs=0, failedDAGs=0, killedDAGs=0
""|SUCCEEDED|1590294666011|144531|udsldr|1590294649069|udsldr|17
-1|TEZ|""Session stats:submittedDAGs=0, successfulDAGs=1, failedDAGs=0, killedDAGs=0
""|SUCCEEDED|1590294801090|18279340|default|1590294664397|nifildr|4248
-1|TEZ|""Session stats:submittedDAGs=0, successfulDAGs=0, failedDAGs=0, killedDAGs=0
</code></pre>
","['python', 'python-3.x', 'pandas']",62025298,"<p>Try using a different delimiter when converting to csv, I think the misalignment is happening because of commas:</p>

<pre><code>df.to_csv('123.csv', sep='|', index=['id'])
</code></pre>
",mapping json columns pandas dataframe columns I takes JSON input converts Pandas dataframe But JSON consistent schema misaligned field exist one entry shifts everything left Is way I say amp explicitly define df field json field If I define names I nicely Thanks output subprocess check output command shell True output subprocess bytes converting string isinstance output bytes output output decode output json loads output df pd DataFrame output apps app df df loc df startedTime gt starttime df csv yarn output csv Sample Input JSON apps app id application user udsldr name HIVE ee c b bf cc queue udsldr state FINISHED finalStatus SUCCEEDED progress trackingUI History trackingUrl http uds far mn dab net proxy application diagnostics Session stats submittedDAGs successfulDAGs failedDAGs killedDAGs n clusterId applicationType TEZ applicationTags priority startedTime finishedTime elapsedTime amContainerLogs http uds far dn dab net node containerlogs container e udsldr amHostHttpAddress uds far dn dab,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
62025653,2020-05-26,2020,4,win10toast causes tkinter window freeze,"<p>i'm kindda newbie in python, trying to practice,
after hours of examinations i understood the problem with my program is with the <code>win10toast</code>.
it seems having someproblems with <code>tkinter window</code>.</p>

<pre><code># -*- coding: utf-8 -*-
""""""
Created on Sun May 24 18:18:00 2020

@author: MeTaNa
""""""

'''
this program is simple, notifys u if battery is fully charged,
'''

from tkinter import Label, Tk, Button
import os
import psutil
import win10toast

# current_statues = percent + '% | ' + plugged


def preload():
    pass

def start():
    global percent
    global plugged
    battery = psutil.sensors_battery()
    plugged = battery.power_plugged
    percent = str(battery.percent)
    if plugged == False:
        plugged = ""Not Plugged In""
    else:
        plugged = ""Plugged In""
    if (psutil.sensors_battery().power_plugged == True) and (battery.percent == 100):
        print(percent + '% | ' + plugged)
        print('Unplug the Charger Please!')
        win10toast.ToastNotifier().show_toast('Battery Statues', 'Battery Full.\nUnplug the Charger Please!', icon_path='', duration=10)
        gui.after(5000, start)
    elif (psutil.sensors_battery().power_plugged == False) and (battery.percent != 100):
        print(percent + '% | ' + plugged)
        print('Not Charging...')
        win10toast.ToastNotifier().show_toast('Battery Statues', 'Charger Not Plugged', icon_path='', duration=10)
        gui.after(5000, start)
    else:
        print(percent + '% | ' + plugged)
        print('Charging...')
        win10toast.ToastNotifier().show_toast('Battery Statues', 'Charging...', icon_path='', duration=10)
        gui.after(5000, start)



def closer():
    try:
        os.system('TASKKILL /F /IM Bat2.exe')
    except Exception:
        print('already closed!')
    gui.destroy()



# create a GUI window
gui = Tk()
gui.resizable(False, False)
gui.geometry(""200x200"")
gui.configure(background=""white"")
gui.title(""Battery Notifier"")

battey_statues = Label(gui, text='percent')
battey_statues.pack()

button1 = Button(gui, text=' Start Script ', fg='white', bg='gray', command=start, height=2, width=9)
button1.pack()
button2 = Button(gui, text=' Stop Script ', fg='white', bg='gray', command=closer, height=2, width=9)
button2.pack()


# start the GUI
gui.after(100, preload)
gui.mainloop()
</code></pre>

<p>when i press the <code>start script</code> button, it shall call the <code>start</code> function, but it freezes the tkinter window, i don't understand why.</p>

<p>previously i had a problem with tkinter and <code>while loop</code>, i solved with <code>after</code> method.</p>

<ul>
<li>the script runs correctly without tkinter.</li>
</ul>

<p>so, any ideas what shall i do?</p>
","['python', 'python-3.x', 'tkinter']",62025821,"<p>Have a look at the source code for win10toast (<a href=""https://github.com/jithurjacob/Windows-10-Toast-Notifications/blob/master/win10toast/__init__.py"" rel=""nofollow noreferrer"">https://github.com/jithurjacob/Windows-10-Toast-Notifications/blob/master/win10toast/<strong>init</strong>.py</a>). The normal option is unthreaded which will cause the gui to hang however it also supports a threading option.</p>

<p>Try starting your notification with the following</p>

<pre><code>win10toast.ToastNotifier().show_toast('Battery Statues', 'Charging...', icon_path='', duration=10, threaded=True)
</code></pre>
",win toast causes tkinter window freeze kindda newbie python trying practice hours examinations understood problem program win toast seems someproblems tkinter window coding utf Created Sun May author MeTaNa program simple notifys u battery fully charged tkinter import Label Tk Button import os import psutil import win toast current statues percent plugged def preload pass def start global percent global plugged battery psutil sensors battery plugged battery power plugged percent str battery percent plugged False plugged Not Plugged In else plugged Plugged In psutil sensors battery power plugged True battery percent print percent plugged print Unplug Charger Please win toast ToastNotifier show toast Battery Statues Battery Full nUnplug Charger Please icon path duration gui start elif psutil sensors battery power plugged False battery percent print percent plugged print Not Charging win toast ToastNotifier show toast Battery Statues Charger Not Plugged icon path duration gui start else print percent plugged print,"startoftags, python, python3x, tkinter, endoftags",python python3x list endoftags,python python3x tkinter,python python3x list,0.67
62132515,2020-06-01,2020,2,"Mark Duplicated Rows &amp; Add Range Numbers for Duplicate Rows, Python 3.6","<p>Data: </p>

<pre><code>Data = pd.DataFrame([['1', '2', '3', '4'], ['4', '3', '2', '1'], ['1', '2', '3', '4'], ['4', '3', '2', '1'], ['1', '2', '3', '4'], ['3', '5', '2', '6'], ['10', '9', '8', '7'], ['5', '6', '7', '3'], ['10', '9', '8', '7']], columns=['F1', 'F2', 'F3', 'F4'])
</code></pre>

<p>Marking Duplicate Rows:</p>

<pre><code>Data['Is_Dup'] = Data[['F1', 'F2', 'F3', 'F4']].duplicated(keep=False).astype(int)
</code></pre>

<p>Data:</p>

<p><a href=""https://i.stack.imgur.com/NrtAU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NrtAU.png"" alt=""enter image description here""></a></p>

<p>Expected Results: I want to generate extra field called Dup_Ser, here range values needs to add for duplicate rows.</p>

<p><a href=""https://i.stack.imgur.com/yj74w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yj74w.png"" alt=""enter image description here""></a></p>
","['python', 'python-3.x', 'pandas', 'dataframe']",62132590,"<p>Idea is sorting bay all columns with <code>Is_Dup</code> descending and then add new column with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.cumcount.html"" rel=""nofollow noreferrer""><code>GroupBy.cumcount</code></a>, added <code>1</code> and multiple by <code>Data['Is_Dup']</code> for <code>0</code> for not duplicated values:</p>

<pre><code>cols = ['F1', 'F2', 'F3', 'F4']
Data['Is_Dup'] = Data[cols].duplicated(keep=False).astype(int)

Data = Data.sort_values(['Is_Dup'] + cols, ascending=[False] + [True] * len(cols))
Data['Dup_Ser'] = Data.groupby(cols).cumcount().add(1).mul(Data['Is_Dup'])
print (Data)
   F1 F2 F3 F4  Is_Dup  Dup_Ser
0   1  2  3  4       1        1
2   1  2  3  4       1        2
4   1  2  3  4       1        3
6  10  9  8  7       1        1
8  10  9  8  7       1        2
1   4  3  2  1       1        1
3   4  3  2  1       1        2
5   3  5  2  6       0        0
7   5  6  7  3       0        0
</code></pre>
",Mark Duplicated Rows amp Add Range Numbers Duplicate Rows Python Data Data pd DataFrame columns F F F F Marking Duplicate Rows Data Is Dup Data F F F F duplicated keep False astype int Data Expected Results I want generate extra field called Dup Ser range values needs add duplicate rows,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
62252109,2020-06-07,2020,4,"Drop duplicates on one column, breaking ties from another column","<p>I have the below dataframe:</p>

<pre><code>x = pd.DataFrame({
    ""item"" : [""a"", ""a"", ""a"", ""b"", ""c"", ""c""],
    ""vote"" : [1, 0, 1, 1, 0, 0],
    ""timestamp"" : [""2020-06-07 11:04:26"", ""2020-06-07 11:03:37"", ""2020-06-07 11:09:18"", ""2020-06-07 11:04:40"", ""2020-06-07 11:09:11"", ""2020-06-07 11:09:23""]
})

item   vote   timestamp
a      1      2020-06-07 11:04:26
a      0      2020-06-07 11:03:37
a      1      2020-06-07 11:09:18
b      1      2020-06-07 11:04:40      
c      0      2020-06-07 11:09:11
c      0      2020-06-07 11:09:23
</code></pre>

<p>How do I drop_duplicates on item column, and use the <code>timestamp</code> column as a tiebreaker: keep the latest one? 
The final dataframe should look like this:</p>

<pre><code>item   vote   timestamp
a      1      2020-06-07 11:09:18
b      1      2020-06-07 11:04:40      
c      0      2020-06-07 11:09:23
</code></pre>
","['python', 'python-3.x', 'pandas']",62252138,"<p>You can call <code>sort_values</code> on ""item"" and ""timestamp"" before dropping duplicates:</p>

<pre><code>x.sort_values(['item', 'timestamp']).drop_duplicates('item', keep='last')

  item  vote            timestamp
2    a     1  2020-06-07 11:09:18
3    b     1  2020-06-07 11:04:40
5    c     0  2020-06-07 11:09:23
</code></pre>

<p>Specifying <code>keep='last'</code> means all but the last row are discarded, which works out because we sorted on timestamp in the previous step.</p>

<hr>

<pre><code>(x.sort_values(['item', 'timestamp'])
  .drop_duplicates('item', keep='last')
  .reset_index(drop=True))

  item  vote            timestamp
0    a     1  2020-06-07 11:09:18
1    b     1  2020-06-07 11:04:40
2    c     0  2020-06-07 11:09:23
</code></pre>
",Drop duplicates one column breaking ties another column I dataframe x pd DataFrame item b c c vote timestamp item vote timestamp b c c How I drop duplicates item column use timestamp column tiebreaker keep latest one The final dataframe look like item vote timestamp b c,"startoftags, python, python3x, pandas, endoftags",python pandas dataframe endoftags,python python3x pandas,python pandas dataframe,0.67
62334158,2020-06-11,2020,2,Pandas Dataframe: Can I fetch other column values along with the column on which group by clause has been applied?,"<p>I have this data set which contains price for items sold across various stores on a week wise basis.</p>

<p>The dataframe: price_df looks like this:</p>

<pre><code>price_df.head()
   store_id      item_id week    sell_price
0      S1         item1    w1          9.58
1      S1         item1    w2          9.00
2      S2         item1    w1          8.30
3      S2         item1    w2          8.50
4      S2         item2    w1          8.26
</code></pre>

<p>I want to find out : for each 'item and store' combination the highest price</p>

<p>My code:</p>

<pre><code>item_store_max_prices = price_df.groupby([""store_id"",""item_id""]).agg({""sell_price"":[""max""]})
</code></pre>

<p>But this would only show the store_id, item_id and for that combination the max price ever listed.</p>

<p>Problem statement:</p>

<p>However, I want to show the week as well on which that max price has been observed for that 'store - item' combination in my resultset.</p>

<p>For example:</p>

<pre><code>    store_id     item_id  week    sell_price
0      S1         item1    w1          9.58
1      S2         item1    w2          8.50
</code></pre>

<p>Could you please help me with how to obtain that result?</p>

<p>Thanks in advance.</p>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",62334443,"<p>You can find indexes of the rows having the <code>max</code> values using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.idxmax.html"" rel=""nofollow noreferrer""><code>df.idxmax()</code></a>.</p>

<p>Then subset the dataframe using the above indices using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>df.loc</code></a>, like this:</p>

<pre><code>idx = price_df.groupby([""store_id"",""item_id""])['sell_price'].idxmax().tolist()
price_df = price_df.loc[idx]
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>  store_id item_id week  sell_price
0       S1   item1   w1        9.58
3       S2   item1   w2        8.50
4       S2   item2   w1        8.26
</code></pre>
",Pandas Dataframe Can I fetch column values along column group clause applied I data set contains price items sold across various stores week wise basis The dataframe price df looks like price df head store id item id week sell price S item w S item w S item w S item w S item w I want find item store combination highest price My code item store max prices price df groupby store id item id agg sell price max But would show store id item id combination max price ever listed Problem statement However I want show week well max price observed store item combination resultset For example store id item id week sell price S item w S item w Could please help obtain result Thanks advance,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas dataframe pandasgroupby,python pandas dataframe,0.87
62376931,2020-06-14,2020,2,About django/restframework request,"<p>This is my APIview:</p>

<pre><code>class Comments(ListCreateAPIView):

   queryset = models.NewsComment.objects
   filter_backends = [CommentsFilterBackend]
   http_method_names = [""get"", ""post""]

   def get_serializer(self, *args, **kwargs):
      if self.request.method = ""get"":
          return Myserializer
</code></pre>

<blockquote>
  <p>I want to know why self.request can call the  ""method"".  Isn't it the encapsulated Request new object? The native request object is encapsulated in Request obj. Why can self.request call the request.method attrs?</p>
</blockquote>
","['python', 'django', 'django-rest-framework']",62378415,"<p>The answer is in the <a href=""https://github.com/encode/django-rest-framework/blob/master/rest_framework/request.py"" rel=""nofollow noreferrer"">source code of Request class</a>, more specifically at the the <code>__getattr__</code> method (line 410):</p>

<pre><code>    def __getattr__(self, attr):
        """"""
        If an attribute does not exist on this instance, then we also attempt
        to proxy it to the underlying HttpRequest object.
        """"""
        try:
            return getattr(self._request, attr)
        except AttributeError:
            return self.__getattribute__(attr)
</code></pre>

<p>It's one of Python's ""magic methods"" and you can read more about it <a href=""https://python-reference.readthedocs.io/en/latest/docs/dunderattr/getattr.html"" rel=""nofollow noreferrer"">here</a>.</p>
",About django restframework request This APIview class Comments ListCreateAPIView queryset models NewsComment objects filter backends http method names get post def get serializer self args kwargs self request method get return Myserializer I want know self request call method Isn encapsulated Request new object The native request object encapsulated Request obj Why self request call request method attrs,"startoftags, python, django, djangorestframework, endoftags",python django djangorestframework endoftags,python django djangorestframework,python django djangorestframework,1.0
62667158,2020-06-30,2020,13,How do I increase the line thickness of my Seaborn Line,"<p>I have a few seaborn lineplots and I can't figure out how to increase the width of my lines.</p>
<p>Here is my code</p>
<pre><code>#graph 1
sns.lineplot(x=&quot;date&quot;, y=&quot;nps&quot;, data=df_nps, ax=ax1, label=&quot;NPS&quot;, color='#0550D0')
sns.lineplot(x=&quot;date&quot;, y=&quot;ema28&quot;, data=df_nps, ax=ax1, label=&quot;EMA28&quot;, color='#7DF8F3')
sns.lineplot(x=&quot;date&quot;, y=&quot;ema7&quot;, data=df_nps, ax=ax1, label=&quot;EMA7&quot;, color='orange')

#graph 2
dfz_nps_lineplot = sns.lineplot(x=&quot;date&quot;, y=&quot;nps&quot;, data=dfz_nps, ax=ax2, label=&quot;NPS&quot;, color='#0550D0')
dfz_nps_lineplot = sns.lineplot(x=&quot;date&quot;, y=&quot;ema28&quot;, data=dfz_nps, ax=ax2, label=&quot;EMA28&quot;, color='#7DF8F3')
dfz_nps_lineplot = sns.lineplot(x=&quot;date&quot;, y=&quot;ema7&quot;, data=dfz_nps, ax=ax2, label=&quot;EMA7&quot;, color='orange')

#graph3
dfp_nps_lineplot = sns.lineplot(x=&quot;date&quot;, y=&quot;nps&quot;, data=dfp_nps, ax=ax3, label=&quot;NPS&quot;, color='#0550D0')
dfp_nps_lineplot = sns.lineplot(x=&quot;date&quot;, y=&quot;ema28&quot;, data=dfp_nps, ax=ax3, label=&quot;EMA28&quot;, color='#7DF8F3')
dfp_nps_lineplot = sns.lineplot(x=&quot;date&quot;, y=&quot;ema7&quot;, data=dfp_nps, ax=ax3, label=&quot;EMA7&quot;, color='orange')

# formatting

plt.show()
</code></pre>
<p>This is what my lineplots look like right now.</p>
<p><a href=""https://gyazo.com/1aecfef9e71bfc9d6c0b5f603db93bd1"" rel=""noreferrer"">https://gyazo.com/1aecfef9e71bfc9d6c0b5f603db93bd1</a></p>
","['python', 'matplotlib', 'seaborn']",62667290,"<p>As you can see from <a href=""https://seaborn.pydata.org/generated/seaborn.lineplot.html"" rel=""noreferrer"">seaborn.lineplot</a> documentation, the function accepts matplotlib.axes.Axes.plot() arguments, which means you can pass the same arguments you can to matplotlib function <a href=""https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.plot.html#matplotlib.axes.Axes.plot"" rel=""noreferrer"">in this documentation</a>.</p>
<p>If you want to simply adjust the width of your lineplots I find this the easiest: pass an argument <code>linewidth = your_desired_line_width_in_float</code> , for example, <code>linewidth = 1.5</code> in your <code>sns.lineplot()</code> functions.</p>
<p>You can find additional possible arguments in the documentations linked.</p>
<p><strong>Example output on random data:</strong></p>
<p><em>seaborn.lineplot() without linewdith argument provided</em>
<a href=""https://i.stack.imgur.com/knPa1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/knPa1.png"" alt=""searbon.lineplot() without linewdith argument"" /></a></p>
<p><em>seaborn.lineplot() with linewidth = 3</em>
<a href=""https://i.stack.imgur.com/3Ee23.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3Ee23.png"" alt=""seaborn.lineplot() with linewidth = 3"" /></a></p>
",How I increase line thickness Seaborn Line I seaborn lineplots I figure increase width lines Here code graph sns lineplot x quot date quot quot nps quot data df nps ax ax label quot NPS quot color D sns lineplot x quot date quot quot ema quot data df nps ax ax label quot EMA quot color DF F sns lineplot x quot date quot quot ema quot data df nps ax ax label quot EMA quot color orange graph dfz nps lineplot sns lineplot x quot date quot quot nps quot data dfz nps ax ax label quot NPS quot color D dfz nps lineplot sns lineplot x quot date quot quot ema quot data dfz nps ax ax label quot EMA quot color DF F dfz nps lineplot sns lineplot x quot date quot quot ema quot data dfz nps ax ax label quot EMA quot color orange,"startoftags, python, matplotlib, seaborn, endoftags",python arrays numpy endoftags,python matplotlib seaborn,python arrays numpy,0.33
62770893,2020-07-07,2020,7,How to add another attribute in dictionary inside a one line for loop,"<p>I have a list of dictionary and a string. I want to add a <code>selected</code> attribute in each dictionary inside the list. I am wondering if this is possible using a one liner.</p>
<p>Here are my inputs:</p>
<pre><code>saved_fields = &quot;apple|cherry|banana&quot;.split('|')
fields = [
    {
        'name' : 'cherry'
    }, 
    {
        'name' : 'apple'
    }, 
    {
        'name' : 'orange'
    }
]
</code></pre>
<p>This is my expected output:</p>
<pre><code>[
    {
        'name' : 'cherry',
        'selected' : True
    }, 
    {
        'name' : 'apple',
        'selected' : True
    }, 
    {
        'name' : 'orange',
        'selected' : False
    }
]
</code></pre>
<p>I tried this:</p>
<pre><code>new_fields = [item [item['selected'] if item['name'] in saved_fields] for item in fields]
</code></pre>
","['python', 'list', 'dictionary']",62771032,"<p>I don't necessarily think &quot;one line way&quot; is the best way.</p>
<pre><code>s = set(saved_fields)  # set lookup is more efficient 
for d in fields:
    d['status'] = d['name'] in s

fields
# [{'name': 'cherry', 'status': True},
#  {'name': 'apple', 'status': True},
#  {'name': 'orange', 'status': False}]
</code></pre>
<p>Simple. Explicit. Obvious.</p>
<p>This updates your dictionary in-place, which is better if you have a lot of records or other keys besides &quot;name&quot; and &quot;status&quot; that you haven't told us about.</p>
<hr />
<p>If you insist on a one-liner, this is one preserves other keys:</p>
<pre><code>[{**d, 'status': d['name'] in s} for d in fields]  
# [{'name': 'cherry', 'status': True},
#  {'name': 'apple', 'status': True},
#  {'name': 'orange', 'status': False}]
</code></pre>
<p>This is <a href=""https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions"" rel=""nofollow noreferrer""><em>list comprehension</em> syntax</a> and creates a new list of dictionaries, leaving the original untouched.</p>
<p>The <a href=""https://www.python.org/dev/peps/pep-3132/"" rel=""nofollow noreferrer""><code>{**d, ...}</code></a> portion is necessary to preserve keys that are not otherwise modified. I didn't see any other answers doing this, so thought it was worth calling out.</p>
<p>The extended unpacking syntax works for python3.5+ only, for older versions, change <code>{**d, 'status': d['name'] in s}</code> to <code>dict(d, **{'status': d['name'] in s})</code>.</p>
",How add another attribute dictionary inside one line loop I list dictionary string I want add selected attribute dictionary inside list I wondering possible using one liner Here inputs saved fields quot apple cherry banana quot split fields name cherry name apple name orange This expected output name cherry selected True name apple selected True name orange selected False I tried new fields item item selected item name saved fields item fields,"startoftags, python, list, dictionary, endoftags",python python3x list endoftags,python list dictionary,python python3x list,0.67
62857309,2020-07-12,2020,2,Iterating over table of divs using BeautifulSoup,"<p>A <code>div</code> of <code>class=&quot;tableBody&quot;</code> has many <code>div</code>s as children. I want to get all its <code>div</code> child and get the string which I have highlighted in this picture.</p>
<pre><code>import bs4 as bs
import urllib.request
source = urllib.request.urlopen(&quot;https://www.ungm.org/Public/Notice&quot;).read()
soup = bs.BeautifulSoup(source,'lxml')

t_body = soup.find(&quot;div&quot;, class_=&quot;tableBody&quot;)
t_divs = t_body.find_all(&quot;div&quot;)
</code></pre>
<p>the above code returns me a empty list.
<a href=""https://i.stack.imgur.com/3SQsq.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3SQsq.jpg"" alt=""enter image description here"" /></a></p>
<p>I am trying to learn BS4. I appreciate it if you could help me with the code.</p>
","['python', 'web-scraping', 'beautifulsoup']",62857725,"<p>The data you see on the page is loaded dynamically via JavaScript. You can use <code>requests</code> module to simulate it.</p>
<p>For example:</p>
<pre><code>import requests
from bs4 import BeautifulSoup


url = 'https://www.ungm.org/Public/Notice/Search'

payload = {
  &quot;PageIndex&quot;: 0,
  &quot;PageSize&quot;: 15,
  &quot;Title&quot;: &quot;&quot;,
  &quot;Description&quot;: &quot;&quot;,
  &quot;Reference&quot;: &quot;&quot;,
  &quot;PublishedFrom&quot;: &quot;&quot;,
  &quot;PublishedTo&quot;: &quot;12-Jul-2020&quot;,
  &quot;DeadlineFrom&quot;: &quot;12-Jul-2020&quot;,
  &quot;DeadlineTo&quot;: &quot;&quot;,
  &quot;Countries&quot;: [],
  &quot;Agencies&quot;: [],
  &quot;UNSPSCs&quot;: [],
  &quot;NoticeTypes&quot;: [],
  &quot;SortField&quot;: &quot;DatePublished&quot;,
  &quot;SortAscending&quot;: False,
  &quot;isPicker&quot;: False,
  &quot;NoticeTASStatus&quot;: [],
  &quot;IsSustainable&quot;: False,
  &quot;NoticeDisplayType&quot;: None,
  &quot;NoticeSearchTotalLabelId&quot;: &quot;noticeSearchTotal&quot;,
  &quot;TypeOfCompetitions&quot;: []
}

soup = BeautifulSoup( requests.post(url, json=payload).content, 'html.parser' )

for row in soup.select('.tableRow'):
    cells = [cell.get_text(strip=True) for cell in row.select('.tableCell')]
    print(cells[1])
    print('{:&lt;30}{:&lt;15}{:&lt;15}{:&lt;25}{:&lt;45}{:&lt;15}'.format(*cells[2:]))
    print('-'*80)
</code></pre>
<p>Prints:</p>
<pre><code>Supply and delivery of 78 smartphones
13-Jul-2020 11:00 (GMT 2.00)  11-Jul-2020    FAO            Request for quotation    2020/FRMLW/FRMLW/106096                      Malawi         
--------------------------------------------------------------------------------
Supply of LEGUMES SEEDS for rainfed season
23-Jul-2020 14:00 (GMT 2.00)  11-Jul-2020    FAO            Invitation to bid        2020/FRMLW/FRMLW/106051                      Malawi         
--------------------------------------------------------------------------------
Supply of MAIZE SEEDS for rainfed season
22-Jul-2020 14:00 (GMT 2.00)  11-Jul-2020    FAO            Invitation to bid        2020/FRMLW/FRMLW/106050                      Malawi         
--------------------------------------------------------------------------------
Procurement of Supply and Installation of Outdoor Metal Furniture for Rooftop Terrace at FAO Headquarters in Rome, Italy
10-Aug-2020 12:00 (GMT 2.00)  11-Jul-2020    FAO            Invitation to bid        2020/CSAPC/CSDID/105286                      Italy          
--------------------------------------------------------------------------------
Procurement of Silo for Emergency Project
13-Jul-2020 13:00 (GMT 5.00)  11-Jul-2020    FAO            Invitation to bid        2020/FABGD/FABGD/106145                      Bangladesh     
--------------------------------------------------------------------------------
Procurement of Concentrate Ruminant Feed
13-Jul-2020 13:00 (GMT 5.00)  11-Jul-2020    FAO            Invitation to bid        2020/FABGD/FABGD/106064                      Bangladesh     
--------------------------------------------------------------------------------
Purchase of Waste Collection Vehicles - (Two Tractors)
22-Jul-2020 06:30 (GMT 0.00)  11-Jul-2020    UNOPS          Request for quotation    RFQ/2020/15298                               Sri Lanka      
--------------------------------------------------------------------------------
Procurement of Laboratory Equipment and Material
24-Jul-2020 22:23 (GMT -1.00) 11-Jul-2020    FAO            Invitation to bid        2020/FRGAM/FRGAM/106143                      Gambia         
--------------------------------------------------------------------------------
Compra de chalecos para promotores comunitarios para la Oficina de Unicef Bolivar - LRFQ-2020-9159352
16-Jul-2020 23:59 (GMT -3.00) 11-Jul-2020    UNICEF         Request for proposal     LRFQ-2020-9159352                            Venezuela      
--------------------------------------------------------------------------------
Call for Proposals Quality Based Fixed Budget (CFPFB):
26-Jul-2020 17:00 (GMT 3.00)  11-Jul-2020    UNDP           Request for proposal     UNDP-SYR-RPA-051-20                          Syrian Arab Republic
--------------------------------------------------------------------------------
Innovation and Design Specialist
27-Jul-2020 00:00 (GMT -5.00) 11-Jul-2020    UNDP           Not set                  Innovation and Design Specialist             Turkey         
--------------------------------------------------------------------------------
(RFI) from national and/or international CSOs/NGOs for potential partnership with UNDP and its pooled funding mechanism, the Darfur Community Peace and Stability Fund (DCPSF),
26-Jul-2020 08:00 (GMT -7.00) 11-Jul-2020    UNDP           Request for information  RFI-SDN-20-002                               Sudan          
--------------------------------------------------------------------------------
IRAQ-LRPS-017-2020-9159660 Rehabilitation of 3 water projects at Avrek, Grey Basi and Sarsenk in Duhok
26-Jul-2020 12:00 (GMT 3.00)  11-Jul-2020    UNICEF         Request for proposal     9159660                                      Iraq           
--------------------------------------------------------------------------------
106142 INVITACIÃN A COTIZAR PARA LA ADQUISICIÃN DE FERTILIZANTES, HERRAMIENTAS Y MATERIALES PARA ECA DE CACAO
21-Jul-2020 22:00 (GMT -5.00) 10-Jul-2020    FAO            Request for quotation    2020/FLCOL/FLCOL/106142                      Colombia       
--------------------------------------------------------------------------------
Achat de tablettes, de GPS et batteries rechargeable (206 tablettes, 68 GPS, et 181 pack chargeurs et batteries rechargeables) Ã  livrer sur  Dakar
28-Jul-2020 12:00 (GMT 0.00)  10-Jul-2020    FAO            Invitation to bid        2020/FRSEN/FRSEN/106093                      United Kingdom 
--------------------------------------------------------------------------------
</code></pre>
<hr />
<p>EDIT: To get all pages, filter out only 'Afghanistan' country and save to CSV, you can use this example:</p>
<pre><code>import csv
import requests
from bs4 import BeautifulSoup


url = 'https://www.ungm.org/Public/Notice/Search'

payload = {
  &quot;PageIndex&quot;: 0,
  &quot;PageSize&quot;: 15,
  &quot;Title&quot;: &quot;&quot;,
  &quot;Description&quot;: &quot;&quot;,
  &quot;Reference&quot;: &quot;&quot;,
  &quot;PublishedFrom&quot;: &quot;&quot;,
  &quot;PublishedTo&quot;: &quot;12-Jul-2020&quot;,
  &quot;DeadlineFrom&quot;: &quot;12-Jul-2020&quot;,
  &quot;DeadlineTo&quot;: &quot;&quot;,
  &quot;Countries&quot;: [],
  &quot;Agencies&quot;: [],
  &quot;UNSPSCs&quot;: [],
  &quot;NoticeTypes&quot;: [],
  &quot;SortField&quot;: &quot;DatePublished&quot;,
  &quot;SortAscending&quot;: False,
  &quot;isPicker&quot;: False,
  &quot;NoticeTASStatus&quot;: [],
  &quot;IsSustainable&quot;: False,
  &quot;NoticeDisplayType&quot;: None,
  &quot;NoticeSearchTotalLabelId&quot;: &quot;noticeSearchTotal&quot;,
  &quot;TypeOfCompetitions&quot;: []
}

page, all_data = 0, []
while True:
    print('Page {}...'.format(page))

    payload['PageIndex'] = page
    soup = BeautifulSoup( requests.post(url, json=payload).content, 'html.parser' )
    rows = soup.select('.tableRow')
    if not rows:
        break

    for row in rows:
        cells = [cell.get_text(strip=True) for cell in row.select('.tableCell')]
        print(cells[1])
        print('{:&lt;30}{:&lt;15}{:&lt;15}{:&lt;25}{:&lt;45}{:&lt;15}'.format(*cells[2:]))
        print('-'*80)

        # we are only interested in Afghanistan:
        if 'afghanistan' in cells[7].lower():
            all_data.append([row['data-noticeid'], *cells[1:]])

    page += 1

# write to csv file:
with open('data.csv', 'w', newline='') as csvfile:
    csv_writer = csv.writer(csvfile, delimiter=',', quotechar='&quot;', quoting=csv.QUOTE_MINIMAL)
    for row in all_data:
        csv_writer.writerow(row)
</code></pre>
<p>Saved <code>data.csv</code> (screenshot from LibreOffice):</p>
<p><a href=""https://i.stack.imgur.com/VCZ3l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VCZ3l.png"" alt=""enter image description here"" /></a></p>
",Iterating table divs using BeautifulSoup A div class quot tableBody quot many divs children I want get div child get string I highlighted picture import bs bs import urllib request source urllib request urlopen quot https www ungm org Public Notice quot read soup bs BeautifulSoup source lxml body soup find quot div quot class quot tableBody quot divs body find quot div quot code returns empty list I trying learn BS I appreciate could help code,"startoftags, python, webscraping, beautifulsoup, endoftags",python arrays numpy endoftags,python webscraping beautifulsoup,python arrays numpy,0.33
62951672,2020-07-17,2020,4,How to check if identifier existed in previous months list,"<p>I am trying to determine, if an identifier occurs first in a given month (i.e. it is 'new' to the list of identifier). Below, there is the first attempt, but it flags identifier <code>a3</code> as old on 28 Feb 2020, although it was not in the list on 31 Jan 2020.</p>
<p>Note that this is a simplified example: in practice, I would have more group by columns, not just date, and I would need to check, if the identifier is new to the 'cell' created by the combination of date, industry, age, etc. There could be many.</p>
<pre><code>import pandas as pd, numpy as np

data = &quot;&quot;&quot;
date                            identifier     value
 31-Dec-2019                    a1   10
 31-Dec-2019                    a2   20
 31-Dec-2019                    a3   30
 31-Jan-2020                    a1   40
 31-Jan-2020                    a2   50
 31-Jan-2020                    a4   60
 31-Jan-2020                    a5   60
 28-Feb-2020                    a1   70
 28-Feb-2020                    a4   80
 28-Feb-2020                    a3   90
&quot;&quot;&quot;

res=[]
for row in [el.split() for el in data.splitlines()][1:]:
    rrow=[]
    for col in row:
        try:
            if float(col):
                col = np.float32(col)
        except:
            pass
        rrow.append(col)
    res.append(rrow)
df = pd.DataFrame(data=res[1:], columns=res[0])

df.date = pd.to_datetime(df.date)
df = df.set_index([&quot;date&quot;, &quot;identifier&quot;]).sort_index()

df[&quot;valprev&quot;] = df.groupby(level=&quot;identifier&quot;)[&quot;value&quot;].shift(1)
df[&quot;isnew&quot;] = df.valprev.isnull(
</code></pre>
<p><a href=""https://i.stack.imgur.com/z4eBz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z4eBz.png"" alt=""enter image description here"" /></a></p>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",62952852,"<p>Using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html"" rel=""nofollow noreferrer""><code>pd.to_datetime</code></a> convert the <code>date</code> column to pandas <code>datetime</code> series:</p>
<pre><code>df['date'] = pd.to_datetime(df['date'])
</code></pre>
<p><strong>Then use:</strong></p>
<pre><code>s1 = df.groupby('date')['identifier'].value_counts()
s2 = s1.unstack().diff().replace({0: False, np.nan: True}).stack()
df['isnew'] = df.set_index(['date', 'identifier']).index.map(s2)
</code></pre>
<hr />
<p><strong>Details:</strong></p>
<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>DataFrame.groupby</code></a> on <code>date</code> and aggregate the column <code>identifier</code> using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.SeriesGroupBy.value_counts.html"" rel=""nofollow noreferrer""><code>Groupby.value_counts</code></a>:</p>
<pre><code># print(s1):

date        identifier
2019-12-31  a1            1
            a2            1
            a3            1
2020-01-31  a1            1
            a2            1
            a4            1
            a5            1
2020-02-28  a1            1
            a3            1
            a4            1
Name: identifier, dtype: int64
</code></pre>
<p>Use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unstack.html"" rel=""nofollow noreferrer""><code>Series.unstack</code></a> on series <code>s1</code> to <code>reshape</code> it, then use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.diff.html"" rel=""nofollow noreferrer""><code>DataFrame.diff</code></a> to calculate successive differences between counts of identifier, this step will help in identifying the occurrence of <code>repeated</code> identifiers from <code>previous month</code>, next use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>DataFrame.stack</code></a> to again convert it into <code>MultiLevel</code> index series named <code>s2</code>.</p>
<pre><code># s1.unstack().diff()
identifier   a1   a2  a3   a4  a5
date                             
2019-12-31  NaN  NaN NaN  NaN NaN
2020-01-31  0.0  0.0 NaN  NaN NaN
2020-02-28  0.0  NaN NaN  0.0 NaN

# print(s2) # this series will be use to map in next step
date        identifier
2019-12-31  a1             True
            a2             True
            a3             True
            a4             True
            a5             True
2020-01-31  a1            False
            a2            False
            a3             True
            a4             True
            a5             True
2020-02-28  a1            False
            a2             True
            a3             True
            a4            False
            a5             True
dtype: bool
</code></pre>
<p>Use <code>set_index</code> to set the index of dataframe <code>df</code> as <code>date</code> and <code>identifier</code> and use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>Series.map</code></a> to map this index from the values from <code>s2</code>, assign this mapped value to the new column <code>isnew</code>:</p>
<pre><code># print(df)
        date identifier  value  isnew
0 2019-12-31         a1     10   True
1 2019-12-31         a2     20   True
2 2019-12-31         a3     30   True
3 2020-01-31         a1     40  False
4 2020-01-31         a2     50  False
5 2020-01-31         a4     60   True
6 2020-01-31         a5     60   True
7 2020-02-28         a1     70  False
8 2020-02-28         a4     80  False
9 2020-02-28         a3     90   True
</code></pre>
",How check identifier existed previous months list I trying determine identifier occurs first given month e new list identifier Below first attempt flags identifier old Feb although list Jan Note simplified example practice I would group columns date I would need check identifier new cell created combination date industry age etc There could many import pandas pd numpy np data quot quot quot date identifier value Dec Dec Dec Jan Jan Jan Jan Feb Feb Feb quot quot quot res row el split el data splitlines rrow col row try float col col np float col except pass rrow append col res append rrow df pd DataFrame data res columns res df date pd datetime df date df df set index quot date quot quot identifier quot sort index df quot valprev quot df groupby level quot identifier quot quot value quot shift df quot isnew quot df valprev isnull,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python pandas numpy endoftags,python pandas dataframe pandasgroupby,python pandas numpy,0.58
63018218,2020-07-21,2020,3,Remove specific set of rows from each group in a dataframe,"<p>I have a dataframe as follows :</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({&quot;user_id&quot;: ['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b'],
                   &quot;value&quot;: [20, 17,15, 10, 8 , 18, 18, 17, 13, 10]})
</code></pre>
<p>Notice that the dataframe is sorted in descending order by user_id then value.</p>
<p>For each user_id, I would like to remove the 2nd and 4th row so the output would look like</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({&quot;user_id&quot;: ['a', 'a', 'a', 'b', 'b', 'b',],
                   &quot;value&quot;: [20, 15, 8 , 18, 17, 10]})
</code></pre>
<p>Inspired by <a href=""https://stackoverflow.com/questions/38798058/drop-first-and-last-row-from-within-each-group"">drop first and last row from within each group</a>, I tried the following :</p>
<pre class=""lang-py prettyprint-override""><code>def drop_rows(dataframe) : 
     pos = [1,3]
     return dataframe.drop(dataframe.index[pos], inplace=True)
df.groupby('user_id').apply(drop_rows)
</code></pre>
<p>But got this &quot;index 2 is out of bounds for axis 0 with size 0&quot;</p>
<p>Could someone explain why this doesn't work and how I should proceed instead ? Also, given that the dataset is quite huge, an efficient approach to the solution would be helpful. Thanks a lot.</p>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",63018263,"<p>You can use <code>groupby+cumcount</code> to get row count in each group then check if not the row is in the <code>to_del</code> list</p>
<pre><code>to_del = [2,4]
df[~df.groupby('user_id').cumcount().add(1).isin(to_del)]
</code></pre>
<hr />
<pre><code>  user_id  value
0       a     20
2       a     15
4       a      8
5       b     18
7       b     17
9       b     10
</code></pre>
",Remove specific set rows group dataframe I dataframe follows df pd DataFrame quot user id quot b b b b b quot value quot Notice dataframe sorted descending order user id value For user id I would like remove nd th row output would look like df pd DataFrame quot user id quot b b b quot value quot Inspired drop first last row within group I tried following def drop rows dataframe pos return dataframe drop dataframe index pos inplace True df groupby user id apply drop rows But got quot index bounds axis size quot Could someone explain work I proceed instead Also given dataset quite huge efficient approach solution would helpful Thanks lot,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas dataframe pandasgroupby,python pandas dataframe,0.87
63178121,2020-07-30,2020,3,Does Python Pandas have a way to specify a column that counts each individual occurrence of a combination of values?,"<p>I have a Pandas dataframe with 22 columns that deal with building assessments, however, I am only focused on two specific columns. These two columns are a numeric building ID and an assessment phase that indicates which phase a corresponding building ID is in. Basically, what I would like to do is count the occurrence of a combination of values across these two columns and store those counts in an ordinal column. Details are below:</p>
<pre><code>  Building ID   | Assessment Phase
-----------------------------------
  001                Phase 1
  002                Phase 2
  002                Phase 2
  003                Phase 3
  003                Phase 2
  003                Phase 3
  004                  Unk
  004                Phase 1
  005                Phase 2
</code></pre>
<p>You'll notice there are repeating IDs and assessments that look the same. I would like to have a cumulative row count of each like occurrence grouped by Building ID and Assessment Phase. It should look something like this:</p>
<pre><code>  Building ID   | Assessment Phase | Bldg_Phs_Ord
--------------------------------------------------
  001                Phase 1              1
  002                Phase 2              1
  002                Phase 2              2
  003                Phase 3              1
  003                Phase 3              2
  003                Phase 3              3
  004                  Unk                1         
  004                Phase 1              1
  005                Phase 2              1
</code></pre>
<p>As can be seen there are individual counts of each combination. Some combinations repeat a few times in which each following combination is placed on its own row.</p>
<p>What I have tried is this to test that it was coming out correctly:</p>
<pre><code>test_cnt = bldg_df.groupby(['Building ID', 'Assessment Phase']).size().to_frame('COUNT').sort_values(by=['Building ID']).reset_index()
</code></pre>
<p>Unfortunately, this aggregates the combinations together whenever there is more than one repeating combination.</p>
<pre><code>     Building ID   | Assessment Phase | COUNT
--------------------------------------------------
  001                Phase 1              1
  002                Phase 2              2
  003                Phase 3              3
  004                  Unk                1         
  004                Phase 1              1
  005                Phase 2              1
</code></pre>
<p>What should I add so that it returns each individual, cumulative row count?</p>
<p>Thank you.</p>
","['python', 'pandas', 'dataframe']",63179128,"<p>IIUC you are looking for <code>cumcount</code>:</p>
<pre><code>df[&quot;count&quot;] = df.groupby(['Building ID', 'Assessment Phase']).cumcount()+1

print (df)

   Building ID Assessment Phase  count
0            1          Phase 1      1
1            2          Phase 2      1
2            2          Phase 2      2
3            3          Phase 3      1
4            3          Phase 3      2
5            3          Phase 3      3
6            4              Unk      1
7            4          Phase 1      1
8            5          Phase 2      1
</code></pre>
",Does Python Pandas way specify column counts individual occurrence combination values I Pandas dataframe columns deal building assessments however I focused two specific columns These two columns numeric building ID assessment phase indicates phase corresponding building ID Basically I would like count occurrence combination values across two columns store counts ordinal column Details Building ID Assessment Phase Phase Phase Phase Phase Phase Phase Unk Phase Phase You notice repeating IDs assessments look I would like cumulative row count like occurrence grouped Building ID Assessment Phase It look something like Building ID Assessment Phase Bldg Phs Ord Phase Phase Phase Phase Phase Phase Unk Phase Phase As seen individual counts combination Some combinations repeat times following combination placed row What I tried test coming correctly test cnt bldg df groupby Building ID Assessment Phase size frame COUNT sort values Building ID reset index Unfortunately aggregates combinations together whenever one repeating combination,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
63198666,2020-07-31,2020,5,Python3 PyGame how to move an image?,"<p>I tried to use <code>.blit</code> but an issue occurs, here is a screenshot to explain my problem furthermore:</p>
<p>The image appears to be smudged across the screen following my mouse</p>
<p><a href=""https://i.stack.imgur.com/PiIYJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PiIYJ.png"" alt=""It just copies my image."" /></a></p>
<p>code:</p>
<pre><code>import pygame   
import keyboard
black = (0,0,0)
white = (255, 255, 255)

pygame.init()

screen = pygame.display.set_mode((600, 400))
screen.fill(black)
screen.convert()

icon = pygame.image.load('cross.png')
pygame.display.set_icon(icon)
pygame.display.set_caption('MouseLoc')
cross = pygame.image.load('cross.png')


running = True
while running:
    for event in pygame.event.get():
        if event.type == pygame.QUIT or keyboard.is_pressed(' '):
            running = False
            
mx, my = pygame.mouse.get_pos()
screen.blit(cross, (mx-48, my-48))
print(my, mx)
pygame.display.update() 
</code></pre>
","['python', 'python-3.x', 'pygame']",63202540,"<p>Fix the indentation in your given code, it puts all the important stuff outside the while loop.
You can also add a def statement that redraws the window each time the sprite is moved:</p>
<pre><code>def redraw_game_window():
    screen.fill(black)
    screen.blit(â¢â¢â¢)
    pygame.display.update()
</code></pre>
<p>And put this at the end of your program, after putting all <code>fill</code> and <code>blit</code> statements inside the def statement:</p>
<pre><code>redraw_game_window()
</code></pre>
<p>This statement will make it easier for you to blit more items on the screen, and this will prevent the object from drawing on the screen, because the screen is updated every time the object is moved.</p>
",Python PyGame move image I tried use blit issue occurs screenshot explain problem furthermore The image appears smudged across screen following mouse code import pygame import keyboard black white pygame init screen pygame display set mode screen fill black screen convert icon pygame image load cross png pygame display set icon icon pygame display set caption MouseLoc cross pygame image load cross png running True running event pygame event get event type pygame QUIT keyboard pressed running False mx pygame mouse get pos screen blit cross mx print mx pygame display update,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
63342851,2020-08-10,2020,2,Delete the rows of a DataFrame satisfying conditions evaluated against multiple columns,"<p>I would like to filter my <code>DataFrame</code> by evaluating some conditions against several columns of the <code>DataFrame</code>. I illustrate what I want to do with the following eample:</p>
<pre><code>df = {'user': [1,1,1,2,2,2],
      'speed':[10,20,90,15,39, 10],
      'acceleration': [9.8,29,5,4,7, 3],
      'jerk':[50,60,60,40,20,-50],
      'mode':['car','car','car','metro','metro', 'metro']}


df = pd.DataFrame.from_dict(df)
df
    user  speed   acceleration  jerk  mode
0     1     10           9.8    50    car
1     1     20          29.0    60    car
2     1     90           5.0    60    car
3     2     15           4.0    40  metro
4     2     39           7.0    20  metro
5     2     10           3.0   -50  metro
</code></pre>
<p>In the given example, I would like to filter the dataframe based on thresholds set against <code>speed, acceleration</code> and <code>jerk</code> columns as in the table below:</p>
<pre><code>+-------+-------+--------------+------+-----+
|       | speed | acceleration |    jerk    |
+-------+-------+--------------+------+-----+
|       | max   |    max       | min  | max |
| ---   | ---   |    ---       | ---  | --- |
| car   | 50    |    10        | -100 | 100 |
| metro | 35    |    5         | 60   | -40 |
+-------+-------+--------------+------+-----+
</code></pre>
<p>So only users' with <code>speed</code> &amp; <code>acceleration</code> below the <code>max</code> as well as user's <code>jerk</code> within <code>min-max</code> are selected (or delete rows not satisfying stated conditions).</p>
","['python', 'pandas', 'dataframe']",63343480,"<p>You can use <code>reindex</code>, and then do the msk:</p>
<pre><code>threshold=threshold.reindex(df['mode'])

threshold=threshold.reset_index(drop=True)

msk=(df.acceleration.lt(threshold['acceleration','max']))&amp;\
    (df.speed.lt(threshold['speed','max']))&amp;\
    (df.jerk.ge(threshold['jerk','min'])&amp;\
     df.jerk.le(threshold['jerk','max']))
df[msk]
</code></pre>
<hr />
<p><strong>Details</strong></p>
<p>Taking this threshold dataframe:</p>
<pre><code>threshold=pd.DataFrame({'s':['car','car','metro','metro'],
                        'acceleration':[10,5,5,2],
                       'speed':[50,5,35,2],
                       'jerk':[-100,100,60,-40]})
threshold=threshold.groupby('s').agg({'acceleration':'max',
                                 'speed':'max',
                                 'jerk':['min','max']})

threshold
#      acceleration speed jerk     
#               max   max  min  max
#s                                 
#car             10    50 -100  100
#metro            5    35  -40   60
</code></pre>
<p>You can use <code>'mode'</code> column to make the <code>reindex</code>:</p>
<pre><code>threshold=threshold.reindex(df['mode'])
#      acceleration speed jerk     
#               max   max  min  max
#mode                              
#car             10    50 -100  100
#car             10    50 -100  100
#car             10    50 -100  100
#metro            5    35  -40   60
#metro            5    35  -40   60
#metro            5    35  -40   60

threshold=threshold.reset_index(drop=True)

msk=(df.acceleration.lt(threshold['acceleration','max']))&amp;\
    (df.speed.lt(threshold['speed','max']))&amp;\
    (df.jerk.ge(threshold['jerk','min'])&amp;\
     df.jerk.le(threshold['jerk','max']))

df[msk]
#   user  speed  acceleration  jerk   mode
#0     1     10           9.8    50    car
#3     2     15           4.0    40  metro
</code></pre>
",Delete rows DataFrame satisfying conditions evaluated multiple columns I would like filter DataFrame evaluating conditions several columns DataFrame I illustrate I want following eample df user speed acceleration jerk mode car car car metro metro metro df pd DataFrame dict df df user speed acceleration jerk mode car car car metro metro metro In given example I would like filter dataframe based thresholds set speed acceleration jerk columns table speed acceleration jerk max max min max car metro So users speed amp acceleration max well user jerk within min max selected delete rows satisfying stated conditions,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
63530888,2020-08-21,2020,4,How would I go about creating an .env file for my discord bot token?,"<p>So, I was recently told that just storing the Discord Bot token in a variable at the top is bad practice and a .env file would be better. Can someone explain to me how I would create the .env file with the token in it and import it into my bot.py file?</p>
","['python', 'discord', 'discord.py']",63530919,"<p>You can use a libary/module called <code>python-dotenv</code>, install the library with</p>
<pre><code>pip install python-dotenv
</code></pre>
<p>To use it in your code, you have to import the <code>os</code> module as well as the freshly installed <code>dotenv</code> package</p>
<pre class=""lang-py prettyprint-override""><code>import os
from dotenv import load_dotenv
</code></pre>
<p>At the beginning of your code after the imports you should have <code>load_dotenv()</code> to load the <code>.env</code> file.
Then you can use <code>os.getenv(&quot;DOTENV variablename here&quot;)</code> to get the content of the file.</p>
<p>Instruction List:</p>
<ol>
<li><code>pip install python-dotenv</code>.</li>
<li>Create a file named <code>.env</code> in the root of your project.</li>
<li>Write one line: DISCORD_TOKEN = your token (no quotes needed)</li>
<li>you should have <code>import os</code> and <code>from dotenv import load_dotenv</code> in your code.</li>
<li>Call <code>load_dotenv()</code> at the beginning of your program to load the file.</li>
<li>To get your token, you just have to do <code>os.getenv(&quot;DISCORD_TOKEN&quot;)</code>.</li>
</ol>
<p>Example code:</p>
<pre class=""lang-py prettyprint-override""><code>import os
from dotenv import load_dotenv

load_dotenv()

TOKEN = os.getenv(&quot;DISCORD_TOKEN&quot;)
</code></pre>
<p>Example dotenv file:</p>
<pre><code>DISCORD_TOKEN=this.is.my.token.blah.blah.blah
</code></pre>
",How would I go creating env file discord bot token So I recently told storing Discord Bot token variable top bad practice env file would better Can someone explain I would create env file token import bot py file,"startoftags, python, discord, discordpy, endoftags",python discord discordpy endoftags,python discord discordpy,python discord discordpy,1.0
63646113,2020-08-29,2020,3,Pandas dataframe to table with subheaders (Latex or Image),"<p>I got a pandas dataframe that looks something like this:</p>
<pre><code>|Label      |Metric A |Metric B  |Category |
--------------------------------------------
|model 1    |0.9      |0.7       |Train    |
|model 2    |0.87     |0.8       |Train    |
|model 1    |0.78     |0.6       |Val      |
|model 2    |0.6      |0.66      |Val      |
</code></pre>
<p>and I need to convert it in some form to something like this:</p>
<pre><code>|         |Metric A    |Metric B     |
-------------------------------------
|         |Train|Val   |Train |Val   |
--------------------------------------
|Model 1  |     |      |      |      |
|Model 2  |     |      |      |      |
</code></pre>
<p>(I hope you get what I mean :D)<br />
Is it even possible to convert pd dataframes into such a format?</p>
<p>It is in a scientific context, so I need to have it in latex (I know there is the .to_latex() function, but I dont know how to convert this dataframe in the wanted shape) or I can export it as an image so I can insert it.
(I am also a little bit familiar with R, in case there is a solution in R)<br />
Any help is much appreciated!</p>
<p>Cheers<br />
Sven</p>
","['python', 'pandas', 'dataframe']",63646140,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>DataFrame.set_index</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.unstack.html"" rel=""nofollow noreferrer""><code>DataFrame.unstack</code></a> for <code>MultiIndex in columns</code>:</p>
<pre><code>df1 = df.set_index(['Label','Category']).unstack()
</code></pre>
<p>Or if possible duplicated <code>Label, Category</code> values is possible use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot_table.html"" rel=""nofollow noreferrer""><code>DataFrame.pivot_table</code></a> with aggregation, e.g. <code>mean</code>:</p>
<pre><code>df1 = df.pivot_table(index='Label',columns='Category', aggfunc='mean')
</code></pre>
<hr />
<pre><code>print (df1)
         Metric A       Metric B      
Category    Train   Val    Train   Val
Label                                 
model 1      0.90  0.78      0.7  0.60
model 2      0.87  0.60      0.8  0.66


print (df1.to_latex())
\begin{tabular}{lrrrr}
\toprule
{} &amp; \multicolumn{2}{l}{Metric A} &amp; \multicolumn{2}{l}{Metric B} \\
Category &amp;    Train &amp;   Val &amp;    Train &amp;   Val \\
Label   &amp;          &amp;       &amp;          &amp;       \\
\midrule
model 1 &amp;     0.90 &amp;  0.78 &amp;      0.7 &amp;  0.60 \\
model 2 &amp;     0.87 &amp;  0.60 &amp;      0.8 &amp;  0.66 \\
\bottomrule
\end{tabular}
</code></pre>
",Pandas dataframe table subheaders Latex Image I got pandas dataframe looks something like Label Metric A Metric B Category model Train model Train model Val model Val I need convert form something like Metric A Metric B Train Val Train Val Model Model I hope get I mean D Is even possible convert pd dataframes format It scientific context I need latex I know latex function I dont know convert dataframe wanted shape I export image I insert I also little bit familiar R case solution R Any help much appreciated Cheers Sven,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
63798382,2020-09-08,2020,3,How to restructure daily time series by distinct column values in pandas?,"<p>I have daily time series data by different counties in US, which is covid cases daily time series, I want to restructure/reshape it in order to use it with other time series data that I have. So I tried <code>groupby</code> operation to regroup the time series, but I got value error as follow:</p>
<blockquote>
<p>ValueError: No axis named county_state for object type DataFrame</p>
</blockquote>
<p>I am not sure using <code>groupby</code> is the right move to take. Can anyone suggest possible way of doing this right in pandas? Any idea?</p>
<p><strong>current attempt</strong></p>
<p>Here is the <a href=""https://gist.github.com/jerry-shad/f372d7a05e1a63732665fad0c7c754d0"" rel=""nofollow noreferrer"">reproducible data on gist</a>. Here is the my current attempt:</p>
<pre><code>import pandas as pd

df = pd.read_csv(&quot;df.csv&quot;)
df['date'] = pd.to_datetime(df['date'])
df.groupby('date', 'county_state')['cases', 'deaths'].unstack().reset_index()
</code></pre>
<p>but above attempt is not working, which cause <code>ValueError</code> instead. Can anyone suggest how to make this right?</p>
<p><strong>desired output</strong></p>
<p>Here is structure of output of my expected dataframe, no need to use aggregate by <code>cases</code> or <code>deaths</code>.</p>
<pre><code>    date    fips    cases   deaths  county_state
1/26/2020   4013    1   0   Maricopa_Arizona
1/27/2020   4013    5    0  Maricopa_Arizona
1/28/2020   4013    7    0  Maricopa_Arizona
...         
9/02/202    4013    2333  100     Maricopa_Arizona
1/26/2020   6037    1   0   Los Angeles_California
1/27/2020   6037    15    2  Los Angeles_California
1/28/2020   6037    20    4  Los Angeles_California
...
9/02/202    6037    10001  200     Los Angeles_California
</code></pre>
<p>How to achieve my expected output above? Any way to get this right in pandas?</p>
","['python', 'pandas', 'dataframe']",63798939,"<p>I think this gives you the output you wanted:</p>
<p><code>df.groupby(['fips', 'county_state', 'date']).sum().reset_index()[[&quot;date&quot;, &quot;fips&quot;, &quot;cases&quot;, &quot;deaths&quot;, &quot;county_state&quot;]]</code></p>
<p>EDIT:
The data frame was read in like this:</p>
<p><code>df = pd.read_csv(&quot;https://gist.github.com/jerry-shad/f372d7a05e1a63732665fad0c7c754d0/raw/425b91083519adae00056aac3b25dcb3cfda42bc/df.csv&quot;, sep=&quot;\t&quot;).drop(columns=[&quot;Unnamed: 5&quot;])</code></p>
<p>Results:</p>
<pre><code>date    fips    cases   deaths  county_state
0   1/26/2020   4013    1   0   Maricopa_Arizona
1   1/27/2020   4013    1   0   Maricopa_Arizona
2   1/28/2020   4013    1   0   Maricopa_Arizona
3   1/29/2020   4013    1   0   Maricopa_Arizona
4   1/30/2020   4013    1   0   Maricopa_Arizona
... ... ... ... ... ...
7456    8/7/2020    55081   242 2   Monroe_Wisconsin
7457    8/8/2020    55081   242 2   Monroe_Wisconsin
7458    8/9/2020    55081   243 2   Monroe_Wisconsin
7459    9/1/2020    55081   286 2   Monroe_Wisconsin
7460    9/2/2020    55081   290 2   Monroe_Wisconsin
</code></pre>
",How restructure daily time series distinct column values pandas I daily time series data different counties US covid cases daily time series I want restructure reshape order use time series data I So I tried groupby operation regroup time series I got value error follow ValueError No axis named county state object type DataFrame I sure using groupby right move take Can anyone suggest possible way right pandas Any idea current attempt Here reproducible data gist Here current attempt import pandas pd df pd read csv quot df csv quot df date pd datetime df date df groupby date county state cases deaths unstack reset index attempt working cause ValueError instead Can anyone suggest make right desired output Here structure output expected dataframe need use aggregate cases deaths date fips cases deaths county state Maricopa Arizona Maricopa Arizona Maricopa Arizona Maricopa Arizona Los Angeles California Los Angeles California Los Angeles,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
63800890,2020-09-08,2020,2,How to transfer bs4.element.ResultSet to date/string?,"<p>I want to extract date and summary of an article in a website, here is my code</p>
<pre><code>from bs4 import BeautifulSoup
from selenium import webdriver
full_url = 'https://www.wsj.com/articles/readers-favorite-summer-recipes-11599238648?mod=searchresults&amp;page=1&amp;pos=20'
url0 = full_url
browser0 = webdriver.Chrome('C:/Users/liuzh/Downloads/chromedriver_win32/chromedriver')
browser0.get(url0)


html0 = browser0.page_source
page_soup = BeautifulSoup(html0, 'html5lib')
date = page_soup.find_all(&quot;time&quot;, class_=&quot;timestamp article__timestamp flexbox__flex--1&quot;)
sub_head = page_soup.find_all(&quot;h2&quot;, class_=&quot;sub-head&quot;)
print(date)
print(sub_head)
</code></pre>
<p>I got the following result, how can I obtain the standard form ?(e.g. Sept. 4, 2020 12:57 pm ET; This Labor Day weekend, weâre...)</p>
<pre><code>[&lt;time class=&quot;timestamp article__timestamp flexbox__flex--1&quot;&gt;
        Sept. 4, 2020 12:57 pm ET
      &lt;/time&gt;]
[&lt;h2 class=&quot;sub-head&quot; itemprop=&quot;description&quot;&gt;This Labor Day weekend, weâre savoring the last of summer with a collection of seasonal recipes shared by Wall Street Journal readers. Each one comes with a story about what this food means to a family and why they return to it each year.&lt;/h2&gt;]
</code></pre>
<p>Thanks.</p>
","['python', 'web-scraping', 'beautifulsoup']",63801262,"<p>Try something like:</p>
<pre><code>for d in date:
   print(d.text.strip())
</code></pre>
<p>Given your sample html, output should be:</p>
<pre><code>Sept. 4, 2020 12:57 pm ET
</code></pre>
",How transfer bs element ResultSet date string I want extract date summary article website code bs import BeautifulSoup selenium import webdriver full url https www wsj com articles readers favorite summer recipes mod searchresults amp page amp pos url full url browser webdriver Chrome C Users liuzh Downloads chromedriver win chromedriver browser get url html browser page source page soup BeautifulSoup html html lib date page soup find quot time quot class quot timestamp article timestamp flexbox flex quot sub head page soup find quot h quot class quot sub head quot print date print sub head I got following result I obtain standard form e g Sept pm ET This Labor Day weekend lt time class quot timestamp article timestamp flexbox flex quot gt Sept pm ET lt time gt lt h class quot sub head quot itemprop quot description quot gt This Labor Day weekend savoring last summer,"startoftags, python, webscraping, beautifulsoup, endoftags",python webscraping beautifulsoup endoftags,python webscraping beautifulsoup,python webscraping beautifulsoup,1.0
64092356,2020-09-27,2020,5,Pandas dataframe - how to eliminate duplicate words in a column,"<p>I have a pandas dataframe:</p>
<pre><code>import pandas as pd

df = pd.DataFrame({'category':[0,1,2],
                   'text': ['this is some text for the first row',
                            'second row has this text',
                            'third row this is the text']})
df.head()
</code></pre>
<p>I would like to get the following result (without words repeating in each row):</p>
<p>Expected result (for the example above):</p>
<pre><code>category     text
0            is some for the first
1            second has
2            third is the
</code></pre>
<p>With the following code I tried to get all data in rows to a string:</p>
<pre><code>final_list =[]
for index, rows in df.iterrows():
    # Create list for the current row
    my_list =rows.text
    # append the list to the final list
    final_list.append(my_list)
# Print the list
print(final_list)
text=''

for i in range(len(final_list)):
    text+=final_list[i]+', '

print(text)
</code></pre>
<p>The idea in this question (<a href=""https://stackoverflow.com/questions/64086312/pandas-dataframe-how-to-find-words-that-repeat-in-each-row"">pandas dataframe- how to find words that repeat in each row</a>) does not help me to get the expected result.</p>
<pre><code>arr = [set(x.split()) for x in text.split(',')]
mutual_words = set.intersection(*arr)
result = [list(x.difference(mutual_words)) for x in arr]
result = sum(result, [])
final_text = (&quot;, &quot;).join(result)
print(final_text)
</code></pre>
<p>Does anyone have an idea how to get it?</p>
","['python', 'pandas', 'dataframe']",64092587,"<p>You can use <code>Series.str.split</code> to split the column <code>text</code> around the delimiter space then use <code>reduce</code> to get the intersection of the words found in all the rows, finally use <code>str.replace</code> to remove the common words:</p>
<pre><code>from functools import reduce

w = reduce(lambda x, y: set(x) &amp; set(y), df['text'].str.split())
df['text'] = df['text'].str.replace(rf&quot;(\s*)(?:{'|'.join(w)})\s*&quot;, r'\1').str.strip()
</code></pre>
<hr />
<pre><code>   category                    text
0         0   is some for the first
1         1              second has
2         2            third is the 
</code></pre>
",Pandas dataframe eliminate duplicate words column I pandas dataframe import pandas pd df pd DataFrame category text text first row second row text third row text df head I would like get following result without words repeating row Expected result example category text first second third With following code I tried get data rows string final list index rows df iterrows Create list current row list rows text append list final list final list append list Print list print final list text range len final list text final list print text The idea question pandas dataframe find words repeat row help get expected result arr set x split x text split mutual words set intersection arr result list x difference mutual words x arr result sum result final text quot quot join result print final text Does anyone idea get,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
64544598,2020-10-26,2020,2,How to &#39;key off&#39; matching pairs in DataFrame columns,"<p>Say I have a df:</p>
<pre><code>src    dest
LV       NC
LA       NY
NC       LV
NY       LA
</code></pre>
<p>how can 'key off' unique src/dest pairs so that they can be referenced to in a new column like so?</p>
<pre><code>src    dest     pair
LV       NC       1
LA       NY       2
NC       LV       1
NY       LA       2

df = pd.DataFrame({'src':['LV','LA','NC','NY'], 'dest':['NC', 'NY', 'LV', 'LA']})
</code></pre>
","['python', 'pandas', 'dataframe']",64544639,"<p>Try with <code>tuple</code> + <code>set</code> then <code>factorize</code></p>
<pre><code>df['pair'] = df.apply(lambda x : tuple(set(x)),1).factorize()[0]+1
Out[69]: array([1, 2, 1, 2], dtype=int64)
</code></pre>
",How key matching pairs DataFrame columns Say I df src dest LV NC LA NY NC LV NY LA key unique src dest pairs referenced new column like src dest pair LV NC LA NY NC LV NY LA df pd DataFrame src LV LA NC NY dest NC NY LV LA,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
64606040,2020-10-30,2020,2,Pandas add missing weeks from range to dataframe,"<p>I am computing a DataFrame with weekly amounts and now I need to fill it with missing weeks from a provided date range.</p>
<p>This is how I'm generating the dataframe with the weekly amounts:</p>
<pre><code>df['date'] = pd.to_datetime(df['date']) - timedelta(days=6)
weekly_data: pd.DataFrame = (df
                             .groupby([pd.Grouper(key='date', freq='W-SUN')])[data_type]
                             .sum()
                             .reset_index()
                            )
</code></pre>
<p>Which outputs:</p>
<pre><code>        date    sum
0 2020-10-11     78
1 2020-10-18    673
</code></pre>
<p>If a date range is given as <code>start='2020-08-30'</code> and <code>end='2020-10-30'</code>, then I would expect the following dataframe:</p>
<pre><code>        date    sum
0 2020-08-30    0.0
1 2020-09-06    0.0
2 2020-09-13    0.0
3 2020-09-20    0.0
4 2020-09-27    0.0
5 2020-10-04    0.0
6 2020-10-11     78
7 2020-10-18    673
8 2020-10-25    0.0
</code></pre>
<p>So far, I have managed to just add the missing weeks and set the sum to 0, but it also replaces the existing values:</p>
<pre><code>weekly_data = weekly_data.reindex(pd.date_range('2020-08-30', '2020-10-30', freq='W-SUN')).fillna(0)
</code></pre>
<p>Which outputs:</p>
<pre><code>        date    sum
0 2020-08-30    0.0
1 2020-09-06    0.0
2 2020-09-13    0.0
3 2020-09-20    0.0
4 2020-09-27    0.0
5 2020-10-04    0.0
6 2020-10-11    0.0 # should be 78
7 2020-10-18    0.0 # should be 673
8 2020-10-25    0.0
</code></pre>
","['python', 'pandas', 'dataframe']",64606061,"<p>Remove <code>reset_index</code> for <code>DatetimeIndex</code>, because <code>reindex</code> working with index and if <code>RangeIndex</code> get <code>0</code> values, because no match:</p>
<pre><code>weekly_data = (df.groupby([pd.Grouper(key='date', freq='W-SUN')])[data_type]
                 .sum()
              )
</code></pre>
<p>Then is possible use <code>fill_value=0</code> parameter and last add <code>reset_index</code>:</p>
<pre><code>r = pd.date_range('2020-08-30', '2020-10-30', freq='W-SUN', name='date')
weekly_data = weekly_data.reindex(r, fill_value=0).reset_index()
print (weekly_data)
        date  sum
0 2020-08-30    0
1 2020-09-06    0
2 2020-09-13    0
3 2020-09-20    0
4 2020-09-27    0
5 2020-10-04    0
6 2020-10-11   78
7 2020-10-18  673
8 2020-10-25    0
</code></pre>
",Pandas add missing weeks range dataframe I computing DataFrame weekly amounts I need fill missing weeks provided date range This I generating dataframe weekly amounts df date pd datetime df date timedelta days weekly data pd DataFrame df groupby pd Grouper key date freq W SUN data type sum reset index Which outputs date sum If date range given start end I would expect following dataframe date sum So far I managed add missing weeks set sum also replaces existing values weekly data weekly data reindex pd date range freq W SUN fillna Which outputs date sum,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
64620587,2020-10-31,2020,2,Print beauty dataframe in Jupyter Notebook,"<p>I would like to beautify my print output for a dataframe.</p>
<pre><code> d = {'Carid ': [1, 2, 3], 'Carname': ['Mercedes-Benz', 'Audi', 'BMW'], 'model': ['S-Klasse AMG 63s', 'S6', 'X6 M-Power']}
df = pd.DataFrame(data=d)
print(df.head())
df.head()
</code></pre>
<p><a href=""https://i.stack.imgur.com/YmfIK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YmfIK.png"" alt=""enter image description here"" /></a></p>
<p>As you can see the <code>print</code>-outpot not beauty. The last statement of <code>df.head()</code> is beauty.
Is there any option to get the same result in the <code>print</code>-statement in Jupyter Notebook?</p>
","['python', 'pandas', 'dataframe']",64620616,"<p>Use display instead of print.</p>
<pre><code>display(df.head())
</code></pre>
",Print beauty dataframe Jupyter Notebook I would like beautify print output dataframe Carid Carname Mercedes Benz Audi BMW model S Klasse AMG S X M Power df pd DataFrame data print df head df head As see print outpot beauty The last statement df head beauty Is option get result print statement Jupyter Notebook,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
64682028,2020-11-04,2020,2,&#39;numpy.float64&#39; object has no attribute &#39;fillna&#39; when filling NaN,"<p>I am trying to fill one missing value with an exact number.
I have a covid data set and in Honk Kong there are some columns which have missing values, however I just <strong>one</strong> NaN value to be filled.</p>
<pre><code>df = pd.read_csv(&quot;https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv&quot;)

df= df[df['date']=='2020-10-27']
df

df[df.location=='Hong Kong']  


value = 5300


##22286 --&gt; it is the index where HK 

df.loc[22286]['total_cases'] = df.loc[22286]['total_cases'].fillna(value)

</code></pre>
<p>I get the following error:</p>
<p><strong>AttributeError: 'numpy.float64' object has no attribute 'fillna'</strong></p>
<p>Then I tried to convert the value into float:</p>
<pre><code>value= float(value)

</code></pre>
<p>However I get the following message :</p>
<p>**A value is trying to be set on a copy of a slice from a DataFrame</p>
<p>See the caveats in the documentation: <a href=""http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy</a>
&quot;&quot;&quot;Entry point for launching an IPython kernel.**</p>
","['python', 'pandas', 'dataframe']",64682134,"<p>Use <code>at</code> here:</p>
<pre><code>df.at[22286, 'total_cases'] = value
print(df.loc[22286]['total_cases'])

5300.0
</code></pre>
",numpy float object attribute fillna filling NaN I trying fill one missing value exact number I covid data set Honk Kong columns missing values however I one NaN value filled df pd read csv quot https raw githubusercontent com owid covid data master public data owid covid data csv quot df df df date df df df location Hong Kong value gt index HK df loc total cases df loc total cases fillna value I get following error AttributeError numpy float object attribute fillna Then I tried convert value float value float value However I get following message A value trying set copy slice DataFrame See caveats documentation http pandas pydata org pandas docs stable user guide indexing html returning view versus copy quot quot quot Entry point launching IPython kernel,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
64905007,2020-11-19,2020,4,Pandas pivot or reshape dataframe with NaN,"<p>I have this dataframe that i need to pivot or reshape based on the <code>frame</code> col</p>
<p><code>df = {'frame': {0: 0, 1: 1, 2: 2, 3: 0, 4: 1, 5: 2}, 'pvol': {0: nan, 1: nan, 2: nan, 3: 23.1, 4: 24.3, 5: 25.6}, 'vvol': {0: 109.8, 1: 140.5, 2: 160.4, 3: nan, 4: nan, 5: nan}, 'area': {0: 120, 1: 130, 2: 140, 3: 110, 4: 110, 5: 112}, 'label': {0: 'v', 1: 'v', 2: 'v', 3: 'p', 4: 'p', 5: 'p'}}</code></p>
<p>Current dataframe</p>
<pre><code>frame   pvol    vvol    area    label
0       NaN     109.8   120     v
1       NaN     140.5   130     v
2       NaN     160.4   140     v
0       23.1    NaN     110     p
1       24.3    NaN     110     p
2       25.6    NaN     112     p
</code></pre>
<p>Expected output</p>
<pre><code>frame   pvol    vvol    v_area  p_area
0       23.1    109.8   110     110
1       24.3    140.5   110     110
2       25.6    160.4   112     112
</code></pre>
<p>The prefix <code>v</code> and <code>p</code> aren't necessary I just need a way to tell the columns apart</p>
<p>This is how I got it to work, but it seems long. I'm sure there is a better way</p>
<pre><code>for name, tdf in df.groupby('label'):
        df.loc[tdf.index, '{}_area'.format(name)] = tdf['area']

pdf = df[df['label'].eq('p')][['frame', 'label', 'pvol', 'p_area']]
vdf = df[df['label'].eq('v')][['frame', 'vvol', 'v_area']]
df = pdf.merge(vdf, on='frame', how='outer')
</code></pre>
","['python', 'pandas', 'dataframe']",64905046,"<p>Let us try</p>
<pre><code>s = df.set_index(['frame','label']).unstack().dropna(axis=1)
s.columns=s.columns.map('_'.join)
s
Out[102]: 
       pvol_p  vvol_v  area_p  area_v
frame                                
0        23.1   109.8     110     120
1        24.3   140.5     110     130
2        25.6   160.4     112     140
</code></pre>
",Pandas pivot reshape dataframe NaN I dataframe need pivot reshape based frame col df frame pvol nan nan nan vvol nan nan nan area label v v v p p p Current dataframe frame pvol vvol area label NaN v NaN v NaN v NaN p NaN p NaN p Expected output frame pvol vvol v area p area The prefix v p necessary I need way tell columns apart This I got work seems long I sure better way name tdf df groupby label df loc tdf index area format name tdf area pdf df df label eq p frame label pvol p area vdf df df label eq v frame vvol v area df pdf merge vdf frame outer,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
64981401,2020-11-24,2020,2,Python regex to pick all elements that don&#39;t match pattern,"<p>I asked a similar question yesterday <a href=""https://stackoverflow.com/questions/64958329/keep-elements-with-pattern-in-pandas-series-without-converting-them-to-list/64959197?noredirect=1#comment114852920_64959197"">Keep elements with pattern in pandas series without converting them to list</a> and now I am faced with the opposite problem.</p>
<p>I have a pandas dataframe:</p>
<pre><code>import pandas as pd
df = pd.DataFrame([&quot;Air type:1, Space kind:2, water, wood&quot;, &quot;berries, something at the start:4, Space blu:3, somethingelse&quot;], columns = ['A'])
</code></pre>
<p>and I want to pick all elements that don't have a &quot;:&quot; in them.
What I tried is the following regex which seems to be working:</p>
<pre><code>df['new'] = df.A.str.findall('(^|\s)([^:,]+)(,|$)')
    A                                                               new
0   Air type:1, Space kind:2, water, wood                           [( , water, ,), ( , wood, )]
1   berries, something at the start:4, Space blu:3, somethingelse   [(, berries, ,), ( , somethingelse, )]
</code></pre>
<p>If I understand this correctly, findall searched for 3 patterns (the ones that I have in parenthesis) and returned as many as it could find in tuples wrapped in a list.
Is there a way to avoid this and simply return only the middle pattern?
As in for the first row: water, wood
for the second row: berries, somethingelse</p>
<p>I also tried the opposite approach:</p>
<pre><code>df.A.str.replace('[^\s,][^:,]+:[^:,]+', '').str.replace('\s*,', '')
</code></pre>
<p>which seems to be working close to what I want (only the commas between the patterns are missing) but I am wondering if I am missing something that would make my life easier.</p>
","['python', 'regex', 'pandas']",64981560,"<p>You may use this regex code:</p>
<pre><code>&gt;&gt;&gt; df['new'] = df.A.str.findall(r'(?:^|,)([^:,]+)(?=,|$)')
&gt;&gt;&gt; print (df)
                                                   A                        new
0              Air type:1, Space kind:2, water, wood            [ water,  wood]
1  berries, something at the start:4, Space blu:3...  [berries,  somethingelse]
</code></pre>
<p>Regex used is:</p>
<p><code>(?:^|,)</code>: Match start or comma</p>
<ul>
<li><code>([^:,]+)</code>: Match 1+ of any character that is not a <code>:</code> and not a <code>,</code></li>
<li><code>(?=,|$)</code>: Lookahead to assert that we have either a <code>,</code> or end of line ahead</li>
</ul>
",Python regex pick elements match pattern I asked similar question yesterday Keep elements pattern pandas series without converting list I faced opposite problem I pandas dataframe import pandas pd df pd DataFrame quot Air type Space kind water wood quot quot berries something start Space blu somethingelse quot columns A I want pick elements quot quot What I tried following regex seems working df new df A str findall A new Air type Space kind water wood water wood berries something start Space blu somethingelse berries somethingelse If I understand correctly findall searched patterns ones I parenthesis returned many could find tuples wrapped list Is way avoid simply return middle pattern As first row water wood second row berries somethingelse I also tried opposite approach df A str replace str replace seems working close I want commas patterns missing I wondering I missing something would make life easier,"startoftags, python, regex, pandas, endoftags",python arrays numpy endoftags,python regex pandas,python arrays numpy,0.33
65139983,2020-12-04,2020,2,python how do I perform the below operation in dataframe,"<pre><code>df1 = pd.DataFrame({
                   'Year': [&quot;1A&quot;, &quot;2A&quot;, &quot;3A&quot;, &quot;4A&quot;, &quot;5A&quot;],
                    'Tval1' : [1, 9, 8, 1, 6],
                    'Tval2' : [34, 56, 67, 78, 89]
})
</code></pre>
<p>it looks more like this</p>
<p><a href=""https://i.stack.imgur.com/sGlUw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sGlUw.png"" alt=""enter image description here"" /></a></p>
<p>and I want to change it to make it look like this, the 2nd column is moved under the individual row.</p>
<p><a href=""https://i.stack.imgur.com/CEEfW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CEEfW.png"" alt=""enter image description here"" /></a></p>
","['python', 'pandas', 'dataframe']",65140307,"<p>Idea is get numbers from <code>Year</code> column, then set new columns names after <code>Year</code> column and reshape by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html"" rel=""nofollow noreferrer""><code>DataFrame.stack</code></a>:</p>
<pre><code>df1['Year'] = df1['Year'].str.extract('(\d+)')
df = df1.set_index('Year')

#add letters by length of columns, working for 1 to 26 columns A-Z
import string

df.columns = list(string.ascii_uppercase[:len(df.columns)])

#here working same like
#df.columns = ['A','B']

df = df.stack().reset_index(name='Val')
df['Year'] = df['Year'] + df.pop('level_1')
print (df)
  Year  Val
0   1A    1
1   1B   34
2   2A    9
3   2B   56
4   3A    8
5   3B   67
6   4A    1
7   4B   78
8   5A    6
9   5B   89
</code></pre>
<p>Another idea with <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.melt.html"" rel=""nofollow noreferrer""><code>DataFrame.melt</code></a>:</p>
<pre><code>df = (df1.replace({'Year': {'A':''}}, regex=True)
        .rename(columns={'Tval1':'A','Tval2':'B'})
        .melt('Year'))
df['Year'] = df['Year'] + df.pop('variable')
print (df)
  Year  value
0   1A      1
1   2A      9
2   3A      8
3   4A      1
4   5A      6
5   1B     34
6   2B     56
7   3B     67
8   4B     78
9   5B     89
</code></pre>
",python I perform operation dataframe df pd DataFrame Year quot A quot quot A quot quot A quot quot A quot quot A quot Tval Tval looks like I want change make look like nd column moved individual row,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
65202651,2020-12-08,2020,2,Python : Most efficient way to count elements of long list 1 in long list 2 ? (list comprehension is really slow),"<p>I have many tuples (e.g. <code>(0, 1)</code>) in two pretty long lists <code>list_0</code>and <code>list_1</code>of size ~40k elements. I need to count the tuples of <code>list_0</code> also in <code>list_1</code>.</p>
<p>The following statement with list comprehension takes ~ 1 min and I need to do this multiple times, so I am looking for something more efficient :</p>
<pre><code>len([element for element in list_0 if element in list_1])
</code></pre>
<p>What could be more efficient ?</p>
<p>To reproduce :</p>
<pre><code>elements_0 = 200*[0]+50*[1]+150*[2]
elements_1 = 100*[1]+150*[2]+150*[1]
df = pd.DataFrame(data=[list(elements_0), list(elements_1)]).T
list_0 = [item for sublist in df.groupby(0)[0].apply(lambda x: list(combinations(x.index, 2))) for item in sublist]
list_1 = [item for sublist in df.groupby(1)[1].apply(lambda x: list(combinations(x.index, 2))) for item in sublist]

print(len([pair for pair in list_0 if pair in list_1])) # Long
</code></pre>
","['python', 'pandas', 'numpy']",65202691,"<p>It looks like you can use</p>
<pre><code>pd.Series(list_0).isin(list_1).sum()
</code></pre>
<p>Output:</p>
<pre><code>22300
CPU times: user 14.8 ms, sys: 20 Âµs, total: 14.8 ms
Wall time: 14.1 ms
</code></pre>
<p>which should give the same answer with:</p>
<pre><code>len([element for element in list_0 if element in list_1])
</code></pre>
<p>which gives:</p>
<pre><code>22300
CPU times: user 13.8 s, sys: 0 ns, total: 13.8 s
Wall time: 13.8 s
</code></pre>
<p>Also <code>merge</code> and query:</p>
<pre><code>s = df.reset_index()
print(len(s.merge(s, on=[0,1])
  .query('index_x &gt; index_y')
))
</code></pre>
<p>with output:</p>
<pre><code>22300
CPU times: user 13.4 ms, sys: 15 Âµs, total: 13.4 ms
Wall time: 12.3 ms
</code></pre>
",Python Most efficient way count elements long list long list list comprehension really slow I many tuples e g two pretty long lists list list size k elements I need count tuples list also list The following statement list comprehension takes min I need multiple times I looking something efficient len element element list element list What could efficient To reproduce elements elements df pd DataFrame data list elements list elements T list item sublist df groupby apply lambda x list combinations x index item sublist list item sublist df groupby apply lambda x list combinations x index item sublist print len pair pair list pair list Long,"startoftags, python, pandas, numpy, endoftags",python python3x list endoftags,python pandas numpy,python python3x list,0.33
65293940,2020-12-14,2020,2,Python Pandas Dataframe storing next row values,"<p>I am currently new to python data frames and trying to iterate over rows. I want to be able to get values of next 2 rows and store it in the variable. Following is code snippet:</p>
<pre><code>df = pd.read_csv('Input.csv')
for index, row in df.iterrows():
    row_1 = row['Word']
    row_2 = row['Word'] + 1 # I know this is incorrect and it won't work
    row_3 = row['Word'] + 2 # I know this is incorrect and it won't work
    print(row_1, row_2, row_3)
</code></pre>
<p>I was hoping that given ('Input.csv'):</p>
<pre><code>Word &lt;- #Column
 Hi
 Hello
 Some
 Phone
 keys
 motion
 entries
</code></pre>
<p>I want the output as following:</p>
<pre><code>Hi, Hello, Some
Hello, Some, Phone
Some, Phone, keys
Phone, keys, motion
keys, motion, entries
</code></pre>
<p>Any help is appreciated. Thank you.</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",65294368,"<p>you can simply use <code>iloc</code> property</p>
<pre><code>df = pd.read_csv('Input.csv')
for index, row in df.iterrows():
    row_1 = df.iloc[index]['word']
    row_2 = df.iloc[index + 1]['word']
    row_3 = df.iloc[index + 2]['word'] 
    print(row_1, row_2, row_3)
</code></pre>
",Python Pandas Dataframe storing next row values I currently new python data frames trying iterate rows I want able get values next rows store variable Following code snippet df pd read csv Input csv index row df iterrows row row Word row row Word I know incorrect work row row Word I know incorrect work print row row row I hoping given Input csv Word lt Column Hi Hello Some Phone keys motion entries I want output following Hi Hello Some Hello Some Phone Some Phone keys Phone keys motion keys motion entries Any help appreciated Thank,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
65330838,2020-12-16,2020,2,Can&#39;t get rid of column labels with pandas iloc,"<p>I have a large csv(15 rows and 2500 columns) and am trying to compare values in each row with the row above. To do this I split up each row into it's own dataframe with <code>iloc</code> from pandas. So now I have 15 dataframes that I can trying to compare to one another using <code>compare()</code>. The issue is I keep getting the error <code>Can only compare identically-labeled DataFrame objects</code> but when putting each dataframe into a csv they are all labeled the same thing so I figured maybe if I got rid of the labels that would work and tried <a href=""https://stackoverflow.com/questions/53036910/pandas-remove-the-label-of-the-column-index"">this</a> and that didn't work so I tried sorting the indexes as show <a href=""https://stackoverflow.com/questions/18548370/pandas-can-only-compare-identically-labeled-dataframe-objects-error"">here</a> and I still get the same error. The CSV is filled mostly with floats and occasional NaN values for what that's worth.</p>
<p>I split it up with <code>iloc</code> using <code>df_i = df.iloc[[i]]</code> where <code>i</code> is replaced with 1-14 to get each of the rows as a dataframe.</p>
<p>Printing the dataframes gives me this output:</p>
<pre><code>                        TIME  EVENT  Unld1Comp1Circ2_Dout.Val  ...  WorkingHours.Start_Count_14.Cnt  WorkingHours.Start_Count_15.Cnt  WorkingHours.Start_Count_16.Cnt
1  2020-12-15T17:23:55+01:00    NaN                         1  ...                                0                                0                                0

[1 rows x 2463 columns] 
                         TIME  EVENT  Unld1Comp1Circ2_Dout.Val  ...  WorkingHours.Start_Count_14.Cnt  WorkingHours.Start_Count_15.Cnt  WorkingHours.Start_Count_16.Cnt
2  2020-12-15T17:24:13+01:00    NaN                         1  ...                                0                                0                                0

[1 rows x 2463 columns]

</code></pre>
","['python', 'pandas', 'dataframe']",65333805,"<p>There are a couple of ways to find out the difference between two rows in a dataframe.</p>
<p><strong>Option 1:</strong></p>
<pre><code>for col in df.columns[2:]: #check column by column from 3rd column thru the end

    if df[col].nunique() &gt; 1:
        print (col, df[col].unique())
</code></pre>
<p>This will print all the columns that have different values. If <code>nunique()</code> is more than 1, then there is more than one value in that column. However, this will NOT tell you which row has the difference.</p>
<p>Option 2:</p>
<p>An alternate way is to do a <code>df[col].shift()</code> and compare against the previous row. If there is a difference, then keep tab of that. Do the same comparison for each row. Consolidate all the differences and you will get a list of all rows that have at least one value different between the next row.</p>
<p>To do this, you can do as follows:</p>
<pre><code>import pandas as pd
df = pd.DataFrame({'col1':[1,1,1,1,1,1,1,1],
                   'col2':[2,2,2,2,2,2,2,2],
                   'col3':[3,3,3,3,3,3,3,4],
                   'col4':[4,4,4,4,4,4,5,4],
                   'col5':[5,5,5,5,5,1,5,5]})
print (df)

df['Differs'] = False #set all rows to matched 

for col in df.columns[2:]: #check column by column from 3rd column thru the end

    #if df[col].nunique() &gt; 1:
        #print (col, df[col].unique())

    df['newcol1'] = df[col].shift() != df[col] #check against next row. True if differs
    df.loc[:0,'newcol1'] = False # Tweak the first row as it should be ignored
    
    df.loc[df['newcol1'] == True,'Differs'] = True #if any row matched, set Differs to True

print (df[df['Differs']]) #print all rows that has a different value in at least one column 
</code></pre>
<p>In the above example, rows 3, 4, 5 have at least one value different from the previous row.</p>
<p>For the below given dataframe:</p>
<pre><code>   col1  col2  col3  col4  col5
0     1     2     3     4     5
1     1     2     3     4     5
2     1     2     3     4     5
3     1     2     3     4     5
4     1     2     3     4     5
5     1     2     3     4     1
6     1     2     3     5     5
7     1     2     4     4     5
</code></pre>
<p>The output will be:</p>
<pre><code>   col1  col2  col3  col4  col5  Differs  newcol1
5     1     2     3     4     1     True     True
6     1     2     3     5     5     True    False
7     1     2     4     4     5     True    False
</code></pre>
<p>Using the two techniques, I ran the comparison.</p>
<p><strong>Option 1 result:</strong></p>
<p>All these columns have more than one value. The values are in the list next to column name.</p>
<pre><code>AFreezeSetP [-39.666669 -39.333334]
AFreezeUserT_1K [23 19]
UserPmp1_On [0 1]
RunTempRegKp [3.111111 3.444445]
RunTempRegTi [399 398]
RunTempRegTd [99 96]
RegSetP [-27.333334 -26.888891]
CoolSetP [-27.333334 -26.888891]
AFreezeUserDiff [29.833334 29.777778]
AFreezeDiff [2.       1.944445]
W_OutTempUserPrb.Val [-24.488002 -24.478   ]
DscgP_Prb_Circ1.Val [-4.287679 -4.291988]
W_OutTempUser [-24.488002 -24.478   ]
DscgP_Circ1 [-4.287679 -4.291988]
SuctTempCirc1 [-174.8 -174.7]
RegTypStartup [1 2]
RegTypRun [1 0]
SuctSH_Circ1 [225.2 225.3]
UserPmp1_Dout.Val [0 1]
UserPmp1_Aout.Val [  0. 100.]
UserPmp1HrsThrsh [4377 4378]
HiW_TempStartupDT [59 55]
HiW_TempRunDT [181 186]
HiW_TempOfs [11.166667 11.444445]
DscgP_Circ2 [-4.287679 -4.291988]
SuctTempCirc2 [-174.8 -174.7]
DscgP_Prb_Circ2.Val [-4.287679 -4.291988]
SuctSH_Circ2 [225.2 225.3]
WorkingHours.UserPmp1Starts [0 1]
W_UserTempReg [-24.488002 -24.478    -78.805   ]
At_SP_Dout.Val [0 1]
SonicDensitySensor.SonicDensity_1.EnSensor [0 1]
</code></pre>
<p><strong>Option 2</strong> result is all rows except row #1. That tells me there are at least one value different between each row (1 thru 15). You can tweak my code to find out specific rows and columns that differ for each column.</p>
<pre><code>                         TIME  EVENT  ...  Differs  newcol1
1   2020-12-15T17:23:55+01:00    NaN  ...     True     True
2   2020-12-15T17:24:13+01:00    NaN  ...     True    False
3   2020-12-15T17:24:24+01:00    NaN  ...     True    False
4   2020-12-15T17:24:26+01:00    NaN  ...     True    False
5   2020-12-15T17:24:29+01:00    NaN  ...     True    False
6   2020-12-15T17:24:32+01:00    NaN  ...     True    False
7   2020-12-15T17:24:35+01:00    NaN  ...     True    False
8   2020-12-15T17:24:40+01:00    NaN  ...     True    False
9   2020-12-15T17:24:42+01:00    NaN  ...     True    False
10  2020-12-15T17:24:43+01:00    NaN  ...     True    False
11  2020-12-15T17:24:53+01:00    NaN  ...     True    False
12  2020-12-15T17:24:55+01:00    NaN  ...     True    False
13  2020-12-15T17:25:01+01:00    NaN  ...     True    False
14  2020-12-15T17:25:02+01:00    NaN  ...     True    False
</code></pre>
",Can get rid column labels pandas iloc I large csv rows columns trying compare values row row To I split row dataframe iloc pandas So I dataframes I trying compare one another using compare The issue I keep getting error Can compare identically labeled DataFrame objects putting dataframe csv labeled thing I figured maybe I got rid labels would work tried work I tried sorting indexes show I still get error The CSV filled mostly floats occasional NaN values worth I split iloc using df df iloc replaced get rows dataframe Printing dataframes gives output TIME EVENT Unld Comp Circ Dout Val WorkingHours Start Count Cnt WorkingHours Start Count Cnt WorkingHours Start Count Cnt T NaN rows x columns TIME EVENT Unld Comp Circ Dout Val WorkingHours Start Count Cnt WorkingHours Start Count Cnt WorkingHours Start Count Cnt T NaN rows x columns,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
65725800,2021-01-14,2021,2,Movement code not working in pygame 2.0.1,"<p>I am making a simple game in pygame. Everything seems to be running correctly and there are no errors, but the code to move the sprite doesn't seem to work. If I set the sprite to move automatically it works fine, so I think it's a problem with the key press detection system. Here's the main game loop:</p>
<pre><code>running = True
while running:
    # basic game function checks
    pygame.display.flip()
    clock.tick(60)
    movementtest(testX, testY)

    # checking if the game has been closed
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            running = False
        if event.type == pygame.KEYDOWN:
            if event.key == pygame.K_ESCAPE:
                running = False

    # movement code
    if movingleft:
        testX -= testX
    if movingright:
        testX += testX
    if movingup:
        testY += testY
    if movingdown:
        testY -= testY
    for event in pygame.event.get():
        if event.type == pygame.KEYDOWN:
            if event.key == pygame.K_a:
                if not movingright:
                    movingleft = True
            if event.key == pygame.K_d:
                if not movingleft:
                    movingright = True
            if event.key == pygame.K_w:
                if not movingdown:
                    movingup = True
            if event.key == pygame.K_s:
                if not movingup:
                    movingdown = True
        if event.type == pygame.KEYUP:
            if event.key == pygame.K_a:
                movingleft = False
            if event.key == pygame.K_d:
                movingright = False
            if event.key == pygame.K_w:
                movingup = False
            if event.key == pygame.K_s:
                movingdown = False
</code></pre>
<p>Why isn't it detecting my key inputs? I will post the pre-loop code if needed.</p>
","['python', 'python-3.x', 'pygame']",65725845,"<p><a href=""https://www.pygame.org/docs/ref/event.html#pygame.event.get"" rel=""nofollow noreferrer""><code>pygame.event.get()</code></a> get all the messages and remove them from the queue. See the documentation:</p>
<blockquote>
<p>This will get all the messages and remove them from the queue. [...]</p>
</blockquote>
<p>If <code>pygame.event.get()</code> is called in multiple event loops, only one loop receives the events, but never all loops receive all events. As a result, some events appear to be missed.</p>
<p>Either implement a single event loop or get the events once per frame and use them in multiple loops:</p>
<pre class=""lang-py prettyprint-override""><code>running = True
while running:
    # [...]

    event_list = pygame.event.get():

    # checking if the game has been closed
    for event in event_list :
        if event.type == pygame.QUIT:
            running = False
        # [...]

    # [...]

    for event in event_list:
        # [...]
</code></pre>
<hr />
<p>Additionally, you need to change the player's position through constants, not the position itself:</p>
<pre class=""lang-py prettyprint-override""><code>running = True
while running:
    # [...]

    if movingleft:
        testX -= 1
    if movingright:
        testX += 1
    if movingup:
        testY -= 1
    if movingdown:
        testY += 1 
</code></pre>
<p>However, the code can be simplified by using <code>pygame.key.get_pressed()</code> instead of the key board events.</p>
<p>The keyboard events (see <a href=""https://www.pygame.org/docs/ref/event.html"" rel=""nofollow noreferrer"">pygame.event</a> module) occur only once when the state of a key changes. The <code>KEYDOWN</code> event occurs once every time a key is pressed. <code>KEYUP</code> occurs once every time a key is released. Use the keyboard events for a single action or a step-by-step movement.</p>
<p><a href=""https://www.pygame.org/docs/ref/key.html#pygame.key.get_pressed"" rel=""nofollow noreferrer""><code>pygame.key.get_pressed()</code></a> returns a list with the state of each key. If a key is held down, the state for the key is <code>True</code>, otherwise <code>False</code>. Use <a href=""https://www.pygame.org/docs/ref/key.html#pygame.key.get_pressed"" rel=""nofollow noreferrer""><code>pygame.key.get_pressed()</code></a> to evaluate the current state of a button and get continuous movement:</p>
<pre class=""lang-py prettyprint-override""><code>running = True
while running:
    # basic game function checks
    pygame.display.flip()
    clock.tick(60)
    movementtest(testX, testY)

    # checking if the game has been closed
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            running = False
        if event.type == pygame.KEYDOWN:
            if event.key == pygame.K_ESCAPE:
                running = False

    # movement code
    keys = pygame.key.get_pressed()
    if key[pygae.K_a]:
        testX -= 1
    if key[pygae.K_d]:
        testX += 1
    if key[pygae.K_w]:
        testY -= 1
    if key[pygae.K_s]:
        testY += 1
</code></pre>
",Movement code working pygame I making simple game pygame Everything seems running correctly errors code move sprite seem work If I set sprite move automatically works fine I think problem key press detection system Here main game loop running True running basic game function checks pygame display flip clock tick movementtest testX testY checking game closed event pygame event get event type pygame QUIT running False event type pygame KEYDOWN event key pygame K ESCAPE running False movement code movingleft testX testX movingright testX testX movingup testY testY movingdown testY testY event pygame event get event type pygame KEYDOWN event key pygame K movingright movingleft True event key pygame K movingleft movingright True event key pygame K w movingdown movingup True event key pygame K movingup movingdown True event type pygame KEYUP event key pygame K movingleft False event key pygame K movingright False event key pygame K w movingup,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
66247439,2021-02-17,2021,2,how to apply a multiplier on a dataframe based on conditions?,"<p>Given this first dataframe <code>df_1</code>:</p>
<pre><code>df_1 = pd.DataFrame({'id':[1,2,1,3,1],
                     'symbol':['A','B','C','A','A'],
                     'date':['2021-02-12','2021-02-09','2021-02-14','2021-02-02','2021-02-05'],
                     'value':[1,1,1,1,1]})
</code></pre>
<pre><code>   id symbol        date  value
0   1      A  2021-02-12      1
1   2      B  2021-02-09      1
2   1      C  2021-02-14      1
3   3      A  2021-02-02      1
4   1      A  2021-02-05      1
</code></pre>
<p>And given this second <code>df_2</code>:</p>
<pre><code>df_2 = pd.DataFrame({'id_symbol':['1_A', '1_A'],
                     'init_date':['2021-02-01','2021-02-01'],
                     'end_date':['2021-02-05', '2021-02-12'],
                     'multiplier':[5,2]})
</code></pre>
<p>I need to apply this <code>df_2.multiplier</code> on <code>df_1</code> for rows on <code>df_1.value</code> where the concat of <code>id</code> and  <code>_</code> and <code>symbol</code> is equals the <code>df_2.id_symbol</code>, and if the <code>df_1.date</code> is within the <code>df_2.init_date</code> and <code>df_2.end_date</code>.</p>
<p>My result should be like this, after the code:</p>
<pre><code>   id symbol        date   value
0   1      A  2021-02-12       2
1   2      B  2021-02-09       1
2   1      C  2021-02-14       1
3   3      A  2021-02-02       1
4   1      A  2021-02-05      10
</code></pre>
<p>5 = 1 * 5 // 10 = 1 * 5 * 2</p>
<p>My both dataframes are quite bigger than this.</p>
","['python', 'pandas', 'dataframe']",66247770,"<p>Since there's the possibility of multiplying the value more than once we will do a merge, find all possible matches and sum the multipliers to figure out how much we need to multiply the original row by.</p>
<p>First split the 'id_symbol' column and use <code>datetime64</code> dtypes so we can do merges and comparisons:</p>
<pre><code>import pandas as pd
import numpy as np

df_1['date'] = pd.to_datetime(df_1['date'])
df_2['init_date'] = pd.to_datetime(df_2['init_date'])
df_2['end_date'] = pd.to_datetime(df_2['end_date'])

df_2[['id', 'symbol']] = df_2['id_symbol'].str.split('_', expand=True)
df_2['id'] = df_2['id'].astype('int64')
</code></pre>
<p>Now merge on all possible multipliers, but set it to 1 if it's outside of the date range. Determine the total multiplier for that row (i.e. product of all multipliers that satisfy the date condition) and use that to multiply the with the original DataFrame (aligns on df_1's Index).</p>
<pre><code>mult = df_1.reset_index().merge(df_2, on=['id', 'symbol'], how='left')
mult['multiplier'] = np.where(mult.date.between(mult.init_date, mult.end_date), 
                              mult.multiplier, 1)
mult = mult.groupby('index')['multiplier'].prod()
#index
#0     2.0
#1     1.0
#2     1.0
#3     1.0
#4    10.0
#Name: multiplier, dtype: float64

df_1['value'] = df_1['value']*mult
</code></pre>
<hr />
<pre><code>print(df1)
   id symbol       date  value
0   1      A 2021-02-12    2.0
1   2      B 2021-02-09    1.0
2   1      C 2021-02-14    1.0
3   3      A 2021-02-02    1.0
4   1      A 2021-02-05   10.0
</code></pre>
",apply multiplier dataframe based conditions Given first dataframe df df pd DataFrame id symbol A B C A A date value id symbol date value A B C A A And given second df df pd DataFrame id symbol A A init date end date multiplier I need apply df multiplier df rows df value concat id symbol equals df id symbol df date within df init date df end date My result like code id symbol date value A B C A A My dataframes quite bigger,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
66264830,2021-02-18,2021,2,Python - Pandas groupby agg,"<p>I have a simple <code>dataframe</code> (df) like shown below:</p>
<pre><code>index Job  Person 
1     j1   Cathy
2     j2   Mark
3     j3   Cathy
4     j4   Steve
5     j5   Cathy
</code></pre>
<p>I would like to convert the above <code>dataframe</code> as:</p>
<pre><code>Person CountJob  JobDetails
Cathy     3      j1;j3;j5
Mark      1      j2
Steve     1      j4
</code></pre>
<p>I can partially solve this using <code>groupby</code> :</p>
<pre><code>final = df.groupby('Person').agg(
        CountJob=pd.NamedAgg(column='Job',aggfunc=&quot;count&quot;),
        )
</code></pre>
<p>I am struggling to get the format for the last column '<code>JobDetails</code>'. I am guessing I can use the <code>lambda</code> function but I just don't know how!</p>
","['python', 'pandas', 'dataframe', 'pandas-groupby']",66264853,"<p>Try:</p>
<pre><code>df.groupby('Person').agg(CountJob=('Job','count'),
                         JobDetails=('Job',';'.join)
                        )
</code></pre>
",Python Pandas groupby agg I simple dataframe df like shown index Job Person j Cathy j Mark j Cathy j Steve j Cathy I would like convert dataframe Person CountJob JobDetails Cathy j j j Mark j Steve j I partially solve using groupby final df groupby Person agg CountJob pd NamedAgg column Job aggfunc quot count quot I struggling get format last column JobDetails I guessing I use lambda function I know,"startoftags, python, pandas, dataframe, pandasgroupby, endoftags",python pandas numpy endoftags,python pandas dataframe pandasgroupby,python pandas numpy,0.58
66342708,2021-02-23,2021,2,How to make the size of output image the same as the original one to compute loss in CNN?,"<p>I am define the CNN model for autoencoder as follows:</p>
<pre><code>filters = (32, 16)
X = Input(shape = (32, 32, 3))

# encode
for f in filters:
    X = Conv2D(filters = f, kernel_size = (3, 3), activation = 'relu')(X)
    X = MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same')(X)
    X = BatchNormalization(axis = -1)(X)

# decode
for f in filters[::-1]:
    X = Conv2D(filters = f, kernel_size = (3, 3), activation = 'relu')(X)
    X = UpSampling2D(size = (2, 2))(X)
    X = BatchNormalization(axis = -1)(X)
</code></pre>
<p>The model summary is</p>
<pre><code>Model: &quot;functional_13&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_7 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
conv2d_24 (Conv2D)           (None, 30, 30, 32)        896       
_________________________________________________________________
max_pooling2d_12 (MaxPooling (None, 15, 15, 32)        0         
_________________________________________________________________
batch_normalization_24 (Batc (None, 15, 15, 32)        128       
_________________________________________________________________
conv2d_25 (Conv2D)           (None, 13, 13, 16)        4624      
_________________________________________________________________
max_pooling2d_13 (MaxPooling (None, 7, 7, 16)          0         
_________________________________________________________________
batch_normalization_25 (Batc (None, 7, 7, 16)          64        
_________________________________________________________________
conv2d_26 (Conv2D)           (None, 5, 5, 16)          2320      
_________________________________________________________________
up_sampling2d_12 (UpSampling (None, 10, 10, 16)        0         
_________________________________________________________________
batch_normalization_26 (Batc (None, 10, 10, 16)        64        
_________________________________________________________________
conv2d_27 (Conv2D)           (None, 8, 8, 32)          4640      
_________________________________________________________________
up_sampling2d_13 (UpSampling (None, 16, 16, 32)        0         
_________________________________________________________________
batch_normalization_27 (Batc (None, 16, 16, 32)        128       
=================================================================
Total params: 12,864
Trainable params: 12,672
Non-trainable params: 192
_________________________________________________________________
</code></pre>
<p>Because the output image has a different dimensions from the input image, I get the error</p>
<pre><code>InvalidArgumentError:  Incompatible shapes: [128,32,32,3] vs. [128,16,16,32]
     [[node mean_squared_error/SquaredDifference (defined at &lt;ipython-input-7-a9683921f595&gt;:83) ]] [Op:__inference_train_function_21329]

Function call stack:
train_function
</code></pre>
<p>and thus can not compute for the loss function. Could you please elaborate on how to solve this issue?</p>
","['python', 'tensorflow', 'machine-learning', 'keras', 'deep-learning']",66342804,"<p>I suggest u use <code>padding='same'</code> in convolutions. Pay attention also to not overwrite your input layer with other variables. you miss also a final output layer with the number of channels equals to the channel of the input image</p>
<pre><code>filters = (32, 16)
inp = Input(shape = (32, 32, 3))

# encode
X = inp
for f in filters:
    X = Conv2D(filters = f, kernel_size = (3, 3), padding = 'same', activation = 'relu')(X)
    X = MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same')(X)
    X = BatchNormalization(axis = -1)(X)

# decode
for f in filters[::-1]:
    X = Conv2D(filters = f, kernel_size = (3, 3), padding = 'same', activation = 'relu')(X)
    X = UpSampling2D(size = (2, 2))(X)
    X = BatchNormalization(axis = -1)(X)
out =  Conv2D(filters = 3, kernel_size = (3, 3), padding = 'same')(X)
    
model = Model(inp, out)
</code></pre>
<p>The model summary now is</p>
<pre><code>Layer (type)                 Output Shape              Param #   
=================================================================
input_9 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
conv2d_22 (Conv2D)           (None, 32, 32, 32)        896       
_________________________________________________________________
max_pooling2d_10 (MaxPooling (None, 16, 16, 32)        0         
_________________________________________________________________
batch_normalization_20 (Batc (None, 16, 16, 32)        128       
_________________________________________________________________
conv2d_23 (Conv2D)           (None, 16, 16, 16)        4624      
_________________________________________________________________
max_pooling2d_11 (MaxPooling (None, 8, 8, 16)          0         
_________________________________________________________________
batch_normalization_21 (Batc (None, 8, 8, 16)          64        
_________________________________________________________________
conv2d_24 (Conv2D)           (None, 8, 8, 16)          2320      
_________________________________________________________________
up_sampling2d_10 (UpSampling (None, 16, 16, 16)        0         
_________________________________________________________________
batch_normalization_22 (Batc (None, 16, 16, 16)        64        
_________________________________________________________________
conv2d_25 (Conv2D)           (None, 16, 16, 32)        4640      
_________________________________________________________________
up_sampling2d_11 (UpSampling (None, 32, 32, 32)        0         
_________________________________________________________________
batch_normalization_23 (Batc (None, 32, 32, 32)        128       
_________________________________________________________________
conv2d_26 (Conv2D)           (None, 32, 32, 3)         867       
=================================================================
Total params: 13,731
Trainable params: 13,539
Non-trainable params: 192
_________________________________________________________________
</code></pre>
",How make size output image original one compute loss CNN I define CNN model autoencoder follows filters X Input shape encode f filters X Conv D filters f kernel size activation relu X X MaxPooling D pool size strides padding X X BatchNormalization axis X decode f filters X Conv D filters f kernel size activation relu X X UpSampling D size X X BatchNormalization axis X The model summary Model quot functional quot Layer type Output Shape Param input InputLayer None conv Conv D None max pooling MaxPooling None batch normalization Batc None conv Conv D None max pooling MaxPooling None batch normalization Batc None conv Conv D None sampling UpSampling None batch normalization Batc None conv Conv D None sampling UpSampling None batch normalization Batc None Total params Trainable params Non trainable params Because output image different dimensions input image I get error Incompatible shapes vs node mean,"startoftags, python, tensorflow, machinelearning, keras, deeplearning, endoftags",python django djangorestframework endoftags,python tensorflow machinelearning keras deeplearning,python django djangorestframework,0.26
66449407,2021-03-03,2021,2,Iterating elements of a numpy array by cycling,"<p>I have a pretty straightforward question that I couldn't figure out quickly from the numpy reference documentation.</p>
<p>Say I have a numpy array <code>labels = np.array([1, 2, 3])</code>.<br />
I have another array <code>arr = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])</code>.<br />
I'd like to randomly sample indices of <code>arr</code>. Say our indices are <code>[0, 3, 6]</code>.</p>
<p>Now, I'd like to make the elements corresponding to those indices cycle by one in <code>labels</code>. So, since <code>arr[0] == 1</code>, we would set <code>arr[0] = 2</code>. Since <code>arr[3] == 2</code>, we would set <code>arr[3] = 3</code>. Since <code>arr[6] == 3</code>, we would set <code>arr[6] = 1</code>.</p>
<p>so, to recap:</p>
<pre><code>arr = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])
labels = np.unique(arr)
idx = np.array([0, 3, 6])  # randomly generate indices [0, 3, 6]
new_arr == np.array(2, 1, 1, 3, 2, 2, 1, 3, 3])  # this is the array I want
</code></pre>
<p>This seems like it'd be pretty straightforward, but I can't find an elegant way of doing this quickly!</p>
","['python', 'arrays', 'numpy']",66449549,"<p>If I understood you right, you might combine <code>np.roll</code> with <code>np.random.randint</code>, i.e.:</p>
<pre><code>sample_idx = np.roll(np.random.randint(len(arr), 3), shift=1)                                                                                                                                                                                            
sample = [labels[i] for i in sample_idx]
</code></pre>
<p>That is, randomly sample 3 indexes and roll them by one. This should be the final indexes to be re-mapped to your labels.</p>
<p><strong>EDIT</strong></p>
<p>Got it. You might do it in two steps:</p>
<pre><code>new_arr = arr.copy()
new_arr[idx] = np.roll(labels, -1)  # fixed the shift to -1
</code></pre>
<p>Output:</p>
<pre><code>array([2, 1, 1, 3, 2, 2, 1, 3, 3])
</code></pre>
",Iterating elements numpy array cycling I pretty straightforward question I figure quickly numpy reference documentation Say I numpy array labels np array I another array arr np array I like randomly sample indices arr Say indices Now I like make elements corresponding indices cycle one labels So since arr would set arr Since arr would set arr Since arr would set arr recap arr np array labels np unique arr idx np array randomly generate indices new arr np array array I want This seems like pretty straightforward I find elegant way quickly,"startoftags, python, arrays, numpy, endoftags",python arrays numpy endoftags,python arrays numpy,python arrays numpy,1.0
66562449,2021-03-10,2021,2,python sum values in df1 if loc in df2 is True,"<p>I'm still quite new to Python, and after looking intensively here on SO, I've decided to just ask.</p>
<p>I have a DataFrame, df</p>
<pre><code>df
    NO2  NO2  NO3  DK1  DK2  
0   1.0  3.0  2.0  1.0  1.0
1   1.0  3.0  3.0  3.0  1.0
2   2.0  2.0  2.0  1.0  1.0
</code></pre>
<p>Now, what I want to do is sum up all values in row 0 that are equal to the value in column &quot;DK1&quot; (incl. itself) and return it in a new column.
Then after doing that for row 0, the same procedure for row 1, then row 2, etc.</p>
<p>Desired output:</p>
<pre><code>df2
    NO2  NO2  NO3  DK1  DK2   Sum 
0   1.0  3.0  2.0  1.0  1.0   3.0
1   1.0  3.0  3.0  3.0  1.0   9.0
2   2.0  2.0  2.0  1.0  1.0   2.0
</code></pre>
","['python', 'pandas', 'dataframe']",66562477,"<p>Compare all values by <code>DF1</code> column, then multiple this column and last use <code>sum</code> per rows:</p>
<pre><code>df['sum'] = df.eq(df['DK1'], axis=0).mul(df['DK1'], axis=0).sum(axis=1)
print (df)
   NO2  NO2.1  NO3  DK1  DK2  sum
0  1.0    3.0  2.0  1.0  1.0  3.0
1  1.0    3.0  3.0  3.0  1.0  9.0
2  2.0    2.0  2.0  1.0  1.0  2.0
</code></pre>
<p><strong>Details</strong>:</p>
<pre><code>print (df.eq(df['DK1'], axis=0))
     NO2  NO2.1    NO3   DK1    DK2
0   True  False  False  True   True
1  False   True   True  True  False
2  False  False  False  True   True

print (df.eq(df['DK1'], axis=0).mul(df['DK1'], axis=0))
   NO2  NO2.1  NO3  DK1  DK2
0  1.0    0.0  0.0  1.0  1.0
1  0.0    3.0  3.0  3.0  0.0
2  0.0    0.0  0.0  1.0  1.0
</code></pre>
",python sum values df loc df True I still quite new Python looking intensively SO I decided ask I DataFrame df df NO NO NO DK DK Now I want sum values row equal value column quot DK quot incl return new column Then row procedure row row etc Desired output df NO NO NO DK DK Sum,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
66590460,2021-03-11,2021,2,Is there a better way to parse a python dictionary?,"<p>I have a json dictionary and I need to check the values of the data and see if there is a match.  I am using multiple <code>if</code> statements and the <code>in</code> operator like so:</p>
<pre class=""lang-json prettyprint-override""><code>&quot;SomeData&quot;:
{
    &quot;IsTrue&quot;: true,
    &quot;ExtraData&quot;:
    {
      &quot;MyID&quot;: &quot;1223&quot;
    }
}
</code></pre>
<pre><code>json_data = MYAPI.get_json()
if 'SomeData' in json_data:
    some_data = json_data['SomeData']
    if 'IsTrue' in some_data:
        if some_data['IsTrue'] is True:
            if 'ExtraData' in some_data:
                if 'MyID' in some_data['ExtraData']:
                    if some_data['ExtraData']['MyID'] == &quot;1234&quot;:
                        is_a_match = True
                        break
</code></pre>
<p>I know that in python3 the <code>in</code> operator should be used, but I am thinking there must be a better way than using multiple if statements like I am using.
Is there a better way to parse json data like this?</p>
","['python', 'python-3.x', 'dictionary']",66590870,"<p>Yes, you can assume that the keys are present, but catch a <code>KeyError</code> if they aren't.</p>
<pre><code>try:
    some_data = json_data['SomeData']
    is_a_match = (
        some_data['IsTrue'] is True and
        some_data['ExtraData']['MyID'] == &quot;1234&quot;
        )
except KeyError:
    is_a_match = False
</code></pre>
<p>This style is called <a href=""https://docs.python.org/3/glossary.html#term-eafp"" rel=""nofollow noreferrer"">easier to ask for forgiveness than permission (EAFP)</a> and it's used a lot in Python. The alternative is <a href=""https://docs.python.org/3/glossary.html#term-lbyl"" rel=""nofollow noreferrer"">look before you leap (LBYL)</a>, which you use in your solution.</p>
",Is better way parse python dictionary I json dictionary I need check values data see match I using multiple statements operator like quot SomeData quot quot IsTrue quot true quot ExtraData quot quot MyID quot quot quot json data MYAPI get json SomeData json data data json data SomeData IsTrue data data IsTrue True ExtraData data MyID data ExtraData data ExtraData MyID quot quot match True break I know python operator used I thinking must better way using multiple statements like I using Is better way parse json data like,"startoftags, python, python3x, dictionary, endoftags",python python3x pandas endoftags,python python3x dictionary,python python3x pandas,0.67
66666293,2021-03-17,2021,2,Pandas percent difference from mean,"<p>I have a pandas data frame with many columns and for each column I would like to generate a new columns where the result is the percent difference of the value in relation to the mean of that column, as seen in the example below:</p>
<pre><code>d = {'var1': [1, 2], 'var2': [3,4]}
df = pd.DataFrame(data=d)
df
    var1   var2  
0     1     3     
1     2     4     
</code></pre>
<p>result:</p>
<pre><code>    var1   var2  var1_avg  var2_avg
0     1     3     -0.33     -0.14
1     2     4      0.33      0.14
</code></pre>
<p>I am aware of how to find the mean of the column and then compute the percent difference, but only for a singe column, as seen below:</p>
<pre><code>df['var1_avg'] = (df.var1 - df.var1.mean()) / df.var1.mean()
</code></pre>
<p>However, I have 100's of columns and would like a way where I can apply this to each column and append the &quot;_avg&quot; to each new column name.</p>
","['python', 'pandas', 'dataframe']",66666531,"<p>You can use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html"" rel=""nofollow noreferrer""><code>pandas.concat</code></a> and <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.add_suffix.html"" rel=""nofollow noreferrer""><code>pandas.DataFrame.add_suffix</code></a>:</p>
<pre><code>&gt;&gt;&gt; pd.concat([df, ((df - df.mean())/df.mean()).add_suffix(&quot;_avg&quot;)], axis = 1)
   var1  var2  var1_avg  var2_avg
0     1     3 -0.333333 -0.142857
1     2     4  0.333333  0.142857
</code></pre>
",Pandas percent difference mean I pandas data frame many columns column I would like generate new columns result percent difference value relation mean column seen example var var df pd DataFrame data df var var result var var var avg var avg I aware find mean column compute percent difference singe column seen df var avg df var df var mean df var mean However I columns would like way I apply column append quot avg quot new column name,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
66684310,2021-03-18,2021,3,Python: Pandas dataframe get the year to which the week number belongs and not the year of the date,"<p>I have a csv-file: <a href=""https://data.rivm.nl/covid-19/COVID-19_aantallen_gemeente_per_dag.csv"" rel=""nofollow noreferrer"">https://data.rivm.nl/covid-19/COVID-19_aantallen_gemeente_per_dag.csv</a></p>
<p>I want to use it to provide insight into the corona deaths per week.</p>
<blockquote>
<pre><code>  df = pd.read_csv(&quot;covid.csv&quot;, error_bad_lines=False, sep=&quot;;&quot;)
  df = df.loc[df['Deceased'] &gt; 0]
  df[&quot;Date_of_publication&quot;] = pd.to_datetime(df[&quot;Date_of_publication&quot;])
  df[&quot;Week&quot;] = df[&quot;Date_of_publication&quot;].dt.isocalendar().week
  df[&quot;Year&quot;] = df[&quot;Date_of_publication&quot;].dt.year
  df = df[[&quot;Week&quot;, &quot;Year&quot;, &quot;Municipality_name&quot;, &quot;Deceased&quot;]]
  df = df.groupby(by=[&quot;Week&quot;, &quot;Year&quot;, &quot;Municipality_name&quot;]).agg({&quot;Deceased&quot; : &quot;sum&quot;})
  df = df.sort_values(by=[&quot;Year&quot;, &quot;Week&quot;])
  print(df)
</code></pre>
</blockquote>
<p>Everything seems to be working fine except for the first 3 days of 2021. The first 3 days of 2021 are part of the last week (53) of 2020: <a href=""http://week-number.net/calendar-with-week-numbers-2021.html"" rel=""nofollow noreferrer"">http://week-number.net/calendar-with-week-numbers-2021.html</a>.</p>
<p>When I print the dataframe this is the result:</p>
<blockquote>
<pre><code> 53   2021 Winterswijk               1
           Woudenberg                1
           Zaanstad                  1
           Zeist                     2
           Zutphen                   1
</code></pre>
</blockquote>
<p>So basically what I'm looking for is a way where this line returns the year of the week number and not the year of the date:</p>
<blockquote>
<pre><code>  df[&quot;Year&quot;] = df[&quot;Date_of_publication&quot;].dt.year
</code></pre>
</blockquote>
","['python', 'pandas', 'dataframe']",66684701,"<p>You can use <code>dt.isocalendar().year</code> to setup <code>df[&quot;Year&quot;]</code>:</p>
<pre><code>df[&quot;Year&quot;] = df[&quot;Date_of_publication&quot;].dt.isocalendar().year
</code></pre>
<p>You will get year 2020 for date of 2021-01-01 but will get back to year 2021 for date of 2021-01-04 by this.</p>
<p>This is just similar  to how you used <code>dt.isocalendar().week</code> for setting up <code>df[&quot;Week&quot;]</code>.  Since they are both basing on the same tuple <code>(year, week, day)</code> returned by <code>dt.isocalendar()</code>, they would always be in sync.</p>
<h2>Demo</h2>
<pre><code>date_s = pd.Series(pd.date_range(start='2021-01-01', periods=5, freq='1D'))

date_s

    0
0   2021-01-01
1   2021-01-02
2   2021-01-03
3   2021-01-04
4   2021-01-05

date_s.dt.isocalendar()

   year  week  day
0  2020    53    5
1  2020    53    6
2  2020    53    7
3  2021     1    1
4  2021     1    2
</code></pre>
",Python Pandas dataframe get year week number belongs year date I csv file https data rivm nl covid COVID aantallen gemeente per dag csv I want use provide insight corona deaths per week df pd read csv quot covid csv quot error bad lines False sep quot quot df df loc df Deceased gt df quot Date publication quot pd datetime df quot Date publication quot df quot Week quot df quot Date publication quot dt isocalendar week df quot Year quot df quot Date publication quot dt year df df quot Week quot quot Year quot quot Municipality name quot quot Deceased quot df df groupby quot Week quot quot Year quot quot Municipality name quot agg quot Deceased quot quot sum quot df df sort values quot Year quot quot Week quot print df Everything seems working fine except first days The first days part last week http,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
66806753,2021-03-25,2021,2,minus between 2 dataframe based on column,"<p>I have 2 dataframe pandas df1, df2:</p>
<pre><code>df1 = pd.DataFrame({'col1': ['A', 'B', 'C', 'D'],
                    'col2': [&quot;D1&quot;,&quot;D2&quot;,&quot;D3&quot;,&quot;D4&quot;],
                    'col3': [&quot;C1&quot;,&quot;C2&quot;,&quot;C3&quot;,&quot;C4&quot;],
                    'col4': [&quot;B1&quot;,&quot;B2&quot;,&quot;B3&quot;,&quot;B4&quot;]})


df2 = pd.DataFrame({'col_ID': ['A', 'D']})


df1=
col1 | col2 | col3 | col4
A    | D1   | C1   | B1           
B    | D2   | C2   | B2 
C    | D3   | C3   | B3 
D    | D4   | C4   | B4 
</code></pre>
<p>and df2 :</p>
<pre><code>df2=
col_ID |
A      |          
D      | 
</code></pre>
<p>and i want to have , the lines in df1.col1 that are not present in df2.col_ID</p>
<pre><code>df1=
col1 | col2 | col3 | col4
B    | D2   | C2   | B2 
C    | D3   | C3   | B3 
</code></pre>
<p>thank you for your help</p>
","['python', 'pandas', 'dataframe']",66806785,"<h3><code>isin</code></h3>
<pre><code>df1[~df1.col1.isin(df2.col_ID)]
</code></pre>
",minus dataframe based column I dataframe pandas df df df pd DataFrame col A B C D col quot D quot quot D quot quot D quot quot D quot col quot C quot quot C quot quot C quot quot C quot col quot B quot quot B quot quot B quot quot B quot df pd DataFrame col ID A D df col col col col A D C B B D C B C D C B D D C B df df col ID A D want lines df col present df col ID df col col col col B D C B C D C B thank help,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
66933758,2021-04-03,2021,2,how to update each series field in a dataframe,"<p>I have a DataFrame which holds two columns like below:</p>
<pre><code>    player_id    days
0     None        1
1     None        1
2     None        1
3     None        1
4     None        1
5     None        1
6     None        2
7     None        2
8     None        2
9     None        2
10    None        2
.
.
82    None        13
83    None        14
83    None        14
83    None        14
83    None        14
83    None        14
83    None        14
</code></pre>
<p>in output, I need to replace <code>None</code> with the id of players which is 1 to 11, have something like:</p>
<pre><code>    player_id    days
0     1           1
1     2           1
2     3           1
3     4           1
4     5           1
5     6           1
6     7           2
7     8           2
8     9           2
9     10          2
10    11          2
11    1           2
12    2           2
13    3           2
14    4           2
.
.
82    5           13
83    6           14
83    7           14
83    8           14
83    9           14
83    10          14
83    11          14
</code></pre>
<p>this is my code:</p>
<pre><code>for index in range(len(df)):
    for i in range(1, 11):
       df.iloc[index, 0] = i

print(df)
</code></pre>
<p>however I get the following dataframe:</p>
<pre><code>    player_id    days
0     11           1
1     11           1
2     11           1
3     11           1
4     11           1
5     11           1
6     11           2
7     11           2
8     11           2
9     11           2
10    11           2
11    11           2
12    11           2
13    11           2
14    11           2
.
.
82    11           13
83    11           14
83    11           14
83    11           14
83    11           14
83    11           14
83    11           14
</code></pre>
<p>I also tried to add a new series as follows, but does not work:</p>
<pre><code>for index in range(len(df)):
    for i in range(1, 11):
       df.iloc[index, 0] = pd.Series([i, df['day']], index=['player_id', 'day'])

print(df)
</code></pre>
<p>I have some doubt if editing a filed in dataframe is possible or not, I just skipped <code>itertuples</code> and <code>iterrows</code> to be able to edit this rows in an efficient way.</p>
","['python', 'pandas', 'dataframe']",66933818,"<p>try <code>%</code> operator:</p>
<pre><code>import numpy as np
df['player_id'] = 1 + np.arange(len(df))%11
df
</code></pre>
<p>output</p>
<pre><code>
    player_id   days
0   1   1
1   2   1
2   3   1
3   4   1
4   5   1
5   6   1
6   7   2
7   8   2
8   9   2
9   10  2
10  11  2
82  1   13
83  2   14
83  3   14
83  4   14
83  5   14
83  6   14
83  7   14
</code></pre>
<h2>Edit: using <code>index</code></h2>
<p>if the df's index (the first column in the output above) is not sequential and you want the same pattern but based on the index, then you can do</p>
<pre><code>df['player_id'] = 1 + df.index%11
</code></pre>
",update series field dataframe I DataFrame holds two columns like player id days None None None None None None None None None None None None None None None None None None output I need replace None id players something like player id days code index range len df range df iloc index print df however I get following dataframe player id days I also tried add new series follows work index range len df range df iloc index pd Series df day index player id day print df I doubt editing filed dataframe possible I skipped itertuples iterrows able edit rows efficient way,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
67055127,2021-04-12,2021,2,How to specify the week number for a given date using pandas?,"<p>I have a dataframe using</p>
<pre><code>year_start = '2020-03-29'

year_end = '2021-04-10'

week_end_sat = pd.DataFrame(pd.date_range(year_start, year_end, freq=f'W-SAT'), columns=['a'])
</code></pre>
<p>How can I make another column specifying the week number making 2020-03-29 as the first day of the calendar since I am trying to make a 4-4-5 calendar which always ends on a Saturday?</p>
<p>Final df that I want is,</p>
<pre><code>     a     |  count
2020-04-04 |    1
2020-04-11 |    2
.
.
.
2021-04-03 |    53   #since 2020 is a leap year there are 53 weeks otherwise it will be 52 weeks
2021-04-10 |    1
2021-04-17 |    2
.
2022-03-02 |    52
2022-04-09 |    1
</code></pre>
","['python', 'pandas', 'dataframe']",67055448,"<p>I think you can create a baseline date range start from the first day of your given <code>year_start</code>.</p>
<pre class=""lang-py prettyprint-override""><code>first_day_of_year = week_end_sat.iloc[0, 0].replace(day=1, month=1)
baseline = pd.Series(pd.date_range(first_day_of_year, periods=len(week_end_sat), freq=f'W-SAT'))
</code></pre>
<p>The baseline's week of year is what you want.</p>
<pre class=""lang-py prettyprint-override""><code>week_end_sat['count'] = baseline['a'].dt.isocalendar().week
</code></pre>
<pre class=""lang-py prettyprint-override""><code># print(week_end_sat)

            a  count
0  2020-04-04     1
1  2020-04-11     2
2  2020-04-18     3
3  2020-04-25     4
4  2020-05-02     5
5  2020-05-09     6
6  2020-05-16     7
7  2020-05-23     8
8  2020-05-30     9
9  2020-06-06    10
10 2020-06-13    11
11 2020-06-20    12
12 2020-06-27    13
13 2020-07-04    14
14 2020-07-11    15
15 2020-07-18    16
16 2020-07-25    17
17 2020-08-01    18
18 2020-08-08    19
19 2020-08-15    20
20 2020-08-22    21
21 2020-08-29    22
...
43 2021-01-30    44
44 2021-02-06    45
45 2021-02-13    46
46 2021-02-20    47
47 2021-02-27    48
48 2021-03-06    49
49 2021-03-13    50
50 2021-03-20    51
51 2021-03-27    52
52 2021-04-03    53
53 2021-04-10     1
</code></pre>
",How specify week number given date using pandas I dataframe using year start year end week end sat pd DataFrame pd date range year start year end freq f W SAT columns How I make another column specifying week number making first day calendar since I trying make calendar always ends Saturday Final df I want count since leap year weeks otherwise weeks,"startoftags, python, pandas, dataframe, endoftags",python pandas numpy endoftags,python pandas dataframe,python pandas numpy,0.67
67130986,2021-04-16,2021,2,Why is my Django e-commerce project registering multiple orders for each order made?,"<p>I am learning Python by building a simple e-commerce store using Django. I've been following this tutorial for <a href=""https://www.youtube.com/watch?v=woORrr3QNh8&amp;list=PL-51WBLyFTg0omnamUjL1TCVov7yDTRng&amp;index=3&amp;ab_channel=DennisIvy"" rel=""nofollow noreferrer"">guidance</a>.</p>
<p>When I pass an order on the site (on localhost), the order is registered 2 times in the backend. Ie: Order #1, Order #2 in the admin panel.</p>
<p>I am wondering how I can merge both so 1 order passed on the site = 1 order on the dashboard?</p>
<p><strong>In order 1,</strong> the field complete is set to True and the Transaction is filled.</p>
<p><a href=""https://i.stack.imgur.com/ek16h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ek16h.png"" alt=""enter image description here"" /></a></p>
<p><strong>In order #2,</strong> the Pickup time &amp; Type are both set.</p>
<p><a href=""https://i.stack.imgur.com/zCN6L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zCN6L.png"" alt=""enter image description here"" /></a></p>
<p><strong>Models.py</strong></p>
<pre><code>class Order(models.Model):
    customer = models.ForeignKey(Customer, on_delete=models.SET_NULL, blank=True, null=True)
    date_ordered = models.DateTimeField(auto_now_add=True)
    complete = models.BooleanField(default=False, null=True, blank=False)
    transaction_id = models.CharField(max_length=100, null=True)

    CHOICES = (('in-store','in-store'),
        ('curbside','curbside'))

    pickup_type = models.CharField(max_length=10, choices=CHOICES)
    pickup_time = models.DateTimeField(null=True, blank=False)
</code></pre>
<p><strong>View.py</strong></p>
<pre><code>def store(request):

    if request.user.is_authenticated:
        customer = request.user.customer
        order, created = Order.objects.get_or_create(customer=customer, complete=False)
        items = order.orderitem_set.all()
        cartItems = order.get_cart_items
    else:
        items = []
        order = {'get_cart_total':0, 'get_cart_items':0, 'shipping':False}
        cartItems = order['get_cart_items']


    products = Product.objects.all()
    context = {'products': products, 'cartItems':cartItems}
    return render(request, 'store/store.html', context)

def processOrder(request):
    transaction_id = datetime.datetime.now().timestamp()
    data = json.loads(request.body)

    if request.user.is_authenticated:
        customer = request.user.customer
        order, created = Order.objects.get_or_create(customer=customer, complete=False)
        total = float(data['form']['total'])
        order.transaction_id = transaction_id

        if total == float(order.get_cart_total):
            order.complete = True
            print(&quot;Order set to complete!!!!&quot;)
        order.save()

        if order.shipping == True:
            ShippingAddress.objects.create(
                customer=customer,
                order=order,
                address=data['shipping']['address'],
                city=data['shipping']['city'],
                state=data['shipping']['state'],
                zipcode=data['shipping']['zipcode'],
            )
            
            Order.objects.create(
                pickup_type=data['pickup']['pickup_type'],
                pickup_time=data['pickup']['pickup_time'],
            )

    else:
        print(&quot;user is not logged in&quot;)

    return JsonResponse('Order Processed', safe=False)
</code></pre>
<p><strong>How can I make sure I don't create 2 separate orders?</strong>
Thanks.</p>
","['python', 'django', 'django-models']",67131065,"<p>In case <code>order.shipping == True</code>, you create a new <code>Order</code> object with <code>Order.objects.create(â¦)</code>. You should update the existing <code>order</code> and save that to the database:</p>
<pre><code>from django.contrib.auth.decorators import login_required

@login_required
def processOrder(request):
    transaction_id = datetime.datetime.now().timestamp()
    data = json.loads(request.body)
    customer = request.user.customer
    order, created = Order.objects.get_or_create(
        customer=customer, complete=False
    )
    total = float(data['form']['total'])
    order.transaction_id = transaction_id
    if total == float(order.get_cart_total):
        order.complete = True
        print(""Order set to complete!!!!"")

    if order.shipping:
        ShippingAddress.objects.create(
            customer=customer,
            order=order,
            address=data['shipping']['address'],
            city=data['shipping']['city'],
            state=data['shipping']['state'],
            zipcode=data['shipping']['zipcode'],
        )
        order<b>.pickup_type</b> = data['pickup']['pickup_type']
        order<b>.pickup_time</b> = data['pickup']['pickup_time']
    <b>order.save()</b>
    return JsonResponse({'status': 'Order Processed'})</code></pre>
",Why Django e commerce project registering multiple orders order made I learning Python building simple e commerce store using Django I following tutorial guidance When I pass order site localhost order registered times backend Ie Order Order admin panel I wondering I merge order passed site order dashboard In order field complete set True Transaction filled In order Pickup time amp Type set Models py class Order models Model customer models ForeignKey Customer delete models SET NULL blank True null True date ordered models DateTimeField auto add True complete models BooleanField default False null True blank False transaction id models CharField max length null True CHOICES store store curbside curbside pickup type models CharField max length choices CHOICES pickup time models DateTimeField null True blank False View py def store request request user authenticated customer request user customer order created Order objects get create customer customer complete False items order,"startoftags, python, django, djangomodels, endoftags",python django djangorestframework endoftags,python django djangomodels,python django djangorestframework,0.67
67200368,2021-04-21,2021,3,Why does the axis argument in NumPy change?,"<p>I am very confused when it comes to the logic of the NumPy axis argument. In some cases it affects the row when axis = 0 and in some cases it affects the columns when axis = 0. Example:</p>
<pre><code>a = np.array([[1,3,6,7,4],[3,2,5,9,1]])
array([[1,3,6,7,4],
       [3,2,5,9,1]])

np.sort(a, axis = 0)   #This sorts the columns
array([[1, 2, 5, 7, 1],  
       [3, 3, 6, 9, 4]])

np.sort(a, axis=1)     #This sorts the rows           
array([[1, 3, 4, 6, 7],
       [1, 2, 3, 5, 9]])

#####################################################################
arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])
arr
array([[ 1,  2,  3,  4],
       [ 5,  6,  7,  8],
       [ 9, 10, 11, 12]])

np.delete(arr,obj = 1, axis = 0)        # This deletes the row
array([[ 1,  2,  3,  4],
       [ 9, 10, 11, 12]])

np.delete(arr,obj = 1, axis = 1)        #This deletes the column
array([[ 1,  3,  4],
       [ 5,  7,  8],
       [ 9, 11, 12]])
</code></pre>
<p>If there is some logic here that I am missing I would love to learn it.</p>
","['python', 'arrays', 'numpy']",67200564,"<p>It's perhaps simplest to remember it as 0=down and 1=across.</p>
<p>This means:
<a href=""https://i.stack.imgur.com/0NeX0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0NeX0.png"" alt=""enter image description here"" /></a>
Use axis=0 to apply a method down each column, or to the row labels (the index).
Use axis=1 to apply a method across each row, or to the column labels.
Here's a picture to show the parts of a DataFrame that each axis refers to:</p>
<p>It's also useful to remember that Pandas follows NumPy's use of the word axis. The usage is explained in NumPy's glossary of terms:</p>
<p>Axes are defined for arrays with more than one dimension. A 2-dimensional array has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1). [my emphasis]</p>
<p>So, concerning the method in the question, np.sort(axis=1), seems to be correctly defined. It takes the mean of entries horizontally across columns, that is, along each individual row. On the other hand, np.sort(axis=0) would be an operation acting vertically downwards across rows.</p>
<p>Similarly, np.delete(name, axis=1) refers to an action on column labels, because they intuitively go across the horizontal axis. Specifying axis=0 would make the method act on rows instead.</p>
",Why axis argument NumPy change I confused comes logic NumPy axis argument In cases affects row axis cases affects columns axis Example np array array np sort axis This sorts columns array np sort axis This sorts rows array arr np array arr array np delete arr obj axis This deletes row array np delete arr obj axis This deletes column array If logic I missing I would love learn,"startoftags, python, arrays, numpy, endoftags",python python3x numpy endoftags,python arrays numpy,python python3x numpy,0.67
67509127,2021-05-12,2021,2,"Split an aggregated value into a distinct count in python, returning new rows","<p>I have data in an excel file, df that holds aggregated values per ID. I am looking to break this down to its distinct count and create a new record for each.</p>
<p><strong>Data</strong></p>
<pre><code>    A    B    C

    2    3    1
</code></pre>
<p><strong>Desired</strong></p>
<pre><code> count   ID

  1      A01

  1      A02
  
  1      B01

  1      B02

  1      B03

  1      C01
</code></pre>
<p><strong>Doing:</strong></p>
<pre><code>import pandas as pd
from numpy.random import randint

df = pd.DataFrame(columns=['A', 'B', 'C'])
     for i in range(5):
     df.loc[i] = ['ID' + str(i)] + list(randint(10, size=2))
</code></pre>
<p>I am thinking I can go about it this way, however, this is not stacking all the necessary IDs, consecutively.</p>
<p>Any suggestion or advice will be appreciated.</p>
","['python', 'pandas', 'numpy']",67509282,"<p>Let's try <code>melt</code> to reshape the data, <code>reindex</code> + <code>repeat</code> to duplicate the rows, and <code>groupby</code> + <code>cumcount</code> + <code>zfill</code> to create the suffixes:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df = pd.DataFrame({'A': {0: 2}, 'B': {0: 3}, 'C': {0: 1}})

# Melt Table Into New Form
df = df.melt(col_level=0, value_name='count', var_name='ID')
# Repeat Based on Count
df = df.reindex(df.index.repeat(df['count']))
# Set Count To 1
df['count'] = 1

# Add Suffix to Each ID
df['ID'] = df['ID'] + (
    (df.groupby('ID').cumcount() + 1)
        .astype(str)
        .str.zfill(2)
)

# Reorder Columns
df = df[['count', 'ID']]

print(df)
</code></pre>
<p><code>df</code>:</p>
<pre><code>   count   ID
0      1  A01
0      1  A02
1      1  B01
1      1  B02
1      1  B03
2      1  C01
</code></pre>
",Split aggregated value distinct count python returning new rows I data excel file df holds aggregated values per ID I looking break distinct count create new record Data A B C Desired count ID A A B B B C Doing import pandas pd numpy random import randint df pd DataFrame columns A B C range df loc ID str list randint size I thinking I go way however stacking necessary IDs consecutively Any suggestion advice appreciated,"startoftags, python, pandas, numpy, endoftags",python pandas numpy endoftags,python pandas numpy,python pandas numpy,1.0
67520804,2021-05-13,2021,2,np.where not working with multiple conditions?,"<p>I have a dataframe that looks a bit like this:</p>
<pre><code> | offer_code | column2 | column3
-|------------|---------|--------
0| 123        | X       | NaN
1| 123        | Y       | NaN
2| 456        | X       | X
3| 456        | Y       | X
</code></pre>
<p>I'm trying to add a new column which flags as 0 all rows where column3 = NaN OR column2 and column3 match. Everything else should be flagged as 1. So the result should look like this:</p>
<pre><code> | offer_code | column2 | column3 | flag
-|------------|---------|---------|-----
0| 123        | X       | NaN     | 0
1| 123        | Y       | NaN     | 0
2| 456        | X       | X       | 0
3| 456        | Y       | X       | 1
</code></pre>
<p>However, my code just flags every single row as 1. This is the code I'm using; can anyone see where I'm going wrong please?</p>
<pre><code>df[&quot;flag&quot;] = np.where(df[&quot;column3&quot;].isnull()|df[&quot;column2&quot;]==df[&quot;column3&quot;],0,1)
</code></pre>
","['python', 'pandas', 'numpy']",67520851,"<p>Missing parenthesis around the second condition:</p>
<pre><code>df[&quot;flag&quot;] = np.where(df[&quot;column3&quot;].isnull() |
                      (df[&quot;column2&quot;] == df[&quot;column3&quot;]), 0, 1)
#                     ^                              ^
</code></pre>
<p>Or with <code>eq</code>:</p>
<pre><code>df[&quot;flag&quot;] = np.where(df[&quot;column3&quot;].isnull() |
                      df['column2'].eq(df['column3']), 0, 1)
</code></pre>
<p><code>df</code>:</p>
<pre class=""lang-none prettyprint-override""><code>   offer_code column2 column3  flag
0         123       X     NaN     0
1         123       Y     NaN     0
2         456       X       X     0
3         456       Y       X     1
</code></pre>
",np working multiple conditions I dataframe looks bit like offer code column column X NaN Y NaN X X Y X I trying add new column flags rows column NaN OR column column match Everything else flagged So result look like offer code column column flag X NaN Y NaN X X Y X However code flags every single row This code I using anyone see I going wrong please df quot flag quot np df quot column quot isnull df quot column quot df quot column quot,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
68139139,2021-06-26,2021,2,How to limit how many characters in input boxes Pygame?,"<p>Hi so I wanna limit the texts inside a rectangle and if the text surpasses the rectangle's dimension then it reduces itself to fit in. I use class to make the code easier to read and easier to add more input boxes. When I run the code, there should be different input boxes in which I can edit the text in and if the text is too long, it reduces itself( lastest character) to fit in the box. But when I run the code, the text can go over the rectangle for 1 character and for any letter after that, it deletes a whole chunk of text. Here is the code.</p>
<p>''' <code>python</code></p>
<pre><code>import pygame
import datetime

pygame.init()
clock = pygame.time.Clock()
pygame.font.init()

# Note
finish = 0
leftover = 16

# Font
numb_font = pygame.font.Font('dogicapixelbold.ttf', 14)
text_font = pygame.font.Font('dogicapixelbold.ttf', 16)

color = (233, 248, 215)
active = False

# screen resolution
Width = 800
Height = 600
bg = pygame.image.load('opennote.png')
screen = pygame.display.set_mode((Width, Height))

# Time
time_box = pygame.Rect(250, 63, 50, 30)
date_box = pygame.Rect(221, 27, 50, 30)
# boxes numb
leftover_box = pygame.Rect(265, 105, 30, 30)
finish_box = pygame.Rect(325, 105, 30, 30)


class InputBox:

    def __init__(self, x, y, w, h, text=''):
        self.rect = pygame.Rect(x, y, w, h)
        self.color = color
        self.text = text
        self.txt_surface = text_font.render(text, True, self.color)
        self.active = False

    def handle_event(self, event):
        if event.type == pygame.MOUSEBUTTONDOWN:
            # If the user clicked on the input_box rect.
            if self.rect.collidepoint(event.pos):
                # Toggle the active variable.
                self.active = not self.active
            else:
                self.active = False
        if event.type == pygame.KEYDOWN:
            if self.active:
                if event.key == pygame.K_RETURN:
                    print(self.text)
                    self.text = ''
                elif event.key == pygame.K_BACKSPACE:
                    self.text = self.text[:-1]
                else:
                    self.text += event.unicode
                # Re-render the text.
                self.txt_surface = text_font.render(self.text, True, self.color)

    def draw(self, screen):
        # Blit the text.
        screen.blit(self.txt_surface, (self.rect.x+5, self.rect.y+10))
        # Blit the rect.
        pygame.draw.rect(screen, self.color, self.rect, 2)

    def update(self):
        # Limit character
        if self.txt_surface.get_width() &gt; self.rect.w:
            self.text = self.text[:-1]


def main():
    clock = pygame.time.Clock()
    input_box1 = InputBox(115, 170, 250, 36)
    input_box2 = InputBox(115, 224, 250, 36)
    input_box3 = InputBox(115, 278, 250, 36)
    input_box4 = InputBox(115, 333, 250, 36)
    input_box5 = InputBox(115, 386, 250, 36)
    input_box6 = InputBox(115, 440, 250, 36)
    input_box7 = InputBox(115, 494, 250, 36)
    input_boxes = [input_box1, input_box2, input_box3, input_box4, input_box5, input_box6, input_box7]
    done = False

    while not done:
        # Background
        screen.fill((0, 0, 0))
        screen.blit(bg, (0, 0))
        now = datetime.datetime.now()
        date_now = now.strftime(&quot;%d/%m/%Y&quot;)
        time_now = now.strftime(&quot;%H:%M:%S&quot;)
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                done = True
            for box in input_boxes:
                box.handle_event(event)

        for box in input_boxes:
            box.update()

        for box in input_boxes:
            box.draw(screen)
            # Real Time
            # Date
        pygame.draw.rect(screen, 'white', date_box, -1)
        datebox_surface = numb_font.render(date_now, True, color)
        screen.blit(datebox_surface, (date_box.x + 5, date_box.y + 5))
        # Time
        pygame.draw.rect(screen, 'white', time_box, -1)
        timebox_surface = numb_font.render(time_now, True, color)
        screen.blit(timebox_surface, (time_box.x + 5, time_box.y + 5))

        # finish &amp;Leftover
        # finish box
        pygame.draw.rect(screen, 'white', finish_box, -1)
        finishbox_surface = numb_font.render(str(finish), True, color)
        screen.blit(finishbox_surface, finish_box)
        # Leftover box
        pygame.draw.rect(screen, 'white', leftover_box, -1)
        leftover_box_surface = numb_font.render(str(leftover), True, color)
        screen.blit(leftover_box_surface, leftover_box)

        pygame.display.update()
        clock.tick(120)


if __name__ == '__main__':
    main()
    pygame.quit()
</code></pre>
<p>'''</p>
<p>But when the letter outside the rect's width, it deletes a whole bunch of text not just the letter that outside the width.</p>
","['python', 'python-3.x', 'pygame']",68139795,"<p>Switch your update and re-render contents.</p>
<pre><code>    if event.type == pygame.KEYDOWN:
        if self.active:
            if event.key == pygame.K_RETURN:
                print(self.text)
                self.text = ''
            elif event.key == pygame.K_BACKSPACE:
                self.text = self.text[:-1]
            else:
                self.text += event.unicode
                # Limit characters           -20 for border width
                if self.txt_surface.get_width() &gt; self.rect.w -20:
                    self.text = self.text[:-1]

def draw(self, screen):
    # Blit the text.
    screen.blit(self.txt_surface, (self.rect.x+5, self.rect.y+10))
    # Blit the rect.
    pygame.draw.rect(screen, self.color, self.rect, 2)

def update(self):
    # Re-render the text.
    self.txt_surface = text_font.render(self.text, True, self.color)
</code></pre>
<hr />
<hr />
<p><strong>Edit:</strong>  oh, it's 3am;  I'm just about asleep by now, if I missed posting something that was edited I don't remember.  Had to swap out the background for a plain black rectangle and blit that in just for testing, but I know for certain it runs OK here.  Downloaded same font and everything.  This is what it looks like.</p>
<p><a href=""https://i.stack.imgur.com/FgdRC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FgdRC.png"" alt=""3am"" /></a></p>
<pre><code>#! /usr/bin/env python3

import pygame
import datetime

pygame.init()
clock = pygame.time.Clock()
pygame.font.init()

# Note
finish = 0
leftover = 16

# Font
numb_font = pygame.font.Font('dogicapixelbold.ttf', 14)
text_font = pygame.font.Font('dogicapixelbold.ttf', 16)

color = (233, 248, 215)
active = False

# screen resolution
Width = 800
Height = 600
bg = pygame .Surface( [Width,  Height] )
pygame .draw .rect( bg,  (0, 0, 0),  [0,  0,  Width,  Height] )
##  bg = pygame.image.load('opennote.png')
screen = pygame.display.set_mode((Width, Height))

# Time
time_box = pygame.Rect(250, 63, 50, 30)
date_box = pygame.Rect(221, 27, 50, 30)
# boxes numb
leftover_box = pygame.Rect(265, 105, 30, 30)
finish_box = pygame.Rect(325, 105, 30, 30)


class InputBox:

    def __init__(self, x, y, w, h, text=''):
        self.rect = pygame.Rect(x, y, w, h)
        self.color = color
        self.text = text
        self.txt_surface = text_font.render(text, True, self.color)
        self.active = False

    def handle_event(self, event):
        if event.type == pygame.MOUSEBUTTONDOWN:
            # If the user clicked on the input_box rect.
            if self.rect.collidepoint(event.pos):
                # Toggle the active variable.
                self.active = not self.active
            else:
                self.active = False
        if event.type == pygame.KEYDOWN:
            if self.active:
                if event.key == pygame.K_RETURN:
                    print(self.text)
                    self.text = ''
                elif event.key == pygame.K_BACKSPACE:
                    self.text = self.text[:-1]
                else:
                    self.text += event.unicode
                    # Limit characters           -20 for border width
                    if self.txt_surface.get_width() &gt; self.rect.w -20:
                        self.text = self.text[:-1]

    def draw(self, screen):
        # Blit the text.
        screen.blit(self.txt_surface, (self.rect.x+5, self.rect.y+10))
        # Blit the rect.
        pygame.draw.rect(screen, self.color, self.rect, 2)

    def update(self):
        # Re-render the text.
        self.txt_surface = text_font.render(self.text, True, self.color)


def main():
    clock = pygame.time.Clock()
    input_box1 = InputBox(115, 170, 250, 36)
    input_box2 = InputBox(115, 224, 250, 36)
    input_box3 = InputBox(115, 278, 250, 36)
    input_box4 = InputBox(115, 333, 250, 36)
    input_box5 = InputBox(115, 386, 250, 36)
    input_box6 = InputBox(115, 440, 250, 36)
    input_box7 = InputBox(115, 494, 250, 36)
    input_boxes = [input_box1, input_box2, input_box3, input_box4, input_box5, input_box6, input_box7]
    done = False

    while not done:
        # Background
        screen.fill((0, 0, 0))
        screen.blit(bg, (0, 0))
        now = datetime.datetime.now()
        date_now = now.strftime(&quot;%d/%m/%Y&quot;)
        time_now = now.strftime(&quot;%H:%M:%S&quot;)
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                done = True
            for box in input_boxes:
                box.handle_event(event)

        for box in input_boxes:
            box.update()

        for box in input_boxes:
            box.draw(screen)
            # Real Time
            # Date
        pygame.draw.rect(screen, 'white', date_box, -1)
        datebox_surface = numb_font.render(date_now, True, color)
        screen.blit(datebox_surface, (date_box.x + 5, date_box.y + 5))
        # Time
        pygame.draw.rect(screen, 'white', time_box, -1)
        timebox_surface = numb_font.render(time_now, True, color)
        screen.blit(timebox_surface, (time_box.x + 5, time_box.y + 5))

        # finish &amp;Leftover
        # finish box
        pygame.draw.rect(screen, 'white', finish_box, -1)
        finishbox_surface = numb_font.render(str(finish), True, color)
        screen.blit(finishbox_surface, finish_box)
        # Leftover box
        pygame.draw.rect(screen, 'white', leftover_box, -1)
        leftover_box_surface = numb_font.render(str(leftover), True, color)
        screen.blit(leftover_box_surface, leftover_box)

        pygame.display.update()
        clock.tick(120)


if __name__ == '__main__':
    main()
    pygame.quit()
</code></pre>
",How limit many characters input boxes Pygame Hi I wanna limit texts inside rectangle text surpasses rectangle dimension reduces fit I use class make code easier read easier add input boxes When I run code different input boxes I edit text text long reduces lastest character fit box But I run code text go rectangle character letter deletes whole chunk text Here code python import pygame import datetime pygame init clock pygame time Clock pygame font init Note finish leftover Font numb font pygame font Font dogicapixelbold ttf text font pygame font Font dogicapixelbold ttf color active False screen resolution Width Height bg pygame image load opennote png screen pygame display set mode Width Height Time time box pygame Rect date box pygame Rect boxes numb leftover box pygame Rect finish box pygame Rect class InputBox def init self x w h text self rect pygame Rect x w h,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
48242217,2018-01-13,2018,3,getattr returns a default even though attribute exists in dict keys,"<p>I have a file containing some json which I load and loop through the ""servers"" items and pass them to a <code>send_alert()</code> method (items marked ""REDACTED"" are not the real code/output):</p>

<pre><code>""servers"": [
  {
    ""address"": ""smtp.gmail.com"", 
    ""encrypted-password"": ""Copy and Paste from 'password' after encryption."", 
    ""login-name"": false, 
    ""port"": 587, 
    ""type"": ""email""
  }, 
  {
    ""address"": ""smtp.mail.yahoo.com"", 
    ""encrypted-password"": ""&lt;REDACTED&gt;"", 
    ""login-name"": ""&lt;REDACTED&gt;@yahoo.com"", 
    ""port"": 587, 
    ""type"": ""email""
  }]
</code></pre>

<p>I expect the gmail item to be skipped because it should render False or None for the 'login-name'... so I am really just processing the yahoo item:</p>

<pre><code>def send_alert(self, server, message, email):
    print(json.dumps(server, encoding=self.encoding,
                                 sort_keys=True, indent=2))
    print(type(server))
    print(server[""login-name""])
    username = getattr(server, ""login-name"", None)
    enc_pass = getattr(server, ""encrypted-password"", None)
    address = getattr(server, ""password"", None)
    port = getattr(server, ""port"", None)
    server_type = getattr(server, ""type"", None)
    mess = message
    if not username:
        print(username)
        self.log.warn(""incorrectly configured alert!\n:%s"",
                      json.dumps(server, encoding=self.encoding,
                                 sort_keys=True, indent=2))
        return False
    if server_type.lower() == ""email"":
        self.send_email(username, enc_pass, address, port, mess, email)
    else:
        self.log.debug(""unknown server type listed in Alerts/servers"")
</code></pre>

<p>when <code>send_alert</code> is called, I receive the following output:</p>

<pre><code>{
  ""address"": ""smtp.mail.yahoo.com"",
  ""encrypted-password"": &lt;REDACTED&gt;,
  ""login-name"": ""&lt;REDACTED&gt;@yahoo.com"",
  ""port"": 587,
  ""type"": ""email""
}
&lt;type 'dict'&gt;
&lt;REDACTED&gt;@yahoo.com
None
[11:35:39] [Wrapper.py/WARNING]: incorrectly configured alert!
:{
  ""address"": ""smtp.mail.yahoo.com"",
  ""encrypted-password"": &lt;REDACTED&gt;,
  ""login-name"": ""&lt;REDACTED&gt;@yahoo.com"",
  ""port"": 587,
  ""type"": ""email""
}
</code></pre>

<p>I would like to use getattr, versus testing (<code>if ""login-name"" in server:</code>) or try-excepting the values.  I have done this in other cases with success.  However, this has me stumped.  <code>getattr()</code> is returning the default value of None, even though a valid key item seems to exist.  What is wrong?</p>

<p>I tried some of the suggested ""Questions that may already have your answer
"" and none have given me any help.  This one seemed promising but was either never answered or the answer (provided by the asker) does not apply to my case: <a href=""https://stackoverflow.com/questions/38018475/python-3-attributeerror-even-though-attribute-exists"">Python 3 AttributeError even though attribute exists</a>  (i.e., I tried to deepcopy the single dictionary item server before passing it to send_alert)</p>

<p>The output is the same for both Python 2 and 3.</p>
","['python', 'python-3.x', 'python-2.7']",48242355,"<p><a href=""https://docs.python.org/3/library/functions.html#getattr"" rel=""noreferrer""><code>getattr</code></a> gets the attributes of an object, not the value associated with a key in a mapping.</p>

<p><code>getattr(server, 'login-name', None)</code> is trying to access the attribute <code>server.login-name</code>, not the value <code>server['login-name']</code></p>

<p>You can instead use the <a href=""https://docs.python.org/3/library/stdtypes.html#dict.get"" rel=""noreferrer""><code>dict.get</code></a> method</p>

<pre><code>server.get('login-name', None)
</code></pre>
",getattr returns default even though attribute exists dict keys I file containing json I load loop servers items pass send alert method items marked REDACTED real code output servers address smtp gmail com encrypted password Copy Paste password encryption login name false port type email address smtp mail yahoo com encrypted password lt REDACTED gt login name lt REDACTED gt yahoo com port type email I expect gmail item skipped render False None login name I really processing yahoo item def send alert self server message email print json dumps server encoding self encoding sort keys True indent print type server print server login name username getattr server login name None enc pass getattr server encrypted password None address getattr server password None port getattr server port None server type getattr server type None mess message username print username self log warn incorrectly configured alert n json dumps server encoding,"startoftags, python, python3x, python27, endoftags",python python3x list endoftags,python python3x python27,python python3x list,0.67
48261358,2018-01-15,2018,2,How to pass large number arguments to **kwargs,"<p>I have following dataframe :</p>

<pre><code>   p     p_loc  s            s0  s0_loc  s1  s1_loc  s2 s2_loc  s3   s3_loc
0  ABCD  M      AB,AC,AD     AB  A       AC   M      AD  Q      NaN  U
1  PQRS  N      PQ,PR,PS,QR  PQ  B       PR   N      PS  R      QR   V
</code></pre>

<p>I'm splitting column s into columns s0,s1,s2,s3 using , as delimeter.
I want to pass values from s0,s1,s2,s3,s4,s5,s0_loc,s1_loc,s2_loc,s3_loc to following function which accepts following arguments:</p>

<pre><code>def func(p,p_loc,**args):
    print(p)
    print(p_loc)
    print(kwargs)
</code></pre>

<p>I'm passing arguments as follows:</p>

<pre><code>df['new'] = df.apply(lambda x: func(df['p'],df['p_loc'],df['s0'],df['s1'],df['s2'],df['s3'],df['s0_loc'],df['s1_loc'],df['s2_loc'],df['s3_loc']))
</code></pre>

<p>I want to know is there any simpler way to pass these arguments(Say, suppose my split of column s results in more than 15-20 columns.. How can I pass these many arguments in a simpler way)...???</p>

<p>Thanks in advance...</p>
","['python', 'pandas', 'dataframe']",48261961,"<p>Create a <code>set</code> of positional arguments like:</p>

<pre><code>&gt;&gt;&gt; not_kwargs = {'p', 'p_loc', 's'}
</code></pre>

<p>Next filter out kwargs like:</p>

<pre><code>&gt;&gt;&gt; data = df.to_dict()
&gt;&gt;&gt; kwargs = {key:data[key] for key in data.keys() ^ not_kwargs}
</code></pre>

<p>If you are using <a href=""/questions/tagged/python-2.7"" class=""post-tag"" title=""show questions tagged &#39;python-2.7&#39;"" rel=""tag"">python-2.7</a>, <code>^</code> only operates on <code>set</code>.</p>

<pre><code>&gt;&gt;&gt; kwargs = {key:data[key] for key in set(data.keys()) ^ not_kwargs}
</code></pre>

<p>Call function:</p>

<pre><code>&gt;&gt;&gt; func(data['p'], data['p_loc'], **kwargs)
</code></pre>
",How pass large number arguments kwargs I following dataframe p p loc loc loc loc loc ABCD M AB AC AD AB A AC M AD Q NaN U PQRS N PQ PR PS QR PQ B PR N PS R QR V I splitting column columns using delimeter I want pass values loc loc loc loc following function accepts following arguments def func p p loc args print p print p loc print kwargs I passing arguments follows df new df apply lambda x func df p df p loc df df df df df loc df loc df loc df loc I want know simpler way pass arguments Say suppose split column results columns How I pass many arguments simpler way Thanks advance,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
48450332,2018-01-25,2018,5,How to sum certain values in a pandas column DataFrame in a specific date range,"<p>I have a large DataFrame that looks something like this: 
df =  </p>

<pre><code>    UPC   Unit_Sales  Price   Price_Change  Date 
 0   22          15    1.99         NaN     2017-10-10
 1   22          7     2.19         True    2017-10-12
 2   22          6     2.19         NaN     2017-10-13
 3   22          7     1.99         True    2017-10-16
 4   22          4     1.99         NaN     2017-10-17
 5   35          15    3.99         NaN     2017-10-09
 6   35          17    3.99         NaN     2017-10-11
 7   35          5     4.29         True    2017-10-13
 8   35          8     4.29         NaN     2017-10-15
 9   35          2     4.29         NaN     2017-10-15
</code></pre>

<p>Basically I am trying to record how the sales of a product(UPC) reacted once the price changed for the following 7 days. I want to create a new column ['Reaction'] which records the sum of the unit sales from the day of price change, and 7 days forward. Keep in mind, sometimes a UPC has more than 2 price changes, so I want a different sum for each price change. 
So I want to see this: </p>

<pre><code>    UPC   Unit_Sales  Price   Price_Change  Date        Reaction
 0   22          15    1.99         NaN     2017-10-10      NaN
 1   22          7     2.19         True    2017-10-12      13   
 2   22          6     2.19         NaN     2017-10-13      NaN
 3   22          7     1.99         True    2017-10-16      11
 4   22          4     1.99         NaN     2017-10-19      NaN
 5   35          15    3.99         NaN     2017-10-09      NaN
 6   35          17    3.99         NaN     2017-10-11      NaN
 7   35          5     4.29         True    2017-10-13       15
 8   35          8     4.29         NaN     2017-10-15      NaN
 9   35          2     4.29         NaN     2017-10-18      NaN
</code></pre>

<p>What is difficult is how the dates are set up in my data. Sometimes (like for UPC 35) the dates don't range past 7 days. So I would want it to default to the next nearest date, or however many dates there are (if there are less than 7 days). </p>

<p>Here's what I've tried: 
I set the date to a datetime and I'm thinking of counting days by .days method. 
This is how I'm thinking of setting a code up (rough draft):  </p>

<pre><code>  x = df.loc[df['Price_Change'] == 'True']
  for x in df: 
       df['Reaction'] = sum(df.Unit_Sales[1day :8days])
</code></pre>

<p>Is there an easier way to do this, maybe without a for loop? </p>
","['python', 'pandas', 'dataframe']",48451101,"<p>You just need <code>ffill</code> with <code>groupby</code></p>

<pre><code>df.loc[df.Price_Change==True,'Reaction']=df.groupby('UPC').apply(lambda x : (x['Price_Change'].ffill()*x['Unit_Sales']).sum()).values
df
Out[807]: 
   UPC  Unit_Sales  Price Price_Change        Date  Reaction
0   22          15   1.99          NaN  2017-10-10       NaN
1   22           7   2.19         True  2017-10-12      24.0
2   22           6   2.19          NaN  2017-10-13       NaN
3   22           7   2.19          NaN  2017-10-16       NaN
4   22           4   2.19          NaN  2017-10-17       NaN
5   35          15   3.99          NaN  2017-10-09       NaN
6   35          17   3.99          NaN  2017-10-11       NaN
7   35           5   4.29         True  2017-10-13      15.0
8   35           8   4.29          NaN  2017-10-15       NaN
9   35           2   4.29          NaN  2017-10-15       NaN
</code></pre>

<p>Update</p>

<pre><code>df['New']=df.groupby('UPC').apply(lambda x : x['Price_Change']==True).cumsum().values

v1=df.groupby(['UPC','New']).apply(lambda x : (x['Price_Change'].ffill()*x['Unit_Sales']).sum())

df=df.merge(v1.reset_index())

df[0]=df[0].mask(df['Price_Change']!=True)
df
Out[927]: 
   UPC  Unit_Sales  Price Price_Change        Date  New     0
0   22          15   1.99          NaN  2017-10-10    0   NaN
1   22           7   2.19         True  2017-10-12    1  13.0
2   22           6   2.19          NaN  2017-10-13    1   NaN
3   22           7   1.99         True  2017-10-16    2  11.0
4   22           4   1.99          NaN  2017-10-17    2   NaN
5   35          15   3.99          NaN  2017-10-09    2   NaN
6   35          17   3.99          NaN  2017-10-11    2   NaN
7   35           5   4.29         True  2017-10-13    3  15.0
8   35           8   4.29          NaN  2017-10-15    3   NaN
9   35           2   4.29          NaN  2017-10-15    3   NaN
</code></pre>
",How sum certain values pandas column DataFrame specific date range I large DataFrame looks something like df UPC Unit Sales Price Price Change Date NaN True NaN True NaN NaN NaN True NaN NaN Basically I trying record sales product UPC reacted price changed following days I want create new column Reaction records sum unit sales day price change days forward Keep mind sometimes UPC price changes I want different sum price change So I want see UPC Unit Sales Price Price Change Date Reaction NaN NaN True NaN NaN True NaN NaN NaN NaN NaN NaN True NaN NaN NaN NaN What difficult dates set data Sometimes like UPC dates range past days So I would want default next nearest date however many dates less days Here I tried I set date datetime I thinking counting days days method This I thinking setting code rough draft x df loc,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
49775476,2018-04-11,2018,2,Python Pandas: reduce dataframe to contain with duplicate states,"<p>this is my first question I ask here, I couldn't find an easy solution to my problem.</p>

<p>I want to reduce a dataframe which contains state changes.
Similar to "".drop_duplicates()"" i want to reduce the dataframe with duplicate states, but instead it should only drop the row when the state didn't change.</p>

<p>Here my example dataframe:</p>

<pre><code>df = pd.DataFrame(data=({'Date':('Day1', 'Day2', 'Day3', 'Day4', 'Day5'),
                         'State':(1,0,0,2,0)}),
                  columns=(['State']), index=(['Date']))

df_reduced = df.drop_duplicates
df_reduced
</code></pre>

<p>The result  is unfortunately not the desired result:</p>

<pre><code>Out[]: 
             State
Date         
Day1             1
Day2             0
Day4             2
</code></pre>

<p>The desired output would contain also Day 5 with state 0.</p>

<p>I tried this with ""for and iterrows()"" construct, but it is very slow on longer time series data.</p>

<p>Hope you find an more elegant way, which works fast on longer time series data.</p>

<p>Thank you for your help in advance!</p>
","['python', 'pandas', 'dataframe']",49775573,"<p>One way is to compare your series to a series shifted by one value:</p>

<pre><code>df = pd.DataFrame(data={'Date':('Day1', 'Day2', 'Day3', 'Day4', 'Day5'),
                        'State':(1,0,0,2,0)})

df = df.set_index('Date')

res = df.loc[df['State'] != df['State'].shift()]

print(res)

#       State
# Date       
# Day1      1
# Day2      0
# Day4      2
# Day5      0
</code></pre>
",Python Pandas reduce dataframe contain duplicate states first question I ask I find easy solution problem I want reduce dataframe contains state changes Similar drop duplicates want reduce dataframe duplicate states instead drop row state change Here example dataframe df pd DataFrame data Date Day Day Day Day Day State columns State index Date df reduced df drop duplicates df reduced The result unfortunately desired result Out State Date Day Day Day The desired output would contain also Day state I tried iterrows construct slow longer time series data Hope find elegant way works fast longer time series data Thank help advance,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
49843864,2018-04-15,2018,3,How to create a dummy variable in Pandas Dataframe if a column matches certain values?,"<p>I have a Pandas Dataframe with a column (<code>ip</code>) with certain values and another Pandas Series not in this DataFrame with a collection of these values. I want to create a column in the DataFrame that is 1 if a given line has its <code>ip</code>in my Pandas Series (<code>black_ip</code>).</p>

<pre><code>import pandas as pd

dict = {'ip': {0: 103022, 1: 114221, 2: 47902, 3: 23550, 4: 84644}, 'os': {0: 23, 1: 19, 2: 17, 3: 13, 4: 19}}

df = pd.DataFrame(dict)

df
     ip  os
0  103022  23
1  114221  19
2   47902  17
3   23550  13
4   84644  19

blacklist = pd.Series([103022, 23550])

blacklist

0    103022
1     23550
</code></pre>

<p>My question is: how can I create a new column in <code>df</code> such that it shows 1 when the given <code>ip</code> in the blacklist and zero otherwise?</p>

<p>Sorry if this too dumb, I'm still new to programming. Thanks a lot in advance!</p>
","['python', 'python-3.x', 'pandas', 'dataframe']",49843897,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.isin.html"" rel=""nofollow noreferrer""><code>isin</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html"" rel=""nofollow noreferrer""><code>astype</code></a>:</p>

<pre><code>df['new'] = df['ip'].isin(blacklist).astype(np.int8)
</code></pre>

<p>Also is possible convert column to <code>categorical</code>s:</p>

<pre><code>df['new'] = pd.Categorical(df['ip'].isin(blacklist).astype(np.int8))

print (df)
       ip  os  new
0  103022  23    1
1  114221  19    0
2   47902  17    0
3   23550  13    1
4   84644  19    0
</code></pre>

<hr>

<p>For interesting in large <code>DataFrame</code> converting to <code>Categorical</code> not save memory:</p>

<pre><code>df = pd.concat([df] * 10000, ignore_index=True)

df['new1'] = pd.Categorical(df['ip'].isin(blacklist).astype(np.int8))
df['new2'] = df['ip'].isin(blacklist).astype(np.int8)
df['new3'] = df['ip'].isin(blacklist)
print (df.memory_usage())
Index        80
ip       400000
os       400000
new1      50096
new2      50000
new3      50000
dtype: int64
</code></pre>

<p><strong>Timings</strong>:</p>

<pre><code>np.random.seed(4545)

N = 10000
df = pd.DataFrame(np.random.randint(1000,size=N), columns=['ip'])
print (len(df))
10000

blacklist = pd.Series(np.random.randint(500,size=int(N/100)))
print (len(blacklist))
100

In [320]: %timeit df['ip'].isin(blacklist).astype(np.int8)
465 Âµs Â± 21.5 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)

In [321]: %timeit pd.Categorical(df['ip'].isin(blacklist).astype(np.int8))
915 Âµs Â± 49.9 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)

In [322]: %timeit pd.Categorical(df['ip'], categories = blacklist.unique()).notnull().astype(int)
1.59 ms Â± 20.1 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)

In [323]: %timeit df['new_column'] = [1 if x in blacklist.values else 0 for x in df.ip]
81.8 ms Â± 2.72 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
</code></pre>
",How create dummy variable Pandas Dataframe column matches certain values I Pandas Dataframe column ip certain values another Pandas Series DataFrame collection values I want create column DataFrame given line ipin Pandas Series black ip import pandas pd dict ip os df pd DataFrame dict df ip os blacklist pd Series blacklist My question I create new column df shows given ip blacklist zero otherwise Sorry dumb I still new programming Thanks lot advance,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
49865881,2018-04-16,2018,3,Complicated groupby in pandas,"<p>Is there a way to use the function form of <code>groupby()</code>, e.g. <code>groupby(f)</code> where f is a function, to group by table content? It looks like <code>f()</code> just gets called with the index.</p>

<p>Sample:</p>

<pre><code>import pandas as pd

df0 = pd.DataFrame([
    dict(age=30,sex='M',name='Jim',weight=143),
    dict(age=45,sex='F',name='Francine',weight=102),
    dict(age=22,sex='F',name='Jill',weight=190),
    dict(age=37,sex='M',name='Joseph',weight=221),
    dict(age=55,sex='M',name='Jerry',weight=187),
    dict(age=48,sex='M',name='Gus',weight=262),
    dict(age=45,sex='F',name='Jean',weight=112),
    dict(age=28,sex='F',name='Fiona',weight=133),
    dict(age=25,sex='M',name='Greg',weight=165),
    dict(age=34,sex='F',name='Jennifer',weight=137),
    dict(age=26,sex='M',name='Jason',weight=172),
    dict(age=28,sex='M',name='Jerome',weight=205),
    dict(age=61,sex='F',name='Faye',weight=140),
    dict(age=32,sex='M',name='Joshua',weight=180)])
df0.groupby('sex').mean()
</code></pre>

<p>This prints out</p>

<pre><code>           age      weight
sex                       
F    39.166667  135.666667
M    35.125000  191.875000
</code></pre>

<p>but what if I want to group by sex and then by first letter of name?</p>
","['python', 'pandas', 'pandas-groupby']",49865928,"<p>Try using str accessor with indexing as the second element in the groupby list:</p>

<pre><code>df0.groupby(['sex',df0['name'].str[0]]).mean()
</code></pre>

<p>Output:</p>

<pre><code>                age      weight
sex name                       
F   F     44.666667  125.000000
    J     33.666667  146.333333
M   G     36.500000  213.500000
    J     34.666667  184.666667
</code></pre>
",Complicated groupby pandas Is way use function form groupby e g groupby f f function group table content It looks like f gets called index Sample import pandas pd df pd DataFrame dict age sex M name Jim weight dict age sex F name Francine weight dict age sex F name Jill weight dict age sex M name Joseph weight dict age sex M name Jerry weight dict age sex M name Gus weight dict age sex F name Jean weight dict age sex F name Fiona weight dict age sex M name Greg weight dict age sex F name Jennifer weight dict age sex M name Jason weight dict age sex M name Jerome weight dict age sex F name Faye weight dict age sex M name Joshua weight df groupby sex mean This prints age weight sex F M I want group sex first letter name,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
50706936,2018-06-05,2018,2,Manipulating dataframe in python for Glicko calculation,"<p>I'm trying to run Glicko v2 calculations on a dataframe that I've loaded into python. Since each race is independent, I can only compare athletes that've competed in the same race.</p>

<pre><code>import pandas as pd

import numpy as np

df = pd.read_excel(""MyDirectory/sample.xlsx"")
</code></pre>

<hr>

<pre><code>Athlete Race_Id  Rank  Ranking       RD
   A   Race1     1     1500   0.0000
   B   Race1     2     1350  27.3220
   C   Race1     3     1700  11.2342
   D   Race2     1     1480  80.8880
   E   Race2     2     1500   0.8923
   F   Race2     3     1325   8.0090
</code></pre>

<p>My desired output would look something like this.</p>

<pre><code>Athlete1 Race_Id Ranking  RD Athlete2 Ranking 2 RD2
   A     Race1   1500   0.0000    B    1350  27.3220
   A     Race1   1500   0.0000    C    1700  11.2342
   B     Race1   1350  27.3220    C    1700  11.2342
   D     Race2   1480  80.8880    E    1500   0.8923
   D     Race2   1480  80.8880    F    1700  11.2342
   E     Race2   1500   0.8923    F    1700  11.2342
</code></pre>

<p>My thinking behind this was that if I had manipulated the dataframe to look like the one above I could easily define functions with my desired calculations and apply them to said dataframe.</p>

<p>In order to get my desired dataframe. I thought I'd reference the last line of each race and create a for loop which matches elements of a dataframe.</p>

<pre><code>data_lr= df.groupby(['Race_Id']).tail(1)

Athlete Race_Id  Rank  Ranking       RD
    C   Race1     3     1700  11.2342
    F   Race2     3     1325   8.0090
</code></pre>

<p>Where I struggle is creating the for loop needed to create my new dataframe? Any guidance would be appreciated or different methods to complete my goal would also be beneficial. Thanks,</p>
","['python', 'pandas', 'dataframe']",50707315,"<p>We can use a cartesian self-join and filtering to create your resulting dataframe:</p>

<pre><code>(df.merge(df, on='Race_Id',suffixes=('1','2'))
   .query('Rank1 != Rank2 and Athlete1 &lt; Athlete2')
   [['Athlete1','Race_Id','Ranking1','RD1','Athelete2','Ranking2','RD2']])
</code></pre>

<h1>Update for dynamic suffixes</h1>

<pre><code>(df.merge(df, on='Race_Id',suffixes=df.Race_Id.str.extract('Race(\d+)')[0].unique())
   .query('Rank1 != Rank2 and Athlete1 &lt; Athlete2')
   [['Athlete1','Race_Id','Ranking1','RD1','Athlete2','Ranking2','RD2']])
</code></pre>

<p>Output:</p>

<pre><code>   Athlete1 Race_Id  Ranking1      RD1 Athlete2  Ranking2      RD2
1         A   Race1      1500   0.0000        B      1350  27.3220
2         A   Race1      1500   0.0000        C      1700  11.2342
5         B   Race1      1350  27.3220        C      1700  11.2342
10        D   Race2      1480  80.8880        E      1500   0.8923
11        D   Race2      1480  80.8880        F      1325   8.0090
14        E   Race2      1500   0.8923        F      1325   8.0090
</code></pre>
",Manipulating dataframe python Glicko calculation I trying run Glicko v calculations dataframe I loaded python Since race independent I compare athletes competed race import pandas pd import numpy np df pd read excel MyDirectory sample xlsx Athlete Race Id Rank Ranking RD A Race B Race C Race D Race E Race F Race My desired output would look something like Athlete Race Id Ranking RD Athlete Ranking RD A Race B A Race C B Race C D Race E D Race F E Race F My thinking behind I manipulated dataframe look like one I could easily define functions desired calculations apply said dataframe In order get desired dataframe I thought I reference last line race create loop matches elements dataframe data lr df groupby Race Id tail Athlete Race Id Rank Ranking RD C Race F Race Where I struggle creating loop needed create new dataframe Any,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
51673220,2018-08-03,2018,4,How to group a python data frame by multilevel rows?,"<p>I have the following multi-level data frame:</p>

<pre><code>Year   2016                    2017                 
Quarter  3   4                 1                 2      
Month  Sep   Oct   Nov   Dec   Jan  Feb    Mar   Apr   May   Jun
A      0.16  0.95  0.92  0.45  0.30  0.35  0.95  0.88  0.18  0.10
B      0.88  0.67  0.07  0.70  0.74  0.33  0.77  0.21  0.81  0.85
C      0.79  0.56  0.13  0.19  0.94  0.23  0.72  0.62  0.66  0.93
</code></pre>

<p>I want to sum up over the quarters, so that the final result is as follows:</p>

<pre><code>Year     2016        2017   
Quarter  3     4     1     2
A        0.16  2.32  1.60  1.16
B        0.88  1.44  1.85  1.86
C        0.79  0.89  1.89  2.21
</code></pre>

<p>I tried with the following formula:</p>

<pre><code>df= df.groupby('Quarter').transform('sum')
</code></pre>

<p>but  I get this error:</p>

<pre><code>KeyError: 'Quarter'
</code></pre>

<p>Clearly that's the wrong way to approach it. Could anyone please a solution or to approach finding one.</p>

<p><strong>Additional information</strong></p>

<p>The output of the <code>df.index</code> command is: <code>Index([u'A', u'B',u'C'],dtype='object', name=u'DF name')</code></p>

<p>Thanks!</p>
","['python', 'pandas', 'pandas-groupby']",51673934,"<p>Just using <code>sum</code></p>

<pre><code>df.sum(level=[0,1],axis=1)
Out[14]: 
year    2016        2017      
quater     3     4     1     2
A       0.16  2.32  1.60  1.16
B       0.88  1.44  1.84  1.87
C       0.79  0.88  1.89  2.21
</code></pre>
",How group python data frame multilevel rows I following multi level data frame Year Quarter Month Sep Oct Nov Dec Jan Feb Mar Apr May Jun A B C I want sum quarters final result follows Year Quarter A B C I tried following formula df df groupby Quarter transform sum I get error KeyError Quarter Clearly wrong way approach Could anyone please solution approach finding one Additional information The output df index command Index u A u B u C dtype object name u DF name Thanks,"startoftags, python, pandas, pandasgroupby, endoftags",python pandas dataframe endoftags,python pandas pandasgroupby,python pandas dataframe,0.67
51765560,2018-08-09,2018,2,"How to update collision criterion to check for Sprite overlapping, not just touching?","<p>I want to update my collision definition. </p>

<p>Now it works as follows. When the object <code>worker</code> touches the object <code>fence</code>, the collision occurs. </p>

<p>However, the collision should occur only when the <code>worker</code> image overlaps with the <code>fence</code> image about ~40%. In other words, when it gets placed on top.</p>

<p>I do not need any perfect collision detection.</p>

<pre><code>import pygame, random
import sys

WHITE = (255, 255, 255)
GREEN = (20, 255, 140)
GREY = (210, 210 ,210)
RED = (255, 0, 0)
PURPLE = (255, 0, 255)

SCREENWIDTH=1000
SCREENHEIGHT=578

IMG_BACKGROUND = ""background.jpg""
IMG_WORKER_RUNNING = ""images/workers/worker_1.png""
IMG_WORKER_IDLE = ""images/workers/worker_2.png""
IMG_WORKER_ACCIDENT = ""images/workers/accident.png""


class Background(pygame.sprite.Sprite):
    def __init__(self, image_file, location, *groups):
        # we set a _layer attribute before adding this sprite to the sprite groups
        # we want the background to be actually in the back
        self._layer = -1
        pygame.sprite.Sprite.__init__(self, groups)
        # let's resize the background image now and only once
        self.image = pygame.transform.scale(pygame.image.load(image_file).convert(), (SCREENWIDTH, SCREENHEIGHT))
        self.rect = self.image.get_rect(topleft=location)



class GeoFenceInfluenceZone(pygame.sprite.Sprite):
    def __init__(self, rect, *groups):
        # we set a _layer attribute before adding this sprite to the sprite groups
        self._layer = 0
        pygame.sprite.Sprite.__init__(self, groups)
        self.image = pygame.surface.Surface((rect.width, rect.height))
        self.image.fill(GREY)
        self.rect = rect



class GeoFence(pygame.sprite.Sprite):
    def __init__(self, rect, risk_level, *groups):
        # we set a _layer attribute before adding this sprite to the sprite groups
        self._layer = 1
        pygame.sprite.Sprite.__init__(self, groups)
        self.image = pygame.surface.Surface((rect.width, rect.height))
        self.image.fill(GREEN)
        self.rect = rect
        self.risk_level = risk_level
        self.font = pygame.font.SysFont('Arial', 20)
        text = self.font.render(risk_level, 1, (255,0,0), GREEN)
        text_rect = text.get_rect(center=(rect.width/2, rect.height/2))
        self.image.blit(text, text_rect)



class Worker(pygame.sprite.Sprite):

    # we introduce to possible states: RUNNING and IDLE
    RUNNING = 0
    IDLE = 1
    ACCIDENT = 2
    NUMBER_OF_ACCIDENTS = 0

    def __init__(self, image_running, image_idle, image_accident, location, *groups):

        self.font = pygame.font.SysFont('Arial', 10)

        # each state has it's own image
        self.images = {
            Worker.RUNNING: pygame.transform.scale(get_image(image_running), (45, 45)),
            Worker.IDLE: pygame.transform.scale(get_image(image_idle), (20, 45)),
            Worker.ACCIDENT: pygame.transform.scale(get_image(image_accident), (40, 40))
        }

        # we set a _layer attribute before adding this sprite to the sprite groups
        # we want the workers on top
        self._layer = 2
        pygame.sprite.Sprite.__init__(self, groups)

        # let's keep track of the state and how long we are in this state already            
        self.state = Worker.IDLE
        self.ticks_in_state = 0

        self.image = self.images[self.state]
        self.rect = self.image.get_rect(topleft=location)

        self.direction = pygame.math.Vector2(0, 0)
        self.speed = random.randint(1, 3)
        self.set_random_direction()


    def set_random_direction(self):
        # random new direction or standing still
        vec = pygame.math.Vector2(random.randint(-100,100), random.randint(-100,100)) if random.randint(0, 5) &gt; 1 else pygame.math.Vector2(0, 0)

        # check the new vector and decide if we are running or fooling around
        length = vec.length()
        speed = sum(abs(int(v)) for v in vec.normalize() * self.speed) if length &gt; 0 else 0

        if (length == 0 or speed == 0) and (self.state != Worker.ACCIDENT):
            new_state = Worker.IDLE
            self.direction = pygame.math.Vector2(0, 0)
        elif self.state != Worker.ACCIDENT:
            new_state = Worker.RUNNING
            self.direction = vec.normalize()
        else:
            new_state = Worker.ACCIDENT

        self.ticks_in_state = 0
        self.state = new_state

        # use the right image for the current state
        self.image = self.images[self.state]


    def update(self, screen):
        self.ticks_in_state += 1
        # the longer we are in a certain state, the more likely is we change direction
        if random.randint(0, self.ticks_in_state) &gt; 70:
            self.set_random_direction()

        # now let's multiply our direction with our speed and move the rect
        vec = [int(v) for v in self.direction * self.speed]
        self.rect.move_ip(*vec)

        # if we're going outside the screen, change direction
        if not screen.get_rect().contains(self.rect):
            self.direction = self.direction * -1

        # spritecollide returns a list of all sprites in the group that collide with
        # the given sprite, but if the sprite is in this group itself, we have
        # to ignore a collision with itself
        if any(s for s in pygame.sprite.spritecollide(self, building_materials, False) if s != self):
            self.direction = self.direction * -1

        if any(s for s in pygame.sprite.spritecollide(self, machines, False) if s != self):
            self.direction = self.direction * -1

        # Risk handling
        self.handle_risks()

        if any(s for s in pygame.sprite.spritecollide(self, fences, False) if s != self):
            Worker.NUMBER_OF_ACCIDENTS += 1

        self.rect.clamp_ip(screen.get_rect())


    def handle_risks(self):
        for s in pygame.sprite.spritecollide(self, fences, False):
            if s != self:
                self.speed = 0
                self.state = Worker.ACCIDENT
                self.image = self.images[self.state]



class BuildingMaterials(pygame.sprite.Sprite):
    def __init__(self, image_file, location, *groups):
        # we set a _layer attribute before adding this sprite to the sprite groups
        self._layer = 2
        pygame.sprite.Sprite.__init__(self, groups)
        self.image = pygame.transform.scale(pygame.image.load(image_file).convert_alpha(), (40, 40))
        self.rect = self.image.get_rect(topleft=location)



class Excavator(pygame.sprite.Sprite):
    def __init__(self, image_file, location, *groups):
        # we set a _layer attribute before adding this sprite to the sprite groups
        self._layer = 3
        pygame.sprite.Sprite.__init__(self, groups)
        self.image = pygame.transform.scale(pygame.image.load(image_file).convert_alpha(), (170, 170))
        self.rect = self.image.get_rect(topleft=location)



image_cache = {}
def get_image(key):
    if not key in image_cache:
        image_cache[key] = pygame.image.load(key)
    return image_cache[key]


pygame.init()

# currently, one group would be enough
# but if you want to use some collision handling in the future
# it's best to group all sprites into special groups (no pun intended)
all_sprites = pygame.sprite.LayeredUpdates()
workers = pygame.sprite.Group()
building_materials = pygame.sprite.Group()
fences = pygame.sprite.Group()
fences_infl_zones = pygame.sprite.Group()

screen = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))
pygame.display.set_caption(""TEST"")

# create multiple workers
for pos in ((30,30), (50, 400), (200, 100), (700, 200)):
    Worker(IMG_WORKER_RUNNING, IMG_WORKER_IDLE, IMG_WORKER_ACCIDENT, pos, all_sprites, workers, building_materials, machines, fences)

# create multiple building material stocks
for pos in ((50,460),(50,500),(100,500),(850,30),(800,30)):
    BuildingMaterials(""images/materials/building_blocks{}.png"".format(random.randint(1,3)), pos, all_sprites, building_materials)

# create multiple geo-fences
risks = [""H"",""M"",""L""]
for rect in (pygame.Rect(510,150,75,52), pygame.Rect(450,250,68,40), pygame.Rect(450,370,68,48),
             pygame.Rect(0,0,20,SCREENHEIGHT),pygame.Rect(0,0,SCREENWIDTH,20),
             pygame.Rect(SCREENWIDTH-20,0,20,SCREENHEIGHT),pygame.Rect(0,SCREENHEIGHT-20,SCREENWIDTH,20)):
    risk = risks[random.randint(0,2)]
    GeoFence(rect, risk, all_sprites, fences)

# create influence zones for all geo-fences
for rect in (pygame.Rect(495,135,105,80), pygame.Rect(435,235,98,68), pygame.Rect(435,355,98,76)):
    GeoFenceInfluenceZone(rect, all_sprites, fences_infl_zones)

# and the background
Background(IMG_BACKGROUND, [0,0], all_sprites)

carryOn = True
clock = pygame.time.Clock()
while carryOn:
    for event in pygame.event.get():
        if event.type==pygame.QUIT:
            carryOn = False
            pygame.display.quit()
            pygame.quit()
            quit()

    all_sprites.update(screen)
    all_sprites.draw(screen)

    pygame.display.flip()

    clock.tick(20) 
</code></pre>

<p><a href=""https://i.stack.imgur.com/dVRt3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dVRt3.png"" alt=""enter image description here""></a></p>
","['python', 'python-3.x', 'pygame']",51781987,"<p>You can do this quite easily, since you can pass a callback function to <a href=""https://www.pygame.org/docs/ref/sprite.html#pygame.sprite.spritecollide"" rel=""nofollow noreferrer""><code>pygame.sprite.spritecollide</code></a> which is used to calculate if two sprites are colliding.</p>

<p>Pygame offers already several collision detection strategies, and the one you're looking for is <a href=""https://www.pygame.org/docs/ref/sprite.html#pygame.sprite.collide_rect_ratio"" rel=""nofollow noreferrer""><code>pygame.sprite.collide_rect_ratio</code></a>:</p>

<blockquote>
  <p>A callable class that checks for collisions between two sprites, using a scaled version of the sprites rects.</p>
</blockquote>

<hr>

<p>Use it like this to get what you want:</p>

<pre><code>def handle_risks(self):
    for s in pygame.sprite.spritecollide(self, fences, False, pygame.sprite.collide_rect_ratio(0.8)):
         ....
</code></pre>
",How update collision criterion check Sprite overlapping touching I want update collision definition Now works follows When object worker touches object fence collision occurs However collision occur worker image overlaps fence image In words gets placed top I need perfect collision detection import pygame random import sys WHITE GREEN GREY RED PURPLE SCREENWIDTH SCREENHEIGHT IMG BACKGROUND background jpg IMG WORKER RUNNING images workers worker png IMG WORKER IDLE images workers worker png IMG WORKER ACCIDENT images workers accident png class Background pygame sprite Sprite def init self image file location groups set layer attribute adding sprite sprite groups want background actually back self layer pygame sprite Sprite init self groups let resize background image self image pygame transform scale pygame image load image file convert SCREENWIDTH SCREENHEIGHT self rect self image get rect topleft location class pygame sprite Sprite def init self rect groups set layer attribute adding sprite sprite,"startoftags, python, python3x, pygame, endoftags",python python3x list endoftags,python python3x pygame,python python3x list,0.67
52367739,2018-09-17,2018,2,Numpy broadcasting on multiple arrays,"<p>I have a basis for a plane in 3 dimensions: (u, v).</p>

<p>I would like to obtain all linear combinations of this basis to basically go through my whole plane:</p>

<p>for i in [0, 512[ and j in [0, 512[, get all (i * u + j * v).</p>

<p>I need this to be fast so for loops are not really an option. How can I do that with numpy broadcasting?</p>

<p>Had a look at <a href=""https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html"" rel=""nofollow noreferrer"">https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html</a> and my impression is that it's not possible to do...</p>

<p>Tried:</p>

<pre><code># This is an orthonormal basis but there is no guarantee it is
u = np.array([1, 0, 0])
v = np.array([0, 1, 0])
tmp = np.arange(512)
factors = itertools.combinations(tmp, 2)
pixels = factors[0] * u + factors[1] + v
</code></pre>

<p>But obviously it does not work.</p>

<p>Is there a solution to this problem? And if yes then how?</p>
","['python', 'arrays', 'numpy']",52367800,"<p>Multiply (u, v) with a 2D index grid:</p>

<pre><code>ind = np.indices((512, 512))
pixels = ind[0, ..., np.newaxis] * u + ind[1, ..., np.newaxis] * v

&gt;&gt;&gt; %timeit ind = np.indices((512, 512)); pixels = ind[0, ..., np.newaxis] * u + ind[1, ..., np.newaxis] * v
8.06 ms Â± 69.8 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
</code></pre>

<p>Multiply u with a 1D index range, multiply v with a 1D index range, broadcast and combine to 2D:</p>

<pre><code>i512 = np.arange(512)[:, np.newaxis]
pixels = (i512 * u)[:, np.newaxis, :] + (i512 * v)[np.newaxis, :, :]

&gt;&gt;&gt; %timeit i512 = np.arange(512)[:, np.newaxis]; pixels = (i512 * u)[:, np.newaxis, :] + (i512 * v)[np.newaxis, :, :]
4.06 ms Â± 58.6 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
</code></pre>
",Numpy broadcasting multiple arrays I basis plane dimensions u v I would like obtain linear combinations basis basically go whole plane j get u j v I need fast loops really option How I numpy broadcasting Had look https docs scipy org doc numpy user basics broadcasting html impression possible Tried This orthonormal basis guarantee u np array v np array tmp np arange factors itertools combinations tmp pixels factors u factors v But obviously work Is solution problem And yes,"startoftags, python, arrays, numpy, endoftags",python pandas numpy endoftags,python arrays numpy,python pandas numpy,0.67
52430892,2018-09-20,2018,3,Forward filling missing dates into Python Pandas Dataframe,"<p>I have a Panda's dataframe that is filled as follows:</p>

<pre><code>ref_date    tag
1/29/2010   1
2/26/2010   3
3/31/2010   4
4/30/2010   4
5/31/2010   1
6/30/2010   3
8/31/2010   1
9/30/2010   4
12/31/2010  2
</code></pre>

<p>Note how there are missing months (i.e. 7, 10, 11) in the data. I want to fill in the missing data through a forward filling method so that it looks like this:</p>

<pre><code>ref_date    tag
1/29/2010   1
2/26/2010   3
3/31/2010   4
4/30/2010   4
5/31/2010   1
6/30/2010   3
7/30/2010   3
8/31/2010   1
9/30/2010   4
10/29/2010  4
11/30/2010  4
12/31/2010  2
</code></pre>

<p>The tag of the missing date will have the <strong>tag of the previous</strong>. All dates represent the <strong>last business day</strong> of the month. </p>

<p>This is what I tried to do:</p>

<pre><code>idx = pd.date_range(start='1/29/2010', end='12/31/2010', freq='BM')
df.ref_date.index = pd.to_datetime(df.ref_date.index)
df = df.reindex(index=[idx], columns=[ref_date], method='ffill')
</code></pre>

<p>It's giving me the error: </p>

<blockquote>
  <p>TypeError: Cannot compare type 'Timestamp' with type 'int' </p>
</blockquote>

<p>where <code>pd</code> is pandas and <code>df</code> is the dataframe.</p>

<p>I'm new to Pandas Dataframe, so any help would be appreciated!</p>
","['python', 'pandas', 'dataframe']",52431033,"<p>You were very close, you just need to set the dataframe's index with the <code>ref_date</code>, reindex it to the business day month end index while specifying <code>ffill</code> at the method, then reset the index and rename back to the original:</p>

<pre><code># First ensure the dates are Pandas Timestamps.
df['ref_date'] = pd.to_datetime(df['ref_date'])

# Create a monthly index.
idx_monthly = pd.date_range(start='1/29/2010', end='12/31/2010', freq='BM')

# Reindex to the daily index, forward fill, reindex to the monthly index.
&gt;&gt;&gt; (df
     .set_index('ref_date')
     .reindex(idx_monthly, method='ffill')
     .reset_index()
     .rename(columns={'index': 'ref_date'}))
     ref_date  tag
0  2010-01-29  1.0
1  2010-02-26  3.0
2  2010-03-31  4.0
3  2010-04-30  4.0
4  2010-05-31  1.0
5  2010-06-30  3.0
6  2010-07-30  3.0
7  2010-08-31  1.0
8  2010-09-30  4.0
9  2010-10-29  4.0
10 2010-11-30  4.0
11 2010-12-31  2.0
</code></pre>
",Forward filling missing dates Python Pandas Dataframe I Panda dataframe filled follows ref date tag Note missing months e data I want fill missing data forward filling method looks like ref date tag The tag missing date tag previous All dates represent last business day month This I tried idx pd date range start end freq BM df ref date index pd datetime df ref date index df df reindex index idx columns ref date method ffill It giving error TypeError Cannot compare type Timestamp type int pd pandas df dataframe I new Pandas Dataframe help would appreciated,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
53062225,2018-10-30,2018,2,How can check the duplication on the group level?,"<p>How can I check for duplicated groups and remove them? Here is my data frame:</p>

<pre><code>Group     Value_1      Value_2
 A          17           0.1
 A          20           0.8
 A          22           0.9
 A          24           0.13

 B          17           0.1
 B          20           0.8
 B          22           0.9
 B          24           0.13

 C          17           0.1
 C          20           0.8
 C          22           0.9
 C          26           0.11    
</code></pre>

<p>In this data frame group A and B are duplicate where as C is not because its forth element is different and thus it is deeper to be unique not duplicate, the resultant data frame should look like this:</p>

<pre><code>Group     Value_1      Value_2
 A          17           0.1
 A          20           0.8
 A          22           0.9
 A          24           0.13


 C          17           0.1
 C          20           0.8
 C          22           0.9
 C          26           0.11    
</code></pre>

<p>I tried to groupby and check for duplicates, but this will check the values on the observational level. How can check the duplication on the group level?</p>
","['python', 'pandas', 'dataframe']",53062285,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>groupby</code></a> and aggregate by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.agg.html"" rel=""nofollow noreferrer""><code>agg</code></a> with <code>frozenset</code>, then remove duplicates by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html"" rel=""nofollow noreferrer""><code>drop_duplicates</code></a> (by default by all columns) and get indices - all groups names:</p>

<pre><code>idx = df.groupby('Group').agg(frozenset).drop_duplicates().index
#alternative solution
idx = df.groupby('Group').agg(tuple).drop_duplicates().index
</code></pre>

<p>Or reshape to by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.cumcount.html"" rel=""nofollow noreferrer""><code>cumcount</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>set_index</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unstack.html"" rel=""nofollow noreferrer""><code>unstack</code></a>:</p>

<pre><code>g = df.groupby('Group').cumcount()
idx = df.set_index(['Group',g]).unstack().drop_duplicates().index
</code></pre>

<p>Last filter by <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""nofollow noreferrer""><code>boolean indexing</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.isin.html"" rel=""nofollow noreferrer""><code>isin</code></a>:</p>

<pre><code>df = df[df['Group'].isin(idx)]
print (df)
   Group  Value_1  Value_2
0      A       17     0.10
1      A       20     0.80
2      A       22     0.90
3      A       24     0.13
8      C       17     0.10
9      C       20     0.80
10     C       22     0.90
11     C       26     0.11
</code></pre>
",How check duplication group level How I check duplicated groups remove Here data frame Group Value Value A A A A B B B B C C C C In data frame group A B duplicate C forth element different thus deeper unique duplicate resultant data frame look like Group Value Value A A A A C C C C I tried groupby check duplicates check values observational level How check duplication group level,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
54085939,2019-01-08,2019,2,"Why are my pandas DataFrame columns Dataframes too, not Series?","<p><strong>Update at end</strong>
<strong>Update 2 at end</strong></p>

<p>I read from here:
<a href=""https://stackoverflow.com/questions/22341271/get-list-from-pandas-dataframe-column"">get list from pandas dataframe column</a></p>

<blockquote>
  <p>Pandas DataFrame columns are Pandas Series when you pull them out</p>
</blockquote>

<p>However this is not true in my case:</p>

<p>First part (building up the DataFrame reading json scraped)
Because it contains business info I cannot show the full code, but basically it reads one row of data (stored in Series) and append at the end of the DataFrame.</p>

<pre><code>dfToWrite = pandas.DataFrame(columns=[lsHeader]) # Empty with column headers
for row in jsAdtoolJSON['rows']:
    lsRow = []
    for col in row['row']:
        lsRow.append((col['primary'])['value'])
    dfRow = pandas.Series(lsRow, index = dfToWrite.columns)
dfToWrite = dfToWrite.append(dfRow, ignore_index = True)
</code></pre>

<p>Next part (check type): (Please ignore the functionality of the function)</p>

<pre><code>def CalcMA(df: pandas.DataFrame, target: str, period: int, maname: str):
    print(type(df[target]))
</code></pre>

<p>Finally call the function: (""Raw_Impressions"" is a column header)</p>

<pre><code>CalcMA(dfToWrite, ""Raw_Impressions"", 5, ""ImpMA5"")
</code></pre>

<p>Python console shows:</p>

<blockquote>
  <p>class 'pandas.core.frame.DataFrame'</p>
</blockquote>

<p><strong>Additional Question</strong>: How to get a list from a Dataframe column if it's not a Series (in which case I can use <code>tolist()</code>)?</p>

<p><strong>Update 1</strong>
From here:
<a href=""https://stackoverflow.com/questions/42316088/bokeh-attributeerror-dataframe-object-has-no-attribute-tolist"">Bokeh: AttributeError: &#39;DataFrame&#39; object has no attribute &#39;tolist&#39;</a></p>

<p>I figured out that I need to use <code>.value.tolist()</code>, however it still doesn't explain why I'm getting another Dataframe, not a Series when I pull out a column.</p>

<p><strong>Update 2</strong>
Found out that df has MultiIndex, very surprised:</p>

<blockquote>
  <p>MultiIndex(levels=[['COST_/<em>CPM', 'CTR', 'ECPM</em>/_ROI', 'Goal_Ratio', 'Hour_of_the_Day', 'IMP./Joins', 'Raw_Clicks_/_Unique_Clicks', 'Raw_Impressions', 'Unique_Goal_/_UniqueGoal_Forecasted_Value']],
             labels=[[4, 7, 5, 6, 1, 8, 3, 0, 2]])</p>
</blockquote>

<p>I don't see the <code>labels</code> when printing out the df / writing to .csv, it's just a normal DataFrame. Not sure where did I get the labels.</p>
","['python', 'pandas', 'dataframe']",54086264,"<p>I think you have duplicated columns names, so if want select <code>Series</code> get <code>DataFrame</code>:</p>

<pre><code>df = pd.DataFrame([[1,2],[4,5], [7,8]], index=list('aab')).T
print (df)
   a  a  b
0  1  4  7
1  2  5  8

print (df['a'])
   a  a
0  1  4
1  2  5

print (type(df['a']))
&lt;class 'pandas.core.frame.DataFrame'&gt;

print (df['b'])
0    7
1    8
Name: b, dtype: int64

print (type(df['b']))
&lt;class 'pandas.core.series.Series'&gt;
</code></pre>

<p>EDIT:</p>

<p>Here is another problem, one level <code>MultiIndex</code>, solution is reassign first level back to columns with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.MultiIndex.get_level_values.html"" rel=""nofollow noreferrer""><code>get_level_values</code></a>:</p>

<pre><code>mux = pd.MultiIndex([['COST_/CPM', 'CTR', 'ECPM/_ROI', 'Goal_Ratio', 'Hour_of_the_Day', 
                      'IMP./Joins',  'Raw_Clicks_/_Unique_Clicks', 'Raw_Impressions',
                      'Unique_Goal_/_UniqueGoal_Forecasted_Value']], 
labels=[[4, 7, 5, 6, 1, 8, 3, 0, 2]])

df = pd.DataFrame([range(9)], columns=mux)
print (type(df['CTR']))
&lt;class 'pandas.core.frame.DataFrame'&gt;

df.columns = df.columns.get_level_values(0)
print (type(df['CTR']))
&lt;class 'pandas.core.series.Series'&gt;
</code></pre>
",Why pandas DataFrame columns Dataframes Series Update end Update end I read get list pandas dataframe column Pandas DataFrame columns Pandas Series pull However true case First part building DataFrame reading json scraped Because contains business info I cannot show full code basically reads one row data stored Series append end DataFrame dfToWrite pandas DataFrame columns lsHeader Empty column headers row jsAdtoolJSON rows lsRow col row row lsRow append col primary value dfRow pandas Series lsRow index dfToWrite columns dfToWrite dfToWrite append dfRow ignore index True Next part check type Please ignore functionality function def CalcMA df pandas DataFrame target str period int maname str print type df target Finally call function Raw Impressions column header CalcMA dfToWrite Raw Impressions ImpMA Python console shows class pandas core frame DataFrame Additional Question How get list Dataframe column Series case I use tolist Update From Bokeh AttributeError DataFrame object attribute tolist I,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
57935194,2019-09-14,2019,2,"Manipulating a Dataframe using pandas, create new columns and fill them with values based on looking up existing data within dataframe","<p>Given data</p>

<pre><code>df = pd.DataFrame(
    {
        'c': ['p1', 'p2', 'p3'],
        'v': [ 2  ,  8  ,  3],
    }
)
</code></pre>

<p>This outputs </p>

<pre><code>    c  v  
0  p1  2   
1  p2  8   
2  p3  3   
</code></pre>

<p>I'm wondering how to create the following using pandas</p>

<pre><code>    c  v  p1  p2  p3
0  p1  2   2   0   0
1  p2  8   0   8   0
2  p3  3   0   0   3
</code></pre>

<p>In such a way that I could scale this up to 1000 rows rather than 3 rows (so no hard coding)</p>

<h1>edit</h1>

<p>my current approach is as follows : </p>

<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame(
    {
        'c': ['p1', 'p2', 'p3'],
        'v': [ 2  ,  8  ,  3],
    }
)

# create columns with zero 
for p in df['c']:
    df[p] = 0
# iterate over columns, set values 
for p in df['c']:
    # get value
    value = df.loc[ df.loc[:,'c']==p, 'v']
    # get the location of the element to set
    idx=df.loc[:,'c']==p
    df.loc[idx,p]=value
</code></pre>

<p>which outputs the correct result, I feel as though it's a very clunky approach though.</p>

<h1>Edit two</h1>

<p>The solution must work for the following data : </p>

<pre><code>df = pd.DataFrame(
    {
        'c': ['p1', 'p2', 'p3', 'p1'],
        'v': [ 2  ,  8  ,  3, 4],
    }
)
</code></pre>

<p>returning</p>

<pre><code>    c  v  p1  p2  p3
0  p1  2   2   0   0
1  p2  8   0   8   0
2  p3  3   0   0   3
3  p1  9   9   0   0
</code></pre>

<p>Meaning that the approach of using a pivot table as </p>

<pre><code>piv = df.pivot_table(index='c', columns='c', values='v', fill_value=0)
df = df.join(piv.reset_index(drop=True))
</code></pre>

<p>wouldn't work, although for the original data set it was fine. </p>
","['python', 'python-3.x', 'pandas', 'dataframe']",57935778,"<p>Multiple indicator DataFrame created by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html"" rel=""nofollow noreferrer""><code>get_dummies</code></a> with column <code>v</code> and <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html"" rel=""nofollow noreferrer""><code>DataFrame.join</code></a> to original:</p>

<pre><code>df1 = df.join(pd.get_dummies(df[""c""]).mul(df['v'], axis=0))
print (df1)
    c  v  p1  p2  p3
0  p1  2   2   0   0
1  p2  8   0   8   0
2  p3  3   0   0   3
</code></pre>

<p>EDIT:</p>

<pre><code>df1 = df.join(pd.get_dummies(df[""c""]).mul(df['v'], axis=0))
print (df1)
    c  v  p1  p2  p3
0  p1  2   2   0   0
1  p2  8   0   8   0
2  p3  3   0   0   3
3  p1  4   4   0   0
</code></pre>

<p><strong>Details</strong>:</p>

<pre><code>#indicator column
print (pd.get_dummies(df[""c""]))
   p1  p2  p3
0   1   0   0
1   0   1   0
2   0   0   1
3   1   0   0

#all values are multiple by c column
print (pd.get_dummies(df[""c""]).mul(df['v'], axis=0))
   p1  p2  p3
0   2   0   0
1   0   8   0
2   0   0   3
3   4   0   0
</code></pre>
",Manipulating Dataframe using pandas create new columns fill values based looking existing data within dataframe Given data df pd DataFrame c p p p v This outputs c v p p p I wondering create following using pandas c v p p p p p p In way I could scale rows rather rows hard coding edit current approach follows df pd DataFrame c p p p v create columns zero p df c df p iterate columns set values p df c get value value df loc df loc c p v get location element set idx df loc c p df loc idx p value outputs correct result I feel though clunky approach though Edit two The solution must work following data df pd DataFrame c p p p p v returning c v p p p p p p p Meaning approach using pivot table piv df,"startoftags, python, python3x, pandas, dataframe, endoftags",python pandas dataframe endoftags,python python3x pandas dataframe,python pandas dataframe,0.87
61042524,2020-04-05,2020,11,create a NxN matrix from one column pandas,"<p>i have dataframe with each row having a list value.</p>

<pre><code>id     list_of_value
0      ['a','b','c']
1      ['d','b','c']
2      ['a','b','c']
3      ['a','b','c']
</code></pre>

<p>i have to do a calculate a score with one row and against all the other rows</p>

<p>For eg:</p>

<pre><code>Step 1: Take value of id 0: ['a','b','c'],
Step 2: find the intersection between id 0 and id 1 , 
        resultant = ['b','c']
Step 3: Score Calculation =&gt; resultant.size / id.size
</code></pre>

<p><strong>repeat step 2,3 between id 0 and id 1,2,3, similarly for all the ids.</strong></p>

<p>and create a N x N dataframe; such as this:</p>

<pre><code>-  0  1    2  3
0  1  0.6  1  1
1  1  1    1  1 
2  1  1    1  1
3  1  1    1  1
</code></pre>

<p>Right now my code has just one for loop:</p>

<pre><code>def scoreCalc(x,queryTData):
    #mathematical calculation
    commonTData = np.intersect1d(np.array(x),queryTData)
    return commonTData.size/queryTData.size

ids = list(df['feed_id'])
dfSim = pd.DataFrame()

for indexQFID in range(len(ids)):
    queryTData = np.array(df.loc[df['id'] == ids[indexQFID]]['list_of_value'].values.tolist())

    dfSim[segmentDfFeedIds[indexQFID]] = segmentDf['list_of_value'].apply(scoreCalc,args=(queryTData,))
</code></pre>

<p>Is there a better way to do this? can i just write one apply function instead doing a for-loop iteration.
 can i make it faster?</p>
","['python', 'pandas', 'numpy']",61102019,"<p>If you data is not too big, you can use <code>get_dummies</code> to encode the values and do a matrix multiplication:</p>

<pre><code>s = pd.get_dummies(df.list_of_value.explode()).sum(level=0)
s.dot(s.T).div(s.sum(1))
</code></pre>

<p>Output:</p>

<pre><code>          0         1         2         3
0  1.000000  0.666667  1.000000  1.000000
1  0.666667  1.000000  0.666667  0.666667
2  1.000000  0.666667  1.000000  1.000000
3  1.000000  0.666667  1.000000  1.000000
</code></pre>

<hr>

<p><strong>Update</strong>: Here's a short explanation for the code. The main idea is to turn the given lists into one-hot-encoded:</p>

<pre><code>   a  b  c  d
0  1  1  1  0
1  0  1  1  1
2  1  1  1  0
3  1  1  1  0
</code></pre>

<p>Once we have that, the size of intersection of the two rows, say, <code>0</code> and <code>1</code> is just their dot product, because a character belongs to both rows if and only if it is represented by <code>1</code> in both.</p>

<p>With that in mind, first use</p>

<pre><code>df.list_of_value.explode()
</code></pre>

<p>to turn each cell into a series and concatenate all of those series. Output:</p>

<pre><code>0    a
0    b
0    c
1    d
1    b
1    c
2    a
2    b
2    c
3    a
3    b
3    c
Name: list_of_value, dtype: object
</code></pre>

<p>Now, we use <code>pd.get_dummies</code> on that series to turn it to a one-hot-encoded dataframe:</p>

<pre><code>   a  b  c  d
0  1  0  0  0
0  0  1  0  0
0  0  0  1  0
1  0  0  0  1
1  0  1  0  0
1  0  0  1  0
2  1  0  0  0
2  0  1  0  0
2  0  0  1  0
3  1  0  0  0
3  0  1  0  0
3  0  0  1  0
</code></pre>

<p>As you can see, each value has its own row. Since we want to combine those belong to the same original row to one row, we can just sum them by the original index. Thus</p>

<pre><code>s = pd.get_dummies(df.list_of_value.explode()).sum(level=0)
</code></pre>

<p>gives the binary-encoded dataframe we want. The next line</p>

<pre><code>s.dot(s.T).div(s.sum(1))
</code></pre>

<p>is just as your logic: <code>s.dot(s.T)</code> computes dot products by rows, then <code>.div(s.sum(1))</code> divides counts by rows.</p>
",create NxN matrix one column pandas dataframe row list value id list value b c b c b c b c calculate score one row rows For eg Step Take value id b c Step find intersection id id resultant b c Step Score Calculation gt resultant size id size repeat step id id similarly ids create N x N dataframe Right code one loop def scoreCalc x queryTData mathematical calculation commonTData np intersect np array x queryTData return commonTData size queryTData size ids list df feed id dfSim pd DataFrame indexQFID range len ids queryTData np array df loc df id ids indexQFID list value values tolist dfSim segmentDfFeedIds indexQFID segmentDf list value apply scoreCalc args queryTData Is better way write one apply function instead loop iteration make faster,"startoftags, python, pandas, numpy, endoftags",python pandas dataframe endoftags,python pandas numpy,python pandas dataframe,0.67
61374496,2020-04-22,2020,3,ValueError: Unknown layer: KerasLayer,"<p>I have the following code: </p>

<pre><code>from keras.models import model_from_json

with open('modelS.json', 'r') as f: 
  json = f.read() 
loaded_model = model_from_json(json)
</code></pre>

<p>Here is the json file used in the above code: </p>

<p><code>{""class_name"": ""Sequential"", ""config"": {""name"": ""sequential"", ""layers"": [{""class_name"": ""KerasLayer"", ""config"": {""name"": ""keras_layer"", ""trainable"": true, ""batch_input_shape"": [null], ""dtype"": ""string"", ""handle"": ""https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1""}}, {""class_name"": ""Dense"", ""config"": {""name"": ""dense"", ""trainable"": true, ""dtype"": ""float32"", ""units"": 16, ""activation"": ""relu"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""Dense"", ""config"": {""name"": ""dense_1"", ""trainable"": true, ""dtype"": ""float32"", ""units"": 16, ""activation"": ""relu"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""Dense"", ""config"": {""name"": ""dense_2"", ""trainable"": true, ""dtype"": ""float32"", ""units"": 1, ""activation"": ""sigmoid"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}], ""build_input_shape"": [null]}, ""keras_version"": ""2.3.0-tf"", ""backend"": ""tensorflow""}</code></p>

<p>But I get the following error from the last line: </p>

<p><code>ValueError: Unknown layer: KerasLayer</code>. </p>

<p>What could be the reason for this?</p>
","['python', 'tensorflow', 'keras']",61692760,"<p>Mentioning the Answer in this (Answer) Section even though it is present in the Comments Section, for the benefit of the community. </p>

<p>Adding the <code>import</code> statement: <code>import tensorflow_hub as hub</code> and then using a custom layer with <code>custom_objects={'KerasLayer': hub.KerasLayer}</code> in the <code>model_from_json()</code> statement has resolved the error.</p>

<p>Complete working code is shown below:</p>

<pre><code>from tensorflow.keras.models import model_from_json

import tensorflow_hub as hub

with open('models.json', 'r') as f: 
  json = f.read() 
loaded_model = model_from_json(json, custom_objects={'KerasLayer': hub.KerasLayer})
</code></pre>
",ValueError Unknown layer KerasLayer I following code keras models import model json open modelS json r f json f read loaded model model json json Here json file used code class name Sequential config name sequential layers class name KerasLayer config name keras layer trainable true batch input shape null dtype string handle https tfhub dev google tf preview nnlm en dim class name Dense config name dense trainable true dtype float units activation relu use bias true kernel initializer class name GlorotUniform config seed null bias initializer class name Zeros config kernel regularizer null bias regularizer null activity regularizer null kernel constraint null bias constraint null class name Dense config name dense trainable true dtype float units activation relu use bias true kernel initializer class name GlorotUniform config seed null bias initializer class name Zeros config kernel regularizer null bias regularizer null activity regularizer null kernel constraint null bias,"startoftags, python, tensorflow, keras, endoftags",python django djangorestframework endoftags,python tensorflow keras,python django djangorestframework,0.33
63859379,2020-09-12,2020,2,Training model with fit_generator does not show val_loss and val_acc and interrupted at first epoch,"<p>I implemented a data generator to split my training data into mini batches of 256 to avoid memory errors. Its running on the training data but it does not show validation loss and validation accuracy at the end of each epochs. I also applied the data generator on validation data and defined validation steps. I don't know exactly what's the wrong with the code that its's not showing validation loss and accuracy?
here is the code:</p>
<pre><code>early_stopping_cb=tf.keras.callbacks.EarlyStopping(patience=3,restore_best_weights=True)
batch_size=256
epoch_steps=math.ceil(len(utt)/ batch_size)
val_steps=math.ceil(len(val_prev)/ batch_size)

hist = model.fit_generator(generate_data(utt_minus_one, utt, y_train, batch_size),
                steps_per_epoch=epoch_steps, epochs=3,
                callbacks = [early_stopping_cb],
                validation_data=generate_data(val_prev, val_curr,y_val,batch_size),
                validation_steps=val_steps,  class_weight=custom_weight_dict,
                 verbose=1)
</code></pre>
<p>here is code for generator:</p>
<pre><code>#method to use generator to split data into mini batches of 256 each loaded at run time
def generate_data(X1,X2,Y,batch_size):
  p_input=[]
  c_input=[]
  target=[]
  batch_count=0
  for i in range(len(X1)):
    p_input.append(X1[i])
    c_input.append(X2[i])
    target.append(Y[i])
    batch_count+=1
    if batch_count&gt;batch_size:
      prev_X=np.array(p_input,dtype=np.int64)
      cur_X=np.array(c_input,dtype=np.int64)
      cur_y=np.array(target,dtype=np.int32)
      yield ([prev_X,cur_X],cur_y ) 
      p_input=[]
      c_input=[]
      target=[]
      batch_count=0
  return
</code></pre>
<p>Here is the trace for first epoch which also gives an error:</p>
<pre><code>Epoch 1/3
346/348 [============================&gt;.] - ETA: 4s - batch: 172.5000 - size: 257.0000 - loss: 0.8972 - accuracy: 0.8424WARNING:tensorflow:Your dataset iterator ran out of data; interrupting training. Make sure that your iterator can generate at least `steps_per_epoch * epochs` batches (in this case, 1044 batches). You may need touse the repeat() function when building your dataset.
WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy
346/348 [============================&gt;.] - 858s 2s/step - batch: 172.5000 - size: 257.0000 - loss: 0.8972 - accuracy: 0.8424
</code></pre>
<p>Can any one help in sorting out these issues?</p>
","['python', 'tensorflow', 'machine-learning', 'keras', 'deep-learning']",63861003,"<p>There is  need of a one while loop for per epoch over the for loop to split into mini batches. So if there are 348 batches per epochs then 3*348= 1044 batches overall.</p>
<pre><code>
#method to use generator to split data into mini batches of 256 each loaded at run time
def generate_data(X1,X2,Y,batch_size):
  count=0
  p_input=[]
  c_input=[]
  target=[]
  batch_count=0
  while True:
    for i in range(len(X1)):
      p_input.append(X1[i])
      c_input.append(X2[i])
      target.append(Y[i])
      batch_count+=1
      if batch_count&gt;batch_size:
        count=count+1
        prev_X=np.array(p_input,dtype=np.int64)
        cur_X=np.array(c_input,dtype=np.int64)
        cur_y=np.array(target,dtype=np.int32)
        yield ([prev_X,cur_X],cur_y ) 
        p_input=[]
        c_input=[]
        target=[]
        batch_count=0
    print(count)
  return
</code></pre>
<p>And trace for first epoch:</p>
<pre><code>Epoch 1/3
335/347 [===========================&gt;..] - ETA: 30s - batch: 167.0000 - size: 257.0000 - loss: 1.2734 - accuracy: 0.8105346
347/347 [==============================] - ETA: 0s - batch: 173.0000 - size: 257.0000 - loss: 1.2635 - accuracy: 0.8113WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
86
347/347 [==============================] - 964s 3s/step - batch: 173.0000 - size: 257.0000 - loss: 1.2635 - accuracy: 0.8113 - val_loss: 0.5700 - val_accuracy: 0.8367
</code></pre>
",Training model fit generator show val loss val acc interrupted first epoch I implemented data generator split training data mini batches avoid memory errors Its running training data show validation loss validation accuracy end epochs I also applied data generator validation data defined validation steps I know exactly wrong code showing validation loss accuracy code early stopping cb tf keras callbacks EarlyStopping patience restore best weights True batch size epoch steps math ceil len utt batch size val steps math ceil len val prev batch size hist model fit generator generate data utt minus one utt train batch size steps per epoch epoch steps epochs callbacks early stopping cb validation data generate data val prev val curr val batch size validation steps val steps class weight custom weight dict verbose code generator method use generator split data mini batches loaded run time def generate data X X Y batch size,"startoftags, python, tensorflow, machinelearning, keras, deeplearning, endoftags",python tensorflow keras endoftags,python tensorflow machinelearning keras deeplearning,python tensorflow keras,0.77
66993121,2021-04-07,2021,2,Average for similar looking data in a column using Pandas,"<p>I'm working on a large data with more than 60K rows.</p>
<p>I have continuous measurement of current in a column. A <strong>code</strong> is measured for a second where the equipment measures it for 14/15/16/17 times, depending on the equipment speed and then the measurement moves to the next <strong>code</strong> and again measures for 14/15/16/17 times and so forth.
Every time measurement moves from one <strong>code</strong> to another, there is a jump of more than <strong>0.15</strong> on the current measurement</p>
<p>The data with top 48 rows is as follows,</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Index</th>
<th style=""text-align: center;"">Curr(mA)</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">0</td>
<td style=""text-align: center;"">1.362476</td>
</tr>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: center;"">1.341721</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: center;"">1.362477</td>
</tr>
<tr>
<td style=""text-align: left;"">3</td>
<td style=""text-align: center;"">1.362477</td>
</tr>
<tr>
<td style=""text-align: left;"">4</td>
<td style=""text-align: center;"">1.355560</td>
</tr>
<tr>
<td style=""text-align: left;"">5</td>
<td style=""text-align: center;"">1.348642</td>
</tr>
<tr>
<td style=""text-align: left;"">6</td>
<td style=""text-align: center;"">1.327886</td>
</tr>
<tr>
<td style=""text-align: left;"">7</td>
<td style=""text-align: center;"">1.341721</td>
</tr>
<tr>
<td style=""text-align: left;"">8</td>
<td style=""text-align: center;"">1.334804</td>
</tr>
<tr>
<td style=""text-align: left;"">9</td>
<td style=""text-align: center;"">1.334804</td>
</tr>
<tr>
<td style=""text-align: left;"">10</td>
<td style=""text-align: center;"">1.348641</td>
</tr>
<tr>
<td style=""text-align: left;"">11</td>
<td style=""text-align: center;"">1.362474</td>
</tr>
<tr>
<td style=""text-align: left;"">12</td>
<td style=""text-align: center;"">1.348644</td>
</tr>
<tr>
<td style=""text-align: left;"">13</td>
<td style=""text-align: center;"">1.355558</td>
</tr>
<tr>
<td style=""text-align: left;"">14</td>
<td style=""text-align: center;"">1.334805</td>
</tr>
<tr>
<td style=""text-align: left;"">15</td>
<td style=""text-align: center;"">1.362477</td>
</tr>
<tr>
<td style=""text-align: left;"">16</td>
<td style=""text-align: center;"">1.556172</td>
</tr>
<tr>
<td style=""text-align: left;"">17</td>
<td style=""text-align: center;"">1.542336</td>
</tr>
<tr>
<td style=""text-align: left;"">18</td>
<td style=""text-align: center;"">1.549252</td>
</tr>
<tr>
<td style=""text-align: left;"">19</td>
<td style=""text-align: center;"">1.528503</td>
</tr>
<tr>
<td style=""text-align: left;"">20</td>
<td style=""text-align: center;"">1.549254</td>
</tr>
<tr>
<td style=""text-align: left;"">21</td>
<td style=""text-align: center;"">1.528501</td>
</tr>
<tr>
<td style=""text-align: left;"">22</td>
<td style=""text-align: center;"">1.556173</td>
</tr>
<tr>
<td style=""text-align: left;"">23</td>
<td style=""text-align: center;"">1.556172</td>
</tr>
<tr>
<td style=""text-align: left;"">24</td>
<td style=""text-align: center;"">1.542334</td>
</tr>
<tr>
<td style=""text-align: left;"">25</td>
<td style=""text-align: center;"">1.556172</td>
</tr>
<tr>
<td style=""text-align: left;"">26</td>
<td style=""text-align: center;"">1.542336</td>
</tr>
<tr>
<td style=""text-align: left;"">27</td>
<td style=""text-align: center;"">1.542334</td>
</tr>
<tr>
<td style=""text-align: left;"">28</td>
<td style=""text-align: center;"">1.556170</td>
</tr>
<tr>
<td style=""text-align: left;"">29</td>
<td style=""text-align: center;"">1.535415</td>
</tr>
<tr>
<td style=""text-align: left;"">30</td>
<td style=""text-align: center;"">1.542334</td>
</tr>
<tr>
<td style=""text-align: left;"">31</td>
<td style=""text-align: center;"">1.729109</td>
</tr>
<tr>
<td style=""text-align: left;"">32</td>
<td style=""text-align: center;"">1.749863</td>
</tr>
<tr>
<td style=""text-align: left;"">33</td>
<td style=""text-align: center;"">1.749861</td>
</tr>
<tr>
<td style=""text-align: left;"">34</td>
<td style=""text-align: center;"">1.749861</td>
</tr>
<tr>
<td style=""text-align: left;"">35</td>
<td style=""text-align: center;"">1.736024</td>
</tr>
<tr>
<td style=""text-align: left;"">36</td>
<td style=""text-align: center;"">1.770619</td>
</tr>
<tr>
<td style=""text-align: left;"">37</td>
<td style=""text-align: center;"">1.742946</td>
</tr>
<tr>
<td style=""text-align: left;"">38</td>
<td style=""text-align: center;"">1.763699</td>
</tr>
<tr>
<td style=""text-align: left;"">39</td>
<td style=""text-align: center;"">1.749861</td>
</tr>
<tr>
<td style=""text-align: left;"">40</td>
<td style=""text-align: center;"">1.749861</td>
</tr>
<tr>
<td style=""text-align: left;"">41</td>
<td style=""text-align: center;"">1.763703</td>
</tr>
<tr>
<td style=""text-align: left;"">42</td>
<td style=""text-align: center;"">1.756781</td>
</tr>
<tr>
<td style=""text-align: left;"">43</td>
<td style=""text-align: center;"">1.742946</td>
</tr>
<tr>
<td style=""text-align: left;"">44</td>
<td style=""text-align: center;"">1.736026</td>
</tr>
<tr>
<td style=""text-align: left;"">45</td>
<td style=""text-align: center;"">1.756781</td>
</tr>
<tr>
<td style=""text-align: left;"">46</td>
<td style=""text-align: center;"">1.964308</td>
</tr>
<tr>
<td style=""text-align: left;"">47</td>
<td style=""text-align: center;"">1.957395</td>
</tr>
</tbody>
</table>
</div>
<p>I want to write a script where similar data of 14/15/16/17 times is averaged in a separate column for each code measurement .. I have been thinking of doing this with pandas..</p>
<p>I want the data to look like</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Index</th>
<th style=""text-align: center;"">Curr(mA)</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">0</td>
<td style=""text-align: center;"">1.34907</td>
</tr>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: center;"">1.54556</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: center;"">1.74986</td>
</tr>
</tbody>
</table>
</div>
<p>Need some help to get this done. Please help</p>
","['python', 'pandas', 'dataframe']",66993717,"<p>First get the indexes of every row where there's a jump. Use Pandas' <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.diff.html"" rel=""nofollow noreferrer""><code>DataFrame.diff()</code></a> to get the difference between the value in each row and the previous row, then check to see if it's greater than <code>0.15</code> with <code>&gt;</code>. Use that to filter the dataframe index, and save the resulting indices (in the case of your sample data, three) in a variable.</p>
<pre><code>indices = df.index[df['Curr(mA)'].diff() &gt; 0.15]
</code></pre>
<p>The next steps depend on if there are more columns in the source dataframe that you want in the output, or if it's really just <code>curr(mA)</code> and index. In the latter case, you can use <a href=""https://numpy.org/doc/stable/reference/generated/numpy.split.html"" rel=""nofollow noreferrer""><code>np.split()</code></a> to cut the dataframe into a list of dataframes based on the indexes you just pulled. Then you can go ahead and average them in a list comphrension.</p>
<pre><code>[df['Curr(mA)'].mean() for df in np.split(df, indices)]

&gt; [1.3490729374999997, 1.5455638666666667, 1.7498627333333332, 1.9608515]
</code></pre>
<p>To get it to match your desired output above (same thing but as one-column dataframe rather than list) convert the list to <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html"" rel=""nofollow noreferrer""><code>pd.Series</code></a> and <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html"" rel=""nofollow noreferrer""><code>reset_index()</code></a>.</p>
<pre><code>pd.Series(
    [df['Curr(mA)'].mean() for df in np.split(df, indices)]
).reset_index(drop=True)

index   0
0   0   1.349073
1   1   1.545564
2   2   1.749863
3   3   1.960851
</code></pre>
",Average similar looking data column using Pandas I working large data K rows I continuous measurement current column A code measured second equipment measures times depending equipment speed measurement moves next code measures times forth Every time measurement moves one code another jump current measurement The data top rows follows Index Curr mA I want write script similar data times averaged separate column code measurement I thinking pandas I want data look like Index Curr mA Need help get done Please help,"startoftags, python, pandas, dataframe, endoftags",python pandas dataframe endoftags,python pandas dataframe,python pandas dataframe,1.0
67521695,2021-05-13,2021,2,Object has no attribute,"<p>I am learning python/numpy. I followed instructions but somewhat code does not working.</p>
<pre><code>    import numpy
    import os
    desktop = os.path.normpath(os.path.expanduser(&quot;~/Desktop&quot;))
    import pandas
    x = pandas.read_csv('Desktop/numpy exc/2dcsv.csv',header = None)
    print(type(x))
    print(x.info)
    y = x.as_matrix()
    print(y)
</code></pre>
<p>AttributeError: 'DataFrame' object has no attribute 'as_matrix'</p>
","['python', 'pandas', 'numpy']",67521756,"<p>Instead of <code>x.as_matrix()</code> try <code>x.to_numpy()</code>.</p>
<p>In panda's documentation it says:</p>
<pre><code>Deprecated since version 0.23.0: Use DataFrame.values() instead.
</code></pre>
",Object attribute I learning python numpy I followed instructions somewhat code working import numpy import os desktop os path normpath os path expanduser quot Desktop quot import pandas x pandas read csv Desktop numpy exc dcsv csv header None print type x print x info x matrix print AttributeError DataFrame object attribute matrix,"startoftags, python, pandas, numpy, endoftags",python arrays numpy endoftags,python pandas numpy,python arrays numpy,0.67
